{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 69,
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Remember to import \"numpy_transforms\" functions if you wish to import these two classes in a different script.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "        label = row[\"class\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image), label\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image, label = self.dataset[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "        batch_labels = np.array(labels)\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 70,
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
<<<<<<< HEAD
    "dataloader = DataLoader(dataset, batch_size=256)"
=======
    "dataloader = DataLoader(dataset, batch_size=256)\n",
    "\n",
    "\n"
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 71,
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(128, 625)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    print(labels.shape)  # Should be [batch_size]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 72,
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 49,
=======
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy.special import softmax\n",
    "\n",
    "# # Sigmoid activation and its derivative\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def sigmoid_derivative(x):\n",
    "#     s = sigmoid(x)\n",
    "#     return s * (1 - s)\n",
    "\n",
    "# # Cross-entropy loss\n",
    "# def cross_entropy_loss(y_true, y_pred):\n",
    "#     y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "#     return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# # Neural Network Class with Softmax in the Output Layer and Sigmoid in Hidden Layers\n",
    "# class NeuralNetwork:\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size):\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "\n",
    "#         layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "#         # Initialize weights and biases\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "#             self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         pre_activations = []\n",
    "\n",
    "#         # Pass through each layer except the output layer\n",
    "#         for i in range(len(self.weights) - 1):\n",
    "#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             pre_activations.append(z)\n",
    "#             a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "#             activations.append(a)\n",
    "\n",
    "#         # Pass through the output layer with softmax\n",
    "#         z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         pre_activations.append(z)\n",
    "#         a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "#         activations.append(a)\n",
    "\n",
    "#         return activations, pre_activations\n",
    "\n",
    "#     def backward(self, X, y, activations, pre_activations):\n",
    "#         grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "#         grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "#         # Start with output layer error\n",
    "#         delta = activations[-1] - y\n",
    "\n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "#             grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "#             if i > 0:\n",
    "#                 delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "\n",
    "#         return grad_w, grad_b\n",
    "\n",
    "#     def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "#         for i in range(len(self.weights)):\n",
    "#             self.weights[i] -= learning_rate * grad_w[i]\n",
    "#             self.biases[i] -= learning_rate * grad_b[i]\n",
    "\n",
    "#     def train(self, batches, epochs, learning_rate):\n",
    "#         for epoch in range(epochs):\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 activations, pre_activations = self.forward(X_batch)\n",
    "#                 grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "#                 self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "#             # Calculate average loss over batches\n",
    "#             loss = 0\n",
    "#             z = 0\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 y_pred, _ = self.forward(X_batch)\n",
    "#                 loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "#                 z += len(y_pred[-1])\n",
    "\n",
    "#             loss /= z\n",
    "#             print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "#     def get_weights(self):\n",
    "#         return self.weights\n",
    "\n",
    "#     def get_biases(self):\n",
    "#         return self.biases\n",
    "\n",
    "# # Example usage:\n",
    "# nn = NeuralNetwork(625, [512, 256, 128, 32], 8)\n",
    "# nn.train(batches, 100, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 1, Loss: 2.2030001288, z: 3200\n",
      "Epoch 2, Loss: 2.1980960210, z: 3200\n",
      "Epoch 3, Loss: 2.1933980726, z: 3200\n",
      "Epoch 4, Loss: 2.1888969749, z: 3200\n",
      "Epoch 5, Loss: 2.1845838621, z: 3200\n",
      "Epoch 6, Loss: 2.1804502915, z: 3200\n",
      "Epoch 7, Loss: 2.1764882236, z: 3200\n",
      "Epoch 8, Loss: 2.1726900032, z: 3200\n",
      "Epoch 9, Loss: 2.1690483410, z: 3200\n",
      "Epoch 10, Loss: 2.1655562961, z: 3200\n",
      "Epoch 11, Loss: 2.1622072592, z: 3200\n",
      "Epoch 12, Loss: 2.1589949363, z: 3200\n",
      "Epoch 13, Loss: 2.1559133332, z: 3200\n",
      "Epoch 14, Loss: 2.1529567408, z: 3200\n",
      "Epoch 15, Loss: 2.1501197207, z: 3200\n",
      "Epoch 16, Loss: 2.1473970921, z: 3200\n",
      "Epoch 17, Loss: 2.1447839185, z: 3200\n",
      "Epoch 18, Loss: 2.1422754957, z: 3200\n",
      "Epoch 19, Loss: 2.1398673399, z: 3200\n",
      "Epoch 20, Loss: 2.1375551769, z: 3200\n",
      "Epoch 21, Loss: 2.1353349313, z: 3200\n",
      "Epoch 22, Loss: 2.1332027167, z: 3200\n",
      "Epoch 23, Loss: 2.1311548258, z: 3200\n",
      "Epoch 24, Loss: 2.1291877218, z: 3200\n",
      "Epoch 25, Loss: 2.1272980297, z: 3200\n",
      "Epoch 26, Loss: 2.1254825282, z: 3200\n",
      "Epoch 27, Loss: 2.1237381417, z: 3200\n",
      "Epoch 28, Loss: 2.1220619336, z: 3200\n",
      "Epoch 29, Loss: 2.1204510990, z: 3200\n",
      "Epoch 30, Loss: 2.1189029581, z: 3200\n",
      "Epoch 31, Loss: 2.1174149504, z: 3200\n",
      "Epoch 32, Loss: 2.1159846283, z: 3200\n",
      "Epoch 33, Loss: 2.1146096523, z: 3200\n",
      "Epoch 34, Loss: 2.1132877849, z: 3200\n",
      "Epoch 35, Loss: 2.1120168861, z: 3200\n",
      "Epoch 36, Loss: 2.1107949089, z: 3200\n",
      "Epoch 37, Loss: 2.1096198943, z: 3200\n",
      "Epoch 38, Loss: 2.1084899675, z: 3200\n",
      "Epoch 39, Loss: 2.1074033336, z: 3200\n",
      "Epoch 40, Loss: 2.1063582741, z: 3200\n",
      "Epoch 41, Loss: 2.1053531432, z: 3200\n",
      "Epoch 42, Loss: 2.1043863644, z: 3200\n",
      "Epoch 43, Loss: 2.1034564271, z: 3200\n",
      "Epoch 44, Loss: 2.1025618839, z: 3200\n",
      "Epoch 45, Loss: 2.1017013475, z: 3200\n",
      "Epoch 46, Loss: 2.1008734879, z: 3200\n",
      "Epoch 47, Loss: 2.1000770300, z: 3200\n",
      "Epoch 48, Loss: 2.0993107509, z: 3200\n",
      "Epoch 49, Loss: 2.0985734777, z: 3200\n",
      "Epoch 50, Loss: 2.0978640854, z: 3200\n",
      "Epoch 51, Loss: 2.0971814944, z: 3200\n",
      "Epoch 52, Loss: 2.0965246688, z: 3200\n",
      "Epoch 53, Loss: 2.0958926145, z: 3200\n",
      "Epoch 54, Loss: 2.0952843771, z: 3200\n",
      "Epoch 55, Loss: 2.0946990406, z: 3200\n",
      "Epoch 56, Loss: 2.0941357253, z: 3200\n",
      "Epoch 57, Loss: 2.0935935868, z: 3200\n",
      "Epoch 58, Loss: 2.0930718137, z: 3200\n",
      "Epoch 59, Loss: 2.0925696272, z: 3200\n",
      "Epoch 60, Loss: 2.0920862788, z: 3200\n",
      "Epoch 61, Loss: 2.0916210495, z: 3200\n",
      "Epoch 62, Loss: 2.0911732486, z: 3200\n",
      "Epoch 63, Loss: 2.0907422125, z: 3200\n",
      "Epoch 64, Loss: 2.0903273033, z: 3200\n",
      "Epoch 65, Loss: 2.0899279081, z: 3200\n",
      "Epoch 66, Loss: 2.0895434377, z: 3200\n",
      "Epoch 67, Loss: 2.0891733259, z: 3200\n",
      "Epoch 68, Loss: 2.0888170283, z: 3200\n",
      "Epoch 69, Loss: 2.0884740217, z: 3200\n",
      "Epoch 70, Loss: 2.0881438029, z: 3200\n",
      "Epoch 71, Loss: 2.0878258882, z: 3200\n",
      "Epoch 72, Loss: 2.0875198125, z: 3200\n",
      "Epoch 73, Loss: 2.0872251286, z: 3200\n",
      "Epoch 74, Loss: 2.0869414063, z: 3200\n",
      "Epoch 75, Loss: 2.0866682322, z: 3200\n",
      "Epoch 76, Loss: 2.0864052083, z: 3200\n",
      "Epoch 77, Loss: 2.0861519521, z: 3200\n",
      "Epoch 78, Loss: 2.0859080957, z: 3200\n",
      "Epoch 79, Loss: 2.0856732850, z: 3200\n",
      "Epoch 80, Loss: 2.0854471798, z: 3200\n",
      "Epoch 81, Loss: 2.0852294524, z: 3200\n",
      "Epoch 82, Loss: 2.0850197878, z: 3200\n",
      "Epoch 83, Loss: 2.0848178830, z: 3200\n",
      "Epoch 84, Loss: 2.0846234464, z: 3200\n",
      "Epoch 85, Loss: 2.0844361975, z: 3200\n",
      "Epoch 86, Loss: 2.0842558665, z: 3200\n",
      "Epoch 87, Loss: 2.0840821938, z: 3200\n",
      "Epoch 88, Loss: 2.0839149295, z: 3200\n",
      "Epoch 89, Loss: 2.0837538333, z: 3200\n",
      "Epoch 90, Loss: 2.0835986741, z: 3200\n",
      "Epoch 91, Loss: 2.0834492292, z: 3200\n",
      "Epoch 92, Loss: 2.0833052846, z: 3200\n",
      "Epoch 93, Loss: 2.0831666343, z: 3200\n",
      "Epoch 94, Loss: 2.0830330801, z: 3200\n",
      "Epoch 95, Loss: 2.0829044313, z: 3200\n",
      "Epoch 96, Loss: 2.0827805042, z: 3200\n",
      "Epoch 97, Loss: 2.0826611224, z: 3200\n",
      "Epoch 98, Loss: 2.0825461159, z: 3200\n",
      "Epoch 99, Loss: 2.0824353210, z: 3200\n",
      "Epoch 100, Loss: 2.0823285806, z: 3200\n",
      "Epoch 101, Loss: 2.0822257430, z: 3200\n",
      "Epoch 102, Loss: 2.0821266626, z: 3200\n",
      "Epoch 103, Loss: 2.0820311991, z: 3200\n",
      "Epoch 104, Loss: 2.0819392175, z: 3200\n",
      "Epoch 105, Loss: 2.0818505880, z: 3200\n",
      "Epoch 106, Loss: 2.0817651856, z: 3200\n",
      "Epoch 107, Loss: 2.0816828900, z: 3200\n",
      "Epoch 108, Loss: 2.0816035853, z: 3200\n",
      "Epoch 109, Loss: 2.0815271603, z: 3200\n",
      "Epoch 110, Loss: 2.0814535077, z: 3200\n",
      "Epoch 111, Loss: 2.0813825243, z: 3200\n",
      "Epoch 112, Loss: 2.0813141107, z: 3200\n",
      "Epoch 113, Loss: 2.0812481715, z: 3200\n",
      "Epoch 114, Loss: 2.0811846147, z: 3200\n",
      "Epoch 115, Loss: 2.0811233517, z: 3200\n",
      "Epoch 116, Loss: 2.0810642973, z: 3200\n",
      "Epoch 117, Loss: 2.0810073695, z: 3200\n",
      "Epoch 118, Loss: 2.0809524895, z: 3200\n",
      "Epoch 119, Loss: 2.0808995813, z: 3200\n",
      "Epoch 120, Loss: 2.0808485717, z: 3200\n",
      "Epoch 121, Loss: 2.0807993905, z: 3200\n",
      "Epoch 122, Loss: 2.0807519698, z: 3200\n",
      "Epoch 123, Loss: 2.0807062447, z: 3200\n",
      "Epoch 124, Loss: 2.0806621522, z: 3200\n",
      "Epoch 125, Loss: 2.0806196321, z: 3200\n",
      "Epoch 126, Loss: 2.0805786262, z: 3200\n",
      "Epoch 127, Loss: 2.0805390787, z: 3200\n",
      "Epoch 128, Loss: 2.0805009356, z: 3200\n",
      "Epoch 129, Loss: 2.0804641452, z: 3200\n",
      "Epoch 130, Loss: 2.0804286576, z: 3200\n",
      "Epoch 131, Loss: 2.0803944248, z: 3200\n",
      "Epoch 132, Loss: 2.0803614006, z: 3200\n",
      "Epoch 133, Loss: 2.0803295405, z: 3200\n",
      "Epoch 134, Loss: 2.0802988017, z: 3200\n",
      "Epoch 135, Loss: 2.0802691430, z: 3200\n"
=======
      "Epoch 1/100, Loss: 2.0880\n",
      "Epoch 2/100, Loss: 2.0794\n",
      "Epoch 3/100, Loss: 2.0785\n",
      "Epoch 4/100, Loss: 2.0766\n",
      "Epoch 5/100, Loss: 2.0744\n",
      "Epoch 6/100, Loss: 2.0711\n",
      "Epoch 7/100, Loss: 2.0647\n",
      "Epoch 8/100, Loss: 2.0499\n",
      "Epoch 9/100, Loss: 2.0283\n",
      "Epoch 10/100, Loss: 1.9714\n",
      "Epoch 11/100, Loss: 1.8837\n",
      "Epoch 12/100, Loss: 1.8390\n",
      "Epoch 13/100, Loss: 1.7436\n",
      "Epoch 14/100, Loss: 1.6736\n",
      "Epoch 15/100, Loss: 1.6128\n",
      "Epoch 16/100, Loss: 1.5672\n",
      "Epoch 17/100, Loss: 1.5275\n",
      "Epoch 18/100, Loss: 1.5076\n",
      "Epoch 19/100, Loss: 1.5031\n",
      "Epoch 20/100, Loss: 1.5162\n",
      "Epoch 21/100, Loss: 1.4468\n",
      "Epoch 22/100, Loss: 1.4330\n",
      "Epoch 23/100, Loss: 1.4403\n",
      "Epoch 24/100, Loss: 1.4145\n",
      "Epoch 25/100, Loss: 1.4032\n",
      "Epoch 26/100, Loss: 1.3978\n",
      "Epoch 27/100, Loss: 1.3907\n",
      "Epoch 28/100, Loss: 1.3971\n",
      "Epoch 29/100, Loss: 1.6107\n",
      "Epoch 30/100, Loss: 1.5274\n",
      "Epoch 31/100, Loss: 1.4049\n",
      "Epoch 32/100, Loss: 1.3819\n",
      "Epoch 33/100, Loss: 1.3737\n",
      "Epoch 34/100, Loss: 1.3699\n",
      "Epoch 35/100, Loss: 1.3689\n",
      "Epoch 36/100, Loss: 1.3581\n",
      "Epoch 37/100, Loss: 1.3478\n",
      "Epoch 38/100, Loss: 1.3462\n",
      "Epoch 39/100, Loss: 1.3496\n",
      "Epoch 40/100, Loss: 1.3449\n",
      "Epoch 41/100, Loss: 1.3305\n",
      "Epoch 42/100, Loss: 1.3229\n",
      "Epoch 43/100, Loss: 1.3245\n",
      "Epoch 44/100, Loss: 1.3291\n",
      "Epoch 45/100, Loss: 1.3255\n",
      "Epoch 46/100, Loss: 1.3104\n",
      "Epoch 47/100, Loss: 1.2981\n",
      "Epoch 48/100, Loss: 1.2948\n",
      "Epoch 49/100, Loss: 1.2970\n",
      "Epoch 50/100, Loss: 1.2991\n",
      "Epoch 51/100, Loss: 1.3007\n",
      "Epoch 52/100, Loss: 1.3027\n",
      "Epoch 53/100, Loss: 1.3024\n",
      "Epoch 54/100, Loss: 1.2960\n",
      "Epoch 55/100, Loss: 1.2847\n",
      "Epoch 56/100, Loss: 1.2772\n",
      "Epoch 57/100, Loss: 1.2735\n",
      "Epoch 58/100, Loss: 1.2686\n",
      "Epoch 59/100, Loss: 1.2626\n",
      "Epoch 60/100, Loss: 1.2627\n",
      "Epoch 61/100, Loss: 1.2694\n",
      "Epoch 62/100, Loss: 1.2750\n",
      "Epoch 63/100, Loss: 1.2678\n",
      "Epoch 64/100, Loss: 1.2259\n",
      "Epoch 65/100, Loss: 1.2102\n",
      "Epoch 66/100, Loss: 1.1983\n",
      "Epoch 67/100, Loss: 1.1800\n",
      "Epoch 68/100, Loss: 1.1701\n",
      "Epoch 69/100, Loss: 1.1673\n",
      "Epoch 70/100, Loss: 1.1589\n",
      "Epoch 71/100, Loss: 1.1617\n",
      "Epoch 72/100, Loss: 1.1569\n",
      "Epoch 73/100, Loss: 1.1534\n",
      "Epoch 74/100, Loss: 1.1393\n",
      "Epoch 75/100, Loss: 1.1389\n",
      "Epoch 76/100, Loss: 1.1472\n",
      "Epoch 77/100, Loss: 1.1335\n",
      "Epoch 78/100, Loss: 1.1347\n",
      "Epoch 79/100, Loss: 1.1180\n",
      "Epoch 80/100, Loss: 1.1177\n",
      "Epoch 81/100, Loss: 1.1239\n",
      "Epoch 82/100, Loss: 1.1333\n",
      "Epoch 83/100, Loss: 1.1118\n",
      "Epoch 84/100, Loss: 1.1100\n",
      "Epoch 85/100, Loss: 1.0985\n",
      "Epoch 86/100, Loss: 1.1031\n",
      "Epoch 87/100, Loss: 1.0968\n",
      "Epoch 88/100, Loss: 1.0935\n",
      "Epoch 89/100, Loss: 1.0928\n",
      "Epoch 90/100, Loss: 1.0902\n",
      "Epoch 91/100, Loss: 1.0875\n",
      "Epoch 92/100, Loss: 1.0854\n",
      "Epoch 93/100, Loss: 1.0824\n",
      "Epoch 94/100, Loss: 1.0791\n",
      "Epoch 95/100, Loss: 1.0769\n",
      "Epoch 96/100, Loss: 1.0764\n",
      "Epoch 97/100, Loss: 1.0762\n",
      "Epoch 98/100, Loss: 1.0741\n",
      "Epoch 99/100, Loss: 1.0727\n",
      "Epoch 100/100, Loss: 1.0734\n"
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# For baseline purpose, running part B code in the same script\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
=======
    "import numpy as np\n",
    "from scipy.special import softmax\n",
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
    "\n",
    "# Sigmoid activation and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
<<<<<<< HEAD
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
=======
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Neural Network Class with Softmax in the Output Layer and Sigmoid in Hidden Layers\n",
<<<<<<< HEAD
    "class NeuralNetwork_Baseline:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        np.random.seed(0)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each layer except the output layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= learning_rate * grad_w[i]\n",
    "            self.biases[i] -= learning_rate * grad_b[i]\n",
    "\n",
    "    def train(self, batches, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while (True):\n",
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
    "            loss /= z\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            epoch += 1\n",
    "            if (time.time() - start_time) > 60:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return activations[-1]\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_biases(self):\n",
    "        return self.biases\n",
    "\n",
    "# Example usage:\n",
    "nn = NeuralNetwork_Baseline(625, [512, 256, 128, 32], 8)\n",
    "nn.train(batches, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Sigmoid activation and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Neural Network Class with Softmax in the Output Layer and Sigmoid in Hidden Layers\n",
    "class NeuralNetwork_Adam:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, init_weights = None, init_biases = None, init_seed = None, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        if (init_seed is None):\n",
    "            self.best_seed = int(time.time())\n",
    "            np.random.seed(self.best_seed)\n",
    "        else:\n",
    "            np.random.seed(init_seed)\n",
=======
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.m_w = []\n",
    "        self.v_w = []\n",
    "        self.m_b = []\n",
    "        self.v_b = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step for Adam\n",
<<<<<<< HEAD
    "        self.best_weights = []\n",
    "        self.best_biases = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if (init_weights is not None) and (init_biases is not None):\n",
    "                self.weights.append(init_weights[i])\n",
    "                self.biases.append(init_biases[i])\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "                self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
=======
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
    "            self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.v_b.append(np.zeros_like(self.biases[-1]))\n",
<<<<<<< HEAD
    "            self.best_weights = self.weights\n",
    "            self.best_biases = self.biases\n",
=======
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each layer except the output layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        self.t += 1  # Increment time step for Adam\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
<<<<<<< HEAD
    "    def train(self, batches, time_of_running, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while(True):\n",
=======
    "    def train(self, batches, epochs, learning_rate):\n",
    "        for epoch in range(epochs):\n",
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
<<<<<<< HEAD
    "            loss /= z\n",
    "            \n",
    "            if (loss < self.best_loss):\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = self.weights\n",
    "                self.best_biases = self.biases\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            epoch += 1\n",
    "            # if time elapsed is greater than 1 minute, break the loop\n",
    "            if time.time() - start_time > 60*time_of_running:\n",
    "                break\n",
=======
    "\n",
    "            loss /= z\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return activations[-1]\n",
<<<<<<< HEAD
    "    \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    \n",
    "    def get_best_biases(self):\n",
    "        return self.best_biases\n",
    "    \n",
    "    def get_best_loss(self):\n",
    "        return self.best_loss\n",
    "    \n",
    "    def get_best_seed(self):\n",
    "        return self.best_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0829553011\n",
      "Epoch 2, Loss: 2.0820056445\n",
      "Epoch 3, Loss: 2.0795830036\n",
      "Epoch 4, Loss: 2.0788180516\n",
      "Epoch 5, Loss: 2.0782540322\n",
      "Epoch 6, Loss: 2.0770604868\n",
      "Epoch 7, Loss: 2.0749511540\n",
      "Epoch 8, Loss: 2.0695981727\n",
      "Epoch 9, Loss: 2.0600528001\n",
      "Epoch 10, Loss: 2.0373417047\n",
      "Epoch 11, Loss: 2.0144082174\n",
      "Epoch 12, Loss: 1.9819438383\n",
      "Epoch 13, Loss: 1.9031988955\n",
      "Epoch 14, Loss: 1.8602880870\n",
      "Epoch 15, Loss: 1.8081300494\n",
      "Epoch 16, Loss: 1.8328444830\n",
      "Epoch 17, Loss: 1.8038334145\n",
      "Epoch 18, Loss: 1.8107438447\n",
      "Epoch 19, Loss: 1.7226336609\n",
      "Epoch 20, Loss: 1.7064005563\n",
      "Epoch 21, Loss: 1.7154906934\n",
      "Epoch 22, Loss: 1.7653693923\n",
      "Epoch 23, Loss: 1.6523139582\n",
      "Epoch 24, Loss: 1.6278921752\n",
      "Epoch 25, Loss: 1.6603661834\n",
      "Epoch 26, Loss: 1.7088567235\n",
      "Epoch 27, Loss: 1.5701571988\n",
      "Epoch 28, Loss: 1.5351827805\n",
      "Epoch 29, Loss: 1.5788307060\n",
      "Epoch 30, Loss: 1.5296322123\n",
      "Epoch 31, Loss: 1.5005064835\n",
      "Epoch 32, Loss: 1.4728495083\n",
      "Epoch 33, Loss: 1.4894198359\n",
      "Epoch 34, Loss: 1.4361946868\n",
      "Epoch 35, Loss: 1.4358583369\n",
      "Epoch 36, Loss: 1.4332840135\n",
      "Epoch 37, Loss: 1.4481714919\n",
      "Epoch 38, Loss: 1.4112367412\n",
      "Epoch 39, Loss: 1.3958363788\n",
      "Epoch 40, Loss: 1.4103387405\n",
      "Epoch 41, Loss: 1.4386432117\n",
      "Epoch 42, Loss: 1.4069989286\n",
      "Epoch 43, Loss: 1.3784132405\n",
      "Epoch 44, Loss: 1.3773773916\n",
      "Epoch 45, Loss: 1.4008965801\n",
      "Epoch 46, Loss: 1.4324213226\n",
      "Epoch 47, Loss: 1.4025852037\n",
      "Epoch 48, Loss: 1.3502862600\n",
      "Epoch 49, Loss: 1.3427620083\n",
      "Epoch 50, Loss: 1.3605534295\n",
      "Epoch 51, Loss: 1.3978547895\n",
      "Epoch 52, Loss: 1.4125147366\n",
      "Epoch 53, Loss: 1.3393771309\n",
      "Epoch 54, Loss: 1.3229401549\n",
      "Epoch 55, Loss: 1.3227024099\n",
      "Epoch 56, Loss: 1.3550391959\n",
      "Epoch 57, Loss: 1.3735913152\n",
      "Epoch 58, Loss: 1.4138940365\n",
      "Epoch 59, Loss: 1.3327130067\n",
      "Epoch 60, Loss: 1.3109420074\n",
      "Epoch 61, Loss: 1.3037962827\n",
      "Epoch 62, Loss: 1.3086054169\n",
      "Epoch 63, Loss: 1.3438990648\n",
      "Epoch 64, Loss: 1.3281012691\n",
      "Epoch 65, Loss: 1.3935196747\n",
      "Epoch 66, Loss: 1.3651568715\n",
      "Epoch 67, Loss: 1.3188472495\n",
      "Epoch 68, Loss: 1.2795437476\n",
      "Epoch 69, Loss: 1.2906259939\n",
      "Epoch 70, Loss: 1.3074493460\n",
      "Epoch 71, Loss: 1.3159475275\n",
      "Epoch 72, Loss: 1.2817542003\n",
      "Epoch 73, Loss: 1.2536147410\n",
      "Epoch 74, Loss: 1.2498576398\n",
      "Epoch 75, Loss: 1.2728439519\n",
      "Epoch 76, Loss: 1.2889830605\n",
      "Epoch 77, Loss: 1.2751309570\n",
      "Epoch 78, Loss: 1.2726792030\n",
      "Epoch 79, Loss: 1.2881788202\n",
      "Epoch 80, Loss: 1.2910920998\n",
      "Epoch 81, Loss: 1.2508882332\n",
      "Epoch 82, Loss: 1.2160495629\n",
      "Epoch 83, Loss: 1.2223087679\n",
      "Epoch 84, Loss: 1.2163363691\n",
      "Epoch 85, Loss: 1.2239681400\n",
      "Epoch 86, Loss: 1.2340297704\n",
      "Epoch 87, Loss: 1.2196430213\n",
      "Epoch 88, Loss: 1.2055319381\n",
      "Epoch 89, Loss: 1.1843638929\n",
      "Epoch 90, Loss: 1.1829954132\n",
      "Epoch 91, Loss: 1.1976632531\n",
      "Epoch 92, Loss: 1.1764741356\n",
      "Epoch 93, Loss: 1.1725823870\n",
      "Epoch 94, Loss: 1.1628063554\n",
      "Epoch 95, Loss: 1.1774463141\n",
      "Epoch 96, Loss: 1.2227115662\n",
      "Epoch 97, Loss: 1.1649044957\n",
      "Epoch 98, Loss: 1.2865292545\n",
      "Epoch 99, Loss: 1.1739641700\n",
      "Epoch 100, Loss: 1.1526985743\n",
      "Epoch 101, Loss: 1.1657647787\n",
      "Epoch 102, Loss: 1.1962807559\n",
      "Epoch 103, Loss: 1.1883370987\n",
      "Epoch 104, Loss: 1.1984466552\n",
      "Epoch 105, Loss: 1.2107089201\n",
      "Epoch 106, Loss: 1.2362394701\n",
      "Epoch 107, Loss: 1.2464415892\n",
      "Epoch 108, Loss: 1.1517689522\n",
      "Epoch 109, Loss: 1.1280912200\n",
      "Epoch 110, Loss: 1.1192240593\n",
      "Epoch 111, Loss: 1.1166710068\n",
      "Epoch 112, Loss: 1.1324888574\n",
      "Epoch 113, Loss: 1.1645597618\n",
      "Epoch 114, Loss: 1.1295018461\n",
      "Epoch 115, Loss: 1.1304211555\n",
      "Epoch 116, Loss: 1.1270057240\n",
      "Epoch 117, Loss: 1.1587896362\n",
      "Epoch 118, Loss: 1.1725877622\n",
      "Epoch 119, Loss: 1.0899846232\n",
      "Epoch 120, Loss: 1.0839909358\n",
      "Epoch 121, Loss: 1.0858953700\n",
      "Epoch 122, Loss: 1.0810741993\n",
      "Epoch 123, Loss: 1.0885607790\n",
      "Epoch 124, Loss: 1.0906618198\n",
      "Epoch 125, Loss: 1.0647577786\n",
      "Epoch 126, Loss: 1.0713156705\n",
      "Epoch 127, Loss: 1.0663911995\n",
      "Epoch 128, Loss: 1.0719266336\n",
      "Epoch 129, Loss: 1.0693872055\n",
      "Epoch 130, Loss: 1.0695040532\n",
      "Epoch 131, Loss: 1.0578961216\n",
      "Epoch 132, Loss: 1.0443989748\n",
      "Epoch 133, Loss: 1.0404984606\n",
      "Epoch 134, Loss: 1.0415450573\n",
      "Epoch 135, Loss: 1.0426617282\n",
      "Epoch 136, Loss: 1.0435856514\n",
      "Epoch 137, Loss: 1.0485324573\n",
      "Epoch 138, Loss: 1.0677809018\n",
      "Epoch 139, Loss: 1.1020181862\n",
      "Epoch 140, Loss: 1.0440316244\n",
      "Epoch 141, Loss: 1.0949015635\n",
      "Epoch 142, Loss: 1.0347503939\n",
      "Epoch 143, Loss: 1.0498968826\n",
      "Epoch 144, Loss: 1.0363735915\n",
      "Epoch 145, Loss: 1.0190026765\n",
      "Epoch 146, Loss: 1.0198015083\n",
      "Epoch 147, Loss: 1.0185669580\n",
      "Epoch 148, Loss: 1.0200275780\n",
      "Epoch 149, Loss: 1.0180178494\n",
      "Epoch 150, Loss: 1.0075391929\n",
      "Epoch 151, Loss: 1.0070468190\n",
      "Epoch 152, Loss: 1.0119051677\n",
      "Epoch 153, Loss: 1.0140813174\n",
      "Epoch 154, Loss: 1.0185839852\n",
      "Epoch 155, Loss: 1.0551432252\n",
      "Epoch 156, Loss: 1.1276274229\n",
      "Epoch 157, Loss: 1.0224536661\n",
      "Epoch 158, Loss: 0.9967375325\n",
      "Epoch 159, Loss: 0.9884699020\n",
      "Epoch 160, Loss: 0.9883526231\n",
      "Epoch 161, Loss: 1.0022859394\n",
      "Epoch 162, Loss: 1.0407538119\n",
      "Epoch 163, Loss: 0.9937585719\n",
      "Epoch 164, Loss: 0.9980456325\n",
      "Epoch 165, Loss: 0.9889634130\n",
      "Epoch 166, Loss: 0.9835989165\n",
      "Epoch 167, Loss: 0.9783082299\n",
      "Epoch 168, Loss: 0.9745718232\n",
      "Epoch 169, Loss: 0.9768421453\n",
      "Epoch 170, Loss: 0.9770275867\n",
      "Epoch 171, Loss: 0.9791316752\n",
      "Epoch 172, Loss: 0.9779292860\n",
      "Epoch 173, Loss: 0.9868300469\n",
      "Epoch 174, Loss: 0.9863441142\n",
      "Epoch 175, Loss: 0.9991571511\n",
      "Epoch 176, Loss: 0.9965529615\n",
      "Epoch 177, Loss: 0.9926752736\n",
      "Epoch 178, Loss: 0.9692433577\n",
      "Epoch 179, Loss: 0.9841286765\n",
      "Epoch 180, Loss: 1.0117364363\n",
      "Epoch 181, Loss: 1.0488414929\n",
      "Epoch 182, Loss: 1.0796583439\n",
      "Epoch 183, Loss: 1.0037641462\n",
      "Epoch 184, Loss: 0.9603113158\n",
      "Epoch 185, Loss: 0.9663812511\n",
      "Epoch 186, Loss: 0.9907609899\n",
      "Epoch 187, Loss: 0.9953703358\n",
      "Epoch 188, Loss: 0.9857121881\n",
      "Epoch 189, Loss: 0.9788938247\n",
      "Epoch 190, Loss: 0.9748378227\n",
      "Epoch 191, Loss: 0.9729920189\n",
      "Epoch 192, Loss: 0.9690351399\n",
      "Epoch 193, Loss: 0.9690482242\n",
      "Epoch 194, Loss: 0.9604817619\n",
      "Epoch 195, Loss: 0.9742431606\n",
      "Epoch 196, Loss: 0.9583271990\n",
      "Epoch 197, Loss: 0.9551894427\n",
      "Epoch 198, Loss: 0.9737744598\n",
      "Epoch 199, Loss: 0.9566824407\n",
      "Epoch 200, Loss: 0.9630127725\n",
      "Epoch 201, Loss: 0.9599542333\n",
      "Epoch 202, Loss: 0.9501058726\n",
      "Epoch 203, Loss: 0.9663385491\n",
      "Epoch 204, Loss: 1.0351215467\n",
      "Epoch 205, Loss: 1.0093023179\n",
      "Epoch 206, Loss: 1.0286726562\n",
      "Epoch 207, Loss: 1.0322808688\n",
      "Epoch 208, Loss: 0.9771933334\n",
      "Epoch 209, Loss: 0.9725534157\n",
      "Epoch 210, Loss: 1.0018432984\n",
      "Epoch 211, Loss: 0.9807993383\n",
      "Epoch 212, Loss: 0.9644158286\n",
      "Epoch 213, Loss: 0.9598846641\n",
      "Epoch 214, Loss: 0.9608185777\n",
      "Epoch 215, Loss: 0.9549067673\n",
      "Epoch 216, Loss: 0.9551995098\n",
      "Epoch 217, Loss: 0.9541047041\n",
      "Epoch 218, Loss: 0.9465795747\n",
      "Epoch 219, Loss: 0.9493430348\n",
      "Epoch 220, Loss: 0.9695654893\n",
      "Epoch 221, Loss: 0.9867972104\n",
      "Epoch 222, Loss: 0.9654813820\n",
      "Epoch 223, Loss: 0.9492898653\n",
      "Epoch 224, Loss: 0.9745697142\n",
      "Epoch 225, Loss: 0.9740944787\n",
      "Epoch 226, Loss: 0.9732680482\n",
      "Epoch 227, Loss: 0.9751755466\n",
      "Epoch 228, Loss: 0.9665866358\n",
      "Epoch 229, Loss: 0.9719068237\n",
      "Epoch 230, Loss: 0.9680444028\n",
      "Epoch 231, Loss: 0.9697539884\n",
      "Epoch 232, Loss: 0.9666689036\n",
      "Epoch 233, Loss: 0.9689263078\n",
      "Epoch 234, Loss: 0.9669686331\n",
      "Epoch 235, Loss: 0.9688286605\n",
      "Epoch 236, Loss: 0.9716014923\n",
      "Epoch 237, Loss: 0.9698691985\n",
      "Epoch 238, Loss: 0.9741923953\n",
      "Epoch 239, Loss: 0.9731796041\n",
      "Epoch 240, Loss: 0.9979260015\n",
      "Epoch 241, Loss: 0.9998711741\n",
      "Epoch 242, Loss: 1.0468516386\n",
      "Epoch 243, Loss: 1.1065561204\n",
      "Epoch 244, Loss: 0.9257224481\n",
      "Epoch 245, Loss: 0.9774390188\n",
      "Epoch 246, Loss: 1.1190945389\n",
      "Epoch 247, Loss: 1.2114310068\n",
      "Epoch 248, Loss: 0.9204688217\n",
      "Epoch 249, Loss: 0.9140999525\n",
      "Epoch 250, Loss: 0.9864005288\n",
      "Epoch 251, Loss: 0.9274098541\n",
      "Epoch 252, Loss: 0.9302828753\n",
      "Epoch 253, Loss: 0.8917991044\n",
      "Epoch 254, Loss: 0.9226051949\n",
      "Epoch 255, Loss: 1.0210566671\n",
      "Epoch 256, Loss: 0.9488564111\n",
      "Epoch 257, Loss: 0.9196183008\n",
      "Epoch 258, Loss: 0.9263165290\n",
      "Epoch 259, Loss: 0.8932304096\n",
      "Epoch 260, Loss: 0.9041834001\n",
      "Epoch 261, Loss: 0.9662765602\n",
      "Epoch 262, Loss: 0.9884249295\n",
      "Epoch 263, Loss: 1.0114610643\n",
      "Epoch 264, Loss: 1.0259344221\n",
      "Epoch 265, Loss: 0.9508688415\n",
      "Epoch 266, Loss: 0.9118366701\n",
      "Epoch 267, Loss: 0.9331660514\n",
      "Epoch 268, Loss: 0.8949384178\n",
      "Epoch 269, Loss: 0.8988945018\n",
      "Epoch 270, Loss: 0.9209373086\n",
      "Epoch 271, Loss: 0.9289231946\n",
      "Epoch 272, Loss: 0.9618637069\n",
      "Epoch 273, Loss: 0.9764322344\n",
      "Epoch 274, Loss: 0.9962429863\n",
      "Epoch 275, Loss: 0.9957373551\n",
      "Epoch 276, Loss: 0.9906866721\n",
      "Epoch 277, Loss: 1.0441609406\n",
      "Epoch 278, Loss: 1.0884297866\n",
      "Epoch 279, Loss: 0.9126778283\n",
      "Epoch 280, Loss: 0.8895079099\n",
      "Epoch 281, Loss: 0.8711102659\n",
      "Epoch 282, Loss: 0.8969903061\n",
      "Epoch 283, Loss: 0.9582292488\n",
      "Epoch 284, Loss: 0.9094563038\n",
      "Epoch 285, Loss: 0.9317932951\n",
      "Epoch 286, Loss: 0.9372480784\n",
      "Epoch 287, Loss: 0.9258409855\n",
      "Epoch 288, Loss: 0.8716802739\n",
      "Epoch 289, Loss: 0.8653157589\n",
      "Epoch 290, Loss: 0.9332094123\n",
      "Epoch 291, Loss: 0.9226173348\n",
      "Epoch 292, Loss: 0.9198219244\n",
      "Epoch 293, Loss: 0.9404786065\n",
      "Epoch 294, Loss: 0.9004718289\n",
      "Epoch 295, Loss: 0.9460964590\n",
      "Epoch 296, Loss: 0.8786798460\n",
      "Epoch 297, Loss: 0.8820008093\n",
      "Epoch 298, Loss: 0.8963004321\n",
      "Epoch 299, Loss: 0.8746876560\n",
      "Epoch 300, Loss: 0.8793568036\n",
      "Epoch 301, Loss: 0.8990397103\n",
      "Epoch 302, Loss: 0.8819254171\n",
      "Epoch 303, Loss: 0.9117284377\n",
      "Epoch 304, Loss: 0.9342491466\n",
      "Epoch 305, Loss: 0.9048430549\n",
      "Epoch 306, Loss: 0.8949397368\n",
      "Epoch 307, Loss: 0.8937646843\n",
      "Epoch 308, Loss: 0.8955451290\n",
      "Epoch 309, Loss: 0.8943491925\n",
      "Epoch 310, Loss: 0.8932016985\n",
      "Epoch 311, Loss: 0.8959032693\n",
      "Epoch 312, Loss: 0.9020273859\n",
      "Epoch 313, Loss: 0.9074677373\n",
      "Epoch 314, Loss: 0.9068685501\n",
      "Epoch 315, Loss: 0.9001313103\n",
      "Epoch 316, Loss: 0.8950311310\n",
      "Epoch 317, Loss: 0.8997396455\n",
      "Epoch 318, Loss: 0.9226739636\n",
      "Epoch 319, Loss: 0.9562987519\n",
      "Epoch 320, Loss: 0.9247429123\n",
      "Epoch 321, Loss: 0.8827938444\n",
      "Epoch 322, Loss: 0.8934013900\n",
      "Epoch 323, Loss: 0.9153468846\n",
      "Epoch 324, Loss: 0.9303597381\n",
      "Epoch 325, Loss: 0.9407972182\n",
      "Epoch 326, Loss: 0.9466395126\n",
      "Epoch 327, Loss: 0.9461767335\n",
      "Epoch 328, Loss: 0.9301655256\n",
      "Epoch 329, Loss: 0.9096510029\n",
      "Epoch 330, Loss: 0.9003900242\n",
      "Epoch 331, Loss: 0.8949583438\n",
      "Epoch 332, Loss: 0.8896406652\n",
      "Epoch 333, Loss: 0.8854111226\n",
      "Epoch 334, Loss: 0.8780620944\n",
      "Epoch 335, Loss: 0.8727211042\n",
      "Epoch 336, Loss: 0.8671734017\n",
      "Epoch 337, Loss: 0.8628918116\n",
      "Epoch 338, Loss: 0.8636795842\n",
      "Epoch 339, Loss: 0.8700070944\n",
      "Epoch 340, Loss: 0.8795479344\n",
      "Epoch 341, Loss: 0.8592617195\n",
      "Epoch 342, Loss: 0.9044164947\n",
      "Epoch 343, Loss: 0.9063731586\n",
      "Epoch 344, Loss: 0.9092827530\n",
      "Epoch 345, Loss: 0.8871755824\n",
      "Epoch 346, Loss: 0.8887877986\n",
      "Epoch 347, Loss: 0.8505601316\n",
      "Epoch 348, Loss: 0.8652267258\n",
      "Epoch 349, Loss: 0.8124946759\n",
      "Epoch 350, Loss: 0.8214958239\n",
      "Epoch 351, Loss: 0.8199416765\n",
      "Epoch 352, Loss: 0.8252653345\n",
      "Epoch 353, Loss: 0.8306290564\n",
      "Epoch 354, Loss: 0.8478611945\n",
      "Epoch 355, Loss: 0.8242203542\n",
      "Epoch 356, Loss: 0.8156106061\n",
      "Epoch 357, Loss: 0.8258270457\n",
      "Epoch 358, Loss: 0.8388197492\n",
      "Epoch 359, Loss: 0.8211326635\n",
      "Epoch 360, Loss: 0.8458522431\n",
      "Epoch 361, Loss: 0.8211799203\n",
      "Epoch 362, Loss: 0.8247875896\n",
      "Epoch 363, Loss: 0.8210264236\n",
      "Epoch 364, Loss: 0.8305361379\n",
      "Epoch 365, Loss: 0.8423236857\n",
      "Epoch 366, Loss: 0.8611722015\n",
      "Epoch 367, Loss: 0.8578287431\n",
      "Epoch 368, Loss: 0.8442012538\n",
      "Epoch 369, Loss: 0.8592391309\n",
      "Epoch 370, Loss: 0.8288865522\n",
      "Epoch 371, Loss: 0.8939909379\n",
      "Epoch 372, Loss: 0.9269771011\n",
      "Epoch 373, Loss: 0.9061609215\n",
      "Epoch 374, Loss: 0.8524734295\n",
      "Epoch 375, Loss: 0.8898450446\n",
      "Epoch 376, Loss: 0.9036867939\n",
      "Epoch 377, Loss: 0.9034355021\n",
      "Epoch 378, Loss: 0.8617298314\n",
      "Epoch 379, Loss: 0.8283988537\n",
      "Epoch 380, Loss: 0.8188703935\n",
      "Epoch 381, Loss: 0.8139370305\n",
      "Epoch 382, Loss: 0.8123695525\n",
      "Epoch 383, Loss: 0.8193102763\n",
      "Epoch 384, Loss: 0.8187682942\n",
      "Epoch 385, Loss: 0.8153426956\n",
      "Epoch 386, Loss: 0.8118548430\n",
      "Epoch 387, Loss: 0.8103359521\n",
      "Epoch 388, Loss: 0.9814411458\n",
      "Epoch 389, Loss: 0.8660772693\n",
      "Epoch 390, Loss: 0.9071249537\n",
      "Epoch 391, Loss: 0.9056599922\n",
      "Epoch 392, Loss: 0.9080205866\n",
      "Epoch 393, Loss: 0.8233918405\n",
      "Epoch 394, Loss: 0.8350308046\n",
      "Epoch 395, Loss: 0.8562130486\n",
      "Epoch 396, Loss: 0.8687025710\n",
      "Epoch 397, Loss: 0.8885700066\n",
      "Epoch 398, Loss: 0.9362007794\n",
      "Epoch 399, Loss: 0.9674377220\n",
      "Epoch 400, Loss: 0.8479786442\n",
      "Epoch 401, Loss: 0.8001894534\n",
      "Epoch 402, Loss: 0.8095172375\n",
      "Epoch 403, Loss: 0.7905108926\n",
      "Epoch 404, Loss: 0.7800259360\n",
      "Epoch 405, Loss: 0.7717528968\n",
      "Epoch 406, Loss: 0.7693047910\n",
      "Epoch 407, Loss: 0.7693888013\n",
      "Epoch 408, Loss: 0.7707469578\n",
      "Epoch 409, Loss: 0.7719314748\n",
      "Epoch 410, Loss: 0.7731079067\n",
      "Epoch 411, Loss: 0.7886011791\n",
      "Epoch 412, Loss: 0.8177306087\n",
      "Epoch 413, Loss: 0.8018748425\n",
      "Epoch 414, Loss: 0.7792665177\n",
      "Epoch 415, Loss: 0.7991032928\n",
      "Epoch 416, Loss: 0.7959261569\n",
      "Epoch 417, Loss: 0.7839002393\n",
      "Epoch 418, Loss: 0.7998404805\n",
      "Epoch 419, Loss: 0.7861152635\n",
      "Epoch 420, Loss: 0.7884781793\n",
      "Epoch 421, Loss: 0.8144787355\n",
      "Epoch 422, Loss: 0.8591371223\n",
      "Epoch 423, Loss: 0.9341096046\n",
      "Epoch 424, Loss: 0.9845059356\n",
      "Epoch 425, Loss: 0.9264200816\n",
      "Epoch 426, Loss: 0.9259303008\n",
      "Epoch 427, Loss: 0.9108765655\n",
      "Epoch 428, Loss: 0.9154178388\n",
      "Epoch 429, Loss: 0.9149059994\n",
      "Epoch 430, Loss: 0.8953345513\n",
      "Epoch 431, Loss: 0.7954248567\n",
      "Epoch 432, Loss: 0.7921717445\n",
      "Epoch 433, Loss: 0.8011190711\n",
      "Epoch 434, Loss: 0.8115713949\n",
      "Epoch 435, Loss: 0.8190980579\n",
      "Epoch 436, Loss: 0.8179380473\n",
      "Epoch 437, Loss: 0.8134332354\n",
      "Epoch 438, Loss: 0.8102719885\n",
      "Epoch 439, Loss: 0.8060302348\n",
      "Epoch 440, Loss: 0.7944459722\n",
      "Epoch 441, Loss: 0.7928493707\n",
      "Epoch 442, Loss: 0.8455821040\n",
      "Epoch 443, Loss: 0.8367712078\n",
      "Epoch 444, Loss: 0.8382206122\n",
      "Epoch 445, Loss: 0.9583094058\n",
      "Epoch 446, Loss: 0.9027664847\n",
      "Epoch 447, Loss: 0.8643603666\n",
      "Epoch 448, Loss: 0.8993767773\n",
      "Epoch 449, Loss: 0.7753996940\n",
      "Epoch 450, Loss: 0.7983591398\n",
      "Epoch 451, Loss: 0.8158400809\n",
      "Epoch 452, Loss: 0.8414959785\n",
      "Epoch 453, Loss: 0.8533328072\n",
      "Epoch 454, Loss: 0.8763109337\n",
      "Epoch 455, Loss: 0.8798000665\n",
      "Epoch 456, Loss: 0.8149291002\n",
      "Epoch 457, Loss: 0.8256280432\n",
      "Epoch 458, Loss: 0.7947033979\n",
      "Epoch 459, Loss: 0.7556009209\n",
      "Epoch 460, Loss: 0.7872655212\n",
      "Epoch 461, Loss: 0.7988221423\n",
      "Epoch 462, Loss: 0.8003909702\n",
      "Epoch 463, Loss: 0.7950429293\n",
      "Epoch 464, Loss: 0.8044411363\n",
      "Epoch 465, Loss: 0.8416767667\n",
      "Epoch 466, Loss: 0.9171927135\n",
      "Epoch 467, Loss: 0.8133023910\n",
      "Epoch 468, Loss: 0.8553125267\n",
      "Epoch 469, Loss: 0.7950884434\n",
      "Epoch 470, Loss: 0.7688418416\n",
      "Epoch 471, Loss: 0.7771003685\n",
      "Epoch 472, Loss: 0.7796206496\n",
      "Epoch 473, Loss: 0.7902966941\n",
      "Epoch 474, Loss: 0.7897207280\n",
      "Epoch 475, Loss: 0.7927278550\n",
      "Epoch 476, Loss: 0.7822962039\n",
      "Epoch 477, Loss: 0.7829737918\n",
      "Epoch 478, Loss: 0.7746375937\n",
      "Epoch 479, Loss: 0.7779651978\n",
      "Epoch 480, Loss: 0.7687249169\n",
      "Epoch 481, Loss: 0.7724450248\n",
      "Epoch 482, Loss: 0.7574723046\n",
      "Epoch 483, Loss: 0.7698502928\n",
      "Epoch 484, Loss: 0.7450602583\n",
      "Epoch 485, Loss: 0.7659745771\n",
      "Epoch 486, Loss: 0.7567703084\n",
      "Epoch 487, Loss: 0.7257179437\n",
      "Epoch 488, Loss: 0.7720907404\n",
      "Epoch 489, Loss: 0.7376527386\n",
      "Epoch 490, Loss: 0.7535997714\n",
      "Epoch 491, Loss: 0.7755391871\n",
      "Epoch 492, Loss: 0.8827797795\n",
      "Epoch 493, Loss: 0.7746410274\n",
      "Epoch 494, Loss: 0.7903316555\n",
      "Epoch 495, Loss: 0.7639092498\n",
      "Epoch 496, Loss: 0.7697880313\n",
      "Epoch 497, Loss: 0.8128672070\n",
      "Epoch 498, Loss: 0.9739403079\n",
      "Epoch 499, Loss: 0.8864264257\n",
      "Epoch 500, Loss: 0.8048329859\n",
      "Epoch 501, Loss: 0.7528106759\n",
      "Epoch 502, Loss: 0.7782817802\n",
      "Epoch 503, Loss: 0.8083648571\n",
      "Epoch 504, Loss: 0.8132398984\n",
      "Epoch 505, Loss: 0.8432010549\n",
      "Epoch 506, Loss: 0.9067888143\n",
      "Epoch 507, Loss: 0.8765579126\n",
      "Epoch 508, Loss: 0.7745759343\n",
      "Epoch 509, Loss: 1.1104553579\n",
      "Epoch 510, Loss: 0.7997613124\n",
      "Epoch 511, Loss: 0.9806902533\n",
      "Epoch 512, Loss: 0.8160325843\n",
      "Epoch 513, Loss: 0.7465079053\n",
      "Epoch 514, Loss: 0.7639394863\n",
      "Epoch 515, Loss: 0.8116663808\n",
      "Epoch 516, Loss: 0.7443584209\n",
      "Epoch 517, Loss: 0.7553716964\n",
      "Epoch 518, Loss: 0.7712263359\n",
      "Epoch 519, Loss: 0.7590000821\n",
      "Epoch 520, Loss: 0.7550783509\n",
      "Epoch 521, Loss: 0.7871440521\n",
      "Epoch 522, Loss: 0.7844726270\n",
      "Epoch 523, Loss: 0.8196812717\n",
      "Epoch 524, Loss: 0.8241618436\n",
      "Epoch 525, Loss: 0.8354250140\n",
      "Epoch 526, Loss: 0.8429475163\n",
      "Epoch 527, Loss: 0.8228021663\n",
      "Epoch 528, Loss: 0.7220037913\n",
      "Epoch 529, Loss: 0.7446889271\n",
      "Epoch 530, Loss: 0.7780982496\n",
      "Epoch 531, Loss: 0.7890342251\n",
      "Epoch 532, Loss: 0.7926419378\n",
      "Epoch 533, Loss: 0.7983143972\n",
      "Epoch 534, Loss: 0.8182947463\n",
      "Epoch 535, Loss: 0.8642521813\n",
      "Epoch 536, Loss: 0.7981476936\n",
      "Epoch 537, Loss: 0.7036415362\n",
      "Epoch 538, Loss: 0.7071291735\n",
      "Epoch 539, Loss: 0.7446581826\n",
      "Epoch 540, Loss: 0.7843582488\n",
      "Epoch 541, Loss: 0.7923261955\n",
      "Epoch 542, Loss: 0.7830908405\n",
      "Epoch 543, Loss: 0.7815839411\n",
      "Epoch 544, Loss: 0.7976360556\n",
      "Epoch 545, Loss: 0.8170419437\n",
      "Epoch 546, Loss: 0.8250229902\n",
      "Epoch 547, Loss: 0.8196523831\n",
      "Epoch 548, Loss: 0.7120636919\n",
      "Epoch 549, Loss: 0.7637554648\n",
      "Epoch 550, Loss: 0.7168334307\n",
      "Epoch 551, Loss: 0.7306978211\n",
      "Epoch 552, Loss: 0.7558305354\n",
      "Epoch 553, Loss: 0.7988759536\n",
      "Epoch 554, Loss: 0.8313324197\n",
      "Epoch 555, Loss: 0.8298506557\n",
      "Epoch 556, Loss: 0.8005351323\n",
      "Epoch 557, Loss: 0.7606566794\n",
      "Epoch 558, Loss: 0.7354239313\n",
      "Epoch 559, Loss: 0.7443014382\n",
      "Epoch 560, Loss: 0.7746913846\n",
      "Epoch 561, Loss: 0.8621987295\n",
      "Epoch 562, Loss: 0.8593756073\n",
      "Epoch 563, Loss: 0.7754528769\n",
      "Epoch 564, Loss: 0.7386755580\n",
      "Epoch 565, Loss: 0.7602942901\n",
      "Epoch 566, Loss: 0.8004438701\n",
      "Epoch 567, Loss: 0.7193135887\n",
      "Epoch 568, Loss: 0.7067019204\n",
      "Epoch 569, Loss: 0.7202208512\n",
      "Epoch 570, Loss: 0.7311562607\n",
      "Epoch 571, Loss: 0.7359482344\n",
      "Epoch 572, Loss: 0.7360445692\n",
      "Epoch 573, Loss: 0.7334155656\n",
      "Epoch 574, Loss: 0.7310674911\n",
      "Epoch 575, Loss: 0.7302658198\n",
      "Epoch 576, Loss: 0.7314891787\n",
      "Epoch 577, Loss: 0.7350190362\n",
      "Epoch 578, Loss: 0.7411174043\n",
      "Epoch 579, Loss: 0.7497337720\n",
      "Epoch 580, Loss: 0.7600107153\n",
      "Epoch 581, Loss: 0.7706307341\n",
      "Epoch 582, Loss: 0.7805807187\n",
      "Epoch 583, Loss: 0.7892816127\n",
      "Epoch 584, Loss: 0.7960689509\n",
      "Epoch 585, Loss: 0.8000321615\n",
      "Epoch 586, Loss: 0.7995940117\n",
      "Epoch 587, Loss: 0.7916288676\n",
      "Epoch 588, Loss: 0.7713087337\n",
      "Epoch 589, Loss: 0.7393163085\n",
      "Epoch 590, Loss: 0.7140119305\n",
      "Epoch 591, Loss: 0.7109259004\n",
      "Epoch 592, Loss: 0.6830883768\n",
      "Epoch 593, Loss: 0.6829963951\n",
      "Epoch 594, Loss: 0.7081025317\n",
      "Epoch 595, Loss: 0.8169448772\n",
      "Epoch 596, Loss: 0.8777284145\n",
      "Epoch 597, Loss: 0.8877600713\n",
      "Epoch 598, Loss: 0.8019890699\n",
      "Epoch 599, Loss: 0.7319778610\n",
      "Epoch 600, Loss: 0.7080948673\n",
      "Epoch 601, Loss: 0.6828629997\n",
      "Epoch 602, Loss: 0.7791761777\n",
      "Epoch 603, Loss: 0.7361808979\n",
      "Epoch 604, Loss: 0.7158451546\n",
      "Epoch 605, Loss: 0.7013484652\n",
      "Epoch 606, Loss: 0.7049193627\n",
      "Epoch 607, Loss: 0.7298768887\n",
      "Epoch 608, Loss: 0.7464523690\n",
      "Epoch 609, Loss: 0.7416160445\n",
      "Epoch 610, Loss: 0.7331741709\n",
      "Epoch 611, Loss: 0.7266409926\n",
      "Epoch 612, Loss: 0.7220420494\n",
      "Epoch 613, Loss: 0.7197557616\n",
      "Epoch 614, Loss: 0.7310749100\n",
      "Epoch 615, Loss: 0.7847951473\n",
      "Epoch 616, Loss: 0.8344928902\n",
      "Epoch 617, Loss: 0.7824391280\n",
      "Epoch 618, Loss: 0.7323352973\n",
      "Epoch 619, Loss: 0.6899304123\n",
      "Epoch 620, Loss: 0.7899593071\n",
      "Epoch 621, Loss: 0.7068505640\n",
      "Epoch 622, Loss: 0.7373142158\n",
      "Epoch 623, Loss: 0.7454315392\n",
      "Epoch 624, Loss: 0.7484459285\n",
      "Epoch 625, Loss: 0.7373253623\n",
      "Epoch 626, Loss: 0.7357962772\n",
      "Epoch 627, Loss: 0.7411317537\n",
      "Epoch 628, Loss: 0.7044810238\n",
      "Epoch 629, Loss: 0.7270710466\n",
      "Epoch 630, Loss: 0.7335462152\n",
      "Epoch 631, Loss: 0.7499083851\n",
      "Epoch 632, Loss: 0.7444239872\n",
      "Epoch 633, Loss: 0.7409107908\n",
      "Epoch 634, Loss: 0.7568095099\n",
      "Epoch 635, Loss: 0.7687261535\n",
      "Epoch 636, Loss: 0.8380099084\n",
      "Epoch 637, Loss: 0.7452495771\n",
      "Epoch 638, Loss: 0.7120992899\n",
      "Epoch 639, Loss: 0.6855688350\n",
      "Epoch 640, Loss: 0.6970262221\n",
      "Epoch 641, Loss: 0.7258507341\n",
      "Epoch 642, Loss: 0.7238591629\n",
      "Epoch 643, Loss: 0.7354603257\n",
      "Epoch 644, Loss: 0.7626401232\n",
      "Epoch 645, Loss: 0.7462904515\n",
      "Epoch 646, Loss: 0.7838928195\n",
      "Epoch 647, Loss: 0.7123703922\n",
      "Epoch 648, Loss: 0.7498533380\n",
      "Epoch 649, Loss: 0.7299464257\n",
      "Epoch 650, Loss: 0.7863998182\n",
      "Epoch 651, Loss: 0.7342192761\n",
      "Epoch 652, Loss: 0.7677838970\n",
      "Epoch 653, Loss: 0.7190095888\n",
      "Epoch 654, Loss: 0.7534061606\n",
      "Epoch 655, Loss: 0.7378408267\n",
      "Epoch 656, Loss: 0.7396992845\n",
      "Epoch 657, Loss: 0.7377598876\n",
      "Epoch 658, Loss: 0.7417882634\n",
      "Epoch 659, Loss: 0.7399941628\n",
      "Epoch 660, Loss: 0.7437432547\n",
      "Epoch 661, Loss: 0.7378951913\n",
      "Epoch 662, Loss: 0.7214092789\n",
      "Epoch 663, Loss: 0.7167284488\n",
      "Epoch 664, Loss: 0.7392437207\n",
      "Epoch 665, Loss: 0.7946808485\n",
      "Epoch 666, Loss: 0.8039089671\n",
      "Epoch 667, Loss: 0.8779893746\n",
      "Epoch 668, Loss: 0.8392702855\n",
      "Epoch 669, Loss: 0.7147535521\n",
      "Epoch 670, Loss: 0.7317743224\n",
      "Epoch 671, Loss: 0.9405705818\n",
      "Epoch 672, Loss: 1.0386937051\n",
      "Epoch 673, Loss: 0.7006882510\n",
      "Epoch 674, Loss: 0.8492621704\n",
      "Epoch 675, Loss: 0.8403247177\n",
      "Epoch 676, Loss: 0.8595147668\n",
      "Epoch 677, Loss: 0.9652869360\n",
      "Epoch 678, Loss: 0.7405703005\n",
      "Epoch 679, Loss: 0.8294558347\n",
      "Epoch 680, Loss: 0.7689944188\n",
      "Epoch 681, Loss: 0.8357809718\n",
      "Epoch 682, Loss: 0.7726983454\n",
      "Epoch 683, Loss: 0.7771130494\n",
      "Epoch 684, Loss: 0.6975240912\n",
      "Epoch 685, Loss: 0.7629183059\n",
      "Epoch 686, Loss: 0.7688025869\n",
      "Epoch 687, Loss: 0.7676346183\n",
      "Epoch 688, Loss: 0.7635469543\n",
      "Epoch 689, Loss: 0.6697673589\n",
      "Epoch 690, Loss: 0.6911547327\n",
      "Epoch 691, Loss: 0.7483322619\n",
      "Epoch 692, Loss: 0.8004283054\n",
      "Epoch 693, Loss: 0.7526638893\n",
      "Epoch 694, Loss: 0.9027268055\n",
      "Epoch 695, Loss: 0.8062957809\n",
      "Epoch 696, Loss: 0.6747606856\n",
      "Epoch 697, Loss: 0.7444556669\n",
      "Epoch 698, Loss: 0.7282518314\n",
      "Epoch 699, Loss: 0.7439248330\n",
      "Epoch 700, Loss: 0.8022966092\n",
      "Epoch 701, Loss: 0.8768642252\n",
      "Epoch 702, Loss: 0.6986440399\n",
      "Epoch 703, Loss: 0.6602410140\n",
      "Epoch 704, Loss: 0.6883126627\n",
      "Epoch 705, Loss: 0.7395830035\n",
      "Epoch 706, Loss: 0.7443963782\n",
      "Epoch 707, Loss: 0.7046444685\n",
      "Epoch 708, Loss: 0.8325048222\n",
      "Epoch 709, Loss: 0.9196374018\n",
      "Epoch 710, Loss: 0.6741154788\n",
      "Epoch 711, Loss: 0.6621247610\n",
      "Epoch 712, Loss: 0.7134162472\n",
      "Epoch 713, Loss: 0.7608149555\n",
      "Epoch 714, Loss: 0.7654520415\n",
      "Epoch 715, Loss: 0.7304434482\n",
      "Epoch 716, Loss: 0.7941413830\n",
      "Epoch 717, Loss: 0.7474819696\n",
      "Epoch 718, Loss: 0.6586708481\n",
      "Epoch 719, Loss: 0.6541164575\n",
      "Epoch 720, Loss: 0.7110844524\n",
      "Epoch 721, Loss: 0.6936924999\n",
      "Epoch 722, Loss: 0.6955435973\n",
      "Epoch 723, Loss: 0.7037215405\n",
      "Epoch 724, Loss: 0.7889598105\n",
      "Epoch 725, Loss: 0.8755642639\n",
      "Epoch 726, Loss: 0.8719345895\n",
      "Epoch 727, Loss: 0.6726491046\n",
      "Epoch 728, Loss: 0.6691857140\n",
      "Epoch 729, Loss: 0.6876413244\n",
      "Epoch 730, Loss: 0.7148274714\n",
      "Epoch 731, Loss: 0.6934946530\n",
      "Epoch 732, Loss: 0.7041932073\n",
      "Epoch 733, Loss: 0.6976337338\n",
      "Epoch 734, Loss: 0.6638985093\n",
      "Epoch 735, Loss: 0.6941160961\n",
      "Epoch 736, Loss: 0.8112367445\n",
      "Epoch 737, Loss: 0.9008598504\n",
      "Epoch 738, Loss: 0.8019637462\n",
      "Epoch 739, Loss: 0.6981094931\n",
      "Epoch 740, Loss: 0.7016845334\n",
      "Epoch 741, Loss: 0.7122680560\n",
      "Epoch 742, Loss: 0.6842749039\n",
      "Epoch 743, Loss: 0.6413064401\n",
      "Epoch 744, Loss: 0.6618537303\n",
      "Epoch 745, Loss: 0.7024419085\n",
      "Epoch 746, Loss: 0.7120319195\n",
      "Epoch 747, Loss: 0.6957461232\n",
      "Epoch 748, Loss: 0.6715387728\n",
      "Epoch 749, Loss: 0.6507803700\n",
      "Epoch 750, Loss: 0.6394645307\n",
      "Epoch 751, Loss: 0.6541809925\n",
      "Epoch 752, Loss: 0.6705325830\n",
      "Epoch 753, Loss: 0.6454803674\n",
      "Epoch 754, Loss: 0.6445640395\n",
      "Epoch 755, Loss: 0.6450028906\n",
      "Epoch 756, Loss: 0.6406543116\n",
      "Epoch 757, Loss: 0.6536020079\n",
      "Epoch 758, Loss: 0.6888080952\n",
      "Epoch 759, Loss: 0.6408464125\n",
      "Epoch 760, Loss: 0.6614509626\n",
      "Epoch 761, Loss: 0.7153774582\n",
      "Epoch 762, Loss: 0.7315994310\n",
      "Epoch 763, Loss: 0.7582712694\n",
      "Epoch 764, Loss: 0.8068725719\n",
      "Epoch 765, Loss: 0.7719114053\n",
      "Epoch 766, Loss: 0.7180086888\n",
      "Epoch 767, Loss: 0.8175921527\n",
      "Epoch 768, Loss: 0.7220208404\n",
      "Epoch 769, Loss: 0.7500127805\n",
      "Epoch 770, Loss: 0.8418473075\n",
      "Epoch 771, Loss: 0.9874107601\n",
      "Epoch 772, Loss: 0.7426446637\n",
      "Epoch 773, Loss: 0.6814685304\n",
      "Epoch 774, Loss: 0.6954271655\n",
      "Epoch 775, Loss: 0.6582575085\n",
      "Epoch 776, Loss: 0.6499218301\n",
      "Epoch 777, Loss: 0.7499135743\n",
      "Epoch 778, Loss: 0.8090796480\n",
      "Epoch 779, Loss: 0.7754686273\n",
      "Epoch 780, Loss: 0.6561456945\n",
      "Epoch 781, Loss: 0.6514472325\n",
      "Epoch 782, Loss: 0.6626666108\n",
      "Epoch 783, Loss: 0.6407274113\n",
      "Epoch 784, Loss: 0.6280494260\n",
      "Epoch 785, Loss: 0.6299797952\n",
      "Epoch 786, Loss: 0.6349354617\n",
      "Epoch 787, Loss: 0.6519543226\n",
      "Epoch 788, Loss: 0.6802634689\n",
      "Epoch 789, Loss: 0.7253020284\n",
      "Epoch 790, Loss: 0.7750771407\n",
      "Epoch 791, Loss: 0.7920649476\n",
      "Epoch 792, Loss: 0.8101739207\n",
      "Epoch 793, Loss: 0.8420132175\n",
      "Epoch 794, Loss: 0.8538567499\n",
      "Epoch 795, Loss: 0.8339568958\n",
      "Epoch 796, Loss: 0.8124366279\n",
      "Epoch 797, Loss: 0.7367759548\n",
      "Epoch 798, Loss: 0.6436703527\n",
      "Epoch 799, Loss: 0.6791452886\n",
      "Epoch 800, Loss: 0.6668245280\n",
      "Epoch 801, Loss: 0.6480987949\n",
      "Epoch 802, Loss: 0.6417801762\n",
      "Epoch 803, Loss: 0.6339494497\n",
      "Epoch 804, Loss: 0.6282272295\n",
      "Epoch 805, Loss: 0.6296313287\n",
      "Epoch 806, Loss: 0.6277413100\n",
      "Epoch 807, Loss: 0.6248370329\n",
      "Epoch 808, Loss: 0.6413129175\n",
      "Epoch 809, Loss: 0.6978789884\n",
      "Epoch 810, Loss: 0.7540201810\n",
      "Epoch 811, Loss: 0.7502273196\n",
      "Epoch 812, Loss: 0.7252613317\n",
      "Epoch 813, Loss: 0.6762038226\n",
      "Epoch 814, Loss: 0.6551867727\n",
      "Epoch 815, Loss: 0.6727063661\n",
      "Epoch 816, Loss: 0.6728569629\n",
      "Epoch 817, Loss: 0.6522487404\n",
      "Epoch 818, Loss: 0.6414667724\n",
      "Epoch 819, Loss: 0.6601628026\n",
      "Epoch 820, Loss: 0.6644331620\n",
      "Epoch 821, Loss: 0.6417941522\n",
      "Epoch 822, Loss: 0.6472599994\n",
      "Epoch 823, Loss: 0.6448815354\n",
      "Epoch 824, Loss: 0.7117861430\n",
      "Epoch 825, Loss: 0.7145206220\n",
      "Epoch 826, Loss: 0.6903186444\n",
      "Epoch 827, Loss: 0.6749154174\n",
      "Epoch 828, Loss: 0.6835474510\n",
      "Epoch 829, Loss: 0.6949894601\n",
      "Epoch 830, Loss: 0.6774946240\n",
      "Epoch 831, Loss: 0.6417605370\n",
      "Epoch 832, Loss: 0.6387691298\n",
      "Epoch 833, Loss: 0.6968347350\n",
      "Epoch 834, Loss: 0.7160728689\n",
      "Epoch 835, Loss: 0.6670231678\n",
      "Epoch 836, Loss: 0.7401686072\n",
      "Epoch 837, Loss: 0.6761349717\n",
      "Epoch 838, Loss: 0.6395153842\n",
      "Epoch 839, Loss: 0.6295368482\n",
      "Epoch 840, Loss: 0.6443803820\n",
      "Epoch 841, Loss: 0.6352427822\n",
      "Epoch 842, Loss: 0.7054692640\n",
      "Epoch 843, Loss: 0.7489777378\n",
      "Epoch 844, Loss: 0.8662056937\n",
      "Epoch 845, Loss: 0.6556914789\n",
      "Epoch 846, Loss: 0.8642677093\n",
      "Epoch 847, Loss: 0.7884936029\n",
      "Epoch 848, Loss: 0.9171527354\n",
      "Epoch 849, Loss: 0.6961016178\n",
      "Epoch 850, Loss: 0.6896824813\n",
      "Epoch 851, Loss: 0.7810510566\n",
      "Epoch 852, Loss: 0.6741842693\n",
      "Epoch 853, Loss: 0.7919330925\n",
      "Epoch 854, Loss: 0.9463197022\n",
      "Epoch 855, Loss: 0.7314142076\n",
      "Epoch 856, Loss: 0.7845653941\n",
      "Epoch 857, Loss: 0.6887213973\n",
      "Epoch 858, Loss: 0.6624290108\n",
      "Epoch 859, Loss: 0.6317867528\n",
      "Epoch 860, Loss: 0.6435649142\n",
      "Epoch 861, Loss: 0.6714774683\n",
      "Epoch 862, Loss: 0.6147897627\n",
      "Epoch 863, Loss: 0.6352261379\n",
      "Epoch 864, Loss: 0.6753090751\n",
      "Epoch 865, Loss: 0.6682775876\n",
      "Epoch 866, Loss: 0.6628543004\n",
      "Epoch 867, Loss: 0.6363514101\n",
      "Epoch 868, Loss: 0.6723294422\n",
      "Epoch 869, Loss: 0.6821655841\n",
      "Epoch 870, Loss: 0.6592954995\n",
      "Epoch 871, Loss: 0.6578034720\n",
      "Epoch 872, Loss: 0.7132295161\n",
      "Epoch 873, Loss: 0.7181268074\n",
      "Epoch 874, Loss: 0.7522759556\n",
      "Epoch 875, Loss: 0.7336569212\n",
      "Epoch 876, Loss: 0.6424386988\n",
      "Epoch 877, Loss: 0.6393492929\n",
      "Epoch 878, Loss: 0.6888528270\n",
      "Epoch 879, Loss: 0.7256109598\n",
      "Epoch 880, Loss: 0.7567131033\n",
      "Epoch 881, Loss: 0.8391551411\n",
      "Epoch 882, Loss: 0.7713842184\n",
      "Epoch 883, Loss: 0.7471719269\n",
      "Epoch 884, Loss: 0.6155669782\n",
      "Epoch 885, Loss: 0.6072142247\n",
      "Epoch 886, Loss: 0.7299923035\n",
      "Epoch 887, Loss: 0.6686519828\n",
      "Epoch 888, Loss: 0.5878037691\n",
      "Epoch 889, Loss: 0.6280411116\n",
      "Epoch 890, Loss: 0.7502228350\n",
      "Epoch 891, Loss: 0.7600863951\n",
      "Epoch 892, Loss: 0.7057158382\n",
      "Epoch 893, Loss: 0.6757337895\n",
      "Epoch 894, Loss: 0.7149705029\n",
      "Epoch 895, Loss: 0.7522348135\n",
      "Epoch 896, Loss: 0.7619108944\n",
      "Epoch 897, Loss: 0.6803457563\n",
      "Epoch 898, Loss: 0.5906093376\n",
      "Epoch 899, Loss: 0.5885699963\n",
      "Epoch 900, Loss: 0.6571350091\n",
      "Epoch 901, Loss: 0.6398839299\n",
      "Epoch 902, Loss: 0.6443093055\n",
      "Epoch 903, Loss: 0.6482858204\n",
      "Epoch 904, Loss: 0.6270253276\n",
      "Epoch 905, Loss: 0.6276519606\n",
      "Epoch 906, Loss: 0.6130275600\n",
      "Epoch 907, Loss: 0.6005459611\n",
      "Epoch 908, Loss: 0.5864697570\n",
      "Epoch 909, Loss: 0.5848731104\n",
      "Epoch 910, Loss: 0.6327421240\n",
      "Epoch 911, Loss: 0.6435893225\n",
      "Epoch 912, Loss: 0.6343635850\n",
      "Epoch 913, Loss: 0.7108983711\n",
      "Epoch 914, Loss: 0.6613345242\n",
      "Epoch 915, Loss: 0.5857778084\n",
      "Epoch 916, Loss: 0.5770992871\n",
      "Epoch 917, Loss: 0.6375322708\n",
      "Epoch 918, Loss: 0.6104361997\n",
      "Epoch 919, Loss: 0.5791871620\n",
      "Epoch 920, Loss: 0.5712164691\n",
      "Epoch 921, Loss: 0.5849962655\n",
      "Epoch 922, Loss: 0.5945299933\n",
      "Epoch 923, Loss: 0.5859448021\n",
      "Epoch 924, Loss: 0.5902981700\n",
      "Epoch 925, Loss: 0.5862847806\n",
      "Epoch 926, Loss: 0.5842815259\n",
      "Epoch 927, Loss: 0.5806200337\n",
      "Epoch 928, Loss: 0.5899897277\n",
      "Epoch 929, Loss: 0.6497544202\n",
      "Epoch 930, Loss: 0.6712535482\n",
      "Epoch 931, Loss: 0.6099418385\n",
      "Epoch 932, Loss: 0.5882103995\n",
      "Epoch 933, Loss: 0.6962231606\n",
      "Epoch 934, Loss: 0.7131850271\n",
      "Epoch 935, Loss: 0.7255703314\n",
      "Epoch 936, Loss: 0.5705914753\n",
      "Epoch 937, Loss: 0.6510205681\n",
      "Epoch 938, Loss: 0.6084475623\n",
      "Epoch 939, Loss: 0.6144455694\n",
      "Epoch 940, Loss: 0.6397055700\n",
      "Epoch 941, Loss: 0.7049517131\n",
      "Epoch 942, Loss: 0.6591931284\n",
      "Epoch 943, Loss: 0.5744203358\n",
      "Epoch 944, Loss: 0.6168976568\n",
      "Epoch 945, Loss: 0.6305989691\n",
      "Epoch 946, Loss: 0.6157446824\n",
      "Epoch 947, Loss: 0.6106820747\n",
      "Epoch 948, Loss: 0.6272865688\n",
      "Epoch 949, Loss: 0.7570788453\n",
      "Epoch 950, Loss: 0.5590211572\n",
      "Epoch 951, Loss: 0.5808223725\n",
      "Epoch 952, Loss: 0.6550884579\n",
      "Epoch 953, Loss: 0.6287673738\n",
      "Epoch 954, Loss: 0.5962978180\n",
      "Epoch 955, Loss: 0.5892932314\n",
      "Epoch 956, Loss: 0.6564020538\n",
      "Epoch 957, Loss: 0.7031766733\n",
      "Epoch 958, Loss: 0.6307180248\n",
      "Epoch 959, Loss: 0.7190364858\n",
      "Epoch 960, Loss: 0.7337320630\n",
      "Epoch 961, Loss: 0.6447168925\n",
      "Epoch 962, Loss: 0.7156797675\n",
      "Epoch 963, Loss: 0.5703749789\n",
      "Epoch 964, Loss: 0.6055193782\n",
      "Epoch 965, Loss: 0.6162136018\n",
      "Epoch 966, Loss: 0.6657035888\n",
      "Epoch 967, Loss: 0.5810564890\n",
      "Epoch 968, Loss: 0.6909588951\n",
      "Epoch 969, Loss: 0.5802880663\n",
      "Epoch 970, Loss: 0.6757864369\n",
      "Epoch 971, Loss: 0.5972076283\n",
      "Epoch 972, Loss: 0.6645533171\n",
      "Epoch 973, Loss: 0.6278707989\n",
      "Epoch 974, Loss: 0.6660971901\n",
      "Epoch 975, Loss: 0.6567146911\n",
      "Epoch 976, Loss: 0.6146237025\n",
      "Epoch 977, Loss: 0.6830319158\n",
      "Epoch 978, Loss: 0.7324780588\n",
      "Epoch 979, Loss: 0.8168815108\n",
      "Epoch 980, Loss: 0.9860108780\n",
      "Epoch 981, Loss: 0.7185374671\n",
      "Epoch 982, Loss: 0.7965260309\n",
      "Epoch 983, Loss: 0.9371406515\n",
      "Epoch 984, Loss: 0.6198764361\n",
      "Epoch 985, Loss: 0.6755633735\n",
      "Epoch 986, Loss: 0.6234320265\n",
      "Epoch 987, Loss: 0.6347560373\n",
      "Epoch 988, Loss: 0.6107820848\n",
      "Epoch 989, Loss: 0.6200275664\n",
      "Epoch 990, Loss: 0.6211023356\n",
      "Epoch 991, Loss: 0.6467064623\n",
      "Epoch 992, Loss: 0.6945262958\n",
      "Epoch 993, Loss: 0.7018377652\n",
      "Epoch 994, Loss: 0.7051230926\n",
      "Epoch 995, Loss: 0.6744384974\n",
      "Epoch 996, Loss: 0.7583495502\n",
      "Epoch 997, Loss: 0.8895541466\n",
      "Epoch 998, Loss: 0.9256835168\n",
      "Epoch 999, Loss: 0.5757299677\n",
      "Epoch 1000, Loss: 0.6442110673\n",
      "Epoch 1001, Loss: 0.6433333894\n",
      "Epoch 1002, Loss: 0.6209562908\n",
      "Epoch 1003, Loss: 0.7186626368\n",
      "Epoch 1004, Loss: 0.7657418112\n",
      "Epoch 1005, Loss: 0.9617455386\n",
      "Epoch 1006, Loss: 0.5670651793\n",
      "Epoch 1007, Loss: 0.6515569895\n",
      "Epoch 1008, Loss: 0.6233348348\n",
      "Epoch 1009, Loss: 0.6293543073\n",
      "Epoch 1010, Loss: 0.7206530593\n",
      "Epoch 1011, Loss: 0.8509243252\n",
      "Epoch 1012, Loss: 0.9244291113\n",
      "Epoch 1013, Loss: 0.5470127763\n",
      "Epoch 1014, Loss: 0.6248184219\n",
      "Epoch 1015, Loss: 0.5931595975\n",
      "Epoch 1016, Loss: 0.6270512962\n",
      "Epoch 1017, Loss: 0.6856798186\n",
      "Epoch 1018, Loss: 0.6349320153\n",
      "Epoch 1019, Loss: 0.5871747175\n",
      "Epoch 1020, Loss: 0.6087367844\n",
      "Epoch 1021, Loss: 0.6668758472\n",
      "Epoch 1022, Loss: 0.6563684582\n",
      "Epoch 1023, Loss: 0.6337816369\n",
      "Epoch 1024, Loss: 0.6167158464\n",
      "Epoch 1025, Loss: 0.6043780617\n",
      "Epoch 1026, Loss: 0.6042782598\n",
      "Epoch 1027, Loss: 0.6239628301\n",
      "Epoch 1028, Loss: 0.5662327299\n",
      "Epoch 1029, Loss: 0.5412915325\n",
      "Epoch 1030, Loss: 0.5644065819\n",
      "Epoch 1031, Loss: 0.5990827616\n",
      "Epoch 1032, Loss: 0.5869942033\n",
      "Epoch 1033, Loss: 0.6462060833\n",
      "Epoch 1034, Loss: 0.7020550467\n",
      "Epoch 1035, Loss: 0.5847090840\n",
      "Epoch 1036, Loss: 0.5400425088\n",
      "Epoch 1037, Loss: 0.5749944061\n",
      "Epoch 1038, Loss: 0.6029482589\n",
      "Epoch 1039, Loss: 0.5797613101\n",
      "Epoch 1040, Loss: 0.5606210992\n",
      "Epoch 1041, Loss: 0.5678175293\n",
      "Epoch 1042, Loss: 0.6062544266\n",
      "Epoch 1043, Loss: 0.7115807753\n",
      "Epoch 1044, Loss: 0.7363853272\n",
      "Epoch 1045, Loss: 0.6302234702\n",
      "Epoch 1046, Loss: 0.5334856312\n",
      "Epoch 1047, Loss: 0.6543989614\n",
      "Epoch 1048, Loss: 0.6487983068\n",
      "Epoch 1049, Loss: 0.6101890041\n",
      "Epoch 1050, Loss: 0.5453168704\n",
      "Epoch 1051, Loss: 0.5649802622\n",
      "Epoch 1052, Loss: 0.5733076599\n",
      "Epoch 1053, Loss: 0.5673897009\n",
      "Epoch 1054, Loss: 0.5984372289\n",
      "Epoch 1055, Loss: 0.5817488394\n",
      "Epoch 1056, Loss: 0.5629771001\n",
      "Epoch 1057, Loss: 0.5357233287\n",
      "Epoch 1058, Loss: 0.5184305121\n",
      "Epoch 1059, Loss: 0.5180551605\n",
      "Epoch 1060, Loss: 0.5671919051\n",
      "Epoch 1061, Loss: 0.6610465455\n",
      "Epoch 1062, Loss: 0.6486121670\n",
      "Epoch 1063, Loss: 0.6150938066\n",
      "Epoch 1064, Loss: 0.6078414949\n",
      "Epoch 1065, Loss: 0.5966721371\n",
      "Epoch 1066, Loss: 0.5864715955\n",
      "Epoch 1067, Loss: 0.5894443977\n",
      "Epoch 1068, Loss: 0.5704289402\n",
      "Epoch 1069, Loss: 0.5534798316\n",
      "Epoch 1070, Loss: 0.5628766477\n",
      "Epoch 1071, Loss: 0.5969297750\n",
      "Epoch 1072, Loss: 0.6124110174\n",
      "Epoch 1073, Loss: 0.6111073724\n",
      "Epoch 1074, Loss: 0.6025214155\n",
      "Epoch 1075, Loss: 0.5607175462\n",
      "Epoch 1076, Loss: 0.5464067027\n",
      "Epoch 1077, Loss: 0.6372869075\n",
      "Epoch 1078, Loss: 0.5620579560\n",
      "Epoch 1079, Loss: 0.5222655025\n",
      "Epoch 1080, Loss: 0.5199909692\n",
      "Epoch 1081, Loss: 0.5285520455\n",
      "Epoch 1082, Loss: 0.5540715041\n",
      "Epoch 1083, Loss: 0.6306147566\n",
      "Epoch 1084, Loss: 0.6067171608\n",
      "Epoch 1085, Loss: 0.5608148123\n",
      "Epoch 1086, Loss: 0.5099921038\n",
      "Epoch 1087, Loss: 0.5431841735\n",
      "Epoch 1088, Loss: 0.6174294048\n",
      "Epoch 1089, Loss: 0.6450435000\n",
      "Epoch 1090, Loss: 0.5228813057\n",
      "Epoch 1091, Loss: 0.5292784030\n",
      "Epoch 1092, Loss: 0.5469422433\n",
      "Epoch 1093, Loss: 0.5338015466\n",
      "Epoch 1094, Loss: 0.5277821428\n",
      "Epoch 1095, Loss: 0.5111462710\n",
      "Epoch 1096, Loss: 0.5052102670\n",
      "Epoch 1097, Loss: 0.5059273266\n",
      "Epoch 1098, Loss: 0.5249977913\n",
      "Epoch 1099, Loss: 0.5829978381\n",
      "Epoch 1100, Loss: 0.6019436145\n",
      "Epoch 1101, Loss: 0.5203748778\n",
      "Epoch 1102, Loss: 0.5224203039\n",
      "Epoch 1103, Loss: 0.5144914351\n",
      "Epoch 1104, Loss: 0.5096415400\n",
      "Epoch 1105, Loss: 0.5051970364\n",
      "Epoch 1106, Loss: 0.5008485475\n",
      "Epoch 1107, Loss: 0.4944049837\n",
      "Epoch 1108, Loss: 0.4910442576\n",
      "Epoch 1109, Loss: 0.4923149667\n",
      "Epoch 1110, Loss: 0.5042375717\n",
      "Epoch 1111, Loss: 0.5316857964\n",
      "Epoch 1112, Loss: 0.5760115666\n",
      "Epoch 1113, Loss: 0.6128254397\n",
      "Epoch 1114, Loss: 0.5755152983\n",
      "Epoch 1115, Loss: 0.5272890292\n",
      "Epoch 1116, Loss: 0.5132470172\n",
      "Epoch 1117, Loss: 0.5142516395\n",
      "Epoch 1118, Loss: 0.5007306201\n",
      "Epoch 1119, Loss: 0.5011202585\n",
      "Epoch 1120, Loss: 0.5014162054\n",
      "Epoch 1121, Loss: 0.4918239283\n",
      "Epoch 1122, Loss: 0.4864765773\n",
      "Epoch 1123, Loss: 0.4844824740\n",
      "Epoch 1124, Loss: 0.4834678528\n",
      "Epoch 1125, Loss: 0.4865046196\n",
      "Epoch 1126, Loss: 0.4900156988\n",
      "Epoch 1127, Loss: 0.4936822367\n",
      "Epoch 1128, Loss: 0.4985189687\n",
      "Epoch 1129, Loss: 0.5042020700\n",
      "Epoch 1130, Loss: 0.5107766677\n",
      "Epoch 1131, Loss: 0.5191520163\n",
      "Epoch 1132, Loss: 0.5294498194\n",
      "Epoch 1133, Loss: 0.5380875255\n",
      "Epoch 1134, Loss: 0.5509639327\n",
      "Epoch 1135, Loss: 0.5856024479\n",
      "Epoch 1136, Loss: 0.5974301336\n",
      "Epoch 1137, Loss: 0.5916802272\n",
      "Epoch 1138, Loss: 0.5444342418\n",
      "Epoch 1139, Loss: 0.5813136218\n",
      "Epoch 1140, Loss: 0.6072264193\n",
      "Epoch 1141, Loss: 0.6012587491\n",
      "Epoch 1142, Loss: 0.5439618472\n",
      "Epoch 1143, Loss: 0.5008324993\n",
      "Epoch 1144, Loss: 0.5710987040\n",
      "Epoch 1145, Loss: 0.5463813246\n",
      "Epoch 1146, Loss: 0.5549929598\n",
      "Epoch 1147, Loss: 0.5416018080\n",
      "Epoch 1148, Loss: 0.5489720913\n",
      "Epoch 1149, Loss: 0.5306130833\n",
      "Epoch 1150, Loss: 0.5201182367\n",
      "Epoch 1151, Loss: 0.5816650403\n",
      "Epoch 1152, Loss: 0.5075138510\n",
      "Epoch 1153, Loss: 0.5268829127\n",
      "Epoch 1154, Loss: 0.5037569899\n",
      "Epoch 1155, Loss: 0.4987729938\n",
      "Epoch 1156, Loss: 0.4999001050\n",
      "Epoch 1157, Loss: 0.5021993194\n",
      "Epoch 1158, Loss: 0.5027426491\n",
      "Epoch 1159, Loss: 0.5024245507\n",
      "Epoch 1160, Loss: 0.5019897962\n",
      "Epoch 1161, Loss: 0.5008612175\n",
      "Epoch 1162, Loss: 0.4978682288\n",
      "Epoch 1163, Loss: 0.4937406594\n",
      "Epoch 1164, Loss: 0.4890167805\n",
      "Epoch 1165, Loss: 0.4842833961\n",
      "Epoch 1166, Loss: 0.4800435123\n",
      "Epoch 1167, Loss: 0.4772955385\n",
      "Epoch 1168, Loss: 0.4742153200\n",
      "Epoch 1169, Loss: 0.4742475503\n",
      "Epoch 1170, Loss: 0.4697094621\n",
      "Epoch 1171, Loss: 0.4689518383\n",
      "Epoch 1172, Loss: 0.4734180338\n",
      "Epoch 1173, Loss: 0.5134115040\n",
      "Epoch 1174, Loss: 0.5428183332\n",
      "Epoch 1175, Loss: 0.4706398288\n",
      "Epoch 1176, Loss: 0.4921658092\n",
      "Epoch 1177, Loss: 0.5257336829\n",
      "Epoch 1178, Loss: 0.5553754733\n",
      "Epoch 1179, Loss: 0.6700084109\n",
      "Epoch 1180, Loss: 0.7413199279\n",
      "Epoch 1181, Loss: 0.6078160795\n",
      "Epoch 1182, Loss: 0.5405431376\n",
      "Epoch 1183, Loss: 0.5066833109\n",
      "Epoch 1184, Loss: 0.5900138885\n",
      "Epoch 1185, Loss: 0.5705394451\n",
      "Epoch 1186, Loss: 0.5390171570\n",
      "Epoch 1187, Loss: 0.5267564527\n",
      "Epoch 1188, Loss: 0.5091208582\n",
      "Epoch 1189, Loss: 0.4722409890\n",
      "Epoch 1190, Loss: 0.4949232888\n",
      "Epoch 1191, Loss: 0.5063166564\n",
      "Epoch 1192, Loss: 0.5038974601\n",
      "Epoch 1193, Loss: 0.5335759766\n",
      "Epoch 1194, Loss: 0.5471987475\n",
      "Epoch 1195, Loss: 0.5291802612\n",
      "Epoch 1196, Loss: 0.4966040772\n",
      "Epoch 1197, Loss: 0.6148594102\n",
      "Epoch 1198, Loss: 0.5408513057\n",
      "Epoch 1199, Loss: 0.5496016441\n",
      "Epoch 1200, Loss: 0.5695630026\n",
      "Epoch 1201, Loss: 0.5223155465\n",
      "Epoch 1202, Loss: 0.5811247004\n",
      "Epoch 1203, Loss: 0.5876640515\n",
      "Epoch 1204, Loss: 0.5111417887\n",
      "Epoch 1205, Loss: 0.4548058095\n",
      "Epoch 1206, Loss: 0.4894309416\n",
      "Epoch 1207, Loss: 0.5797381219\n",
      "Epoch 1208, Loss: 0.5568337487\n",
      "Epoch 1209, Loss: 0.5475263874\n",
      "Epoch 1210, Loss: 0.4827353588\n",
      "Epoch 1211, Loss: 0.4635847329\n",
      "Epoch 1212, Loss: 0.4704961889\n",
      "Epoch 1213, Loss: 0.4774240466\n",
      "Epoch 1214, Loss: 0.4805204787\n",
      "Epoch 1215, Loss: 0.4777901470\n",
      "Epoch 1216, Loss: 0.5431690167\n",
      "Epoch 1217, Loss: 0.6227919845\n",
      "Epoch 1218, Loss: 0.6068364928\n",
      "Epoch 1219, Loss: 0.5465610217\n",
      "Epoch 1220, Loss: 0.5719536342\n",
      "Epoch 1221, Loss: 0.5205283067\n",
      "Epoch 1222, Loss: 0.4909761716\n",
      "Epoch 1223, Loss: 0.4945071724\n",
      "Epoch 1224, Loss: 0.5961765185\n",
      "Epoch 1225, Loss: 0.5752253514\n",
      "Epoch 1226, Loss: 0.4847945169\n",
      "Epoch 1227, Loss: 0.5742222563\n",
      "Epoch 1228, Loss: 0.4922598901\n",
      "Epoch 1229, Loss: 0.4904895070\n",
      "Epoch 1230, Loss: 0.6827656714\n",
      "Epoch 1231, Loss: 0.6749186040\n",
      "Epoch 1232, Loss: 0.5733817408\n",
      "Epoch 1233, Loss: 0.5007644484\n",
      "Epoch 1234, Loss: 0.4976327845\n",
      "Epoch 1235, Loss: 0.5188255507\n",
      "Epoch 1236, Loss: 0.6312124122\n",
      "Epoch 1237, Loss: 0.4758591349\n",
      "Epoch 1238, Loss: 0.4757659159\n",
      "Epoch 1239, Loss: 0.5488196035\n",
      "Epoch 1240, Loss: 0.5537763535\n",
      "Epoch 1241, Loss: 0.5853147863\n",
      "Epoch 1242, Loss: 0.6042908647\n",
      "Epoch 1243, Loss: 0.6928021876\n",
      "Epoch 1244, Loss: 0.9936986987\n",
      "Epoch 1245, Loss: 0.5759201700\n",
      "Epoch 1246, Loss: 0.5101410836\n",
      "Epoch 1247, Loss: 0.5601179352\n",
      "Epoch 1248, Loss: 0.5955679817\n",
      "Epoch 1249, Loss: 0.5607279041\n",
      "Epoch 1250, Loss: 0.5552743385\n",
      "Epoch 1251, Loss: 0.6987189399\n",
      "Epoch 1252, Loss: 0.8575745194\n",
      "Epoch 1253, Loss: 0.7380805277\n",
      "Epoch 1254, Loss: 0.6391713316\n",
      "Epoch 1255, Loss: 0.8610769119\n",
      "Epoch 1256, Loss: 0.6052235896\n",
      "Epoch 1257, Loss: 0.5048211534\n",
      "Epoch 1258, Loss: 0.5497556747\n",
      "Epoch 1259, Loss: 0.5718413810\n",
      "Epoch 1260, Loss: 0.5634259490\n",
      "Epoch 1261, Loss: 0.6573683922\n",
      "Epoch 1262, Loss: 0.7479719166\n",
      "Epoch 1263, Loss: 0.9621935556\n",
      "Epoch 1264, Loss: 0.5596469860\n",
      "Epoch 1265, Loss: 0.6116060224\n",
      "Epoch 1266, Loss: 0.5653339153\n",
      "Epoch 1267, Loss: 0.5366749761\n",
      "Epoch 1268, Loss: 0.5986981197\n",
      "Epoch 1269, Loss: 0.6340300877\n",
      "Epoch 1270, Loss: 0.5827401461\n",
      "Epoch 1271, Loss: 0.5587987966\n",
      "Epoch 1272, Loss: 0.6632685808\n",
      "Epoch 1273, Loss: 0.9340478812\n",
      "Epoch 1274, Loss: 0.8610554597\n",
      "Epoch 1275, Loss: 0.5617116313\n",
      "Epoch 1276, Loss: 0.6849183573\n",
      "Epoch 1277, Loss: 0.4924743196\n",
      "Epoch 1278, Loss: 0.5362690071\n",
      "Epoch 1279, Loss: 0.5422262074\n",
      "Epoch 1280, Loss: 0.5304391957\n",
      "Epoch 1281, Loss: 0.5464573622\n",
      "Epoch 1282, Loss: 0.6101457631\n",
      "Epoch 1283, Loss: 0.6503581842\n",
      "Epoch 1284, Loss: 0.5798632379\n",
      "Epoch 1285, Loss: 0.8235959807\n",
      "Epoch 1286, Loss: 1.0787748466\n",
      "Epoch 1287, Loss: 0.5303553688\n",
      "Epoch 1288, Loss: 0.5836216878\n",
      "Epoch 1289, Loss: 0.5810684379\n",
      "Epoch 1290, Loss: 0.5196759289\n",
      "Epoch 1291, Loss: 0.5800900986\n",
      "Epoch 1292, Loss: 0.5512092954\n",
      "Epoch 1293, Loss: 0.5126709982\n",
      "Epoch 1294, Loss: 0.5328097784\n",
      "Epoch 1295, Loss: 0.5444787978\n",
      "Epoch 1296, Loss: 0.5880468272\n",
      "Epoch 1297, Loss: 0.6816965366\n",
      "Epoch 1298, Loss: 0.6898061204\n",
      "Epoch 1299, Loss: 0.9625081166\n",
      "Epoch 1300, Loss: 0.5353237907\n",
      "Epoch 1301, Loss: 0.6705421486\n",
      "Epoch 1302, Loss: 0.6588392998\n",
      "Epoch 1303, Loss: 0.6639468078\n",
      "Epoch 1304, Loss: 0.6228980665\n",
      "Epoch 1305, Loss: 0.6238935817\n",
      "Epoch 1306, Loss: 0.6795639763\n",
      "Epoch 1307, Loss: 0.7300014156\n",
      "Epoch 1308, Loss: 0.6184973465\n",
      "Epoch 1309, Loss: 0.5366142370\n",
      "Epoch 1310, Loss: 0.5537747945\n",
      "Epoch 1311, Loss: 0.5717817683\n",
      "Epoch 1312, Loss: 0.5827598861\n",
      "Epoch 1313, Loss: 0.6788750996\n",
      "Epoch 1314, Loss: 0.7323338876\n",
      "Epoch 1315, Loss: 0.6420510567\n",
      "Epoch 1316, Loss: 0.5514270508\n",
      "Epoch 1317, Loss: 0.4704048205\n",
      "Epoch 1318, Loss: 0.4756000631\n",
      "Epoch 1319, Loss: 0.5324937338\n",
      "Epoch 1320, Loss: 0.6109003719\n",
      "Epoch 1321, Loss: 0.8098615946\n",
      "Epoch 1322, Loss: 0.6076012054\n",
      "Epoch 1323, Loss: 0.4473807437\n",
      "Epoch 1324, Loss: 0.6226062424\n",
      "Epoch 1325, Loss: 0.5730043539\n",
      "Epoch 1326, Loss: 0.5268825248\n",
      "Epoch 1327, Loss: 0.5146630553\n",
      "Epoch 1328, Loss: 0.5157340496\n",
      "Epoch 1329, Loss: 0.5382492158\n",
      "Epoch 1330, Loss: 0.4912503091\n",
      "Epoch 1331, Loss: 0.4902117796\n",
      "Epoch 1332, Loss: 0.5099250287\n",
      "Epoch 1333, Loss: 0.5211003796\n",
      "Epoch 1334, Loss: 0.5469138437\n",
      "Epoch 1335, Loss: 0.5624691818\n",
      "Epoch 1336, Loss: 0.5538914389\n",
      "Epoch 1337, Loss: 0.5407845879\n",
      "Epoch 1338, Loss: 0.5385469274\n",
      "Epoch 1339, Loss: 0.5356303460\n",
      "Epoch 1340, Loss: 0.5177184490\n",
      "Epoch 1341, Loss: 0.4929953555\n",
      "Epoch 1342, Loss: 0.4940726255\n",
      "Epoch 1343, Loss: 0.5691640559\n",
      "Epoch 1344, Loss: 0.6373077909\n",
      "Epoch 1345, Loss: 0.5897144847\n",
      "Epoch 1346, Loss: 0.6307667689\n",
      "Epoch 1347, Loss: 0.7052792158\n",
      "Epoch 1348, Loss: 0.8497427928\n",
      "Epoch 1349, Loss: 0.6881339042\n",
      "Epoch 1350, Loss: 0.5336784483\n",
      "Epoch 1351, Loss: 0.6739634390\n",
      "Epoch 1352, Loss: 0.5505069077\n",
      "Epoch 1353, Loss: 0.4309225651\n",
      "Epoch 1354, Loss: 0.6183257902\n",
      "Epoch 1355, Loss: 0.5605605022\n",
      "Epoch 1356, Loss: 0.5832461726\n",
      "Epoch 1357, Loss: 0.5353367401\n",
      "Epoch 1358, Loss: 0.4832629305\n",
      "Epoch 1359, Loss: 0.5027092533\n",
      "Epoch 1360, Loss: 0.5047265523\n",
      "Epoch 1361, Loss: 0.4840204901\n",
      "Epoch 1362, Loss: 0.4863641209\n",
      "Epoch 1363, Loss: 0.4921117417\n",
      "Epoch 1364, Loss: 0.4895355746\n",
      "Epoch 1365, Loss: 0.4893482801\n",
      "Epoch 1366, Loss: 0.4906880442\n",
      "Epoch 1367, Loss: 0.4889375224\n",
      "Epoch 1368, Loss: 0.4920018857\n",
      "Epoch 1369, Loss: 0.5016811795\n",
      "Epoch 1370, Loss: 0.5220300118\n",
      "Epoch 1371, Loss: 0.5718011144\n",
      "Epoch 1372, Loss: 0.5957775107\n",
      "Epoch 1373, Loss: 0.6552130653\n",
      "Epoch 1374, Loss: 0.6018490275\n",
      "Epoch 1375, Loss: 0.4457716610\n",
      "Epoch 1376, Loss: 0.5359597090\n",
      "Epoch 1377, Loss: 0.5309939876\n",
      "Epoch 1378, Loss: 0.5877728392\n",
      "Epoch 1379, Loss: 0.5559512025\n",
      "Epoch 1380, Loss: 0.5543428698\n",
      "Epoch 1381, Loss: 0.6164413620\n",
      "Epoch 1382, Loss: 0.6069823105\n",
      "Epoch 1383, Loss: 0.6193998876\n",
      "Epoch 1384, Loss: 0.5900092815\n",
      "Epoch 1385, Loss: 0.6100497009\n",
      "Epoch 1386, Loss: 0.5946270332\n",
      "Epoch 1387, Loss: 0.5843212851\n",
      "Epoch 1388, Loss: 0.5370995771\n",
      "Epoch 1389, Loss: 0.4966642659\n",
      "Epoch 1390, Loss: 0.5034782519\n",
      "Epoch 1391, Loss: 0.5695903443\n",
      "Epoch 1392, Loss: 0.7158783863\n",
      "Epoch 1393, Loss: 0.7484284681\n",
      "Epoch 1394, Loss: 0.6811412091\n",
      "Epoch 1395, Loss: 0.6414424691\n",
      "Epoch 1396, Loss: 0.6144460016\n",
      "Epoch 1397, Loss: 0.6521891402\n",
      "Epoch 1398, Loss: 0.7004126866\n",
      "Epoch 1399, Loss: 0.9467569403\n",
      "Epoch 1400, Loss: 1.1897155853\n",
      "Epoch 1401, Loss: 1.2509415039\n",
      "Epoch 1402, Loss: 0.5771379308\n",
      "Epoch 1403, Loss: 0.6056134662\n",
      "Epoch 1404, Loss: 0.6669058556\n",
      "Epoch 1405, Loss: 0.5797973455\n",
      "Epoch 1406, Loss: 0.5521286193\n",
      "Epoch 1407, Loss: 0.5561580081\n",
      "Epoch 1408, Loss: 0.5230298712\n",
      "Epoch 1409, Loss: 0.5736846253\n",
      "Epoch 1410, Loss: 0.6663648677\n",
      "Epoch 1411, Loss: 0.5899193964\n",
      "Epoch 1412, Loss: 0.5117911291\n",
      "Epoch 1413, Loss: 0.6271914246\n",
      "Epoch 1414, Loss: 0.5909937375\n",
      "Epoch 1415, Loss: 0.5692126528\n",
      "Epoch 1416, Loss: 0.5770423178\n",
      "Epoch 1417, Loss: 0.8195428934\n",
      "Epoch 1418, Loss: 0.6531758827\n",
      "Epoch 1419, Loss: 0.6539191989\n",
      "Epoch 1420, Loss: 0.5882604337\n",
      "Epoch 1421, Loss: 0.7281659913\n",
      "Epoch 1422, Loss: 0.6025547522\n",
      "Epoch 1423, Loss: 0.5891772626\n",
      "Epoch 1424, Loss: 0.6964564627\n",
      "Epoch 1425, Loss: 0.5866821447\n",
      "Epoch 1426, Loss: 0.6457698495\n",
      "Epoch 1427, Loss: 0.5695869480\n",
      "Epoch 1428, Loss: 0.5889206621\n",
      "Epoch 1429, Loss: 0.6951774799\n",
      "Epoch 1430, Loss: 0.5681353398\n",
      "Epoch 1431, Loss: 0.5750184967\n",
      "Epoch 1432, Loss: 0.5131075504\n",
      "Epoch 1433, Loss: 0.5527599355\n",
      "Epoch 1434, Loss: 0.5654793802\n",
      "Epoch 1435, Loss: 0.6488616764\n",
      "Epoch 1436, Loss: 0.7176408535\n",
      "Epoch 1437, Loss: 0.6174832768\n",
      "Epoch 1438, Loss: 0.5253280878\n",
      "Epoch 1439, Loss: 0.5087855935\n",
      "Epoch 1440, Loss: 0.5571862240\n",
      "Epoch 1441, Loss: 0.6377374560\n",
      "Epoch 1442, Loss: 0.5240516530\n",
      "Epoch 1443, Loss: 0.6492972327\n",
      "Epoch 1444, Loss: 0.6756733804\n",
      "Epoch 1445, Loss: 0.5766885058\n",
      "Epoch 1446, Loss: 0.6561128183\n",
      "Epoch 1447, Loss: 0.6941968299\n",
      "Epoch 1448, Loss: 0.5351172463\n",
      "Epoch 1449, Loss: 0.4809666188\n",
      "Epoch 1450, Loss: 0.5485898623\n",
      "Epoch 1451, Loss: 0.6333048363\n",
      "Epoch 1452, Loss: 0.6452556494\n",
      "Epoch 1453, Loss: 0.5298345814\n",
      "Epoch 1454, Loss: 0.5003340836\n",
      "Epoch 1455, Loss: 0.5238264299\n",
      "Epoch 1456, Loss: 0.5832631320\n",
      "Epoch 1457, Loss: 0.7128013412\n",
      "Epoch 1458, Loss: 0.5823792258\n",
      "Epoch 1459, Loss: 0.5027554449\n",
      "Epoch 1460, Loss: 0.5308379585\n",
      "Epoch 1461, Loss: 0.4787043837\n",
      "Epoch 1462, Loss: 0.5052050133\n",
      "Epoch 1463, Loss: 0.5680708238\n",
      "Epoch 1464, Loss: 0.5615053065\n",
      "Epoch 1465, Loss: 0.6892384326\n",
      "Epoch 1466, Loss: 0.5781277101\n",
      "Epoch 1467, Loss: 0.5449622857\n",
      "Epoch 1468, Loss: 0.4977549060\n",
      "Epoch 1469, Loss: 0.4778019269\n",
      "Epoch 1470, Loss: 0.5253578447\n",
      "Epoch 1471, Loss: 0.5650760132\n",
      "Epoch 1472, Loss: 0.6623226020\n",
      "Epoch 1473, Loss: 0.6668414592\n",
      "Epoch 1474, Loss: 0.4701408657\n",
      "Epoch 1475, Loss: 0.4698987385\n",
      "Epoch 1476, Loss: 0.4747863594\n",
      "Epoch 1477, Loss: 0.4697460016\n",
      "Epoch 1478, Loss: 0.4439369533\n",
      "Epoch 1479, Loss: 0.4433069404\n",
      "Epoch 1480, Loss: 0.4579851748\n",
      "Epoch 1481, Loss: 0.5071334049\n",
      "Epoch 1482, Loss: 0.5745537489\n",
      "Epoch 1483, Loss: 0.5915896706\n",
      "Epoch 1484, Loss: 0.6193603216\n",
      "Epoch 1485, Loss: 0.5575250805\n",
      "Epoch 1486, Loss: 0.4686423714\n",
      "Epoch 1487, Loss: 0.4856690095\n",
      "Epoch 1488, Loss: 0.4968460045\n",
      "Epoch 1489, Loss: 0.5234145081\n",
      "Epoch 1490, Loss: 0.6271876269\n",
      "Epoch 1491, Loss: 0.5604488956\n",
      "Epoch 1492, Loss: 0.4813205744\n",
      "Epoch 1493, Loss: 0.4854934165\n",
      "Epoch 1494, Loss: 0.5048352015\n",
      "Epoch 1495, Loss: 0.5205741426\n",
      "Epoch 1496, Loss: 0.5479897537\n",
      "Epoch 1497, Loss: 0.5677911341\n",
      "Epoch 1498, Loss: 0.5168047569\n",
      "Epoch 1499, Loss: 0.5018135313\n",
      "Epoch 1500, Loss: 0.4724172368\n",
      "Epoch 1501, Loss: 0.4466149846\n",
      "Epoch 1502, Loss: 0.4754271610\n",
      "Epoch 1503, Loss: 0.5616642048\n",
      "Epoch 1504, Loss: 0.6662086505\n",
      "Epoch 1505, Loss: 0.6044177085\n",
      "Epoch 1506, Loss: 0.5418567791\n",
      "Epoch 1507, Loss: 0.4702713463\n",
      "Epoch 1508, Loss: 0.4553585720\n",
      "Epoch 1509, Loss: 0.4702386588\n",
      "Epoch 1510, Loss: 0.4871946472\n",
      "Epoch 1511, Loss: 0.4857774806\n",
      "Epoch 1512, Loss: 0.4817078697\n",
      "Epoch 1513, Loss: 0.4773418753\n",
      "Epoch 1514, Loss: 0.4594727158\n",
      "Epoch 1515, Loss: 0.4242475570\n",
      "Epoch 1516, Loss: 0.4136703360\n",
      "Epoch 1517, Loss: 0.4100584512\n",
      "Epoch 1518, Loss: 0.5151964337\n",
      "Epoch 1519, Loss: 0.4937898988\n",
      "Epoch 1520, Loss: 0.5303256412\n",
      "Epoch 1521, Loss: 0.4719205432\n",
      "Epoch 1522, Loss: 0.3967996131\n",
      "Epoch 1523, Loss: 0.4120416959\n",
      "Epoch 1524, Loss: 0.4210567828\n",
      "Epoch 1525, Loss: 0.4500331099\n",
      "Epoch 1526, Loss: 0.5769406525\n",
      "Epoch 1527, Loss: 0.5756539014\n",
      "Epoch 1528, Loss: 0.4737857449\n",
      "Epoch 1529, Loss: 0.5487006331\n",
      "Epoch 1530, Loss: 0.4763472933\n",
      "Epoch 1531, Loss: 0.5620436403\n",
      "Epoch 1532, Loss: 0.5805631395\n",
      "Epoch 1533, Loss: 0.5654208137\n",
      "Epoch 1534, Loss: 0.6757132022\n",
      "Epoch 1535, Loss: 0.7163716185\n",
      "Epoch 1536, Loss: 0.5291191204\n",
      "Epoch 1537, Loss: 0.4714638116\n",
      "Epoch 1538, Loss: 0.5266449473\n",
      "Epoch 1539, Loss: 0.6182632685\n",
      "Epoch 1540, Loss: 0.5075609942\n",
      "Epoch 1541, Loss: 0.5304082829\n",
      "Epoch 1542, Loss: 0.5497290249\n",
      "Epoch 1543, Loss: 0.5638697084\n",
      "Epoch 1544, Loss: 0.5550255029\n",
      "Epoch 1545, Loss: 0.4960381351\n",
      "Epoch 1546, Loss: 0.5656186941\n",
      "Epoch 1547, Loss: 0.5098181115\n",
      "Epoch 1548, Loss: 0.4624520237\n",
      "Epoch 1549, Loss: 0.5097492089\n",
      "Epoch 1550, Loss: 0.5950196563\n",
      "Epoch 1551, Loss: 0.4668541571\n",
      "Epoch 1552, Loss: 0.4457977653\n",
      "Epoch 1553, Loss: 0.4332675283\n",
      "Epoch 1554, Loss: 0.6475555153\n",
      "Epoch 1555, Loss: 0.5026114794\n",
      "Epoch 1556, Loss: 0.4592161787\n",
      "Epoch 1557, Loss: 0.4427809996\n",
      "Epoch 1558, Loss: 0.4748950442\n",
      "Epoch 1559, Loss: 0.5317041077\n",
      "Epoch 1560, Loss: 0.5855613699\n",
      "Epoch 1561, Loss: 0.6711768513\n",
      "Epoch 1562, Loss: 0.7292270351\n",
      "Epoch 1563, Loss: 0.5818973516\n",
      "Epoch 1564, Loss: 0.4439424634\n",
      "Epoch 1565, Loss: 0.4866986716\n",
      "Epoch 1566, Loss: 0.6227607853\n",
      "Epoch 1567, Loss: 0.6454243515\n",
      "Epoch 1568, Loss: 0.5627135538\n",
      "Epoch 1569, Loss: 0.4569149577\n",
      "Epoch 1570, Loss: 0.4979946544\n",
      "Epoch 1571, Loss: 0.5170033228\n",
      "Epoch 1572, Loss: 0.5473679815\n",
      "Epoch 1573, Loss: 0.5963997021\n",
      "Epoch 1574, Loss: 0.4348108689\n",
      "Epoch 1575, Loss: 0.4107715960\n",
      "Epoch 1576, Loss: 0.4229154405\n",
      "Epoch 1577, Loss: 0.4922720847\n",
      "Epoch 1578, Loss: 0.6176119848\n",
      "Epoch 1579, Loss: 0.5369839554\n",
      "Epoch 1580, Loss: 0.5799247524\n",
      "Epoch 1581, Loss: 0.4480722843\n",
      "Epoch 1582, Loss: 0.4537866405\n",
      "Epoch 1583, Loss: 0.4814530992\n",
      "Epoch 1584, Loss: 0.5029947134\n",
      "Epoch 1585, Loss: 0.5701813677\n",
      "Epoch 1586, Loss: 0.6178337657\n",
      "Epoch 1587, Loss: 0.4609702699\n",
      "Epoch 1588, Loss: 0.4522606420\n",
      "Epoch 1589, Loss: 0.4788647448\n",
      "Epoch 1590, Loss: 0.4416013474\n",
      "Epoch 1591, Loss: 0.4299115501\n",
      "Epoch 1592, Loss: 0.4469661953\n",
      "Epoch 1593, Loss: 0.4670051942\n",
      "Epoch 1594, Loss: 0.5072557697\n",
      "Epoch 1595, Loss: 0.5164165484\n",
      "Epoch 1596, Loss: 0.4659137150\n",
      "Epoch 1597, Loss: 0.3880524027\n",
      "Epoch 1598, Loss: 0.3723431018\n",
      "Epoch 1599, Loss: 0.4061100269\n",
      "Epoch 1600, Loss: 0.4112099716\n",
      "Epoch 1601, Loss: 0.4070803837\n",
      "Epoch 1602, Loss: 0.4734599162\n",
      "Epoch 1603, Loss: 0.5791302560\n",
      "Epoch 1604, Loss: 0.4953326185\n",
      "Epoch 1605, Loss: 0.5775141838\n",
      "Epoch 1606, Loss: 0.4551529480\n",
      "Epoch 1607, Loss: 0.4850020137\n",
      "Epoch 1608, Loss: 0.4902103547\n",
      "Epoch 1609, Loss: 0.5665514513\n",
      "Epoch 1610, Loss: 0.6247191332\n",
      "Epoch 1611, Loss: 0.5022016692\n",
      "Epoch 1612, Loss: 0.4070675957\n",
      "Epoch 1613, Loss: 0.3724356348\n",
      "Epoch 1614, Loss: 0.3937908048\n",
      "Epoch 1615, Loss: 0.4306889760\n",
      "Epoch 1616, Loss: 0.4352011531\n",
      "Epoch 1617, Loss: 0.4401545136\n",
      "Epoch 1618, Loss: 0.4637636971\n",
      "Epoch 1619, Loss: 0.4944737891\n",
      "Epoch 1620, Loss: 0.5270710599\n",
      "Epoch 1621, Loss: 0.5730191424\n",
      "Epoch 1622, Loss: 0.6185740757\n",
      "Epoch 1623, Loss: 0.5199154924\n",
      "Epoch 1624, Loss: 0.4129336648\n",
      "Epoch 1625, Loss: 0.3932878855\n",
      "Epoch 1626, Loss: 0.4242396262\n",
      "Epoch 1627, Loss: 0.4841664853\n",
      "Epoch 1628, Loss: 0.5088683722\n",
      "Epoch 1629, Loss: 0.4627891492\n",
      "Epoch 1630, Loss: 0.4274799746\n",
      "Epoch 1631, Loss: 0.4868649711\n",
      "Epoch 1632, Loss: 0.3967397757\n",
      "Epoch 1633, Loss: 0.3957852583\n",
      "Epoch 1634, Loss: 0.4049580085\n",
      "Epoch 1635, Loss: 0.4035016704\n",
      "Epoch 1636, Loss: 0.4125404931\n",
      "Epoch 1637, Loss: 0.4373949817\n",
      "Epoch 1638, Loss: 0.4492607576\n",
      "Epoch 1639, Loss: 0.4291710500\n",
      "Epoch 1640, Loss: 0.4626312505\n",
      "Epoch 1641, Loss: 0.4656268809\n",
      "Epoch 1642, Loss: 0.4248717510\n",
      "Epoch 1643, Loss: 0.4071110771\n",
      "Epoch 1644, Loss: 0.4116924100\n",
      "Epoch 1645, Loss: 0.4639649782\n",
      "Epoch 1646, Loss: 0.4909570259\n",
      "Epoch 1647, Loss: 0.4192947712\n",
      "Epoch 1648, Loss: 0.3498826183\n",
      "Epoch 1649, Loss: 0.3364923030\n",
      "Epoch 1650, Loss: 0.3429758334\n",
      "Epoch 1651, Loss: 0.3427881531\n",
      "Epoch 1652, Loss: 0.3504317497\n",
      "Epoch 1653, Loss: 0.3713468455\n",
      "Epoch 1654, Loss: 0.3713039999\n",
      "Epoch 1655, Loss: 0.3474195539\n",
      "Epoch 1656, Loss: 0.4105357827\n",
      "Epoch 1657, Loss: 0.4042905455\n",
      "Epoch 1658, Loss: 0.3311570440\n",
      "Epoch 1659, Loss: 0.3400911605\n",
      "Epoch 1660, Loss: 0.3323738805\n",
      "Epoch 1661, Loss: 0.3512461751\n",
      "Epoch 1662, Loss: 0.3729306503\n",
      "Epoch 1663, Loss: 0.3850805146\n",
      "Epoch 1664, Loss: 0.3514520301\n",
      "Epoch 1665, Loss: 0.3527285171\n",
      "Epoch 1666, Loss: 0.3742305508\n",
      "Epoch 1667, Loss: 0.4033379578\n",
      "Epoch 1668, Loss: 0.4421119706\n",
      "Epoch 1669, Loss: 0.3866089171\n",
      "Epoch 1670, Loss: 0.4493357769\n",
      "Epoch 1671, Loss: 0.4753460686\n",
      "Epoch 1672, Loss: 0.5288834361\n",
      "Epoch 1673, Loss: 0.4987553408\n",
      "Epoch 1674, Loss: 0.5629499346\n",
      "Epoch 1675, Loss: 0.6359488658\n",
      "Epoch 1676, Loss: 0.5576907502\n",
      "Epoch 1677, Loss: 0.5775623415\n",
      "Epoch 1678, Loss: 0.7053999960\n",
      "Epoch 1679, Loss: 0.6693290031\n",
      "Epoch 1680, Loss: 0.5438365382\n",
      "Epoch 1681, Loss: 0.4421745552\n",
      "Epoch 1682, Loss: 0.3622246052\n",
      "Epoch 1683, Loss: 0.3559985846\n",
      "Epoch 1684, Loss: 0.3783858159\n",
      "Epoch 1685, Loss: 0.3719383230\n",
      "Epoch 1686, Loss: 0.4133156953\n",
      "Epoch 1687, Loss: 0.4396546090\n",
      "Epoch 1688, Loss: 0.4712615085\n",
      "Epoch 1689, Loss: 0.5068742502\n",
      "Epoch 1690, Loss: 0.5488764427\n",
      "Epoch 1691, Loss: 0.5021576537\n",
      "Epoch 1692, Loss: 0.5045764640\n",
      "Epoch 1693, Loss: 0.4977234488\n",
      "Epoch 1694, Loss: 0.3717846507\n",
      "Epoch 1695, Loss: 0.4424257582\n",
      "Epoch 1696, Loss: 0.3674624468\n",
      "Epoch 1697, Loss: 0.3671870419\n",
      "Epoch 1698, Loss: 0.3727547288\n",
      "Epoch 1699, Loss: 0.3711105836\n",
      "Epoch 1700, Loss: 0.3646993433\n",
      "Epoch 1701, Loss: 0.3737266536\n",
      "Epoch 1702, Loss: 0.3776209296\n",
      "Epoch 1703, Loss: 0.3643755553\n",
      "Epoch 1704, Loss: 0.3651494343\n",
      "Epoch 1705, Loss: 0.3777843038\n",
      "Epoch 1706, Loss: 0.3581458486\n",
      "Epoch 1707, Loss: 0.3465836072\n",
      "Epoch 1708, Loss: 0.3489342900\n",
      "Epoch 1709, Loss: 0.3376564485\n",
      "Epoch 1710, Loss: 0.3380213437\n",
      "Epoch 1711, Loss: 0.3496110092\n",
      "Epoch 1712, Loss: 0.3682748795\n",
      "Epoch 1713, Loss: 0.3786648064\n",
      "Epoch 1714, Loss: 0.3924166242\n",
      "Epoch 1715, Loss: 0.4002868577\n",
      "Epoch 1716, Loss: 0.4128137268\n",
      "Epoch 1717, Loss: 0.3767396474\n",
      "Epoch 1718, Loss: 0.3270099394\n",
      "Epoch 1719, Loss: 0.3320942222\n",
      "Epoch 1720, Loss: 0.3633494132\n",
      "Epoch 1721, Loss: 0.3514395172\n",
      "Epoch 1722, Loss: 0.3588237875\n",
      "Epoch 1723, Loss: 0.3442809543\n",
      "Epoch 1724, Loss: 0.3474547774\n",
      "Epoch 1725, Loss: 0.3588518826\n",
      "Epoch 1726, Loss: 0.3779621288\n",
      "Epoch 1727, Loss: 0.3847390790\n",
      "Epoch 1728, Loss: 0.4056579648\n",
      "Epoch 1729, Loss: 0.4204447329\n",
      "Epoch 1730, Loss: 0.3719969722\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_weights_init = []\n",
    "best_biases_init = []\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "best_seed = 0\n",
    "\n",
    "for _ in range (1):\n",
    "    nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8)\n",
    "    nn.train(batches, 13, 0.001)\n",
    "    if nn.get_best_loss() < best_loss:\n",
    "        best_loss = nn.get_best_loss()\n",
    "        best_weights = nn.get_best_weights()\n",
    "        best_biases = nn.get_best_biases()\n",
    "        best_seed = nn.get_best_seed()"
=======
    "\n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_biases(self):\n",
    "        return self.biases\n",
    "\n",
    "# Example usage:\n",
    "nn = NeuralNetwork(625, [512, 256, 128, 32], 8)\n",
    "nn.train(batches, 100, 0.001)\n"
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32700993939589185\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "# print(best_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.4185276484\n",
      "Epoch 2, Loss: 1.1282646851\n",
      "Epoch 3, Loss: 0.7309074188\n",
      "Epoch 4, Loss: 0.6247790516\n",
      "Epoch 5, Loss: 0.4332603115\n",
      "Epoch 6, Loss: 0.4081079146\n",
      "Epoch 7, Loss: 0.3888599424\n",
      "Epoch 8, Loss: 0.3779536149\n",
      "Epoch 9, Loss: 0.3808083469\n",
      "Epoch 10, Loss: 0.3780970939\n",
      "Epoch 11, Loss: 0.4397415875\n",
      "Epoch 12, Loss: 0.4661906919\n",
      "Epoch 13, Loss: 0.4056532174\n",
      "Epoch 14, Loss: 0.4509562482\n",
      "Epoch 15, Loss: 0.4449019688\n",
      "Epoch 16, Loss: 0.4613219688\n",
      "Epoch 17, Loss: 0.4654153454\n",
      "Epoch 18, Loss: 0.4455619615\n",
      "Epoch 19, Loss: 0.4571067082\n",
      "Epoch 20, Loss: 0.4523868662\n",
      "Epoch 21, Loss: 0.4458695895\n",
      "Epoch 22, Loss: 0.4393289771\n",
      "Epoch 23, Loss: 0.4481930830\n",
      "Epoch 24, Loss: 0.4917528787\n",
      "Epoch 25, Loss: 0.4904827158\n",
      "Epoch 26, Loss: 0.4175613221\n",
      "Epoch 27, Loss: 0.4147259132\n",
      "Epoch 28, Loss: 0.4634246556\n",
      "Epoch 29, Loss: 0.4842470942\n",
      "Epoch 30, Loss: 0.4012346787\n",
      "Epoch 31, Loss: 0.3568365409\n",
      "Epoch 32, Loss: 0.4522432975\n",
      "Epoch 33, Loss: 0.4004028806\n",
      "Epoch 34, Loss: 0.4118879041\n",
      "Epoch 35, Loss: 0.3422370264\n",
      "Epoch 36, Loss: 0.4180495403\n",
      "Epoch 37, Loss: 0.3831858135\n",
      "Epoch 38, Loss: 0.4238288387\n",
      "Epoch 39, Loss: 0.4862894090\n",
      "Epoch 40, Loss: 0.4521047211\n",
      "Epoch 41, Loss: 0.4424714345\n",
      "Epoch 42, Loss: 0.4362433538\n",
      "Epoch 43, Loss: 0.4866150890\n",
      "Epoch 44, Loss: 0.5080272212\n",
      "Epoch 45, Loss: 0.4127301449\n",
      "Epoch 46, Loss: 0.4251502036\n",
      "Epoch 47, Loss: 0.4999993565\n",
      "Epoch 48, Loss: 0.6014983345\n",
      "Epoch 49, Loss: 0.4180800622\n",
      "Epoch 50, Loss: 0.4293531838\n",
      "Epoch 51, Loss: 0.4098463399\n",
      "Epoch 52, Loss: 0.4024289278\n",
      "Epoch 53, Loss: 0.4949035074\n",
      "Epoch 54, Loss: 0.5296370989\n",
      "Epoch 55, Loss: 0.4800948611\n",
      "Epoch 56, Loss: 0.5319467492\n",
      "Epoch 57, Loss: 0.4440213984\n",
      "Epoch 58, Loss: 0.7254089500\n",
      "Epoch 59, Loss: 0.5103300179\n",
      "Epoch 60, Loss: 0.4898393608\n",
      "Epoch 61, Loss: 0.4122522259\n",
      "Epoch 62, Loss: 0.5268121553\n",
      "Epoch 63, Loss: 0.4173841622\n",
      "Epoch 64, Loss: 0.6102395396\n",
      "Epoch 65, Loss: 0.4255820725\n",
      "Epoch 66, Loss: 0.5963727661\n",
      "Epoch 67, Loss: 0.4577491444\n",
      "Epoch 68, Loss: 0.5401762852\n",
      "Epoch 69, Loss: 0.5192068913\n",
      "Epoch 70, Loss: 0.5068531923\n",
      "Epoch 71, Loss: 0.3915785226\n",
      "Epoch 72, Loss: 0.6200352445\n",
      "Epoch 73, Loss: 0.4940070499\n",
      "Epoch 74, Loss: 0.4745535756\n",
      "Epoch 75, Loss: 0.4497579159\n",
      "Epoch 76, Loss: 0.4859896578\n",
      "Epoch 77, Loss: 0.4205965762\n",
      "Epoch 78, Loss: 0.7401565489\n",
      "Epoch 79, Loss: 0.4940382305\n",
      "Epoch 80, Loss: 0.4050385700\n",
      "Epoch 81, Loss: 0.4005797838\n",
      "Epoch 82, Loss: 0.6035218406\n",
      "Epoch 83, Loss: 0.5255294454\n",
      "Epoch 84, Loss: 0.4327396699\n",
      "Epoch 85, Loss: 0.4612157697\n",
      "Epoch 86, Loss: 0.4242665175\n",
      "Epoch 87, Loss: 0.4770947544\n",
      "Epoch 88, Loss: 0.4550676636\n",
      "Epoch 89, Loss: 0.4661861294\n",
      "Epoch 90, Loss: 0.4488464202\n",
      "Epoch 91, Loss: 0.4191746877\n",
      "Epoch 92, Loss: 0.4118901952\n",
      "Epoch 93, Loss: 0.4317930743\n",
      "Epoch 94, Loss: 0.4794946206\n",
      "Epoch 95, Loss: 0.5266176502\n",
      "Epoch 96, Loss: 0.4738710535\n",
      "Epoch 97, Loss: 0.4285478706\n",
      "Epoch 98, Loss: 0.4407212991\n",
      "Epoch 99, Loss: 0.4741259522\n",
      "Epoch 100, Loss: 0.5037728874\n",
      "Epoch 101, Loss: 0.4557394612\n",
      "Epoch 102, Loss: 0.4494932359\n",
      "Epoch 103, Loss: 0.4103248139\n",
      "Epoch 104, Loss: 0.4553231393\n",
      "Epoch 105, Loss: 0.4512122228\n",
      "Epoch 106, Loss: 0.4099006224\n",
      "Epoch 107, Loss: 0.4213580047\n",
      "Epoch 108, Loss: 0.4544221743\n",
      "Epoch 109, Loss: 0.4209165906\n",
      "Epoch 110, Loss: 0.4359786723\n",
      "Epoch 111, Loss: 0.4633853300\n",
      "Epoch 112, Loss: 0.5362433370\n",
      "Epoch 113, Loss: 0.5340799228\n",
      "Epoch 114, Loss: 0.4603079526\n",
      "Epoch 115, Loss: 0.4243903924\n",
      "Epoch 116, Loss: 0.3761595917\n",
      "Epoch 117, Loss: 0.3916245878\n",
      "Epoch 118, Loss: 0.4328256976\n",
      "Epoch 119, Loss: 0.3399752259\n",
      "Epoch 120, Loss: 0.3728368494\n",
      "Epoch 121, Loss: 0.3368397529\n",
      "Epoch 122, Loss: 0.3600966028\n",
      "Epoch 123, Loss: 0.3764310814\n",
      "Epoch 124, Loss: 0.4088421207\n",
      "Epoch 125, Loss: 0.4430574044\n",
      "Epoch 126, Loss: 0.5257351680\n",
      "Epoch 127, Loss: 0.4119722784\n",
      "Epoch 128, Loss: 0.3695005291\n",
      "Epoch 129, Loss: 0.3827115055\n",
      "Epoch 130, Loss: 0.4450320446\n",
      "Epoch 131, Loss: 0.4521132684\n",
      "Epoch 132, Loss: 0.5270055327\n",
      "Epoch 133, Loss: 0.5193046961\n",
      "Epoch 134, Loss: 0.4453827430\n",
      "Epoch 135, Loss: 0.5052858835\n",
      "Epoch 136, Loss: 0.4296367872\n",
      "Epoch 137, Loss: 0.5123164419\n",
      "Epoch 138, Loss: 0.4839870491\n",
      "Epoch 139, Loss: 0.6273506970\n",
      "Epoch 140, Loss: 0.4144554126\n",
      "Epoch 141, Loss: 0.4677372823\n",
      "Epoch 142, Loss: 0.4969792870\n",
      "Epoch 143, Loss: 0.5212160288\n",
      "Epoch 144, Loss: 0.5594935513\n",
      "Epoch 145, Loss: 0.5761790825\n",
      "Epoch 146, Loss: 0.5582765768\n",
      "Epoch 147, Loss: 0.5929891204\n",
      "Epoch 148, Loss: 0.5202958983\n",
      "Epoch 149, Loss: 0.4641572397\n",
      "Epoch 150, Loss: 0.6523694716\n",
      "Epoch 151, Loss: 0.4831506543\n",
      "Epoch 152, Loss: 0.6339156628\n",
      "Epoch 153, Loss: 0.6844424839\n",
      "Epoch 154, Loss: 0.6054929746\n",
      "Epoch 155, Loss: 0.4995012353\n",
      "Epoch 156, Loss: 0.5257070012\n",
      "Epoch 157, Loss: 0.3326314261\n",
      "Epoch 158, Loss: 0.3838968352\n",
      "Epoch 159, Loss: 0.3991443005\n",
      "Epoch 160, Loss: 0.3833434805\n",
      "Epoch 161, Loss: 0.4733293317\n",
      "Epoch 162, Loss: 0.4984042669\n",
      "Epoch 163, Loss: 0.5058634280\n",
      "Epoch 164, Loss: 0.5602938612\n",
      "Epoch 165, Loss: 0.4835770442\n",
      "Epoch 166, Loss: 0.6257472005\n",
      "Epoch 167, Loss: 0.5194254056\n",
      "Epoch 168, Loss: 0.4881528610\n",
      "Epoch 169, Loss: 0.5654371741\n",
      "Epoch 170, Loss: 0.5531027217\n",
      "Epoch 171, Loss: 0.5050280871\n",
      "Epoch 172, Loss: 0.5372393385\n",
      "Epoch 173, Loss: 0.5961034572\n",
      "Epoch 174, Loss: 0.6334594322\n",
      "Epoch 175, Loss: 0.6828122749\n",
      "Epoch 176, Loss: 0.7451671614\n",
      "Epoch 177, Loss: 0.6547584674\n",
      "Epoch 178, Loss: 0.3959255167\n",
      "Epoch 179, Loss: 0.3729916968\n",
      "Epoch 180, Loss: 0.4963921837\n",
      "Epoch 181, Loss: 0.3996891095\n",
      "Epoch 182, Loss: 0.3978724264\n",
      "Epoch 183, Loss: 0.3872264604\n",
      "Epoch 184, Loss: 0.3698016046\n",
      "Epoch 185, Loss: 0.4118213526\n",
      "Epoch 186, Loss: 0.4574098197\n",
      "Epoch 187, Loss: 0.5096467057\n",
      "Epoch 188, Loss: 0.5000600125\n",
      "Epoch 189, Loss: 0.4200053840\n",
      "Epoch 190, Loss: 0.3764220194\n",
      "Epoch 191, Loss: 0.4074707621\n",
      "Epoch 192, Loss: 0.4238662989\n",
      "Epoch 193, Loss: 0.4237722606\n",
      "Epoch 194, Loss: 0.4126934875\n",
      "Epoch 195, Loss: 0.3876703357\n",
      "Epoch 196, Loss: 0.3511502299\n",
      "Epoch 197, Loss: 0.3430507213\n",
      "Epoch 198, Loss: 0.4443177897\n",
      "Epoch 199, Loss: 0.4327940932\n",
      "Epoch 200, Loss: 0.4186626854\n",
      "Epoch 201, Loss: 0.4045912113\n",
      "Epoch 202, Loss: 0.3706461825\n",
      "Epoch 203, Loss: 0.3689288557\n",
      "Epoch 204, Loss: 0.3776515696\n",
      "Epoch 205, Loss: 0.4007468353\n",
      "Epoch 206, Loss: 0.4288853493\n",
      "Epoch 207, Loss: 0.5036556916\n",
      "Epoch 208, Loss: 0.5390972040\n",
      "Epoch 209, Loss: 0.3957142091\n",
      "Epoch 210, Loss: 0.3564066329\n",
      "Epoch 211, Loss: 0.3594151625\n",
      "Epoch 212, Loss: 0.4182433707\n",
      "Epoch 213, Loss: 0.4233323775\n",
      "Epoch 214, Loss: 0.4193422668\n",
      "Epoch 215, Loss: 0.6091982035\n",
      "Epoch 216, Loss: 0.5158022569\n",
      "Epoch 217, Loss: 0.5435261687\n",
      "Epoch 218, Loss: 0.3849326295\n",
      "Epoch 219, Loss: 0.5267794853\n",
      "Epoch 220, Loss: 0.3687439597\n",
      "Epoch 221, Loss: 0.3732817289\n",
      "Epoch 222, Loss: 0.4919718698\n",
      "Epoch 223, Loss: 0.5575036461\n",
      "Epoch 224, Loss: 0.7195476877\n",
      "Epoch 225, Loss: 0.4218740283\n",
      "Epoch 226, Loss: 0.4058808226\n",
      "Epoch 227, Loss: 0.3634848652\n",
      "Epoch 228, Loss: 0.3855537523\n",
      "Epoch 229, Loss: 0.4690884319\n",
      "Epoch 230, Loss: 0.4894622312\n",
      "Epoch 231, Loss: 0.4880149895\n",
      "Epoch 232, Loss: 0.4898704416\n",
      "Epoch 233, Loss: 0.5001593661\n",
      "Epoch 234, Loss: 0.4263241775\n",
      "Epoch 235, Loss: 0.4631813436\n",
      "Epoch 236, Loss: 0.4501242174\n",
      "Epoch 237, Loss: 0.4414554287\n",
      "Epoch 238, Loss: 0.4410091340\n",
      "Epoch 239, Loss: 0.4360152918\n",
      "Epoch 240, Loss: 0.4472278019\n",
      "Epoch 241, Loss: 0.4738950918\n",
      "Epoch 242, Loss: 0.4959018634\n",
      "Epoch 243, Loss: 0.4601308256\n",
      "Epoch 244, Loss: 0.4138126439\n",
      "Epoch 245, Loss: 0.4932999101\n",
      "Epoch 246, Loss: 0.7106992612\n",
      "Epoch 247, Loss: 0.4222784613\n",
      "Epoch 248, Loss: 0.3943565071\n",
      "Epoch 249, Loss: 0.5007457112\n",
      "Epoch 250, Loss: 0.4271512396\n",
      "Epoch 251, Loss: 0.3121890946\n",
      "Epoch 252, Loss: 0.3427115975\n",
      "Epoch 253, Loss: 0.4125393075\n",
      "Epoch 254, Loss: 0.4599813774\n",
      "Epoch 255, Loss: 0.4528790637\n",
      "Epoch 256, Loss: 0.4124230147\n",
      "Epoch 257, Loss: 0.3974298497\n",
      "Epoch 258, Loss: 0.3508822936\n",
      "Epoch 259, Loss: 0.3755228261\n",
      "Epoch 260, Loss: 0.3651311033\n",
      "Epoch 261, Loss: 0.4229069213\n",
      "Epoch 262, Loss: 0.3298186337\n",
      "Epoch 263, Loss: 0.3521832554\n",
      "Epoch 264, Loss: 0.4918741716\n",
      "Epoch 265, Loss: 0.3896725176\n",
      "Epoch 266, Loss: 0.3886615843\n",
      "Epoch 267, Loss: 0.3077372138\n",
      "Epoch 268, Loss: 0.3588645058\n",
      "Epoch 269, Loss: 0.3647405262\n",
      "Epoch 270, Loss: 0.4354082733\n",
      "Epoch 271, Loss: 0.4087249730\n",
      "Epoch 272, Loss: 0.3569114120\n",
      "Epoch 273, Loss: 0.5032458988\n",
      "Epoch 274, Loss: 0.7225822939\n",
      "Epoch 275, Loss: 0.9507486293\n",
      "Epoch 276, Loss: 0.4036660690\n",
      "Epoch 277, Loss: 0.3472539286\n",
      "Epoch 278, Loss: 0.4724612470\n",
      "Epoch 279, Loss: 0.3331879525\n",
      "Epoch 280, Loss: 0.3261357217\n",
      "Epoch 281, Loss: 0.4222973818\n",
      "Epoch 282, Loss: 0.3797098722\n",
      "Epoch 283, Loss: 0.4073702717\n",
      "Epoch 284, Loss: 0.4356475561\n",
      "Epoch 285, Loss: 0.3403252170\n",
      "Epoch 286, Loss: 0.3587729380\n",
      "Epoch 287, Loss: 0.4478979481\n",
      "Epoch 288, Loss: 0.4952428152\n",
      "Epoch 289, Loss: 0.3708251520\n",
      "Epoch 290, Loss: 0.3201606026\n",
      "Epoch 291, Loss: 0.3527485888\n",
      "Epoch 292, Loss: 0.4152373653\n",
      "Epoch 293, Loss: 0.4699431065\n",
      "Epoch 294, Loss: 0.4928075472\n",
      "Epoch 295, Loss: 0.4593896217\n",
      "Epoch 296, Loss: 0.4302290750\n",
      "Epoch 297, Loss: 0.4283856836\n",
      "Epoch 298, Loss: 0.4388490025\n",
      "Epoch 299, Loss: 0.5186753990\n",
      "Epoch 300, Loss: 0.6533148402\n",
      "Epoch 301, Loss: 0.5824108150\n",
      "Epoch 302, Loss: 0.4950025540\n",
      "Epoch 303, Loss: 0.4113003743\n",
      "Epoch 304, Loss: 0.4827124171\n",
      "Epoch 305, Loss: 0.5535272739\n",
      "Epoch 306, Loss: 0.5455671434\n",
      "Epoch 307, Loss: 0.4666329803\n",
      "Epoch 308, Loss: 0.4768105583\n",
      "Epoch 309, Loss: 0.5191037427\n",
      "Epoch 310, Loss: 0.5372505611\n",
      "Epoch 311, Loss: 0.4259921189\n",
      "Epoch 312, Loss: 0.4158459761\n",
      "Epoch 313, Loss: 0.3224718048\n",
      "Epoch 314, Loss: 0.3419843174\n",
      "Epoch 315, Loss: 0.3585858932\n",
      "Epoch 316, Loss: 0.4670153786\n",
      "Epoch 317, Loss: 0.5935455833\n",
      "Epoch 318, Loss: 0.4807184776\n",
      "Epoch 319, Loss: 0.3542153844\n",
      "Epoch 320, Loss: 0.5024156696\n",
      "Epoch 321, Loss: 0.4843208120\n",
      "Epoch 322, Loss: 0.4078407688\n",
      "Epoch 323, Loss: 0.3790194870\n",
      "Epoch 324, Loss: 0.4789085310\n",
      "Epoch 325, Loss: 0.4847611978\n",
      "Epoch 326, Loss: 0.3949173380\n",
      "Epoch 327, Loss: 0.4684413779\n",
      "Epoch 328, Loss: 0.3961316761\n",
      "Epoch 329, Loss: 0.6995350665\n",
      "Epoch 330, Loss: 0.7422148023\n",
      "Epoch 331, Loss: 0.3828134966\n",
      "Epoch 332, Loss: 0.5087706632\n",
      "Epoch 333, Loss: 0.6102372023\n",
      "Epoch 334, Loss: 0.6515855743\n",
      "Epoch 335, Loss: 0.7138198615\n",
      "Epoch 336, Loss: 0.5834966565\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8, init_weights=best_weights, init_biases=best_biases, init_seed=best_seed)\n",
    "nn.train(batches, 3, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6684433530744485\n"
     ]
    }
   ],
   "source": [
    "print(nn.get_best_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 75,
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Number of layers in the Neural Network\n",
<<<<<<< HEAD
    "N = 5  # Example value, replace with the actual number of layers\n",
=======
    "N = 4  # Example value, replace with the actual number of layers\n",
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
    "\n",
    "# Initialize the dictionary\n",
    "weights_dict = {\n",
    "    'weights': {},\n",
    "    'bias': {}\n",
    "}\n",
    "\n",
<<<<<<< HEAD
    "weights = nn.get_best_weights()\n",
    "biases = nn.get_best_biases()\n",
=======
    "weights = nn.get_weights()\n",
    "biases = nn.get_biases()\n",
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
    "\n",
    "# Populate the weights and bias dictionaries\n",
    "for i in range(N):\n",
    "    weights_dict['weights'][f'fc{i+1}'] = weights[i]\n",
    "    weights_dict['bias'][f'fc{i+1}'] = biases[i].flatten()\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = load_pickle(\"weights.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fc1', 'fc2', 'fc3', 'fc4'])\n"
     ]
    }
   ],
   "source": [
    "print(w['bias'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.12"
=======
   "version": "3.12.3"
>>>>>>> 3948090a285ac1b88e1d93155df559fe401c9504
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
