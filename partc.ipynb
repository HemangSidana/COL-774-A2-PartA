{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Remember to import \"numpy_transforms\" functions if you wish to import these two classes in a different script.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "        label = row[\"class\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image), label\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image, label = self.dataset[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "        batch_labels = np.array(labels)\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(128, 625)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    print(labels.shape)  # Should be [batch_size]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.2030001288\n",
      "Epoch 2, Loss: 2.1980960210\n",
      "Epoch 3, Loss: 2.1933980726\n",
      "Epoch 4, Loss: 2.1888969749\n",
      "Epoch 5, Loss: 2.1845838621\n",
      "Epoch 6, Loss: 2.1804502915\n",
      "Epoch 7, Loss: 2.1764882236\n",
      "Epoch 8, Loss: 2.1726900032\n",
      "Epoch 9, Loss: 2.1690483410\n",
      "Epoch 10, Loss: 2.1655562961\n",
      "Epoch 11, Loss: 2.1622072592\n",
      "Epoch 12, Loss: 2.1589949363\n",
      "Epoch 13, Loss: 2.1559133332\n",
      "Epoch 14, Loss: 2.1529567408\n",
      "Epoch 15, Loss: 2.1501197207\n",
      "Epoch 16, Loss: 2.1473970921\n",
      "Epoch 17, Loss: 2.1447839185\n",
      "Epoch 18, Loss: 2.1422754957\n",
      "Epoch 19, Loss: 2.1398673399\n",
      "Epoch 20, Loss: 2.1375551769\n",
      "Epoch 21, Loss: 2.1353349313\n",
      "Epoch 22, Loss: 2.1332027167\n",
      "Epoch 23, Loss: 2.1311548258\n",
      "Epoch 24, Loss: 2.1291877218\n",
      "Epoch 25, Loss: 2.1272980297\n",
      "Epoch 26, Loss: 2.1254825282\n",
      "Epoch 27, Loss: 2.1237381417\n",
      "Epoch 28, Loss: 2.1220619336\n",
      "Epoch 29, Loss: 2.1204510990\n",
      "Epoch 30, Loss: 2.1189029581\n",
      "Epoch 31, Loss: 2.1174149504\n",
      "Epoch 32, Loss: 2.1159846283\n",
      "Epoch 33, Loss: 2.1146096523\n",
      "Epoch 34, Loss: 2.1132877849\n",
      "Epoch 35, Loss: 2.1120168861\n",
      "Epoch 36, Loss: 2.1107949089\n",
      "Epoch 37, Loss: 2.1096198943\n",
      "Epoch 38, Loss: 2.1084899675\n",
      "Epoch 39, Loss: 2.1074033336\n",
      "Epoch 40, Loss: 2.1063582741\n",
      "Epoch 41, Loss: 2.1053531432\n",
      "Epoch 42, Loss: 2.1043863644\n",
      "Epoch 43, Loss: 2.1034564271\n",
      "Epoch 44, Loss: 2.1025618839\n",
      "Epoch 45, Loss: 2.1017013475\n",
      "Epoch 46, Loss: 2.1008734879\n",
      "Epoch 47, Loss: 2.1000770300\n",
      "Epoch 48, Loss: 2.0993107509\n",
      "Epoch 49, Loss: 2.0985734777\n",
      "Epoch 50, Loss: 2.0978640854\n",
      "Epoch 51, Loss: 2.0971814944\n",
      "Epoch 52, Loss: 2.0965246688\n",
      "Epoch 53, Loss: 2.0958926145\n",
      "Epoch 54, Loss: 2.0952843771\n",
      "Epoch 55, Loss: 2.0946990406\n",
      "Epoch 56, Loss: 2.0941357253\n",
      "Epoch 57, Loss: 2.0935935868\n",
      "Epoch 58, Loss: 2.0930718137\n",
      "Epoch 59, Loss: 2.0925696272\n",
      "Epoch 60, Loss: 2.0920862788\n",
      "Epoch 61, Loss: 2.0916210495\n",
      "Epoch 62, Loss: 2.0911732486\n",
      "Epoch 63, Loss: 2.0907422125\n",
      "Epoch 64, Loss: 2.0903273033\n",
      "Epoch 65, Loss: 2.0899279081\n",
      "Epoch 66, Loss: 2.0895434377\n",
      "Epoch 67, Loss: 2.0891733259\n",
      "Epoch 68, Loss: 2.0888170283\n",
      "Epoch 69, Loss: 2.0884740217\n",
      "Epoch 70, Loss: 2.0881438029\n",
      "Epoch 71, Loss: 2.0878258882\n",
      "Epoch 72, Loss: 2.0875198125\n",
      "Epoch 73, Loss: 2.0872251286\n",
      "Epoch 74, Loss: 2.0869414063\n",
      "Epoch 75, Loss: 2.0866682322\n",
      "Epoch 76, Loss: 2.0864052083\n",
      "Epoch 77, Loss: 2.0861519521\n",
      "Epoch 78, Loss: 2.0859080957\n",
      "Epoch 79, Loss: 2.0856732850\n",
      "Epoch 80, Loss: 2.0854471798\n",
      "Epoch 81, Loss: 2.0852294524\n",
      "Epoch 82, Loss: 2.0850197878\n",
      "Epoch 83, Loss: 2.0848178830\n",
      "Epoch 84, Loss: 2.0846234464\n",
      "Epoch 85, Loss: 2.0844361975\n",
      "Epoch 86, Loss: 2.0842558665\n",
      "Epoch 87, Loss: 2.0840821938\n",
      "Epoch 88, Loss: 2.0839149295\n",
      "Epoch 89, Loss: 2.0837538333\n",
      "Epoch 90, Loss: 2.0835986741\n",
      "Epoch 91, Loss: 2.0834492292\n",
      "Epoch 92, Loss: 2.0833052846\n",
      "Epoch 93, Loss: 2.0831666343\n",
      "Epoch 94, Loss: 2.0830330801\n",
      "Epoch 95, Loss: 2.0829044313\n",
      "Epoch 96, Loss: 2.0827805042\n",
      "Epoch 97, Loss: 2.0826611224\n",
      "Epoch 98, Loss: 2.0825461159\n",
      "Epoch 99, Loss: 2.0824353210\n",
      "Epoch 100, Loss: 2.0823285806\n",
      "Epoch 101, Loss: 2.0822257430\n",
      "Epoch 102, Loss: 2.0821266626\n",
      "Epoch 103, Loss: 2.0820311991\n",
      "Epoch 104, Loss: 2.0819392175\n",
      "Epoch 105, Loss: 2.0818505880\n",
      "Epoch 106, Loss: 2.0817651856\n",
      "Epoch 107, Loss: 2.0816828900\n",
      "Epoch 108, Loss: 2.0816035853\n",
      "Epoch 109, Loss: 2.0815271603\n",
      "Epoch 110, Loss: 2.0814535077\n",
      "Epoch 111, Loss: 2.0813825243\n",
      "Epoch 112, Loss: 2.0813141107\n",
      "Epoch 113, Loss: 2.0812481715\n",
      "Epoch 114, Loss: 2.0811846147\n",
      "Epoch 115, Loss: 2.0811233517\n",
      "Epoch 116, Loss: 2.0810642973\n",
      "Epoch 117, Loss: 2.0810073695\n",
      "Epoch 118, Loss: 2.0809524895\n",
      "Epoch 119, Loss: 2.0808995813\n",
      "Epoch 120, Loss: 2.0808485717\n",
      "Epoch 121, Loss: 2.0807993905\n",
      "Epoch 122, Loss: 2.0807519698\n",
      "Epoch 123, Loss: 2.0807062447\n",
      "Epoch 124, Loss: 2.0806621522\n",
      "Epoch 125, Loss: 2.0806196321\n",
      "Epoch 126, Loss: 2.0805786262\n",
      "Epoch 127, Loss: 2.0805390787\n",
      "Epoch 128, Loss: 2.0805009356\n",
      "Epoch 129, Loss: 2.0804641452\n",
      "Epoch 130, Loss: 2.0804286576\n",
      "Epoch 131, Loss: 2.0803944248\n",
      "Epoch 132, Loss: 2.0803614006\n",
      "Epoch 133, Loss: 2.0803295405\n",
      "Epoch 134, Loss: 2.0802988017\n",
      "Epoch 135, Loss: 2.0802691430\n",
      "Epoch 136, Loss: 2.0802405248\n",
      "Epoch 137, Loss: 2.0802129088\n",
      "Epoch 138, Loss: 2.0801862584\n",
      "Epoch 139, Loss: 2.0801605380\n",
      "Epoch 140, Loss: 2.0801357137\n",
      "Epoch 141, Loss: 2.0801117528\n",
      "Epoch 142, Loss: 2.0800886235\n",
      "Epoch 143, Loss: 2.0800662956\n",
      "Epoch 144, Loss: 2.0800447398\n",
      "Epoch 145, Loss: 2.0800239279\n",
      "Epoch 146, Loss: 2.0800038329\n",
      "Epoch 147, Loss: 2.0799844286\n",
      "Epoch 148, Loss: 2.0799656899\n",
      "Epoch 149, Loss: 2.0799475927\n",
      "Epoch 150, Loss: 2.0799301135\n",
      "Epoch 151, Loss: 2.0799132301\n",
      "Epoch 152, Loss: 2.0798969209\n",
      "Epoch 153, Loss: 2.0798811650\n",
      "Epoch 154, Loss: 2.0798659424\n",
      "Epoch 155, Loss: 2.0798512338\n",
      "Epoch 156, Loss: 2.0798370208\n",
      "Epoch 157, Loss: 2.0798232855\n",
      "Epoch 158, Loss: 2.0798100105\n",
      "Epoch 159, Loss: 2.0797971795\n",
      "Epoch 160, Loss: 2.0797847764\n",
      "Epoch 161, Loss: 2.0797727858\n",
      "Epoch 162, Loss: 2.0797611930\n",
      "Epoch 163, Loss: 2.0797499837\n",
      "Epoch 164, Loss: 2.0797391443\n",
      "Epoch 165, Loss: 2.0797286614\n",
      "Epoch 166, Loss: 2.0797185224\n",
      "Epoch 167, Loss: 2.0797087150\n",
      "Epoch 168, Loss: 2.0796992274\n",
      "Epoch 169, Loss: 2.0796900482\n",
      "Epoch 170, Loss: 2.0796811665\n",
      "Epoch 171, Loss: 2.0796725718\n",
      "Epoch 172, Loss: 2.0796642538\n",
      "Epoch 173, Loss: 2.0796562028\n",
      "Epoch 174, Loss: 2.0796484093\n",
      "Epoch 175, Loss: 2.0796408644\n",
      "Epoch 176, Loss: 2.0796335591\n",
      "Epoch 177, Loss: 2.0796264852\n",
      "Epoch 178, Loss: 2.0796196344\n",
      "Epoch 179, Loss: 2.0796129989\n",
      "Epoch 180, Loss: 2.0796065713\n",
      "Epoch 181, Loss: 2.0796003442\n",
      "Epoch 182, Loss: 2.0795943106\n",
      "Epoch 183, Loss: 2.0795884639\n",
      "Epoch 184, Loss: 2.0795827974\n",
      "Epoch 185, Loss: 2.0795773050\n",
      "Epoch 186, Loss: 2.0795719805\n",
      "Epoch 187, Loss: 2.0795668183\n",
      "Epoch 188, Loss: 2.0795618126\n",
      "Epoch 189, Loss: 2.0795569581\n",
      "Epoch 190, Loss: 2.0795522495\n",
      "Epoch 191, Loss: 2.0795476819\n",
      "Epoch 192, Loss: 2.0795432504\n",
      "Epoch 193, Loss: 2.0795389503\n",
      "Epoch 194, Loss: 2.0795347772\n",
      "Epoch 195, Loss: 2.0795307267\n",
      "Epoch 196, Loss: 2.0795267946\n",
      "Epoch 197, Loss: 2.0795229769\n",
      "Epoch 198, Loss: 2.0795192698\n",
      "Epoch 199, Loss: 2.0795156694\n",
      "Epoch 200, Loss: 2.0795121722\n",
      "Epoch 201, Loss: 2.0795087746\n",
      "Epoch 202, Loss: 2.0795054734\n",
      "Epoch 203, Loss: 2.0795022653\n",
      "Epoch 204, Loss: 2.0794991471\n",
      "Epoch 205, Loss: 2.0794961159\n",
      "Epoch 206, Loss: 2.0794931688\n",
      "Epoch 207, Loss: 2.0794903029\n",
      "Epoch 208, Loss: 2.0794875155\n",
      "Epoch 209, Loss: 2.0794848040\n",
      "Epoch 210, Loss: 2.0794821660\n",
      "Epoch 211, Loss: 2.0794795990\n",
      "Epoch 212, Loss: 2.0794771006\n",
      "Epoch 213, Loss: 2.0794746686\n",
      "Epoch 214, Loss: 2.0794723008\n",
      "Epoch 215, Loss: 2.0794699952\n",
      "Epoch 216, Loss: 2.0794677496\n",
      "Epoch 217, Loss: 2.0794655621\n",
      "Epoch 218, Loss: 2.0794634309\n",
      "Epoch 219, Loss: 2.0794613541\n",
      "Epoch 220, Loss: 2.0794593299\n",
      "Epoch 221, Loss: 2.0794573567\n",
      "Epoch 222, Loss: 2.0794554328\n",
      "Epoch 223, Loss: 2.0794535566\n",
      "Epoch 224, Loss: 2.0794517266\n",
      "Epoch 225, Loss: 2.0794499413\n",
      "Epoch 226, Loss: 2.0794481994\n",
      "Epoch 227, Loss: 2.0794464993\n",
      "Epoch 228, Loss: 2.0794448398\n",
      "Epoch 229, Loss: 2.0794432197\n",
      "Epoch 230, Loss: 2.0794416376\n",
      "Epoch 231, Loss: 2.0794400924\n",
      "Epoch 232, Loss: 2.0794385829\n",
      "Epoch 233, Loss: 2.0794371079\n",
      "Epoch 234, Loss: 2.0794356665\n",
      "Epoch 235, Loss: 2.0794342576\n",
      "Epoch 236, Loss: 2.0794328802\n",
      "Epoch 237, Loss: 2.0794315332\n",
      "Epoch 238, Loss: 2.0794302158\n",
      "Epoch 239, Loss: 2.0794289270\n",
      "Epoch 240, Loss: 2.0794276660\n",
      "Epoch 241, Loss: 2.0794264320\n",
      "Epoch 242, Loss: 2.0794252240\n",
      "Epoch 243, Loss: 2.0794240413\n",
      "Epoch 244, Loss: 2.0794228832\n",
      "Epoch 245, Loss: 2.0794217489\n",
      "Epoch 246, Loss: 2.0794206376\n",
      "Epoch 247, Loss: 2.0794195488\n",
      "Epoch 248, Loss: 2.0794184816\n",
      "Epoch 249, Loss: 2.0794174355\n",
      "Epoch 250, Loss: 2.0794164099\n",
      "Epoch 251, Loss: 2.0794154041\n",
      "Epoch 252, Loss: 2.0794144176\n",
      "Epoch 253, Loss: 2.0794134497\n",
      "Epoch 254, Loss: 2.0794125000\n",
      "Epoch 255, Loss: 2.0794115679\n",
      "Epoch 256, Loss: 2.0794106529\n",
      "Epoch 257, Loss: 2.0794097545\n",
      "Epoch 258, Loss: 2.0794088722\n",
      "Epoch 259, Loss: 2.0794080055\n",
      "Epoch 260, Loss: 2.0794071541\n",
      "Epoch 261, Loss: 2.0794063174\n",
      "Epoch 262, Loss: 2.0794054951\n",
      "Epoch 263, Loss: 2.0794046867\n",
      "Epoch 264, Loss: 2.0794038918\n",
      "Epoch 265, Loss: 2.0794031101\n",
      "Epoch 266, Loss: 2.0794023412\n",
      "Epoch 267, Loss: 2.0794015847\n",
      "Epoch 268, Loss: 2.0794008402\n",
      "Epoch 269, Loss: 2.0794001076\n",
      "Epoch 270, Loss: 2.0793993863\n",
      "Epoch 271, Loss: 2.0793986761\n",
      "Epoch 272, Loss: 2.0793979768\n",
      "Epoch 273, Loss: 2.0793972879\n",
      "Epoch 274, Loss: 2.0793966092\n",
      "Epoch 275, Loss: 2.0793959405\n",
      "Epoch 276, Loss: 2.0793952814\n",
      "Epoch 277, Loss: 2.0793946317\n",
      "Epoch 278, Loss: 2.0793939912\n",
      "Epoch 279, Loss: 2.0793933595\n",
      "Epoch 280, Loss: 2.0793927365\n",
      "Epoch 281, Loss: 2.0793921220\n",
      "Epoch 282, Loss: 2.0793915156\n",
      "Epoch 283, Loss: 2.0793909173\n",
      "Epoch 284, Loss: 2.0793903267\n",
      "Epoch 285, Loss: 2.0793897436\n",
      "Epoch 286, Loss: 2.0793891680\n",
      "Epoch 287, Loss: 2.0793885995\n",
      "Epoch 288, Loss: 2.0793880380\n",
      "Epoch 289, Loss: 2.0793874834\n",
      "Epoch 290, Loss: 2.0793869353\n",
      "Epoch 291, Loss: 2.0793863938\n",
      "Epoch 292, Loss: 2.0793858585\n",
      "Epoch 293, Loss: 2.0793853294\n",
      "Epoch 294, Loss: 2.0793848062\n",
      "Epoch 295, Loss: 2.0793842889\n",
      "Epoch 296, Loss: 2.0793837772\n",
      "Epoch 297, Loss: 2.0793832711\n",
      "Epoch 298, Loss: 2.0793827704\n",
      "Epoch 299, Loss: 2.0793822750\n",
      "Epoch 300, Loss: 2.0793817847\n",
      "Epoch 301, Loss: 2.0793812994\n",
      "Epoch 302, Loss: 2.0793808190\n",
      "Epoch 303, Loss: 2.0793803433\n",
      "Epoch 304, Loss: 2.0793798723\n",
      "Epoch 305, Loss: 2.0793794059\n",
      "Epoch 306, Loss: 2.0793789438\n",
      "Epoch 307, Loss: 2.0793784861\n",
      "Epoch 308, Loss: 2.0793780325\n",
      "Epoch 309, Loss: 2.0793775831\n",
      "Epoch 310, Loss: 2.0793771377\n",
      "Epoch 311, Loss: 2.0793766962\n",
      "Epoch 312, Loss: 2.0793762585\n",
      "Epoch 313, Loss: 2.0793758246\n",
      "Epoch 314, Loss: 2.0793753943\n",
      "Epoch 315, Loss: 2.0793749676\n",
      "Epoch 316, Loss: 2.0793745443\n",
      "Epoch 317, Loss: 2.0793741244\n",
      "Epoch 318, Loss: 2.0793737078\n",
      "Epoch 319, Loss: 2.0793732945\n",
      "Epoch 320, Loss: 2.0793728843\n",
      "Epoch 321, Loss: 2.0793724773\n",
      "Epoch 322, Loss: 2.0793720732\n",
      "Epoch 323, Loss: 2.0793716721\n",
      "Epoch 324, Loss: 2.0793712738\n",
      "Epoch 325, Loss: 2.0793708784\n",
      "Epoch 326, Loss: 2.0793704857\n",
      "Epoch 327, Loss: 2.0793700958\n",
      "Epoch 328, Loss: 2.0793697084\n",
      "Epoch 329, Loss: 2.0793693236\n",
      "Epoch 330, Loss: 2.0793689413\n",
      "Epoch 331, Loss: 2.0793685615\n",
      "Epoch 332, Loss: 2.0793681841\n",
      "Epoch 333, Loss: 2.0793678090\n",
      "Epoch 334, Loss: 2.0793674363\n",
      "Epoch 335, Loss: 2.0793670657\n",
      "Epoch 336, Loss: 2.0793666974\n",
      "Epoch 337, Loss: 2.0793663312\n",
      "Epoch 338, Loss: 2.0793659671\n",
      "Epoch 339, Loss: 2.0793656051\n",
      "Epoch 340, Loss: 2.0793652451\n",
      "Epoch 341, Loss: 2.0793648871\n",
      "Epoch 342, Loss: 2.0793645309\n",
      "Epoch 343, Loss: 2.0793641767\n",
      "Epoch 344, Loss: 2.0793638243\n",
      "Epoch 345, Loss: 2.0793634737\n",
      "Epoch 346, Loss: 2.0793631249\n",
      "Epoch 347, Loss: 2.0793627778\n",
      "Epoch 348, Loss: 2.0793624324\n",
      "Epoch 349, Loss: 2.0793620887\n",
      "Epoch 350, Loss: 2.0793617466\n",
      "Epoch 351, Loss: 2.0793614060\n",
      "Epoch 352, Loss: 2.0793610671\n",
      "Epoch 353, Loss: 2.0793607296\n",
      "Epoch 354, Loss: 2.0793603937\n",
      "Epoch 355, Loss: 2.0793600592\n",
      "Epoch 356, Loss: 2.0793597261\n",
      "Epoch 357, Loss: 2.0793593944\n",
      "Epoch 358, Loss: 2.0793590641\n",
      "Epoch 359, Loss: 2.0793587352\n",
      "Epoch 360, Loss: 2.0793584075\n",
      "Epoch 361, Loss: 2.0793580812\n",
      "Epoch 362, Loss: 2.0793577561\n",
      "Epoch 363, Loss: 2.0793574323\n",
      "Epoch 364, Loss: 2.0793571096\n",
      "Epoch 365, Loss: 2.0793567882\n",
      "Epoch 366, Loss: 2.0793564679\n",
      "Epoch 367, Loss: 2.0793561488\n",
      "Epoch 368, Loss: 2.0793558307\n",
      "Epoch 369, Loss: 2.0793555138\n",
      "Epoch 370, Loss: 2.0793551980\n",
      "Epoch 371, Loss: 2.0793548832\n",
      "Epoch 372, Loss: 2.0793545694\n",
      "Epoch 373, Loss: 2.0793542567\n",
      "Epoch 374, Loss: 2.0793539449\n",
      "Epoch 375, Loss: 2.0793536341\n",
      "Epoch 376, Loss: 2.0793533243\n",
      "Epoch 377, Loss: 2.0793530154\n",
      "Epoch 378, Loss: 2.0793527074\n",
      "Epoch 379, Loss: 2.0793524003\n",
      "Epoch 380, Loss: 2.0793520941\n",
      "Epoch 381, Loss: 2.0793517887\n",
      "Epoch 382, Loss: 2.0793514842\n",
      "Epoch 383, Loss: 2.0793511806\n",
      "Epoch 384, Loss: 2.0793508777\n",
      "Epoch 385, Loss: 2.0793505756\n",
      "Epoch 386, Loss: 2.0793502744\n",
      "Epoch 387, Loss: 2.0793499739\n",
      "Epoch 388, Loss: 2.0793496741\n",
      "Epoch 389, Loss: 2.0793493751\n",
      "Epoch 390, Loss: 2.0793490768\n",
      "Epoch 391, Loss: 2.0793487792\n",
      "Epoch 392, Loss: 2.0793484824\n"
     ]
    }
   ],
   "source": [
    "# # For baseline purpose, running part B code in the same script\n",
    "\n",
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# # Sigmoid activation and its derivative\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def sigmoid_derivative(x):\n",
    "#     s = sigmoid(x)\n",
    "#     return s * (1 - s)\n",
    "\n",
    "# def softmax(x, axis=None):\n",
    "#     exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "#     return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# # Cross-entropy loss\n",
    "# def cross_entropy_loss(y_true, y_pred):\n",
    "#     y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "#     return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# # Neural Network Class with Softmax in the Output Layer and Sigmoid in Hidden Layers\n",
    "# class NeuralNetwork_Baseline:\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size):\n",
    "#         np.random.seed(0)\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "\n",
    "#         layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "#         # Initialize weights and biases\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "#             self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         pre_activations = []\n",
    "\n",
    "#         # Pass through each layer except the output layer\n",
    "#         for i in range(len(self.weights) - 1):\n",
    "#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             pre_activations.append(z)\n",
    "#             a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "#             activations.append(a)\n",
    "\n",
    "#         # Pass through the output layer with softmax\n",
    "#         z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         pre_activations.append(z)\n",
    "#         a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "#         activations.append(a)\n",
    "\n",
    "#         return activations, pre_activations\n",
    "\n",
    "#     def backward(self, X, y, activations, pre_activations):\n",
    "#         grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "#         grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "#         # Start with output layer error\n",
    "#         delta = activations[-1] - y\n",
    "\n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "#             grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "#             if i > 0:\n",
    "#                 delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "\n",
    "#         return grad_w, grad_b\n",
    "\n",
    "#     def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "#         for i in range(len(self.weights)):\n",
    "#             self.weights[i] -= learning_rate * grad_w[i]\n",
    "#             self.biases[i] -= learning_rate * grad_b[i]\n",
    "\n",
    "#     def train(self, batches, learning_rate):\n",
    "#         start_time = time.time()\n",
    "#         epoch = 0\n",
    "#         while (True):\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 activations, pre_activations = self.forward(X_batch)\n",
    "#                 grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "#                 self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "#             # Calculate average loss over batches\n",
    "#             loss = 0\n",
    "#             z = 0\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 y_pred, _ = self.forward(X_batch)\n",
    "#                 loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "#                 z += len(y_pred[-1])\n",
    "#             loss /= z\n",
    "#             print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "#             epoch += 1\n",
    "#             if (time.time() - start_time) > 60:\n",
    "#                 break\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "#     def get_weights(self):\n",
    "#         return self.weights\n",
    "\n",
    "#     def get_biases(self):\n",
    "#         return self.biases\n",
    "\n",
    "# # Example usage:\n",
    "# nn = NeuralNetwork_Baseline(625, [512, 256, 128, 32], 8)\n",
    "# nn.train(batches, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Sigmoid activation and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Neural Network Class with Softmax in the Output Layer and Sigmoid in Hidden Layers\n",
    "class NeuralNetwork_Adam:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, init_weights = None, init_biases = None, init_seed = None, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        if (init_seed is None):\n",
    "            self.best_seed = int(time.time())\n",
    "            np.random.seed(self.best_seed)\n",
    "        else:\n",
    "            np.random.seed(init_seed)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.m_w = []\n",
    "        self.v_w = []\n",
    "        self.m_b = []\n",
    "        self.v_b = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step for Adam\n",
    "        self.best_weights = []\n",
    "        self.best_biases = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if (init_weights is not None) and (init_biases is not None):\n",
    "                self.weights.append(init_weights[i])\n",
    "                self.biases.append(init_biases[i])\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "                self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "            self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.best_weights = self.weights\n",
    "            self.best_biases = self.biases\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each layer except the output layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        self.t += 1  # Increment time step for Adam\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def train(self, batches, time_of_running, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while(True):\n",
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
    "            loss /= z\n",
    "            \n",
    "            if (loss < self.best_loss):\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = self.weights\n",
    "                self.best_biases = self.biases\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            epoch += 1\n",
    "            # if time elapsed is greater than 1 minute, break the loop\n",
    "            if time.time() - start_time > 60*time_of_running:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    \n",
    "    def get_best_biases(self):\n",
    "        return self.best_biases\n",
    "    \n",
    "    def get_best_loss(self):\n",
    "        return self.best_loss\n",
    "    \n",
    "    def get_best_seed(self):\n",
    "        return self.best_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0914506361\n",
      "Epoch 2, Loss: 2.0788259000\n",
      "Epoch 3, Loss: 2.0766124286\n",
      "Epoch 4, Loss: 2.0722483331\n",
      "Epoch 5, Loss: 2.0627044792\n",
      "Epoch 6, Loss: 2.0420303345\n",
      "Epoch 7, Loss: 2.0245986179\n",
      "Epoch 8, Loss: 2.0011135797\n",
      "Epoch 9, Loss: 1.9676149203\n",
      "Epoch 10, Loss: 1.9433233631\n",
      "Epoch 11, Loss: 1.9180561117\n",
      "Epoch 12, Loss: 1.8204787833\n",
      "Epoch 13, Loss: 1.8416444746\n",
      "Epoch 14, Loss: 1.7882767608\n",
      "Epoch 15, Loss: 1.7437465632\n",
      "Epoch 16, Loss: 1.7597514839\n",
      "Epoch 17, Loss: 1.8105201544\n",
      "Epoch 18, Loss: 1.6848965516\n",
      "Epoch 19, Loss: 1.6429073254\n",
      "Epoch 20, Loss: 1.6708231001\n",
      "Epoch 21, Loss: 1.7065059722\n",
      "Epoch 22, Loss: 1.5739130629\n",
      "Epoch 23, Loss: 1.5228905646\n",
      "Epoch 24, Loss: 1.5334547629\n",
      "Epoch 25, Loss: 1.5319941543\n",
      "Epoch 26, Loss: 1.4619217270\n",
      "Epoch 27, Loss: 1.4458115915\n",
      "Epoch 28, Loss: 1.4526393194\n",
      "Epoch 29, Loss: 1.4775548328\n",
      "Epoch 30, Loss: 1.4356503640\n",
      "Epoch 31, Loss: 1.4035938578\n",
      "Epoch 32, Loss: 1.4031061251\n",
      "Epoch 33, Loss: 1.4189242016\n",
      "Epoch 34, Loss: 1.4309642413\n",
      "Epoch 35, Loss: 1.3831736024\n",
      "Epoch 36, Loss: 1.3648219179\n",
      "Epoch 37, Loss: 1.3674654774\n",
      "Epoch 38, Loss: 1.3830316304\n",
      "Epoch 39, Loss: 1.3891675491\n",
      "Epoch 40, Loss: 1.3544413084\n",
      "Epoch 41, Loss: 1.3315143186\n",
      "Epoch 42, Loss: 1.3278134055\n",
      "Epoch 43, Loss: 1.3323951168\n",
      "Epoch 44, Loss: 1.3371309417\n",
      "Epoch 45, Loss: 1.3443197374\n",
      "Epoch 46, Loss: 1.3288178870\n",
      "Epoch 47, Loss: 1.2961607824\n",
      "Epoch 48, Loss: 1.2842025943\n",
      "Epoch 49, Loss: 1.2861707220\n",
      "Epoch 50, Loss: 1.2949293981\n",
      "Epoch 51, Loss: 1.2764291388\n",
      "Epoch 52, Loss: 1.2736954032\n",
      "Epoch 53, Loss: 1.2985155870\n",
      "Epoch 54, Loss: 1.3261425355\n",
      "Epoch 55, Loss: 1.3216636538\n",
      "Epoch 56, Loss: 1.2970364801\n",
      "Epoch 57, Loss: 1.2843531543\n",
      "Epoch 58, Loss: 1.2640939476\n",
      "Epoch 59, Loss: 1.2634602774\n",
      "Epoch 60, Loss: 1.2658303420\n",
      "Epoch 61, Loss: 1.2501969989\n",
      "Epoch 62, Loss: 1.2173752755\n",
      "Epoch 63, Loss: 1.2020118920\n",
      "Epoch 64, Loss: 1.1970944187\n",
      "Epoch 65, Loss: 1.1914204995\n",
      "Epoch 66, Loss: 1.1757320736\n",
      "Epoch 67, Loss: 1.1654447838\n",
      "Epoch 68, Loss: 1.1597236779\n",
      "Epoch 69, Loss: 1.1549435299\n",
      "Epoch 70, Loss: 1.1503361924\n",
      "Epoch 71, Loss: 1.1447039813\n",
      "Epoch 72, Loss: 1.1394836882\n",
      "Epoch 73, Loss: 1.1353368913\n",
      "Epoch 74, Loss: 1.1325504020\n",
      "Epoch 75, Loss: 1.1300100193\n",
      "Epoch 76, Loss: 1.1248005640\n",
      "Epoch 77, Loss: 1.1168783328\n",
      "Epoch 78, Loss: 1.1121264635\n",
      "Epoch 79, Loss: 1.1116716049\n",
      "Epoch 80, Loss: 1.1130058042\n",
      "Epoch 81, Loss: 1.1066085818\n",
      "Epoch 82, Loss: 1.0999873851\n",
      "Epoch 83, Loss: 1.0955571671\n",
      "Epoch 84, Loss: 1.0928425291\n",
      "Epoch 85, Loss: 1.0917622813\n",
      "Epoch 86, Loss: 1.0901198972\n",
      "Epoch 87, Loss: 1.0883030920\n",
      "Epoch 88, Loss: 1.0845278867\n",
      "Epoch 89, Loss: 1.0795035164\n",
      "Epoch 90, Loss: 1.0765271993\n",
      "Epoch 91, Loss: 1.0746182463\n",
      "Epoch 92, Loss: 1.0724601679\n",
      "Epoch 93, Loss: 1.0704727730\n",
      "Epoch 94, Loss: 1.0703124989\n",
      "Epoch 95, Loss: 1.0664194690\n",
      "Epoch 96, Loss: 1.0636057717\n",
      "Epoch 97, Loss: 1.0595186467\n",
      "Epoch 98, Loss: 1.0572646292\n",
      "Epoch 99, Loss: 1.0547182729\n",
      "Epoch 100, Loss: 1.0530185875\n",
      "Epoch 101, Loss: 1.0508900727\n",
      "Epoch 102, Loss: 1.0502214313\n",
      "Epoch 103, Loss: 1.0478532812\n",
      "Epoch 104, Loss: 1.0452627028\n",
      "Epoch 105, Loss: 1.0420425070\n",
      "Epoch 106, Loss: 1.0382440622\n",
      "Epoch 107, Loss: 1.0364087374\n",
      "Epoch 108, Loss: 1.0333881373\n",
      "Epoch 109, Loss: 1.0319113828\n",
      "Epoch 110, Loss: 1.0296211440\n",
      "Epoch 111, Loss: 1.0279761453\n",
      "Epoch 112, Loss: 1.0301300182\n",
      "Epoch 113, Loss: 1.0238052843\n",
      "Epoch 114, Loss: 1.0287681201\n",
      "Epoch 115, Loss: 1.0221164324\n",
      "Epoch 116, Loss: 1.0155069060\n",
      "Epoch 117, Loss: 1.0650259507\n",
      "Epoch 118, Loss: 1.0138862341\n",
      "Epoch 119, Loss: 1.1775928477\n",
      "Epoch 120, Loss: 1.0255200762\n",
      "Epoch 121, Loss: 1.0630826209\n",
      "Epoch 122, Loss: 1.0306621521\n",
      "Epoch 123, Loss: 1.0541644209\n",
      "Epoch 124, Loss: 1.0256574376\n",
      "Epoch 125, Loss: 1.0141347486\n",
      "Epoch 126, Loss: 1.0278643631\n",
      "Epoch 127, Loss: 1.0134634216\n",
      "Epoch 128, Loss: 1.0181099574\n",
      "Epoch 129, Loss: 1.0274537325\n",
      "Epoch 130, Loss: 1.0870708736\n",
      "Epoch 131, Loss: 1.0196881587\n",
      "Epoch 132, Loss: 1.0194406464\n",
      "Epoch 133, Loss: 1.0218850104\n",
      "Epoch 134, Loss: 1.0160540852\n",
      "Epoch 135, Loss: 1.0473122232\n",
      "Epoch 136, Loss: 1.0516462341\n",
      "Epoch 137, Loss: 1.0307004747\n",
      "Epoch 138, Loss: 1.0075518229\n",
      "Epoch 139, Loss: 1.0220028040\n",
      "Epoch 140, Loss: 1.0512811050\n",
      "Epoch 141, Loss: 1.0222890920\n",
      "Epoch 142, Loss: 1.0146576507\n",
      "Epoch 143, Loss: 1.0145663057\n",
      "Epoch 144, Loss: 1.0123748607\n",
      "Epoch 145, Loss: 1.0063189474\n",
      "Epoch 146, Loss: 0.9899420356\n",
      "Epoch 147, Loss: 0.9947159752\n",
      "Epoch 148, Loss: 0.9944997177\n",
      "Epoch 149, Loss: 0.9925916261\n",
      "Epoch 150, Loss: 0.9914322426\n",
      "Epoch 151, Loss: 0.9944766999\n",
      "Epoch 152, Loss: 0.9913655967\n",
      "Epoch 153, Loss: 0.9963105273\n",
      "Epoch 154, Loss: 1.0037606429\n",
      "Epoch 155, Loss: 0.9840985376\n",
      "Epoch 156, Loss: 1.0095696562\n",
      "Epoch 157, Loss: 1.0030660081\n",
      "Epoch 158, Loss: 1.0013132713\n",
      "Epoch 159, Loss: 1.0108681849\n",
      "Epoch 160, Loss: 1.0255381907\n",
      "Epoch 161, Loss: 1.0280160123\n",
      "Epoch 162, Loss: 1.0477372203\n",
      "Epoch 163, Loss: 1.0767594501\n",
      "Epoch 164, Loss: 1.0462577186\n",
      "Epoch 165, Loss: 1.0109643980\n",
      "Epoch 166, Loss: 1.0048727749\n",
      "Epoch 167, Loss: 0.9610241656\n",
      "Epoch 168, Loss: 0.9617716714\n",
      "Epoch 169, Loss: 0.9604657002\n",
      "Epoch 170, Loss: 0.9607088044\n",
      "Epoch 171, Loss: 0.9563722631\n",
      "Epoch 172, Loss: 0.9464020447\n",
      "Epoch 173, Loss: 0.9392122972\n",
      "Epoch 174, Loss: 0.9349266390\n",
      "Epoch 175, Loss: 0.9312595156\n",
      "Epoch 176, Loss: 0.9376778388\n",
      "Epoch 177, Loss: 0.9335250252\n",
      "Epoch 178, Loss: 0.9406257485\n",
      "Epoch 179, Loss: 0.9496826780\n",
      "Epoch 180, Loss: 2.0504209470\n",
      "Epoch 181, Loss: 1.3891773714\n",
      "Epoch 182, Loss: 1.1389812898\n",
      "Epoch 183, Loss: 1.0256993016\n",
      "Epoch 184, Loss: 0.9760313006\n",
      "Epoch 185, Loss: 0.9607070434\n",
      "Epoch 186, Loss: 0.9385621597\n",
      "Epoch 187, Loss: 0.9386644361\n",
      "Epoch 188, Loss: 0.9374343226\n",
      "Epoch 189, Loss: 0.9373389334\n",
      "Epoch 190, Loss: 0.9370530408\n",
      "Epoch 191, Loss: 0.9374122442\n",
      "Epoch 192, Loss: 0.9389927582\n",
      "Epoch 193, Loss: 0.9429826181\n",
      "Epoch 194, Loss: 0.9467819798\n",
      "Epoch 195, Loss: 0.9435634941\n",
      "Epoch 196, Loss: 0.9411800520\n",
      "Epoch 197, Loss: 0.9350095752\n",
      "Epoch 198, Loss: 0.9352682033\n",
      "Epoch 199, Loss: 0.9300985016\n",
      "Epoch 200, Loss: 0.9333310941\n",
      "Epoch 201, Loss: 0.9385408344\n",
      "Epoch 202, Loss: 0.9390917024\n",
      "Epoch 203, Loss: 0.9534761366\n",
      "Epoch 204, Loss: 0.9221212063\n",
      "Epoch 205, Loss: 0.9161197246\n",
      "Epoch 206, Loss: 0.9090177937\n",
      "Epoch 207, Loss: 0.9160989985\n",
      "Epoch 208, Loss: 0.9266273754\n",
      "Epoch 209, Loss: 0.9202946367\n",
      "Epoch 210, Loss: 0.9209220932\n",
      "Epoch 211, Loss: 0.9368947507\n",
      "Epoch 212, Loss: 0.9217558644\n",
      "Epoch 213, Loss: 0.9248487764\n",
      "Epoch 214, Loss: 0.9113499382\n",
      "Epoch 215, Loss: 0.9132967254\n",
      "Epoch 216, Loss: 0.9133390224\n",
      "Epoch 217, Loss: 0.9130599372\n",
      "Epoch 218, Loss: 0.9145694783\n",
      "Epoch 219, Loss: 0.9059548771\n",
      "Epoch 220, Loss: 0.9212019725\n",
      "Epoch 221, Loss: 0.9135952304\n",
      "Epoch 222, Loss: 0.9177827894\n",
      "Epoch 223, Loss: 0.9080338220\n",
      "Epoch 224, Loss: 0.9198973712\n",
      "Epoch 225, Loss: 0.9173882222\n",
      "Epoch 226, Loss: 0.9183284992\n",
      "Epoch 227, Loss: 0.9050277023\n",
      "Epoch 228, Loss: 0.9172801914\n",
      "Epoch 229, Loss: 0.9153673686\n",
      "Epoch 230, Loss: 0.9114363338\n",
      "Epoch 231, Loss: 0.9097163665\n",
      "Epoch 232, Loss: 0.9082110525\n",
      "Epoch 233, Loss: 0.8965156251\n",
      "Epoch 234, Loss: 0.9031079738\n",
      "Epoch 235, Loss: 0.9006126548\n",
      "Epoch 236, Loss: 0.8968301856\n",
      "Epoch 237, Loss: 0.8989440984\n",
      "Epoch 238, Loss: 0.8951943900\n",
      "Epoch 239, Loss: 0.9265329995\n",
      "Epoch 240, Loss: 0.9327227430\n",
      "Epoch 241, Loss: 0.9069161951\n",
      "Epoch 242, Loss: 0.9014704720\n",
      "Epoch 243, Loss: 0.8877693403\n",
      "Epoch 244, Loss: 0.9017432828\n",
      "Epoch 245, Loss: 0.9027323061\n",
      "Epoch 246, Loss: 0.8835945300\n",
      "Epoch 247, Loss: 0.8998084811\n",
      "Epoch 248, Loss: 0.9058903960\n",
      "Epoch 249, Loss: 0.8804027941\n",
      "Epoch 250, Loss: 0.8890615087\n",
      "Epoch 251, Loss: 0.9345935682\n",
      "Epoch 252, Loss: 0.9105509255\n",
      "Epoch 253, Loss: 0.8999986114\n",
      "Epoch 254, Loss: 0.9352158134\n",
      "Epoch 255, Loss: 0.9571954976\n",
      "Epoch 256, Loss: 1.0646690928\n",
      "Epoch 257, Loss: 1.0164651717\n",
      "Epoch 258, Loss: 0.8969678637\n",
      "Epoch 259, Loss: 0.9630810332\n",
      "Epoch 260, Loss: 0.8966647434\n",
      "Epoch 261, Loss: 0.8725310613\n",
      "Epoch 262, Loss: 0.8657685775\n",
      "Epoch 263, Loss: 0.8829657567\n",
      "Epoch 264, Loss: 1.1666308227\n",
      "Epoch 265, Loss: 0.8728236621\n",
      "Epoch 266, Loss: 0.9034027955\n",
      "Epoch 267, Loss: 0.9845527086\n",
      "Epoch 268, Loss: 1.0024738209\n",
      "Epoch 269, Loss: 0.9784003413\n",
      "Epoch 270, Loss: 0.8864090507\n",
      "Epoch 271, Loss: 0.9203414867\n",
      "Epoch 272, Loss: 1.0032492828\n",
      "Epoch 273, Loss: 0.8801338630\n",
      "Epoch 274, Loss: 0.9045117095\n",
      "Epoch 275, Loss: 0.8735076913\n",
      "Epoch 276, Loss: 0.8769385671\n",
      "Epoch 277, Loss: 0.9363950441\n",
      "Epoch 278, Loss: 0.9615793418\n",
      "Epoch 279, Loss: 1.0010312097\n",
      "Epoch 280, Loss: 1.0480624753\n",
      "Epoch 281, Loss: 0.8704061640\n",
      "Epoch 282, Loss: 0.8659490947\n",
      "Epoch 283, Loss: 0.9179397614\n",
      "Epoch 284, Loss: 0.9371125466\n",
      "Epoch 285, Loss: 0.9149471673\n",
      "Epoch 286, Loss: 0.9190227646\n",
      "Epoch 287, Loss: 0.9025636237\n",
      "Epoch 288, Loss: 0.8537412962\n",
      "Epoch 289, Loss: 0.8649947678\n",
      "Epoch 290, Loss: 0.9019304625\n",
      "Epoch 291, Loss: 0.9172385494\n",
      "Epoch 292, Loss: 0.8555377780\n",
      "Epoch 293, Loss: 0.8592872907\n",
      "Epoch 294, Loss: 0.8602991032\n",
      "Epoch 295, Loss: 0.8448717188\n",
      "Epoch 296, Loss: 0.8318261442\n",
      "Epoch 297, Loss: 0.8354642321\n",
      "Epoch 298, Loss: 0.8887701804\n",
      "Epoch 299, Loss: 1.1529975275\n",
      "Epoch 300, Loss: 0.8716385993\n",
      "Epoch 301, Loss: 0.8952666924\n",
      "Epoch 302, Loss: 0.8760764597\n",
      "Epoch 303, Loss: 0.8762053035\n",
      "Epoch 304, Loss: 0.9376901683\n",
      "Epoch 305, Loss: 0.9672583753\n",
      "Epoch 306, Loss: 0.9183031359\n",
      "Epoch 307, Loss: 0.8356834933\n",
      "Epoch 308, Loss: 0.8617457458\n",
      "Epoch 309, Loss: 0.9114562658\n",
      "Epoch 310, Loss: 0.8742806540\n",
      "Epoch 311, Loss: 0.8310319305\n",
      "Epoch 312, Loss: 0.8571196113\n",
      "Epoch 313, Loss: 0.8960274040\n",
      "Epoch 314, Loss: 0.8664540937\n",
      "Epoch 315, Loss: 0.8379011543\n",
      "Epoch 316, Loss: 0.8256448536\n",
      "Epoch 317, Loss: 0.8427025039\n",
      "Epoch 318, Loss: 0.8815062867\n",
      "Epoch 319, Loss: 1.0743801812\n",
      "Epoch 320, Loss: 0.8712259519\n",
      "Epoch 321, Loss: 0.8273456840\n",
      "Epoch 322, Loss: 0.9111400262\n",
      "Epoch 323, Loss: 0.8372027590\n",
      "Epoch 324, Loss: 0.8144325886\n",
      "Epoch 325, Loss: 0.8383907599\n",
      "Epoch 326, Loss: 0.8792099064\n",
      "Epoch 327, Loss: 0.8632776995\n",
      "Epoch 328, Loss: 0.8485187985\n",
      "Epoch 329, Loss: 0.8526379330\n",
      "Epoch 330, Loss: 0.8304585883\n",
      "Epoch 331, Loss: 0.8340946518\n",
      "Epoch 332, Loss: 0.8373853333\n",
      "Epoch 333, Loss: 0.8569999313\n",
      "Epoch 334, Loss: 0.8567033564\n",
      "Epoch 335, Loss: 0.8528624191\n",
      "Epoch 336, Loss: 0.8322682773\n",
      "Epoch 337, Loss: 0.8212672840\n",
      "Epoch 338, Loss: 0.8197810740\n",
      "Epoch 339, Loss: 0.8276708646\n",
      "Epoch 340, Loss: 0.8425326546\n",
      "Epoch 341, Loss: 0.8582930917\n",
      "Epoch 342, Loss: 0.8498072228\n",
      "Epoch 343, Loss: 0.8172167326\n",
      "Epoch 344, Loss: 0.8029330593\n",
      "Epoch 345, Loss: 0.8043142158\n",
      "Epoch 346, Loss: 0.8227143340\n",
      "Epoch 347, Loss: 0.8450940857\n",
      "Epoch 348, Loss: 0.8495927234\n",
      "Epoch 349, Loss: 0.8512633381\n",
      "Epoch 350, Loss: 0.8689417317\n",
      "Epoch 351, Loss: 0.8857225340\n",
      "Epoch 352, Loss: 0.8673600896\n",
      "Epoch 353, Loss: 0.8589996317\n",
      "Epoch 354, Loss: 0.8546737869\n",
      "Epoch 355, Loss: 0.8587994571\n",
      "Epoch 356, Loss: 0.8736249698\n",
      "Epoch 357, Loss: 0.8957355286\n",
      "Epoch 358, Loss: 0.9250298998\n",
      "Epoch 359, Loss: 0.9177597965\n",
      "Epoch 360, Loss: 0.7926116782\n",
      "Epoch 361, Loss: 0.7789279964\n",
      "Epoch 362, Loss: 0.7860963057\n",
      "Epoch 363, Loss: 0.7812978328\n",
      "Epoch 364, Loss: 0.7884855460\n",
      "Epoch 365, Loss: 0.7985563596\n",
      "Epoch 366, Loss: 0.8464319651\n",
      "Epoch 367, Loss: 0.9170369794\n",
      "Epoch 368, Loss: 0.8790035217\n",
      "Epoch 369, Loss: 0.8642100318\n",
      "Epoch 370, Loss: 0.8001102144\n",
      "Epoch 371, Loss: 0.7691727985\n",
      "Epoch 372, Loss: 0.7648907811\n",
      "Epoch 373, Loss: 0.7946637472\n",
      "Epoch 374, Loss: 0.8389098538\n",
      "Epoch 375, Loss: 0.8369800982\n",
      "Epoch 376, Loss: 0.8446653747\n",
      "Epoch 377, Loss: 0.8517604114\n",
      "Epoch 378, Loss: 0.8779357192\n",
      "Epoch 379, Loss: 0.9093994230\n",
      "Epoch 380, Loss: 0.9376575218\n",
      "Epoch 381, Loss: 0.8638022471\n",
      "Epoch 382, Loss: 0.7784523940\n",
      "Epoch 383, Loss: 0.7741788416\n",
      "Epoch 384, Loss: 0.7846955958\n",
      "Epoch 385, Loss: 0.8162252878\n",
      "Epoch 386, Loss: 0.8260437692\n",
      "Epoch 387, Loss: 0.8430551264\n",
      "Epoch 388, Loss: 0.8923074170\n",
      "Epoch 389, Loss: 0.8682366339\n",
      "Epoch 390, Loss: 0.7782216632\n",
      "Epoch 391, Loss: 0.9592570779\n",
      "Epoch 392, Loss: 1.1117932396\n",
      "Epoch 393, Loss: 0.8784315484\n",
      "Epoch 394, Loss: 0.9512532051\n",
      "Epoch 395, Loss: 0.8590827321\n",
      "Epoch 396, Loss: 0.8340842934\n",
      "Epoch 397, Loss: 0.7883429723\n",
      "Epoch 398, Loss: 0.7866077633\n",
      "Epoch 399, Loss: 0.7890775347\n",
      "Epoch 400, Loss: 0.7841991269\n",
      "Epoch 401, Loss: 0.8037873111\n",
      "Epoch 402, Loss: 0.8295578028\n",
      "Epoch 403, Loss: 0.8158664554\n",
      "Epoch 404, Loss: 0.7662130833\n",
      "Epoch 405, Loss: 0.7861304298\n",
      "Epoch 406, Loss: 0.8465284296\n",
      "Epoch 407, Loss: 0.8338076809\n",
      "Epoch 408, Loss: 0.7668274875\n",
      "Epoch 409, Loss: 0.7599606969\n",
      "Epoch 410, Loss: 0.7894394168\n",
      "Epoch 411, Loss: 0.8494359846\n",
      "Epoch 412, Loss: 0.8638799067\n",
      "Epoch 413, Loss: 0.7850453283\n",
      "Epoch 414, Loss: 0.7597817806\n",
      "Epoch 415, Loss: 0.7862469138\n",
      "Epoch 416, Loss: 0.8057340742\n",
      "Epoch 417, Loss: 0.7713527645\n",
      "Epoch 418, Loss: 0.7644884175\n",
      "Epoch 419, Loss: 0.8190850942\n",
      "Epoch 420, Loss: 0.9069172933\n",
      "Epoch 421, Loss: 0.8376158245\n",
      "Epoch 422, Loss: 0.8663438048\n",
      "Epoch 423, Loss: 0.9638250032\n",
      "Epoch 424, Loss: 0.9898238582\n",
      "Epoch 425, Loss: 0.9733210592\n",
      "Epoch 426, Loss: 0.9037604479\n",
      "Epoch 427, Loss: 0.9162245056\n",
      "Epoch 428, Loss: 0.7598222806\n",
      "Epoch 429, Loss: 0.7599518592\n",
      "Epoch 430, Loss: 0.7764600553\n",
      "Epoch 431, Loss: 0.7723579967\n",
      "Epoch 432, Loss: 0.7593572236\n",
      "Epoch 433, Loss: 0.7844545968\n",
      "Epoch 434, Loss: 0.8186940823\n",
      "Epoch 435, Loss: 0.8537085967\n",
      "Epoch 436, Loss: 0.7824386802\n",
      "Epoch 437, Loss: 0.7603064844\n",
      "Epoch 438, Loss: 0.8089212714\n",
      "Epoch 439, Loss: 0.8637258953\n",
      "Epoch 440, Loss: 0.7909945065\n",
      "Epoch 441, Loss: 0.7377628266\n",
      "Epoch 442, Loss: 0.7390712192\n",
      "Epoch 443, Loss: 0.7672214437\n",
      "Epoch 444, Loss: 0.8190126578\n",
      "Epoch 445, Loss: 0.8577696407\n",
      "Epoch 446, Loss: 0.8239761356\n",
      "Epoch 447, Loss: 0.7704250943\n",
      "Epoch 448, Loss: 0.7885330092\n",
      "Epoch 449, Loss: 0.8220511560\n",
      "Epoch 450, Loss: 0.8463150343\n",
      "Epoch 451, Loss: 0.8104158809\n",
      "Epoch 452, Loss: 0.7883754849\n",
      "Epoch 453, Loss: 0.8203154933\n",
      "Epoch 454, Loss: 0.8883383128\n",
      "Epoch 455, Loss: 0.9118220842\n",
      "Epoch 456, Loss: 0.7923969949\n",
      "Epoch 457, Loss: 0.8011006074\n",
      "Epoch 458, Loss: 0.8659671824\n",
      "Epoch 459, Loss: 0.8654004166\n",
      "Epoch 460, Loss: 0.7549853978\n",
      "Epoch 461, Loss: 0.7337734695\n",
      "Epoch 462, Loss: 0.7466762110\n",
      "Epoch 463, Loss: 0.7506595218\n",
      "Epoch 464, Loss: 0.7607416836\n",
      "Epoch 465, Loss: 0.7799360063\n",
      "Epoch 466, Loss: 0.8136186822\n",
      "Epoch 467, Loss: 0.8369720526\n",
      "Epoch 468, Loss: 0.7961726345\n",
      "Epoch 469, Loss: 0.7420517346\n",
      "Epoch 470, Loss: 0.7885692975\n",
      "Epoch 471, Loss: 0.8665250460\n",
      "Epoch 472, Loss: 0.9165003474\n",
      "Epoch 473, Loss: 0.8937565647\n",
      "Epoch 474, Loss: 0.7575270316\n",
      "Epoch 475, Loss: 0.7338964844\n",
      "Epoch 476, Loss: 0.7296988036\n",
      "Epoch 477, Loss: 0.7599486517\n",
      "Epoch 478, Loss: 0.8045615860\n",
      "Epoch 479, Loss: 0.8141222634\n",
      "Epoch 480, Loss: 0.7506095460\n",
      "Epoch 481, Loss: 0.7249236145\n",
      "Epoch 482, Loss: 0.7286459786\n",
      "Epoch 483, Loss: 0.7398564951\n",
      "Epoch 484, Loss: 0.7440109545\n",
      "Epoch 485, Loss: 0.7512839316\n",
      "Epoch 486, Loss: 0.7668063900\n",
      "Epoch 487, Loss: 0.7913103447\n",
      "Epoch 488, Loss: 0.8157312660\n",
      "Epoch 489, Loss: 0.8268802019\n",
      "Epoch 490, Loss: 0.8014852880\n",
      "Epoch 491, Loss: 0.7727208150\n",
      "Epoch 492, Loss: 0.7713661584\n",
      "Epoch 493, Loss: 0.7830186642\n",
      "Epoch 494, Loss: 0.7766338152\n",
      "Epoch 495, Loss: 0.7774010056\n",
      "Epoch 496, Loss: 0.8042214732\n",
      "Epoch 497, Loss: 0.7936898273\n",
      "Epoch 498, Loss: 0.7644647373\n",
      "Epoch 499, Loss: 0.7565755827\n",
      "Epoch 500, Loss: 0.7615701836\n",
      "Epoch 501, Loss: 0.7728538884\n",
      "Epoch 502, Loss: 0.7832371863\n",
      "Epoch 503, Loss: 0.7907591362\n",
      "Epoch 504, Loss: 0.7982880984\n",
      "Epoch 505, Loss: 0.8080705712\n",
      "Epoch 506, Loss: 0.8192293491\n",
      "Epoch 507, Loss: 0.8293084651\n",
      "Epoch 508, Loss: 0.8368181612\n",
      "Epoch 509, Loss: 0.8413046907\n",
      "Epoch 510, Loss: 0.8424544883\n",
      "Epoch 511, Loss: 0.8395297959\n",
      "Epoch 512, Loss: 0.8335043472\n",
      "Epoch 513, Loss: 0.8557213686\n",
      "Epoch 514, Loss: 0.9747052683\n",
      "Epoch 515, Loss: 0.7667129492\n",
      "Epoch 516, Loss: 0.7031899703\n",
      "Epoch 517, Loss: 0.7833688542\n",
      "Epoch 518, Loss: 0.7765064099\n",
      "Epoch 519, Loss: 0.8085517673\n",
      "Epoch 520, Loss: 0.8081245126\n",
      "Epoch 521, Loss: 0.8673969851\n",
      "Epoch 522, Loss: 0.8973498445\n",
      "Epoch 523, Loss: 0.7795106275\n",
      "Epoch 524, Loss: 0.7187843928\n",
      "Epoch 525, Loss: 0.7197842141\n",
      "Epoch 526, Loss: 0.7629150511\n",
      "Epoch 527, Loss: 0.7925409306\n",
      "Epoch 528, Loss: 0.8009762394\n",
      "Epoch 529, Loss: 0.8028095596\n",
      "Epoch 530, Loss: 0.8115869289\n",
      "Epoch 531, Loss: 0.8251975262\n",
      "Epoch 532, Loss: 0.8552096890\n",
      "Epoch 533, Loss: 0.8674024859\n",
      "Epoch 534, Loss: 1.1382336952\n",
      "Epoch 535, Loss: 0.7862246330\n",
      "Epoch 536, Loss: 0.7252431442\n",
      "Epoch 537, Loss: 0.7167717524\n",
      "Epoch 538, Loss: 0.7273383494\n",
      "Epoch 539, Loss: 0.7402913887\n",
      "Epoch 540, Loss: 0.7610406523\n",
      "Epoch 541, Loss: 0.7722312144\n",
      "Epoch 542, Loss: 0.7802231323\n",
      "Epoch 543, Loss: 0.7670939246\n",
      "Epoch 544, Loss: 0.7372340821\n",
      "Epoch 545, Loss: 0.7030526930\n",
      "Epoch 546, Loss: 0.6822401733\n",
      "Epoch 547, Loss: 0.6848383536\n",
      "Epoch 548, Loss: 0.7005073949\n",
      "Epoch 549, Loss: 0.7231856972\n",
      "Epoch 550, Loss: 0.7332438233\n",
      "Epoch 551, Loss: 0.7228408174\n",
      "Epoch 552, Loss: 0.7164754516\n",
      "Epoch 553, Loss: 0.7179009498\n",
      "Epoch 554, Loss: 0.7177865609\n",
      "Epoch 555, Loss: 0.7153087301\n",
      "Epoch 556, Loss: 0.7143606847\n",
      "Epoch 557, Loss: 0.7144839300\n",
      "Epoch 558, Loss: 0.7150169506\n",
      "Epoch 559, Loss: 0.7151004360\n",
      "Epoch 560, Loss: 0.7130478089\n",
      "Epoch 561, Loss: 0.7078714795\n",
      "Epoch 562, Loss: 0.7005148784\n",
      "Epoch 563, Loss: 0.6936125228\n",
      "Epoch 564, Loss: 0.6892449640\n",
      "Epoch 565, Loss: 0.6878423783\n",
      "Epoch 566, Loss: 0.6887864616\n",
      "Epoch 567, Loss: 0.6912067371\n",
      "Epoch 568, Loss: 0.6948260143\n",
      "Epoch 569, Loss: 0.6995970073\n",
      "Epoch 570, Loss: 0.7042286195\n",
      "Epoch 571, Loss: 0.7096114085\n",
      "Epoch 572, Loss: 0.7391052019\n",
      "Epoch 573, Loss: 0.7418045523\n",
      "Epoch 574, Loss: 0.7515435517\n",
      "Epoch 575, Loss: 0.8433927967\n",
      "Epoch 576, Loss: 0.8793547494\n",
      "Epoch 577, Loss: 0.7687868206\n",
      "Epoch 578, Loss: 0.7099703140\n",
      "Epoch 579, Loss: 0.7189490695\n",
      "Epoch 580, Loss: 0.6759792800\n",
      "Epoch 581, Loss: 0.6681882112\n",
      "Epoch 582, Loss: 0.6614330139\n",
      "Epoch 583, Loss: 0.6715215799\n",
      "Epoch 584, Loss: 0.6865605965\n",
      "Epoch 585, Loss: 0.7299991153\n",
      "Epoch 586, Loss: 0.7297946093\n",
      "Epoch 587, Loss: 0.7206522722\n",
      "Epoch 588, Loss: 0.7258579548\n",
      "Epoch 589, Loss: 0.7362287232\n",
      "Epoch 590, Loss: 0.7608552261\n",
      "Epoch 591, Loss: 0.7158805662\n",
      "Epoch 592, Loss: 1.0765559034\n",
      "Epoch 593, Loss: 0.9294810105\n",
      "Epoch 594, Loss: 0.7829880587\n",
      "Epoch 595, Loss: 0.6852558090\n",
      "Epoch 596, Loss: 0.7148128256\n",
      "Epoch 597, Loss: 0.6914550459\n",
      "Epoch 598, Loss: 0.6830386293\n",
      "Epoch 599, Loss: 0.7044973470\n",
      "Epoch 600, Loss: 0.7263573781\n",
      "Epoch 601, Loss: 0.7374379488\n",
      "Epoch 602, Loss: 0.7240882501\n",
      "Epoch 603, Loss: 0.7041662963\n",
      "Epoch 604, Loss: 0.7007940968\n",
      "Epoch 605, Loss: 0.7093459543\n",
      "Epoch 606, Loss: 0.7145693199\n",
      "Epoch 607, Loss: 0.7146311257\n",
      "Epoch 608, Loss: 0.7062758906\n",
      "Epoch 609, Loss: 0.6813155493\n",
      "Epoch 610, Loss: 0.6660728259\n",
      "Epoch 611, Loss: 0.6892551047\n",
      "Epoch 612, Loss: 0.7278827493\n",
      "Epoch 613, Loss: 0.6592634934\n",
      "Epoch 614, Loss: 0.6645937951\n",
      "Epoch 615, Loss: 0.6970158705\n",
      "Epoch 616, Loss: 0.6914695695\n",
      "Epoch 617, Loss: 0.6587846689\n",
      "Epoch 618, Loss: 0.6647458719\n",
      "Epoch 619, Loss: 0.6696544413\n",
      "Epoch 620, Loss: 0.6801929858\n",
      "Epoch 621, Loss: 0.6927955869\n",
      "Epoch 622, Loss: 0.7585841530\n",
      "Epoch 623, Loss: 0.7607618037\n",
      "Epoch 624, Loss: 0.8627696969\n",
      "Epoch 625, Loss: 0.6495243428\n",
      "Epoch 626, Loss: 0.6654177847\n",
      "Epoch 627, Loss: 0.6949769145\n",
      "Epoch 628, Loss: 0.7275558584\n",
      "Epoch 629, Loss: 0.9978122655\n",
      "Epoch 630, Loss: 0.8319951532\n",
      "Epoch 631, Loss: 0.6897963529\n",
      "Epoch 632, Loss: 0.7137746126\n",
      "Epoch 633, Loss: 0.7520378694\n",
      "Epoch 634, Loss: 0.8148476891\n",
      "Epoch 635, Loss: 0.8120890437\n",
      "Epoch 636, Loss: 0.6961269099\n",
      "Epoch 637, Loss: 0.6574810727\n",
      "Epoch 638, Loss: 0.7362710110\n",
      "Epoch 639, Loss: 0.7452407499\n",
      "Epoch 640, Loss: 0.8181227776\n",
      "Epoch 641, Loss: 0.9569407293\n",
      "Epoch 642, Loss: 0.7547678844\n",
      "Epoch 643, Loss: 0.8164392992\n",
      "Epoch 644, Loss: 0.7143959966\n",
      "Epoch 645, Loss: 0.6858905096\n",
      "Epoch 646, Loss: 0.7127982858\n",
      "Epoch 647, Loss: 0.7068348992\n",
      "Epoch 648, Loss: 0.7025244655\n",
      "Epoch 649, Loss: 0.6858536321\n",
      "Epoch 650, Loss: 0.6802844361\n",
      "Epoch 651, Loss: 0.6922940321\n",
      "Epoch 652, Loss: 0.7053412209\n",
      "Epoch 653, Loss: 0.7139114114\n",
      "Epoch 654, Loss: 0.7248003939\n",
      "Epoch 655, Loss: 0.7337047575\n",
      "Epoch 656, Loss: 0.7286669881\n",
      "Epoch 657, Loss: 0.7025392841\n",
      "Epoch 658, Loss: 0.6687425894\n",
      "Epoch 659, Loss: 0.6449692877\n",
      "Epoch 660, Loss: 0.6430285453\n",
      "Epoch 661, Loss: 0.6677246191\n",
      "Epoch 662, Loss: 0.6709027001\n",
      "Epoch 663, Loss: 0.7173533489\n",
      "Epoch 664, Loss: 0.7154723398\n",
      "Epoch 665, Loss: 0.7428246459\n",
      "Epoch 666, Loss: 0.7367806249\n",
      "Epoch 667, Loss: 0.7356963316\n",
      "Epoch 668, Loss: 0.7084921451\n",
      "Epoch 669, Loss: 0.6830317729\n",
      "Epoch 670, Loss: 0.6933194673\n",
      "Epoch 671, Loss: 0.7032281657\n",
      "Epoch 672, Loss: 0.6899329458\n",
      "Epoch 673, Loss: 0.6927444262\n",
      "Epoch 674, Loss: 0.7095397672\n",
      "Epoch 675, Loss: 0.7306353402\n",
      "Epoch 676, Loss: 0.7257040564\n",
      "Epoch 677, Loss: 0.7315249586\n",
      "Epoch 678, Loss: 0.7709427714\n",
      "Epoch 679, Loss: 0.7740455719\n",
      "Epoch 680, Loss: 0.8263843931\n",
      "Epoch 681, Loss: 0.7610079159\n",
      "Epoch 682, Loss: 0.7276331131\n",
      "Epoch 683, Loss: 0.6829603548\n",
      "Epoch 684, Loss: 0.6722651883\n",
      "Epoch 685, Loss: 0.6590832279\n",
      "Epoch 686, Loss: 0.6813302135\n",
      "Epoch 687, Loss: 0.6460158658\n",
      "Epoch 688, Loss: 0.6253445194\n",
      "Epoch 689, Loss: 0.6389568399\n",
      "Epoch 690, Loss: 0.6763772565\n",
      "Epoch 691, Loss: 0.6950923073\n",
      "Epoch 692, Loss: 0.6910604679\n",
      "Epoch 693, Loss: 0.7130949446\n",
      "Epoch 694, Loss: 0.6915233833\n",
      "Epoch 695, Loss: 0.7464079100\n",
      "Epoch 696, Loss: 0.7610992582\n",
      "Epoch 697, Loss: 0.7230188226\n",
      "Epoch 698, Loss: 0.6613825445\n",
      "Epoch 699, Loss: 0.6722845743\n",
      "Epoch 700, Loss: 0.6869713866\n",
      "Epoch 701, Loss: 0.6734436535\n",
      "Epoch 702, Loss: 0.7104065090\n",
      "Epoch 703, Loss: 0.7292614201\n",
      "Epoch 704, Loss: 0.7287921069\n",
      "Epoch 705, Loss: 0.7290019374\n",
      "Epoch 706, Loss: 0.7116559318\n",
      "Epoch 707, Loss: 0.8044048767\n",
      "Epoch 708, Loss: 0.8522012945\n",
      "Epoch 709, Loss: 0.7008434396\n",
      "Epoch 710, Loss: 0.6164126347\n",
      "Epoch 711, Loss: 0.6306376594\n",
      "Epoch 712, Loss: 0.6693078125\n",
      "Epoch 713, Loss: 0.6387347291\n",
      "Epoch 714, Loss: 0.6391474971\n",
      "Epoch 715, Loss: 0.6405177235\n",
      "Epoch 716, Loss: 0.6795476919\n",
      "Epoch 717, Loss: 0.6331041816\n",
      "Epoch 718, Loss: 0.6249346132\n",
      "Epoch 719, Loss: 0.6078319841\n",
      "Epoch 720, Loss: 0.6074615648\n",
      "Epoch 721, Loss: 0.6242755370\n",
      "Epoch 722, Loss: 0.6658355991\n",
      "Epoch 723, Loss: 0.7388872836\n",
      "Epoch 724, Loss: 0.7634600048\n",
      "Epoch 725, Loss: 0.7173810126\n",
      "Epoch 726, Loss: 0.6965344592\n",
      "Epoch 727, Loss: 0.6886576495\n",
      "Epoch 728, Loss: 0.6777202159\n",
      "Epoch 729, Loss: 0.6691214800\n",
      "Epoch 730, Loss: 0.6602130968\n",
      "Epoch 731, Loss: 0.6568577827\n",
      "Epoch 732, Loss: 0.6608215970\n",
      "Epoch 733, Loss: 0.6644513543\n",
      "Epoch 734, Loss: 0.6743621672\n",
      "Epoch 735, Loss: 0.6899207056\n",
      "Epoch 736, Loss: 0.6824413157\n",
      "Epoch 737, Loss: 0.6606522669\n",
      "Epoch 738, Loss: 0.6348040525\n",
      "Epoch 739, Loss: 0.7307169910\n",
      "Epoch 740, Loss: 0.7485853389\n",
      "Epoch 741, Loss: 0.7884861096\n",
      "Epoch 742, Loss: 0.7155645394\n",
      "Epoch 743, Loss: 0.6261748917\n",
      "Epoch 744, Loss: 0.6151503557\n",
      "Epoch 745, Loss: 0.6470345406\n",
      "Epoch 746, Loss: 0.7022689398\n",
      "Epoch 747, Loss: 0.7677486743\n",
      "Epoch 748, Loss: 0.7181048035\n",
      "Epoch 749, Loss: 0.7205887270\n",
      "Epoch 750, Loss: 0.7432553450\n",
      "Epoch 751, Loss: 0.7563636303\n",
      "Epoch 752, Loss: 1.2571992098\n",
      "Epoch 753, Loss: 0.7797563442\n",
      "Epoch 754, Loss: 0.6863187855\n",
      "Epoch 755, Loss: 0.6780697576\n",
      "Epoch 756, Loss: 0.6879988972\n",
      "Epoch 757, Loss: 0.6129377819\n",
      "Epoch 758, Loss: 0.7079792814\n",
      "Epoch 759, Loss: 0.6544483842\n",
      "Epoch 760, Loss: 0.6177170457\n",
      "Epoch 761, Loss: 0.7089670381\n",
      "Epoch 762, Loss: 0.6649296675\n",
      "Epoch 763, Loss: 0.6430822467\n",
      "Epoch 764, Loss: 0.6732577332\n",
      "Epoch 765, Loss: 0.6755335053\n",
      "Epoch 766, Loss: 0.6615121889\n",
      "Epoch 767, Loss: 0.7078670315\n",
      "Epoch 768, Loss: 0.6576007721\n",
      "Epoch 769, Loss: 0.6308509784\n",
      "Epoch 770, Loss: 0.6534270312\n",
      "Epoch 771, Loss: 0.6628213477\n",
      "Epoch 772, Loss: 0.6535659595\n",
      "Epoch 773, Loss: 0.6453719981\n",
      "Epoch 774, Loss: 0.6290707463\n",
      "Epoch 775, Loss: 0.6631335582\n",
      "Epoch 776, Loss: 0.7539242575\n",
      "Epoch 777, Loss: 0.8399354722\n",
      "Epoch 778, Loss: 0.9706100303\n",
      "Epoch 779, Loss: 0.7479086744\n",
      "Epoch 780, Loss: 0.6313024905\n",
      "Epoch 781, Loss: 0.6586712904\n",
      "Epoch 782, Loss: 0.7037549884\n",
      "Epoch 783, Loss: 0.6284274104\n",
      "Epoch 784, Loss: 0.6294819109\n",
      "Epoch 785, Loss: 0.6476968096\n",
      "Epoch 786, Loss: 0.6747763175\n",
      "Epoch 787, Loss: 0.7343392059\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_weights_init = []\n",
    "best_biases_init = []\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "best_seed = 0\n",
    "\n",
    "for _ in range (1):\n",
    "    nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8)\n",
    "    nn.train(batches, 3, 0.001)\n",
    "    if nn.get_best_loss() < best_loss:\n",
    "        best_loss = nn.get_best_loss()\n",
    "        best_weights = nn.get_best_weights()\n",
    "        best_biases = nn.get_best_biases()\n",
    "        best_seed = nn.get_best_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6074615648081181\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "# print(best_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8, init_weights=best_weights, init_biases=best_biases, init_seed=best_seed)\n",
    "# nn.train(batches, 3, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.get_best_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Number of layers in the Neural Network\n",
    "N = 5  # Example value, replace with the actual number of layers\n",
    "\n",
    "# Initialize the dictionary\n",
    "weights_dict = {\n",
    "    'weights': {},\n",
    "    'bias': {}\n",
    "}\n",
    "\n",
    "weights = nn.get_best_weights()\n",
    "biases = nn.get_best_biases()\n",
    "\n",
    "# Populate the weights and bias dictionaries\n",
    "for i in range(N):\n",
    "    weights_dict['weights'][f'fc{i+1}'] = weights[i]\n",
    "    weights_dict['bias'][f'fc{i+1}'] = biases[i].flatten()\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
