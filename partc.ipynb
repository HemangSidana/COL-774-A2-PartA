{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Remember to import \"numpy_transforms\" functions if you wish to import these two classes in a different script.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "        label = row[\"class\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image), label\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image, label = self.dataset[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "        batch_labels = np.array(labels)\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(128, 625)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    print(labels.shape)  # Should be [batch_size]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For baseline purpose, running part B code in the same script\n",
    "\n",
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# # Sigmoid activation and its derivative\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def sigmoid_derivative(x):\n",
    "#     s = sigmoid(x)\n",
    "#     return s * (1 - s)\n",
    "\n",
    "# def softmax(x, axis=None):\n",
    "#     exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "#     return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# # Cross-entropy loss\n",
    "# def cross_entropy_loss(y_true, y_pred):\n",
    "#     y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "#     return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# # Neural Network Class with Softmax in the Output Layer and Sigmoid in Hidden Layers\n",
    "# class NeuralNetwork_Baseline:\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size):\n",
    "#         np.random.seed(0)\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "\n",
    "#         layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "#         # Initialize weights and biases\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "#             self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         pre_activations = []\n",
    "\n",
    "#         # Pass through each layer except the output layer\n",
    "#         for i in range(len(self.weights) - 1):\n",
    "#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             pre_activations.append(z)\n",
    "#             a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "#             activations.append(a)\n",
    "\n",
    "#         # Pass through the output layer with softmax\n",
    "#         z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         pre_activations.append(z)\n",
    "#         a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "#         activations.append(a)\n",
    "\n",
    "#         return activations, pre_activations\n",
    "\n",
    "#     def backward(self, X, y, activations, pre_activations):\n",
    "#         grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "#         grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "#         # Start with output layer error\n",
    "#         delta = activations[-1] - y\n",
    "\n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "#             grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "#             if i > 0:\n",
    "#                 delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "\n",
    "#         return grad_w, grad_b\n",
    "\n",
    "#     def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "#         for i in range(len(self.weights)):\n",
    "#             self.weights[i] -= learning_rate * grad_w[i]\n",
    "#             self.biases[i] -= learning_rate * grad_b[i]\n",
    "\n",
    "#     def train(self, batches, learning_rate):\n",
    "#         start_time = time.time()\n",
    "#         epoch = 0\n",
    "#         while (True):\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 activations, pre_activations = self.forward(X_batch)\n",
    "#                 grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "#                 self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "#             # Calculate average loss over batches\n",
    "#             loss = 0\n",
    "#             z = 0\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 y_pred, _ = self.forward(X_batch)\n",
    "#                 loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "#                 z += len(y_pred[-1])\n",
    "#             loss /= z\n",
    "#             print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "#             epoch += 1\n",
    "#             if (time.time() - start_time) > 60:\n",
    "#                 break\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "#     def get_weights(self):\n",
    "#         return self.weights\n",
    "\n",
    "#     def get_biases(self):\n",
    "#         return self.biases\n",
    "\n",
    "# # Example usage:\n",
    "# nn = NeuralNetwork_Baseline(625, [512, 256, 128, 32], 8)\n",
    "# nn.train(batches, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "# import copy\n",
    "\n",
    "# Sigmoid activation and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Neural Network Class with Softmax in the Output Layer and Sigmoid in Hidden Layers\n",
    "class NeuralNetwork_Adam:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, init_weights = None, init_biases = None, init_seed = None, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        # if (init_seed is None):\n",
    "        #     self.best_seed = int(time.time())\n",
    "        #     np.random.seed(self.best_seed)\n",
    "        # else:\n",
    "        #     np.random.seed(init_seed)\n",
    "        np.random.seed(0)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.m_w = []\n",
    "        self.v_w = []\n",
    "        self.m_b = []\n",
    "        self.v_b = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step for Adam\n",
    "        self.best_weights = []\n",
    "        self.best_biases = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if (init_weights is not None) and (init_biases is not None):\n",
    "                self.weights.append(init_weights[i])\n",
    "                self.biases.append(init_biases[i])\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "                self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "            self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "            # self.best_weights = self.weights\n",
    "            # self.best_biases = self.biases\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each layer except the output layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        self.t += 1  # Increment time step for Adam\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def train(self, batches, time_of_running, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while(epoch<1880):\n",
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
    "            loss /= z\n",
    "            \n",
    "            if (loss < self.best_loss):\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = [np.copy(w) for w in self.weights]  # Use np.copy to create independent copies\n",
    "                self.best_biases = [np.copy(b) for b in self.biases]  # Use np.copy for biases\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            epoch += 1\n",
    "            if time.time() - start_time > 60*time_of_running:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    \n",
    "    def get_best_biases(self):\n",
    "        return self.best_biases\n",
    "    \n",
    "    def get_best_loss(self):\n",
    "        return self.best_loss\n",
    "    \n",
    "    # def get_best_seed(self):\n",
    "    #     return self.best_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0926818227\n",
      "Epoch 2, Loss: 2.0819257024\n",
      "Epoch 3, Loss: 2.0812945793\n",
      "Epoch 4, Loss: 2.0790259720\n",
      "Epoch 5, Loss: 2.0779594084\n",
      "Epoch 6, Loss: 2.0762348022\n",
      "Epoch 7, Loss: 2.0735369435\n",
      "Epoch 8, Loss: 2.0708790493\n",
      "Epoch 9, Loss: 2.0683421468\n",
      "Epoch 10, Loss: 2.0658023146\n",
      "Epoch 11, Loss: 2.0607006061\n",
      "Epoch 12, Loss: 2.0508949941\n",
      "Epoch 13, Loss: 2.0324973727\n",
      "Epoch 14, Loss: 2.0005332596\n",
      "Epoch 15, Loss: 1.9420067377\n",
      "Epoch 16, Loss: 1.9219856443\n",
      "Epoch 17, Loss: 1.8436966249\n",
      "Epoch 18, Loss: 1.7944831792\n",
      "Epoch 19, Loss: 1.7526193634\n",
      "Epoch 20, Loss: 1.7378466578\n",
      "Epoch 21, Loss: 1.6744006855\n",
      "Epoch 22, Loss: 1.6232124826\n",
      "Epoch 23, Loss: 1.5793994640\n",
      "Epoch 24, Loss: 1.5495773344\n",
      "Epoch 25, Loss: 1.5417146101\n",
      "Epoch 26, Loss: 1.5085892098\n",
      "Epoch 27, Loss: 1.4920912484\n",
      "Epoch 28, Loss: 1.4462005270\n",
      "Epoch 29, Loss: 1.4580501029\n",
      "Epoch 30, Loss: 1.4321023100\n",
      "Epoch 31, Loss: 1.4461850705\n",
      "Epoch 32, Loss: 1.3932146462\n",
      "Epoch 33, Loss: 1.4275718761\n",
      "Epoch 34, Loss: 1.3760990636\n",
      "Epoch 35, Loss: 1.3714031818\n",
      "Epoch 36, Loss: 1.3549908504\n",
      "Epoch 37, Loss: 1.3575575079\n",
      "Epoch 38, Loss: 1.3347977647\n",
      "Epoch 39, Loss: 1.3249143350\n",
      "Epoch 40, Loss: 1.3332214534\n",
      "Epoch 41, Loss: 1.3846206799\n",
      "Epoch 42, Loss: 1.3554321289\n",
      "Epoch 43, Loss: 1.3300838352\n",
      "Epoch 44, Loss: 1.3008415557\n",
      "Epoch 45, Loss: 1.2956595824\n",
      "Epoch 46, Loss: 1.3255619486\n",
      "Epoch 47, Loss: 1.3299933415\n",
      "Epoch 48, Loss: 1.3055549650\n",
      "Epoch 49, Loss: 1.3050241827\n",
      "Epoch 50, Loss: 1.2532128917\n",
      "Epoch 51, Loss: 1.2437731429\n",
      "Epoch 52, Loss: 1.2487967925\n",
      "Epoch 53, Loss: 1.2266167905\n",
      "Epoch 54, Loss: 1.2549605947\n",
      "Epoch 55, Loss: 1.2415318365\n",
      "Epoch 56, Loss: 1.2366288273\n",
      "Epoch 57, Loss: 1.2264130855\n",
      "Epoch 58, Loss: 1.2117353386\n",
      "Epoch 59, Loss: 1.2030777325\n",
      "Epoch 60, Loss: 1.2146030837\n",
      "Epoch 61, Loss: 1.2407251391\n",
      "Epoch 62, Loss: 1.3249208432\n",
      "Epoch 63, Loss: 1.3392351615\n",
      "Epoch 64, Loss: 1.2520066125\n",
      "Epoch 65, Loss: 1.2192421482\n",
      "Epoch 66, Loss: 1.2095278562\n",
      "Epoch 67, Loss: 1.1596299013\n",
      "Epoch 68, Loss: 1.1603410246\n",
      "Epoch 69, Loss: 1.1637829271\n",
      "Epoch 70, Loss: 1.1850193434\n",
      "Epoch 71, Loss: 1.2073673726\n",
      "Epoch 72, Loss: 1.1559479407\n",
      "Epoch 73, Loss: 1.1258601233\n",
      "Epoch 74, Loss: 1.1388587875\n",
      "Epoch 75, Loss: 1.1266865886\n",
      "Epoch 76, Loss: 1.1299518827\n",
      "Epoch 77, Loss: 1.1577619791\n",
      "Epoch 78, Loss: 1.2302959250\n",
      "Epoch 79, Loss: 1.1464947694\n",
      "Epoch 80, Loss: 1.1022869579\n",
      "Epoch 81, Loss: 1.1091795059\n",
      "Epoch 82, Loss: 1.1067884878\n",
      "Epoch 83, Loss: 1.0940062741\n",
      "Epoch 84, Loss: 1.1000176625\n",
      "Epoch 85, Loss: 1.1240826752\n",
      "Epoch 86, Loss: 1.1448133286\n",
      "Epoch 87, Loss: 1.1150554994\n",
      "Epoch 88, Loss: 1.1045922501\n",
      "Epoch 89, Loss: 1.0970059838\n",
      "Epoch 90, Loss: 1.1584202487\n",
      "Epoch 91, Loss: 1.1154663959\n",
      "Epoch 92, Loss: 1.1421615996\n",
      "Epoch 93, Loss: 1.0849722734\n",
      "Epoch 94, Loss: 1.1004276670\n",
      "Epoch 95, Loss: 1.0582825632\n",
      "Epoch 96, Loss: 1.0560314829\n",
      "Epoch 97, Loss: 1.0589662720\n",
      "Epoch 98, Loss: 1.0613506492\n",
      "Epoch 99, Loss: 1.0632692883\n",
      "Epoch 100, Loss: 1.0697849254\n",
      "Epoch 101, Loss: 1.0933494530\n",
      "Epoch 102, Loss: 1.0954009519\n",
      "Epoch 103, Loss: 1.0683801761\n",
      "Epoch 104, Loss: 1.0454001520\n",
      "Epoch 105, Loss: 1.0407327368\n",
      "Epoch 106, Loss: 1.0278405967\n",
      "Epoch 107, Loss: 1.0208759054\n",
      "Epoch 108, Loss: 1.0277542197\n",
      "Epoch 109, Loss: 1.0362520590\n",
      "Epoch 110, Loss: 1.0436145771\n",
      "Epoch 111, Loss: 1.0439706501\n",
      "Epoch 112, Loss: 1.0531863286\n",
      "Epoch 113, Loss: 1.0636788757\n",
      "Epoch 114, Loss: 1.0541547246\n",
      "Epoch 115, Loss: 1.0946988340\n",
      "Epoch 116, Loss: 1.1044050782\n",
      "Epoch 117, Loss: 1.0855069987\n",
      "Epoch 118, Loss: 1.0294796075\n",
      "Epoch 119, Loss: 1.0271870520\n",
      "Epoch 120, Loss: 1.0282638575\n",
      "Epoch 121, Loss: 1.0225519398\n",
      "Epoch 122, Loss: 1.0103535278\n",
      "Epoch 123, Loss: 1.0146115876\n",
      "Epoch 124, Loss: 1.0205568641\n",
      "Epoch 125, Loss: 1.0449410354\n",
      "Epoch 126, Loss: 0.9943382219\n",
      "Epoch 127, Loss: 0.9937224255\n",
      "Epoch 128, Loss: 0.9938391854\n",
      "Epoch 129, Loss: 1.0007638454\n",
      "Epoch 130, Loss: 0.9955048687\n",
      "Epoch 131, Loss: 1.0099160538\n",
      "Epoch 132, Loss: 0.9934150586\n",
      "Epoch 133, Loss: 0.9722506585\n",
      "Epoch 134, Loss: 0.9893142397\n",
      "Epoch 135, Loss: 1.0070858787\n",
      "Epoch 136, Loss: 0.9702695970\n",
      "Epoch 137, Loss: 0.9657840700\n",
      "Epoch 138, Loss: 0.9979836027\n",
      "Epoch 139, Loss: 1.0068685025\n",
      "Epoch 140, Loss: 1.0102832705\n",
      "Epoch 141, Loss: 1.0246342429\n",
      "Epoch 142, Loss: 1.0404402963\n",
      "Epoch 143, Loss: 1.0460310854\n",
      "Epoch 144, Loss: 0.9970559367\n",
      "Epoch 145, Loss: 0.9777902766\n",
      "Epoch 146, Loss: 0.9537366603\n",
      "Epoch 147, Loss: 0.9536552604\n",
      "Epoch 148, Loss: 0.9926302428\n",
      "Epoch 149, Loss: 1.0023275424\n",
      "Epoch 150, Loss: 0.9715401020\n",
      "Epoch 151, Loss: 0.9899697466\n",
      "Epoch 152, Loss: 1.0654430763\n",
      "Epoch 153, Loss: 0.9665665492\n",
      "Epoch 154, Loss: 0.9782564999\n",
      "Epoch 155, Loss: 0.9600275136\n",
      "Epoch 156, Loss: 0.9445793914\n",
      "Epoch 157, Loss: 0.9545069708\n",
      "Epoch 158, Loss: 0.9512181774\n",
      "Epoch 159, Loss: 0.9510295591\n",
      "Epoch 160, Loss: 0.9515298581\n",
      "Epoch 161, Loss: 0.9533542675\n",
      "Epoch 162, Loss: 0.9618881219\n",
      "Epoch 163, Loss: 0.9720403097\n",
      "Epoch 164, Loss: 1.0107496563\n",
      "Epoch 165, Loss: 1.0718951152\n",
      "Epoch 166, Loss: 1.0239146038\n",
      "Epoch 167, Loss: 0.9629880708\n",
      "Epoch 168, Loss: 0.9438309010\n",
      "Epoch 169, Loss: 0.9883265973\n",
      "Epoch 170, Loss: 0.9861646117\n",
      "Epoch 171, Loss: 0.9745083644\n",
      "Epoch 172, Loss: 0.9371655554\n",
      "Epoch 173, Loss: 0.9461782911\n",
      "Epoch 174, Loss: 0.9356871600\n",
      "Epoch 175, Loss: 0.9534306090\n",
      "Epoch 176, Loss: 0.9547902629\n",
      "Epoch 177, Loss: 0.9623298964\n",
      "Epoch 178, Loss: 0.9540769836\n",
      "Epoch 179, Loss: 0.9403479777\n",
      "Epoch 180, Loss: 1.0040174593\n",
      "Epoch 181, Loss: 1.0129548804\n",
      "Epoch 182, Loss: 0.8962774427\n",
      "Epoch 183, Loss: 0.9770899636\n",
      "Epoch 184, Loss: 0.9007362317\n",
      "Epoch 185, Loss: 0.9323705726\n",
      "Epoch 186, Loss: 0.9416111893\n",
      "Epoch 187, Loss: 0.9301588621\n",
      "Epoch 188, Loss: 0.9542618447\n",
      "Epoch 189, Loss: 0.9332465754\n",
      "Epoch 190, Loss: 0.9729648862\n",
      "Epoch 191, Loss: 1.0354553087\n",
      "Epoch 192, Loss: 0.9103068655\n",
      "Epoch 193, Loss: 1.0444314861\n",
      "Epoch 194, Loss: 0.8937138813\n",
      "Epoch 195, Loss: 0.9118849771\n",
      "Epoch 196, Loss: 1.0539509683\n",
      "Epoch 197, Loss: 0.9655628021\n",
      "Epoch 198, Loss: 0.8983502977\n",
      "Epoch 199, Loss: 1.0405217659\n",
      "Epoch 200, Loss: 1.0015310613\n",
      "Epoch 201, Loss: 0.9207748846\n",
      "Epoch 202, Loss: 0.8738966721\n",
      "Epoch 203, Loss: 0.9095561684\n",
      "Epoch 204, Loss: 0.9215985002\n",
      "Epoch 205, Loss: 0.9425314910\n",
      "Epoch 206, Loss: 0.9138559965\n",
      "Epoch 207, Loss: 0.8949156483\n",
      "Epoch 208, Loss: 0.9240973725\n",
      "Epoch 209, Loss: 0.8968893171\n",
      "Epoch 210, Loss: 0.8768038205\n",
      "Epoch 211, Loss: 0.8682644331\n",
      "Epoch 212, Loss: 0.8692602649\n",
      "Epoch 213, Loss: 0.8961382597\n",
      "Epoch 214, Loss: 0.8824678404\n",
      "Epoch 215, Loss: 0.9013659480\n",
      "Epoch 216, Loss: 0.8998717046\n",
      "Epoch 217, Loss: 0.9418971011\n",
      "Epoch 218, Loss: 0.9349361522\n",
      "Epoch 219, Loss: 0.9080981999\n",
      "Epoch 220, Loss: 0.8646629075\n",
      "Epoch 221, Loss: 0.8742594424\n",
      "Epoch 222, Loss: 0.8367644551\n",
      "Epoch 223, Loss: 0.8304613196\n",
      "Epoch 224, Loss: 0.8160456891\n",
      "Epoch 225, Loss: 0.8257546960\n",
      "Epoch 226, Loss: 0.8368854777\n",
      "Epoch 227, Loss: 0.8686427858\n",
      "Epoch 228, Loss: 0.8727525890\n",
      "Epoch 229, Loss: 0.8562513079\n",
      "Epoch 230, Loss: 0.8557869425\n",
      "Epoch 231, Loss: 0.8233924996\n",
      "Epoch 232, Loss: 0.8409346725\n",
      "Epoch 233, Loss: 0.8312699184\n",
      "Epoch 234, Loss: 0.8501327265\n",
      "Epoch 235, Loss: 0.8250482513\n",
      "Epoch 236, Loss: 0.8009113962\n",
      "Epoch 237, Loss: 0.8086099960\n",
      "Epoch 238, Loss: 0.8222997837\n",
      "Epoch 239, Loss: 0.8492502792\n",
      "Epoch 240, Loss: 0.8703481887\n",
      "Epoch 241, Loss: 0.8312902512\n",
      "Epoch 242, Loss: 0.8698172034\n",
      "Epoch 243, Loss: 0.8134469831\n",
      "Epoch 244, Loss: 0.8038639879\n",
      "Epoch 245, Loss: 0.8166010358\n",
      "Epoch 246, Loss: 0.8227027236\n",
      "Epoch 247, Loss: 0.8099753792\n",
      "Epoch 248, Loss: 0.7909117421\n",
      "Epoch 249, Loss: 0.7862663247\n",
      "Epoch 250, Loss: 0.7802493475\n",
      "Epoch 251, Loss: 0.8005496957\n",
      "Epoch 252, Loss: 0.7822900971\n",
      "Epoch 253, Loss: 0.7738254875\n",
      "Epoch 254, Loss: 0.7711432001\n",
      "Epoch 255, Loss: 0.7717193434\n",
      "Epoch 256, Loss: 0.7819385071\n",
      "Epoch 257, Loss: 0.7764944646\n",
      "Epoch 258, Loss: 0.7586508424\n",
      "Epoch 259, Loss: 0.7642678700\n",
      "Epoch 260, Loss: 0.7715957425\n",
      "Epoch 261, Loss: 0.7696933476\n",
      "Epoch 262, Loss: 0.7617000322\n",
      "Epoch 263, Loss: 0.7601507440\n",
      "Epoch 264, Loss: 0.7741475353\n",
      "Epoch 265, Loss: 0.7650481343\n",
      "Epoch 266, Loss: 0.7630129831\n",
      "Epoch 267, Loss: 0.7617724903\n",
      "Epoch 268, Loss: 0.7498547387\n",
      "Epoch 269, Loss: 0.7641194162\n",
      "Epoch 270, Loss: 0.7528722760\n",
      "Epoch 271, Loss: 0.7468060602\n",
      "Epoch 272, Loss: 0.7587018394\n",
      "Epoch 273, Loss: 0.7564478922\n",
      "Epoch 274, Loss: 0.7489519551\n",
      "Epoch 275, Loss: 0.7445947856\n",
      "Epoch 276, Loss: 0.7350522268\n",
      "Epoch 277, Loss: 0.7418650999\n",
      "Epoch 278, Loss: 0.7474176190\n",
      "Epoch 279, Loss: 0.7407890877\n",
      "Epoch 280, Loss: 0.7447151008\n",
      "Epoch 281, Loss: 0.7326396568\n",
      "Epoch 282, Loss: 0.7273285673\n",
      "Epoch 283, Loss: 0.7527692670\n",
      "Epoch 284, Loss: 0.7432050355\n",
      "Epoch 285, Loss: 0.8124149891\n",
      "Epoch 286, Loss: 0.7409412522\n",
      "Epoch 287, Loss: 0.7306071084\n",
      "Epoch 288, Loss: 0.7340566165\n",
      "Epoch 289, Loss: 0.7211677172\n",
      "Epoch 290, Loss: 0.7128620193\n",
      "Epoch 291, Loss: 0.7142147748\n",
      "Epoch 292, Loss: 0.7173697255\n",
      "Epoch 293, Loss: 0.7210394819\n",
      "Epoch 294, Loss: 0.7172169001\n",
      "Epoch 295, Loss: 0.7101867330\n",
      "Epoch 296, Loss: 0.7150793184\n",
      "Epoch 297, Loss: 0.7409773899\n",
      "Epoch 298, Loss: 0.7610295054\n",
      "Epoch 299, Loss: 0.7119337976\n",
      "Epoch 300, Loss: 0.9007413529\n",
      "Epoch 301, Loss: 0.8128797760\n",
      "Epoch 302, Loss: 0.9818312356\n",
      "Epoch 303, Loss: 0.9232874568\n",
      "Epoch 304, Loss: 0.8769388834\n",
      "Epoch 305, Loss: 0.9033099213\n",
      "Epoch 306, Loss: 0.8741968141\n",
      "Epoch 307, Loss: 1.0897084527\n",
      "Epoch 308, Loss: 0.8241100191\n",
      "Epoch 309, Loss: 0.7948904248\n",
      "Epoch 310, Loss: 0.9848300769\n",
      "Epoch 311, Loss: 0.7867693777\n",
      "Epoch 312, Loss: 0.8152699270\n",
      "Epoch 313, Loss: 0.8293281553\n",
      "Epoch 314, Loss: 0.7484717672\n",
      "Epoch 315, Loss: 0.7600563497\n",
      "Epoch 316, Loss: 0.7150090565\n",
      "Epoch 317, Loss: 0.7167831190\n",
      "Epoch 318, Loss: 0.7219296468\n",
      "Epoch 319, Loss: 0.7458544235\n",
      "Epoch 320, Loss: 0.7165740644\n",
      "Epoch 321, Loss: 0.7503433852\n",
      "Epoch 322, Loss: 0.7926790841\n",
      "Epoch 323, Loss: 0.9438251443\n",
      "Epoch 324, Loss: 0.7240597584\n",
      "Epoch 325, Loss: 0.7278589807\n",
      "Epoch 326, Loss: 0.7507662019\n",
      "Epoch 327, Loss: 0.7966965567\n",
      "Epoch 328, Loss: 0.7565572853\n",
      "Epoch 329, Loss: 0.7800413162\n",
      "Epoch 330, Loss: 0.7444147840\n",
      "Epoch 331, Loss: 0.7397429319\n",
      "Epoch 332, Loss: 0.7404560248\n",
      "Epoch 333, Loss: 0.7136641630\n",
      "Epoch 334, Loss: 0.7093106164\n",
      "Epoch 335, Loss: 0.7003990883\n",
      "Epoch 336, Loss: 0.7042398784\n",
      "Epoch 337, Loss: 0.7493750697\n",
      "Epoch 338, Loss: 0.8386943269\n",
      "Epoch 339, Loss: 0.6922162840\n",
      "Epoch 340, Loss: 0.8324068246\n",
      "Epoch 341, Loss: 0.7108846774\n",
      "Epoch 342, Loss: 0.7242375019\n",
      "Epoch 343, Loss: 0.6964696402\n",
      "Epoch 344, Loss: 0.6975834465\n",
      "Epoch 345, Loss: 0.6845336244\n",
      "Epoch 346, Loss: 0.6851159236\n",
      "Epoch 347, Loss: 0.6898277234\n",
      "Epoch 348, Loss: 0.6861768607\n",
      "Epoch 349, Loss: 0.6866864821\n",
      "Epoch 350, Loss: 0.6746158954\n",
      "Epoch 351, Loss: 0.6776576612\n",
      "Epoch 352, Loss: 0.6653498969\n",
      "Epoch 353, Loss: 0.6662617419\n",
      "Epoch 354, Loss: 0.6713161371\n",
      "Epoch 355, Loss: 0.6534262996\n",
      "Epoch 356, Loss: 0.6669897490\n",
      "Epoch 357, Loss: 0.6690362049\n",
      "Epoch 358, Loss: 0.6639714135\n",
      "Epoch 359, Loss: 0.6808103419\n",
      "Epoch 360, Loss: 0.6780678240\n",
      "Epoch 361, Loss: 0.6471183221\n",
      "Epoch 362, Loss: 0.6626763892\n",
      "Epoch 363, Loss: 0.6605326243\n",
      "Epoch 364, Loss: 0.6633610404\n",
      "Epoch 365, Loss: 0.6595590555\n",
      "Epoch 366, Loss: 0.6795081130\n",
      "Epoch 367, Loss: 0.6891739972\n",
      "Epoch 368, Loss: 0.6691242725\n",
      "Epoch 369, Loss: 0.6434550501\n",
      "Epoch 370, Loss: 0.6438195414\n",
      "Epoch 371, Loss: 0.6869178975\n",
      "Epoch 372, Loss: 0.6843199265\n",
      "Epoch 373, Loss: 0.7323959406\n",
      "Epoch 374, Loss: 0.6737567092\n",
      "Epoch 375, Loss: 0.6911832745\n",
      "Epoch 376, Loss: 0.6758219777\n",
      "Epoch 377, Loss: 0.6452968866\n",
      "Epoch 378, Loss: 0.6733121068\n",
      "Epoch 379, Loss: 0.6475342523\n",
      "Epoch 380, Loss: 0.6467751222\n",
      "Epoch 381, Loss: 0.6665525600\n",
      "Epoch 382, Loss: 0.8385451670\n",
      "Epoch 383, Loss: 0.7793217536\n",
      "Epoch 384, Loss: 0.7079098537\n",
      "Epoch 385, Loss: 0.6887739095\n",
      "Epoch 386, Loss: 0.6843524791\n",
      "Epoch 387, Loss: 0.6374808484\n",
      "Epoch 388, Loss: 0.6342042007\n",
      "Epoch 389, Loss: 0.6360077479\n",
      "Epoch 390, Loss: 0.6544662102\n",
      "Epoch 391, Loss: 0.7479840037\n",
      "Epoch 392, Loss: 0.8038501311\n",
      "Epoch 393, Loss: 0.6899701217\n",
      "Epoch 394, Loss: 0.7419991494\n",
      "Epoch 395, Loss: 0.6617287971\n",
      "Epoch 396, Loss: 0.6316835823\n",
      "Epoch 397, Loss: 0.6772046522\n",
      "Epoch 398, Loss: 0.6349940478\n",
      "Epoch 399, Loss: 0.6149165597\n",
      "Epoch 400, Loss: 0.6291725134\n",
      "Epoch 401, Loss: 0.6576501848\n",
      "Epoch 402, Loss: 0.6481812102\n",
      "Epoch 403, Loss: 0.6495548506\n",
      "Epoch 404, Loss: 0.6082487384\n",
      "Epoch 405, Loss: 0.6064436007\n",
      "Epoch 406, Loss: 0.6395575379\n",
      "Epoch 407, Loss: 0.6394517488\n",
      "Epoch 408, Loss: 0.6378561167\n",
      "Epoch 409, Loss: 0.6112796778\n",
      "Epoch 410, Loss: 0.6296918656\n",
      "Epoch 411, Loss: 0.7094907107\n",
      "Epoch 412, Loss: 0.6466531158\n",
      "Epoch 413, Loss: 0.6183902201\n",
      "Epoch 414, Loss: 0.6613818351\n",
      "Epoch 415, Loss: 0.6650829195\n",
      "Epoch 416, Loss: 0.6331195035\n",
      "Epoch 417, Loss: 0.6657471996\n",
      "Epoch 418, Loss: 0.8217283493\n",
      "Epoch 419, Loss: 0.6843578383\n",
      "Epoch 420, Loss: 0.7615124235\n",
      "Epoch 421, Loss: 0.6325011733\n",
      "Epoch 422, Loss: 0.6404715076\n",
      "Epoch 423, Loss: 0.6913189713\n",
      "Epoch 424, Loss: 0.6542863108\n",
      "Epoch 425, Loss: 0.6388668690\n",
      "Epoch 426, Loss: 0.6268740926\n",
      "Epoch 427, Loss: 0.6956700555\n",
      "Epoch 428, Loss: 0.6584306637\n",
      "Epoch 429, Loss: 0.7404503858\n",
      "Epoch 430, Loss: 0.6364033032\n",
      "Epoch 431, Loss: 0.5962245619\n",
      "Epoch 432, Loss: 0.6922403387\n",
      "Epoch 433, Loss: 0.6646438849\n",
      "Epoch 434, Loss: 0.6020415937\n",
      "Epoch 435, Loss: 0.6362202718\n",
      "Epoch 436, Loss: 0.6310786771\n",
      "Epoch 437, Loss: 0.6200935048\n",
      "Epoch 438, Loss: 0.6312939600\n",
      "Epoch 439, Loss: 0.6837231107\n",
      "Epoch 440, Loss: 0.6698002029\n",
      "Epoch 441, Loss: 0.6454227885\n",
      "Epoch 442, Loss: 0.6145500268\n",
      "Epoch 443, Loss: 0.6033690153\n",
      "Epoch 444, Loss: 0.6224254809\n",
      "Epoch 445, Loss: 0.6248762201\n",
      "Epoch 446, Loss: 0.6209884103\n",
      "Epoch 447, Loss: 0.7299874119\n",
      "Epoch 448, Loss: 0.6467095973\n",
      "Epoch 449, Loss: 0.7103936406\n",
      "Epoch 450, Loss: 0.6178102227\n",
      "Epoch 451, Loss: 0.6846978135\n",
      "Epoch 452, Loss: 0.6627671324\n",
      "Epoch 453, Loss: 0.6136927156\n",
      "Epoch 454, Loss: 0.6106129079\n",
      "Epoch 455, Loss: 0.5875463996\n",
      "Epoch 456, Loss: 0.6300030630\n",
      "Epoch 457, Loss: 0.5903404153\n",
      "Epoch 458, Loss: 0.6248445703\n",
      "Epoch 459, Loss: 0.7098042328\n",
      "Epoch 460, Loss: 0.6195890427\n",
      "Epoch 461, Loss: 0.6060718862\n",
      "Epoch 462, Loss: 0.7121205389\n",
      "Epoch 463, Loss: 0.7362460276\n",
      "Epoch 464, Loss: 0.6200943995\n",
      "Epoch 465, Loss: 0.7210447914\n",
      "Epoch 466, Loss: 0.6499800596\n",
      "Epoch 467, Loss: 0.7044475597\n",
      "Epoch 468, Loss: 0.6180480755\n",
      "Epoch 469, Loss: 0.7281740331\n",
      "Epoch 470, Loss: 0.6626280567\n",
      "Epoch 471, Loss: 0.6389149546\n",
      "Epoch 472, Loss: 0.6155522744\n",
      "Epoch 473, Loss: 0.6934265718\n",
      "Epoch 474, Loss: 0.6771031698\n",
      "Epoch 475, Loss: 0.6076626647\n",
      "Epoch 476, Loss: 0.6143450175\n",
      "Epoch 477, Loss: 0.6233820283\n",
      "Epoch 478, Loss: 0.6366601068\n",
      "Epoch 479, Loss: 0.6370317153\n",
      "Epoch 480, Loss: 0.6467989103\n",
      "Epoch 481, Loss: 0.6462441948\n",
      "Epoch 482, Loss: 0.6150378631\n",
      "Epoch 483, Loss: 0.5983699000\n",
      "Epoch 484, Loss: 0.6102048701\n",
      "Epoch 485, Loss: 0.5868782563\n",
      "Epoch 486, Loss: 0.6135726404\n",
      "Epoch 487, Loss: 0.6246766518\n",
      "Epoch 488, Loss: 0.6307459161\n",
      "Epoch 489, Loss: 0.6448369739\n",
      "Epoch 490, Loss: 0.6801557774\n",
      "Epoch 491, Loss: 0.7445281669\n",
      "Epoch 492, Loss: 0.6341848636\n",
      "Epoch 493, Loss: 0.8917551348\n",
      "Epoch 494, Loss: 0.7152891821\n",
      "Epoch 495, Loss: 0.9026194300\n",
      "Epoch 496, Loss: 1.0177136342\n",
      "Epoch 497, Loss: 0.9982777960\n",
      "Epoch 498, Loss: 0.7645949124\n",
      "Epoch 499, Loss: 0.9030414195\n",
      "Epoch 500, Loss: 0.8356549345\n",
      "Epoch 501, Loss: 0.7230913420\n",
      "Epoch 502, Loss: 0.7330889821\n",
      "Epoch 503, Loss: 0.6402415512\n",
      "Epoch 504, Loss: 0.6891825532\n",
      "Epoch 505, Loss: 0.7087970547\n",
      "Epoch 506, Loss: 0.6846964359\n",
      "Epoch 507, Loss: 0.6460150893\n",
      "Epoch 508, Loss: 0.6786601765\n",
      "Epoch 509, Loss: 0.7281722366\n",
      "Epoch 510, Loss: 0.7460474390\n",
      "Epoch 511, Loss: 0.6213433523\n",
      "Epoch 512, Loss: 0.7145806590\n",
      "Epoch 513, Loss: 0.9050030583\n",
      "Epoch 514, Loss: 0.6847358281\n",
      "Epoch 515, Loss: 0.9708979401\n",
      "Epoch 516, Loss: 0.7153740259\n",
      "Epoch 517, Loss: 0.7140443980\n",
      "Epoch 518, Loss: 0.7329604067\n",
      "Epoch 519, Loss: 0.6447812459\n",
      "Epoch 520, Loss: 0.6219280013\n",
      "Epoch 521, Loss: 0.6512356387\n",
      "Epoch 522, Loss: 0.5890891199\n",
      "Epoch 523, Loss: 0.6286161719\n",
      "Epoch 524, Loss: 0.7243989399\n",
      "Epoch 525, Loss: 0.6513526081\n",
      "Epoch 526, Loss: 0.6581708049\n",
      "Epoch 527, Loss: 0.6529675164\n",
      "Epoch 528, Loss: 0.6754930894\n",
      "Epoch 529, Loss: 0.6165671701\n",
      "Epoch 530, Loss: 0.5968030226\n",
      "Epoch 531, Loss: 0.6246673318\n",
      "Epoch 532, Loss: 0.6529845694\n",
      "Epoch 533, Loss: 0.6141469736\n",
      "Epoch 534, Loss: 0.6073467282\n",
      "Epoch 535, Loss: 0.5648233579\n",
      "Epoch 536, Loss: 0.5365098290\n",
      "Epoch 537, Loss: 0.5573310306\n",
      "Epoch 538, Loss: 0.5703060663\n",
      "Epoch 539, Loss: 0.5850081965\n",
      "Epoch 540, Loss: 0.6136051158\n",
      "Epoch 541, Loss: 0.6600636618\n",
      "Epoch 542, Loss: 0.6001188567\n",
      "Epoch 543, Loss: 0.5531086185\n",
      "Epoch 544, Loss: 0.5216776256\n",
      "Epoch 545, Loss: 0.5660255166\n",
      "Epoch 546, Loss: 0.5926477861\n",
      "Epoch 547, Loss: 0.5432976437\n",
      "Epoch 548, Loss: 0.6137251199\n",
      "Epoch 549, Loss: 0.6462584481\n",
      "Epoch 550, Loss: 0.6412577078\n",
      "Epoch 551, Loss: 0.6484870174\n",
      "Epoch 552, Loss: 0.6447820471\n",
      "Epoch 553, Loss: 0.5578535967\n",
      "Epoch 554, Loss: 0.5528125332\n",
      "Epoch 555, Loss: 0.5138731741\n",
      "Epoch 556, Loss: 0.5174215909\n",
      "Epoch 557, Loss: 0.5418794764\n",
      "Epoch 558, Loss: 0.5659649503\n",
      "Epoch 559, Loss: 0.6226698022\n",
      "Epoch 560, Loss: 0.7049862682\n",
      "Epoch 561, Loss: 0.5239998684\n",
      "Epoch 562, Loss: 0.5573007040\n",
      "Epoch 563, Loss: 0.5423472915\n",
      "Epoch 564, Loss: 0.5767676717\n",
      "Epoch 565, Loss: 0.5347196801\n",
      "Epoch 566, Loss: 0.5230427191\n",
      "Epoch 567, Loss: 0.5625945235\n",
      "Epoch 568, Loss: 0.5727813810\n",
      "Epoch 569, Loss: 0.5414599841\n",
      "Epoch 570, Loss: 0.5235452440\n",
      "Epoch 571, Loss: 0.5158266935\n",
      "Epoch 572, Loss: 0.5093453532\n",
      "Epoch 573, Loss: 0.5321073248\n",
      "Epoch 574, Loss: 0.6248269254\n",
      "Epoch 575, Loss: 0.6061616524\n",
      "Epoch 576, Loss: 0.5308643120\n",
      "Epoch 577, Loss: 0.8676966555\n",
      "Epoch 578, Loss: 0.5732497545\n",
      "Epoch 579, Loss: 0.6054392256\n",
      "Epoch 580, Loss: 0.5975964126\n",
      "Epoch 581, Loss: 0.5361618483\n",
      "Epoch 582, Loss: 0.5412012238\n",
      "Epoch 583, Loss: 0.5510750392\n",
      "Epoch 584, Loss: 0.5280904994\n",
      "Epoch 585, Loss: 0.5418352011\n",
      "Epoch 586, Loss: 0.5305254835\n",
      "Epoch 587, Loss: 0.4991185020\n",
      "Epoch 588, Loss: 0.5019226036\n",
      "Epoch 589, Loss: 0.5014430809\n",
      "Epoch 590, Loss: 0.5072394563\n",
      "Epoch 591, Loss: 0.5562776268\n",
      "Epoch 592, Loss: 0.6350280457\n",
      "Epoch 593, Loss: 0.5277109976\n",
      "Epoch 594, Loss: 0.4925536271\n",
      "Epoch 595, Loss: 0.5023161215\n",
      "Epoch 596, Loss: 0.4986510911\n",
      "Epoch 597, Loss: 0.5205434428\n",
      "Epoch 598, Loss: 0.5616714789\n",
      "Epoch 599, Loss: 0.5377848953\n",
      "Epoch 600, Loss: 0.5485669244\n",
      "Epoch 601, Loss: 0.5307250974\n",
      "Epoch 602, Loss: 0.6422336891\n",
      "Epoch 603, Loss: 0.5454638322\n",
      "Epoch 604, Loss: 0.4947363695\n",
      "Epoch 605, Loss: 0.5236344241\n",
      "Epoch 606, Loss: 0.5717175695\n",
      "Epoch 607, Loss: 0.6028789482\n",
      "Epoch 608, Loss: 0.4995588491\n",
      "Epoch 609, Loss: 0.6628690077\n",
      "Epoch 610, Loss: 0.5248412852\n",
      "Epoch 611, Loss: 0.5374439726\n",
      "Epoch 612, Loss: 0.5283005388\n",
      "Epoch 613, Loss: 0.5072582200\n",
      "Epoch 614, Loss: 0.4788822565\n",
      "Epoch 615, Loss: 0.4714527533\n",
      "Epoch 616, Loss: 0.4748632440\n",
      "Epoch 617, Loss: 0.4912363430\n",
      "Epoch 618, Loss: 0.5039683461\n",
      "Epoch 619, Loss: 0.4958129673\n",
      "Epoch 620, Loss: 0.5190502719\n",
      "Epoch 621, Loss: 0.5891067094\n",
      "Epoch 622, Loss: 0.5150035741\n",
      "Epoch 623, Loss: 0.4953590167\n",
      "Epoch 624, Loss: 0.4822928965\n",
      "Epoch 625, Loss: 0.4600796901\n",
      "Epoch 626, Loss: 0.4797583242\n",
      "Epoch 627, Loss: 0.6217397295\n",
      "Epoch 628, Loss: 0.7325683519\n",
      "Epoch 629, Loss: 0.6615920606\n",
      "Epoch 630, Loss: 0.5490647070\n",
      "Epoch 631, Loss: 0.4711823630\n",
      "Epoch 632, Loss: 0.5796510331\n",
      "Epoch 633, Loss: 0.5103453411\n",
      "Epoch 634, Loss: 0.4678009162\n",
      "Epoch 635, Loss: 0.4870069103\n",
      "Epoch 636, Loss: 0.4763642733\n",
      "Epoch 637, Loss: 0.4621350866\n",
      "Epoch 638, Loss: 0.4872162882\n",
      "Epoch 639, Loss: 0.5363349526\n",
      "Epoch 640, Loss: 0.5231272145\n",
      "Epoch 641, Loss: 0.4999637328\n",
      "Epoch 642, Loss: 0.4709676482\n",
      "Epoch 643, Loss: 0.5239467374\n",
      "Epoch 644, Loss: 0.5533275058\n",
      "Epoch 645, Loss: 0.5746714958\n",
      "Epoch 646, Loss: 0.5080803888\n",
      "Epoch 647, Loss: 0.4837875245\n",
      "Epoch 648, Loss: 0.5493825489\n",
      "Epoch 649, Loss: 0.6351132653\n",
      "Epoch 650, Loss: 0.6469578164\n",
      "Epoch 651, Loss: 0.6751746933\n",
      "Epoch 652, Loss: 0.6654993768\n",
      "Epoch 653, Loss: 0.5977667936\n",
      "Epoch 654, Loss: 0.6737775113\n",
      "Epoch 655, Loss: 0.6530279465\n",
      "Epoch 656, Loss: 0.5998553240\n",
      "Epoch 657, Loss: 0.6682067607\n",
      "Epoch 658, Loss: 0.6046206683\n",
      "Epoch 659, Loss: 0.5069375861\n",
      "Epoch 660, Loss: 0.5550738459\n",
      "Epoch 661, Loss: 0.4855624158\n",
      "Epoch 662, Loss: 0.4952526710\n",
      "Epoch 663, Loss: 0.4716828912\n",
      "Epoch 664, Loss: 0.5395017205\n",
      "Epoch 665, Loss: 0.5570572176\n",
      "Epoch 666, Loss: 0.6242343492\n",
      "Epoch 667, Loss: 0.5410264906\n",
      "Epoch 668, Loss: 0.6413299254\n",
      "Epoch 669, Loss: 0.4927294483\n",
      "Epoch 670, Loss: 0.6123826422\n",
      "Epoch 671, Loss: 0.7521358299\n",
      "Epoch 672, Loss: 0.5104152933\n",
      "Epoch 673, Loss: 0.5382302007\n",
      "Epoch 674, Loss: 0.5616837280\n",
      "Epoch 675, Loss: 0.5215941755\n",
      "Epoch 676, Loss: 0.4631471608\n",
      "Epoch 677, Loss: 0.4856830851\n",
      "Epoch 678, Loss: 0.4903795009\n",
      "Epoch 679, Loss: 0.5299718095\n",
      "Epoch 680, Loss: 0.5359941740\n",
      "Epoch 681, Loss: 0.5675725390\n",
      "Epoch 682, Loss: 0.4689150638\n",
      "Epoch 683, Loss: 0.5168256282\n",
      "Epoch 684, Loss: 0.5267730501\n",
      "Epoch 685, Loss: 0.5758833082\n",
      "Epoch 686, Loss: 0.5491623853\n",
      "Epoch 687, Loss: 0.5518109250\n",
      "Epoch 688, Loss: 0.7853021242\n",
      "Epoch 689, Loss: 0.5673450853\n",
      "Epoch 690, Loss: 0.6546370337\n",
      "Epoch 691, Loss: 0.5162408631\n",
      "Epoch 692, Loss: 0.4755580732\n",
      "Epoch 693, Loss: 0.5107492758\n",
      "Epoch 694, Loss: 0.4733667049\n",
      "Epoch 695, Loss: 0.5281693511\n",
      "Epoch 696, Loss: 0.5408244424\n",
      "Epoch 697, Loss: 0.5180892565\n",
      "Epoch 698, Loss: 0.5708951391\n",
      "Epoch 699, Loss: 0.4463795865\n",
      "Epoch 700, Loss: 0.5267097854\n",
      "Epoch 701, Loss: 0.5682452774\n",
      "Epoch 702, Loss: 0.5565168001\n",
      "Epoch 703, Loss: 0.4486409724\n",
      "Epoch 704, Loss: 0.4756399004\n",
      "Epoch 705, Loss: 0.5357268814\n",
      "Epoch 706, Loss: 0.5548844316\n",
      "Epoch 707, Loss: 0.6063457275\n",
      "Epoch 708, Loss: 0.4908051484\n",
      "Epoch 709, Loss: 0.4505156374\n",
      "Epoch 710, Loss: 0.5634671917\n",
      "Epoch 711, Loss: 0.7056560482\n",
      "Epoch 712, Loss: 0.6299768361\n",
      "Epoch 713, Loss: 0.5759090846\n",
      "Epoch 714, Loss: 0.4682581736\n",
      "Epoch 715, Loss: 0.4542430978\n",
      "Epoch 716, Loss: 0.4381619645\n",
      "Epoch 717, Loss: 0.5720208980\n",
      "Epoch 718, Loss: 0.5676469762\n",
      "Epoch 719, Loss: 0.4878546212\n",
      "Epoch 720, Loss: 0.5363248565\n",
      "Epoch 721, Loss: 0.4920607978\n",
      "Epoch 722, Loss: 0.5444988841\n",
      "Epoch 723, Loss: 0.4842321250\n",
      "Epoch 724, Loss: 0.5073299193\n",
      "Epoch 725, Loss: 0.5276944424\n",
      "Epoch 726, Loss: 0.6536287130\n",
      "Epoch 727, Loss: 0.6367302009\n",
      "Epoch 728, Loss: 0.5311772535\n",
      "Epoch 729, Loss: 0.5339634521\n",
      "Epoch 730, Loss: 0.4797759912\n",
      "Epoch 731, Loss: 0.4543935801\n",
      "Epoch 732, Loss: 0.4853600799\n",
      "Epoch 733, Loss: 0.4929548719\n",
      "Epoch 734, Loss: 0.4286518185\n",
      "Epoch 735, Loss: 0.4576242788\n",
      "Epoch 736, Loss: 0.5067597457\n",
      "Epoch 737, Loss: 0.5102016981\n",
      "Epoch 738, Loss: 0.4502316765\n",
      "Epoch 739, Loss: 0.4624357778\n",
      "Epoch 740, Loss: 0.5028663806\n",
      "Epoch 741, Loss: 0.4613120085\n",
      "Epoch 742, Loss: 0.4806023124\n",
      "Epoch 743, Loss: 0.4789100043\n",
      "Epoch 744, Loss: 0.4433647814\n",
      "Epoch 745, Loss: 0.4810070245\n",
      "Epoch 746, Loss: 0.4689054747\n",
      "Epoch 747, Loss: 0.4934803026\n",
      "Epoch 748, Loss: 0.5165401486\n",
      "Epoch 749, Loss: 0.4603047324\n",
      "Epoch 750, Loss: 0.4617825562\n",
      "Epoch 751, Loss: 0.4327021259\n",
      "Epoch 752, Loss: 0.4287369771\n",
      "Epoch 753, Loss: 0.4605165534\n",
      "Epoch 754, Loss: 0.4407973587\n",
      "Epoch 755, Loss: 0.4324865452\n",
      "Epoch 756, Loss: 0.4352827890\n",
      "Epoch 757, Loss: 0.4982608181\n",
      "Epoch 758, Loss: 0.5359097199\n",
      "Epoch 759, Loss: 0.5837743414\n",
      "Epoch 760, Loss: 0.4083410706\n",
      "Epoch 761, Loss: 0.4451298638\n",
      "Epoch 762, Loss: 0.4381905292\n",
      "Epoch 763, Loss: 0.4687786242\n",
      "Epoch 764, Loss: 0.5342473638\n",
      "Epoch 765, Loss: 0.4730991065\n",
      "Epoch 766, Loss: 0.5504243667\n",
      "Epoch 767, Loss: 0.5051352387\n",
      "Epoch 768, Loss: 0.4944990444\n",
      "Epoch 769, Loss: 0.5862082825\n",
      "Epoch 770, Loss: 0.4646518326\n",
      "Epoch 771, Loss: 0.5809342698\n",
      "Epoch 772, Loss: 0.4618685910\n",
      "Epoch 773, Loss: 0.4684834538\n",
      "Epoch 774, Loss: 0.5955075924\n",
      "Epoch 775, Loss: 0.5712378423\n",
      "Epoch 776, Loss: 0.6204189969\n",
      "Epoch 777, Loss: 0.4601007632\n",
      "Epoch 778, Loss: 0.4819220521\n",
      "Epoch 779, Loss: 0.5050836354\n",
      "Epoch 780, Loss: 0.5984481230\n",
      "Epoch 781, Loss: 0.6068027325\n",
      "Epoch 782, Loss: 0.4950532771\n",
      "Epoch 783, Loss: 0.6213286819\n",
      "Epoch 784, Loss: 0.6084879142\n",
      "Epoch 785, Loss: 0.9161049269\n",
      "Epoch 786, Loss: 0.6112379442\n",
      "Epoch 787, Loss: 0.6165460882\n",
      "Epoch 788, Loss: 0.4953143760\n",
      "Epoch 789, Loss: 0.6375975511\n",
      "Epoch 790, Loss: 0.5046281610\n",
      "Epoch 791, Loss: 0.8938860826\n",
      "Epoch 792, Loss: 0.8767224723\n",
      "Epoch 793, Loss: 0.6965655927\n",
      "Epoch 794, Loss: 0.6876762868\n",
      "Epoch 795, Loss: 0.7072182631\n",
      "Epoch 796, Loss: 0.5371696360\n",
      "Epoch 797, Loss: 0.5191158300\n",
      "Epoch 798, Loss: 0.4880450655\n",
      "Epoch 799, Loss: 0.5226752938\n",
      "Epoch 800, Loss: 0.4716414767\n",
      "Epoch 801, Loss: 0.4544197806\n",
      "Epoch 802, Loss: 0.5038929239\n",
      "Epoch 803, Loss: 0.4508579662\n",
      "Epoch 804, Loss: 0.4833212656\n",
      "Epoch 805, Loss: 0.4695967923\n",
      "Epoch 806, Loss: 0.5375223483\n",
      "Epoch 807, Loss: 0.4749135454\n",
      "Epoch 808, Loss: 0.4706084875\n",
      "Epoch 809, Loss: 0.4258196191\n",
      "Epoch 810, Loss: 0.4092471029\n",
      "Epoch 811, Loss: 0.4887061987\n",
      "Epoch 812, Loss: 0.4623968457\n",
      "Epoch 813, Loss: 0.4706658862\n",
      "Epoch 814, Loss: 0.5566207621\n",
      "Epoch 815, Loss: 0.5478356289\n",
      "Epoch 816, Loss: 0.4351869068\n",
      "Epoch 817, Loss: 0.4433224707\n",
      "Epoch 818, Loss: 0.4728569699\n",
      "Epoch 819, Loss: 0.4559184163\n",
      "Epoch 820, Loss: 0.3926355688\n",
      "Epoch 821, Loss: 0.4213938296\n",
      "Epoch 822, Loss: 0.4181244404\n",
      "Epoch 823, Loss: 0.4229509557\n",
      "Epoch 824, Loss: 0.4485952956\n",
      "Epoch 825, Loss: 0.5221156091\n",
      "Epoch 826, Loss: 0.6611975729\n",
      "Epoch 827, Loss: 0.8601621630\n",
      "Epoch 828, Loss: 0.6128076880\n",
      "Epoch 829, Loss: 0.4909559365\n",
      "Epoch 830, Loss: 0.4374010020\n",
      "Epoch 831, Loss: 0.4564532333\n",
      "Epoch 832, Loss: 0.4682679747\n",
      "Epoch 833, Loss: 0.4364719709\n",
      "Epoch 834, Loss: 0.4286954696\n",
      "Epoch 835, Loss: 0.4052707235\n",
      "Epoch 836, Loss: 0.4360356317\n",
      "Epoch 837, Loss: 0.4863213812\n",
      "Epoch 838, Loss: 0.4731090921\n",
      "Epoch 839, Loss: 0.4400121527\n",
      "Epoch 840, Loss: 0.6353307392\n",
      "Epoch 841, Loss: 0.5784864090\n",
      "Epoch 842, Loss: 0.5221327481\n",
      "Epoch 843, Loss: 0.5521641986\n",
      "Epoch 844, Loss: 0.4399847076\n",
      "Epoch 845, Loss: 0.5131032622\n",
      "Epoch 846, Loss: 0.3836370780\n",
      "Epoch 847, Loss: 0.4735722205\n",
      "Epoch 848, Loss: 0.4091992545\n",
      "Epoch 849, Loss: 0.3956096662\n",
      "Epoch 850, Loss: 0.4427361866\n",
      "Epoch 851, Loss: 0.4284165312\n",
      "Epoch 852, Loss: 0.4818552166\n",
      "Epoch 853, Loss: 0.3903018445\n",
      "Epoch 854, Loss: 0.4271892217\n",
      "Epoch 855, Loss: 0.3993308534\n",
      "Epoch 856, Loss: 0.5382274524\n",
      "Epoch 857, Loss: 0.5768073798\n",
      "Epoch 858, Loss: 0.4034244922\n",
      "Epoch 859, Loss: 0.4206817599\n",
      "Epoch 860, Loss: 0.4430425458\n",
      "Epoch 861, Loss: 0.4049419283\n",
      "Epoch 862, Loss: 0.4871883846\n",
      "Epoch 863, Loss: 0.4308852639\n",
      "Epoch 864, Loss: 0.5130883703\n",
      "Epoch 865, Loss: 0.4272129136\n",
      "Epoch 866, Loss: 0.4049368883\n",
      "Epoch 867, Loss: 0.4306776440\n",
      "Epoch 868, Loss: 0.4537810025\n",
      "Epoch 869, Loss: 0.4045615377\n",
      "Epoch 870, Loss: 0.3777038761\n",
      "Epoch 871, Loss: 0.4434953230\n",
      "Epoch 872, Loss: 0.4666636793\n",
      "Epoch 873, Loss: 0.4804421684\n",
      "Epoch 874, Loss: 0.4712111249\n",
      "Epoch 875, Loss: 0.4094189873\n",
      "Epoch 876, Loss: 0.6601355245\n",
      "Epoch 877, Loss: 0.5471988825\n",
      "Epoch 878, Loss: 0.4442088797\n",
      "Epoch 879, Loss: 0.4163939674\n",
      "Epoch 880, Loss: 0.4119101589\n",
      "Epoch 881, Loss: 0.4389173217\n",
      "Epoch 882, Loss: 0.4944565793\n",
      "Epoch 883, Loss: 0.4510763706\n",
      "Epoch 884, Loss: 0.4530479345\n",
      "Epoch 885, Loss: 0.5094485535\n",
      "Epoch 886, Loss: 0.5236335137\n",
      "Epoch 887, Loss: 0.5117284234\n",
      "Epoch 888, Loss: 0.5457344320\n",
      "Epoch 889, Loss: 0.6634125473\n",
      "Epoch 890, Loss: 0.4213198201\n",
      "Epoch 891, Loss: 0.5144909837\n",
      "Epoch 892, Loss: 0.4848260426\n",
      "Epoch 893, Loss: 0.4292174400\n",
      "Epoch 894, Loss: 0.5983798945\n",
      "Epoch 895, Loss: 0.4074565907\n",
      "Epoch 896, Loss: 0.5690262322\n",
      "Epoch 897, Loss: 0.5480299466\n",
      "Epoch 898, Loss: 0.5162454868\n",
      "Epoch 899, Loss: 0.4246798866\n",
      "Epoch 900, Loss: 0.5235523847\n",
      "Epoch 901, Loss: 0.4968907681\n",
      "Epoch 902, Loss: 0.5335628810\n",
      "Epoch 903, Loss: 0.5152900877\n",
      "Epoch 904, Loss: 0.4354458414\n",
      "Epoch 905, Loss: 0.6266537915\n",
      "Epoch 906, Loss: 0.4963228354\n",
      "Epoch 907, Loss: 0.4897262745\n",
      "Epoch 908, Loss: 0.4726106360\n",
      "Epoch 909, Loss: 0.5592464363\n",
      "Epoch 910, Loss: 0.5425888182\n",
      "Epoch 911, Loss: 0.7298705579\n",
      "Epoch 912, Loss: 0.4569365085\n",
      "Epoch 913, Loss: 0.5132259162\n",
      "Epoch 914, Loss: 0.5841719772\n",
      "Epoch 915, Loss: 0.4738798359\n",
      "Epoch 916, Loss: 0.5198477385\n",
      "Epoch 917, Loss: 0.6415244759\n",
      "Epoch 918, Loss: 0.8180746759\n",
      "Epoch 919, Loss: 0.6290205337\n",
      "Epoch 920, Loss: 0.5428071383\n",
      "Epoch 921, Loss: 0.7566033544\n",
      "Epoch 922, Loss: 0.6859530029\n",
      "Epoch 923, Loss: 0.6639814596\n",
      "Epoch 924, Loss: 0.6154083989\n",
      "Epoch 925, Loss: 0.7423307126\n",
      "Epoch 926, Loss: 0.6761906874\n",
      "Epoch 927, Loss: 0.4620429138\n",
      "Epoch 928, Loss: 0.6192487982\n",
      "Epoch 929, Loss: 0.4539155603\n",
      "Epoch 930, Loss: 0.4931558055\n",
      "Epoch 931, Loss: 0.4074769217\n",
      "Epoch 932, Loss: 0.4539345639\n",
      "Epoch 933, Loss: 0.4785569636\n",
      "Epoch 934, Loss: 0.5680483218\n",
      "Epoch 935, Loss: 0.5615545068\n",
      "Epoch 936, Loss: 0.4902432588\n",
      "Epoch 937, Loss: 0.4351170598\n",
      "Epoch 938, Loss: 0.4517587140\n",
      "Epoch 939, Loss: 0.3927344655\n",
      "Epoch 940, Loss: 0.5092346009\n",
      "Epoch 941, Loss: 0.5484580482\n",
      "Epoch 942, Loss: 0.3894681293\n",
      "Epoch 943, Loss: 0.3569372001\n",
      "Epoch 944, Loss: 0.3921076447\n",
      "Epoch 945, Loss: 0.5342507980\n",
      "Epoch 946, Loss: 0.4981799843\n",
      "Epoch 947, Loss: 0.4187014744\n",
      "Epoch 948, Loss: 0.4389785270\n",
      "Epoch 949, Loss: 0.5140875761\n",
      "Epoch 950, Loss: 0.4353721267\n",
      "Epoch 951, Loss: 0.3679419445\n",
      "Epoch 952, Loss: 0.3587179172\n",
      "Epoch 953, Loss: 0.3624165403\n",
      "Epoch 954, Loss: 0.5489906781\n",
      "Epoch 955, Loss: 0.5435843309\n",
      "Epoch 956, Loss: 0.4762702294\n",
      "Epoch 957, Loss: 0.4321461293\n",
      "Epoch 958, Loss: 0.4024708744\n",
      "Epoch 959, Loss: 0.4474515592\n",
      "Epoch 960, Loss: 0.4757343126\n",
      "Epoch 961, Loss: 0.4364667284\n",
      "Epoch 962, Loss: 0.4526618903\n",
      "Epoch 963, Loss: 0.4488932114\n",
      "Epoch 964, Loss: 0.5624966075\n",
      "Epoch 965, Loss: 0.5412800699\n",
      "Epoch 966, Loss: 0.5637974406\n",
      "Epoch 967, Loss: 0.4598146285\n",
      "Epoch 968, Loss: 0.4877492096\n",
      "Epoch 969, Loss: 0.4156143250\n",
      "Epoch 970, Loss: 0.4227100863\n",
      "Epoch 971, Loss: 0.3988254932\n",
      "Epoch 972, Loss: 0.3725500579\n",
      "Epoch 973, Loss: 0.5011896480\n",
      "Epoch 974, Loss: 0.6119124167\n",
      "Epoch 975, Loss: 0.4286427053\n",
      "Epoch 976, Loss: 0.6448022752\n",
      "Epoch 977, Loss: 0.3429026264\n",
      "Epoch 978, Loss: 0.4544939608\n",
      "Epoch 979, Loss: 0.3811643031\n",
      "Epoch 980, Loss: 0.4290743693\n",
      "Epoch 981, Loss: 0.4158093616\n",
      "Epoch 982, Loss: 0.4543965420\n",
      "Epoch 983, Loss: 0.4010163606\n",
      "Epoch 984, Loss: 0.4140209621\n",
      "Epoch 985, Loss: 0.5282345699\n",
      "Epoch 986, Loss: 0.4439094839\n",
      "Epoch 987, Loss: 0.4369693357\n",
      "Epoch 988, Loss: 0.3757705203\n",
      "Epoch 989, Loss: 0.4520063332\n",
      "Epoch 990, Loss: 0.5237924648\n",
      "Epoch 991, Loss: 0.4072047480\n",
      "Epoch 992, Loss: 0.3659300720\n",
      "Epoch 993, Loss: 0.3693862242\n",
      "Epoch 994, Loss: 0.3525731976\n",
      "Epoch 995, Loss: 0.4412774633\n",
      "Epoch 996, Loss: 0.4614698224\n",
      "Epoch 997, Loss: 0.3523440450\n",
      "Epoch 998, Loss: 0.4044120011\n",
      "Epoch 999, Loss: 0.3815012470\n",
      "Epoch 1000, Loss: 0.3192545233\n",
      "Epoch 1001, Loss: 0.4437611705\n",
      "Epoch 1002, Loss: 0.4773830275\n",
      "Epoch 1003, Loss: 0.3534348273\n",
      "Epoch 1004, Loss: 0.4924186030\n",
      "Epoch 1005, Loss: 0.3992492007\n",
      "Epoch 1006, Loss: 0.3585438754\n",
      "Epoch 1007, Loss: 0.5776022119\n",
      "Epoch 1008, Loss: 0.6266480769\n",
      "Epoch 1009, Loss: 0.5343923843\n",
      "Epoch 1010, Loss: 0.3742880153\n",
      "Epoch 1011, Loss: 0.5699846046\n",
      "Epoch 1012, Loss: 0.3907436711\n",
      "Epoch 1013, Loss: 0.4474808935\n",
      "Epoch 1014, Loss: 0.4840158478\n",
      "Epoch 1015, Loss: 0.4225006066\n",
      "Epoch 1016, Loss: 0.4254465010\n",
      "Epoch 1017, Loss: 0.4031938137\n",
      "Epoch 1018, Loss: 0.5173344256\n",
      "Epoch 1019, Loss: 0.5090512930\n",
      "Epoch 1020, Loss: 0.5949824445\n",
      "Epoch 1021, Loss: 0.3772135492\n",
      "Epoch 1022, Loss: 0.4477199541\n",
      "Epoch 1023, Loss: 0.3671356204\n",
      "Epoch 1024, Loss: 0.3778476881\n",
      "Epoch 1025, Loss: 0.4172277708\n",
      "Epoch 1026, Loss: 0.3786140511\n",
      "Epoch 1027, Loss: 0.4428175708\n",
      "Epoch 1028, Loss: 0.5729705892\n",
      "Epoch 1029, Loss: 0.4473863079\n",
      "Epoch 1030, Loss: 0.6528360068\n",
      "Epoch 1031, Loss: 0.3816307112\n",
      "Epoch 1032, Loss: 0.4875529246\n",
      "Epoch 1033, Loss: 0.4521734090\n",
      "Epoch 1034, Loss: 0.4558593546\n",
      "Epoch 1035, Loss: 0.4233153432\n",
      "Epoch 1036, Loss: 0.3641364557\n",
      "Epoch 1037, Loss: 0.3836907190\n",
      "Epoch 1038, Loss: 0.4217625450\n",
      "Epoch 1039, Loss: 0.4353240748\n",
      "Epoch 1040, Loss: 0.4121782562\n",
      "Epoch 1041, Loss: 0.4230787108\n",
      "Epoch 1042, Loss: 0.3674014021\n",
      "Epoch 1043, Loss: 0.5170837758\n",
      "Epoch 1044, Loss: 0.5824245344\n",
      "Epoch 1045, Loss: 0.4566463747\n",
      "Epoch 1046, Loss: 0.4687834254\n",
      "Epoch 1047, Loss: 0.4341642873\n",
      "Epoch 1048, Loss: 0.3549887265\n",
      "Epoch 1049, Loss: 0.5122370527\n",
      "Epoch 1050, Loss: 0.3800130853\n",
      "Epoch 1051, Loss: 0.3549260332\n",
      "Epoch 1052, Loss: 0.4033807955\n",
      "Epoch 1053, Loss: 0.3985954175\n",
      "Epoch 1054, Loss: 0.4909198903\n",
      "Epoch 1055, Loss: 0.4762021557\n",
      "Epoch 1056, Loss: 0.4254764710\n",
      "Epoch 1057, Loss: 0.4141943727\n",
      "Epoch 1058, Loss: 0.3675501939\n",
      "Epoch 1059, Loss: 0.3460439085\n",
      "Epoch 1060, Loss: 0.4222226947\n",
      "Epoch 1061, Loss: 0.5035832372\n",
      "Epoch 1062, Loss: 0.3445748233\n",
      "Epoch 1063, Loss: 0.3578491701\n",
      "Epoch 1064, Loss: 0.3660819422\n",
      "Epoch 1065, Loss: 0.4732882305\n",
      "Epoch 1066, Loss: 0.3627836527\n",
      "Epoch 1067, Loss: 0.3259926434\n",
      "Epoch 1068, Loss: 0.3698246868\n",
      "Epoch 1069, Loss: 0.4198530164\n",
      "Epoch 1070, Loss: 0.3481485480\n",
      "Epoch 1071, Loss: 0.3613931610\n",
      "Epoch 1072, Loss: 0.4039158209\n",
      "Epoch 1073, Loss: 0.4937086667\n",
      "Epoch 1074, Loss: 0.4626925358\n",
      "Epoch 1075, Loss: 0.3401529754\n",
      "Epoch 1076, Loss: 0.3154571681\n",
      "Epoch 1077, Loss: 0.3638502271\n",
      "Epoch 1078, Loss: 0.5319680323\n",
      "Epoch 1079, Loss: 0.2888574107\n",
      "Epoch 1080, Loss: 0.4460755719\n",
      "Epoch 1081, Loss: 0.3346247942\n",
      "Epoch 1082, Loss: 0.3116381195\n",
      "Epoch 1083, Loss: 0.4463949113\n",
      "Epoch 1084, Loss: 0.3773360131\n",
      "Epoch 1085, Loss: 0.3652365440\n",
      "Epoch 1086, Loss: 0.4121889855\n",
      "Epoch 1087, Loss: 0.4844823546\n",
      "Epoch 1088, Loss: 0.3305122812\n",
      "Epoch 1089, Loss: 0.4568490757\n",
      "Epoch 1090, Loss: 0.5183429312\n",
      "Epoch 1091, Loss: 0.4521521871\n",
      "Epoch 1092, Loss: 0.5535210065\n",
      "Epoch 1093, Loss: 0.5027201174\n",
      "Epoch 1094, Loss: 0.5537168319\n",
      "Epoch 1095, Loss: 0.5397150580\n",
      "Epoch 1096, Loss: 0.3732393141\n",
      "Epoch 1097, Loss: 0.4283450572\n",
      "Epoch 1098, Loss: 0.3718995286\n",
      "Epoch 1099, Loss: 0.3568461801\n",
      "Epoch 1100, Loss: 0.4126380298\n",
      "Epoch 1101, Loss: 0.4055121037\n",
      "Epoch 1102, Loss: 0.3527286640\n",
      "Epoch 1103, Loss: 0.3981708313\n",
      "Epoch 1104, Loss: 0.3664013144\n",
      "Epoch 1105, Loss: 0.3373551498\n",
      "Epoch 1106, Loss: 0.3558388122\n",
      "Epoch 1107, Loss: 0.3703356022\n",
      "Epoch 1108, Loss: 0.4658834966\n",
      "Epoch 1109, Loss: 0.6206085955\n",
      "Epoch 1110, Loss: 0.5354920194\n",
      "Epoch 1111, Loss: 0.3554506801\n",
      "Epoch 1112, Loss: 0.3587145406\n",
      "Epoch 1113, Loss: 0.4314838684\n",
      "Epoch 1114, Loss: 0.4059888786\n",
      "Epoch 1115, Loss: 0.2809749779\n",
      "Epoch 1116, Loss: 0.5509301033\n",
      "Epoch 1117, Loss: 0.4457870456\n",
      "Epoch 1118, Loss: 0.4503432348\n",
      "Epoch 1119, Loss: 0.4657937635\n",
      "Epoch 1120, Loss: 0.4043656468\n",
      "Epoch 1121, Loss: 0.3397168390\n",
      "Epoch 1122, Loss: 0.4692928742\n",
      "Epoch 1123, Loss: 0.3789203819\n",
      "Epoch 1124, Loss: 0.4051010971\n",
      "Epoch 1125, Loss: 0.5170442132\n",
      "Epoch 1126, Loss: 0.3797181567\n",
      "Epoch 1127, Loss: 0.5669155341\n",
      "Epoch 1128, Loss: 0.5231550660\n",
      "Epoch 1129, Loss: 0.3966411504\n",
      "Epoch 1130, Loss: 0.5161049582\n",
      "Epoch 1131, Loss: 0.6393479303\n",
      "Epoch 1132, Loss: 0.5245855495\n",
      "Epoch 1133, Loss: 0.3814678570\n",
      "Epoch 1134, Loss: 0.5471600682\n",
      "Epoch 1135, Loss: 0.3073814816\n",
      "Epoch 1136, Loss: 0.6652838217\n",
      "Epoch 1137, Loss: 0.6746117672\n",
      "Epoch 1138, Loss: 0.6519144871\n",
      "Epoch 1139, Loss: 0.4644316006\n",
      "Epoch 1140, Loss: 0.4679247563\n",
      "Epoch 1141, Loss: 0.6157915247\n",
      "Epoch 1142, Loss: 0.5504392933\n",
      "Epoch 1143, Loss: 0.5034714423\n",
      "Epoch 1144, Loss: 0.4679742446\n",
      "Epoch 1145, Loss: 0.5705575437\n",
      "Epoch 1146, Loss: 0.5491522576\n",
      "Epoch 1147, Loss: 0.5940411243\n",
      "Epoch 1148, Loss: 0.4682653529\n",
      "Epoch 1149, Loss: 0.5847844264\n",
      "Epoch 1150, Loss: 0.4005572820\n",
      "Epoch 1151, Loss: 0.6555218811\n",
      "Epoch 1152, Loss: 0.5753988867\n",
      "Epoch 1153, Loss: 0.5312962747\n",
      "Epoch 1154, Loss: 0.4788540688\n",
      "Epoch 1155, Loss: 0.5718291613\n",
      "Epoch 1156, Loss: 0.4454913936\n",
      "Epoch 1157, Loss: 0.4278232250\n",
      "Epoch 1158, Loss: 0.5859792666\n",
      "Epoch 1159, Loss: 0.5789399899\n",
      "Epoch 1160, Loss: 0.7355211044\n",
      "Epoch 1161, Loss: 0.7403988572\n",
      "Epoch 1162, Loss: 0.5947969208\n",
      "Epoch 1163, Loss: 0.6271618954\n",
      "Epoch 1164, Loss: 0.5820247401\n",
      "Epoch 1165, Loss: 0.6335958632\n",
      "Epoch 1166, Loss: 0.5742373798\n",
      "Epoch 1167, Loss: 0.4071831370\n",
      "Epoch 1168, Loss: 0.5334353872\n",
      "Epoch 1169, Loss: 0.4588878707\n",
      "Epoch 1170, Loss: 0.4695366354\n",
      "Epoch 1171, Loss: 0.4419468364\n",
      "Epoch 1172, Loss: 0.3722750282\n",
      "Epoch 1173, Loss: 0.3300210593\n",
      "Epoch 1174, Loss: 0.3318293718\n",
      "Epoch 1175, Loss: 0.4080612637\n",
      "Epoch 1176, Loss: 0.5454186007\n",
      "Epoch 1177, Loss: 0.6564887834\n",
      "Epoch 1178, Loss: 0.3229285140\n",
      "Epoch 1179, Loss: 0.3570973284\n",
      "Epoch 1180, Loss: 0.3720111513\n",
      "Epoch 1181, Loss: 0.3158945806\n",
      "Epoch 1182, Loss: 0.3844899897\n",
      "Epoch 1183, Loss: 0.6167093570\n",
      "Epoch 1184, Loss: 0.3605561589\n",
      "Epoch 1185, Loss: 0.3458891044\n",
      "Epoch 1186, Loss: 0.3917773447\n",
      "Epoch 1187, Loss: 0.4064537516\n",
      "Epoch 1188, Loss: 0.3317626725\n",
      "Epoch 1189, Loss: 0.3548367543\n",
      "Epoch 1190, Loss: 0.4323181152\n",
      "Epoch 1191, Loss: 0.3365550377\n",
      "Epoch 1192, Loss: 0.3644944053\n",
      "Epoch 1193, Loss: 0.3804475537\n",
      "Epoch 1194, Loss: 0.3100531070\n",
      "Epoch 1195, Loss: 0.3852192231\n",
      "Epoch 1196, Loss: 0.3467490067\n",
      "Epoch 1197, Loss: 0.2943744796\n",
      "Epoch 1198, Loss: 0.2769821299\n",
      "Epoch 1199, Loss: 0.3869383184\n",
      "Epoch 1200, Loss: 0.6369261658\n",
      "Epoch 1201, Loss: 0.4752575424\n",
      "Epoch 1202, Loss: 0.5359960634\n",
      "Epoch 1203, Loss: 0.5968144123\n",
      "Epoch 1204, Loss: 0.3358517643\n",
      "Epoch 1205, Loss: 0.5550060574\n",
      "Epoch 1206, Loss: 0.4325668662\n",
      "Epoch 1207, Loss: 0.3946299676\n",
      "Epoch 1208, Loss: 0.3408558275\n",
      "Epoch 1209, Loss: 0.3897098527\n",
      "Epoch 1210, Loss: 0.3831387414\n",
      "Epoch 1211, Loss: 0.4343433967\n",
      "Epoch 1212, Loss: 0.3515071596\n",
      "Epoch 1213, Loss: 0.3421493769\n",
      "Epoch 1214, Loss: 0.3124146269\n",
      "Epoch 1215, Loss: 0.3404503730\n",
      "Epoch 1216, Loss: 0.3733819711\n",
      "Epoch 1217, Loss: 0.3042791597\n",
      "Epoch 1218, Loss: 0.2917503987\n",
      "Epoch 1219, Loss: 0.3876633630\n",
      "Epoch 1220, Loss: 0.5959421689\n",
      "Epoch 1221, Loss: 0.4644221886\n",
      "Epoch 1222, Loss: 0.3483020880\n",
      "Epoch 1223, Loss: 0.3132524913\n",
      "Epoch 1224, Loss: 0.2817857633\n",
      "Epoch 1225, Loss: 0.3318239582\n",
      "Epoch 1226, Loss: 0.4201065228\n",
      "Epoch 1227, Loss: 0.5865334409\n",
      "Epoch 1228, Loss: 0.4526757081\n",
      "Epoch 1229, Loss: 0.4359819541\n",
      "Epoch 1230, Loss: 0.3569939466\n",
      "Epoch 1231, Loss: 0.3550443227\n",
      "Epoch 1232, Loss: 0.3538920724\n",
      "Epoch 1233, Loss: 0.4200549572\n",
      "Epoch 1234, Loss: 0.4295292495\n",
      "Epoch 1235, Loss: 0.3337698869\n",
      "Epoch 1236, Loss: 0.5193043984\n",
      "Epoch 1237, Loss: 0.6031616620\n",
      "Epoch 1238, Loss: 0.3208714052\n",
      "Epoch 1239, Loss: 0.3033352462\n",
      "Epoch 1240, Loss: 0.3742722405\n",
      "Epoch 1241, Loss: 0.2450977314\n",
      "Epoch 1242, Loss: 0.3120297279\n",
      "Epoch 1243, Loss: 0.3664495721\n",
      "Epoch 1244, Loss: 0.3732573477\n",
      "Epoch 1245, Loss: 0.4341288563\n",
      "Epoch 1246, Loss: 0.4555143396\n",
      "Epoch 1247, Loss: 0.4376459673\n",
      "Epoch 1248, Loss: 0.3502678934\n",
      "Epoch 1249, Loss: 0.2694465703\n",
      "Epoch 1250, Loss: 0.3409844588\n",
      "Epoch 1251, Loss: 0.3164768350\n",
      "Epoch 1252, Loss: 0.4720362481\n",
      "Epoch 1253, Loss: 0.3200675570\n",
      "Epoch 1254, Loss: 0.2914995476\n",
      "Epoch 1255, Loss: 0.3163381829\n",
      "Epoch 1256, Loss: 0.4036485821\n",
      "Epoch 1257, Loss: 0.3317215888\n",
      "Epoch 1258, Loss: 0.3961907257\n",
      "Epoch 1259, Loss: 0.4043181134\n",
      "Epoch 1260, Loss: 0.3664407674\n",
      "Epoch 1261, Loss: 0.3786167684\n",
      "Epoch 1262, Loss: 0.5601685863\n",
      "Epoch 1263, Loss: 0.4217911471\n",
      "Epoch 1264, Loss: 0.2788394393\n",
      "Epoch 1265, Loss: 0.3336766003\n",
      "Epoch 1266, Loss: 0.3335110860\n",
      "Epoch 1267, Loss: 0.4383348650\n",
      "Epoch 1268, Loss: 0.3546391888\n",
      "Epoch 1269, Loss: 0.2975708410\n",
      "Epoch 1270, Loss: 0.3892247095\n",
      "Epoch 1271, Loss: 0.5546271492\n",
      "Epoch 1272, Loss: 0.5758599805\n",
      "Epoch 1273, Loss: 0.3190103153\n",
      "Epoch 1274, Loss: 0.2575237053\n",
      "Epoch 1275, Loss: 0.4290809582\n",
      "Epoch 1276, Loss: 0.6268904966\n",
      "Epoch 1277, Loss: 0.3153392406\n",
      "Epoch 1278, Loss: 0.3990921128\n",
      "Epoch 1279, Loss: 0.6073577213\n",
      "Epoch 1280, Loss: 0.2680069359\n",
      "Epoch 1281, Loss: 0.5017677970\n",
      "Epoch 1282, Loss: 0.2942722767\n",
      "Epoch 1283, Loss: 0.4849725934\n",
      "Epoch 1284, Loss: 0.3211735350\n",
      "Epoch 1285, Loss: 0.3614597160\n",
      "Epoch 1286, Loss: 0.3391406292\n",
      "Epoch 1287, Loss: 0.4000058874\n",
      "Epoch 1288, Loss: 0.4135222955\n",
      "Epoch 1289, Loss: 0.3749720914\n",
      "Epoch 1290, Loss: 0.3366517079\n",
      "Epoch 1291, Loss: 0.3569439340\n",
      "Epoch 1292, Loss: 0.4446238403\n",
      "Epoch 1293, Loss: 0.4940720374\n",
      "Epoch 1294, Loss: 0.4374648696\n",
      "Epoch 1295, Loss: 0.4092030091\n",
      "Epoch 1296, Loss: 0.4234080593\n",
      "Epoch 1297, Loss: 0.2940724154\n",
      "Epoch 1298, Loss: 0.3594029880\n",
      "Epoch 1299, Loss: 0.3847917613\n",
      "Epoch 1300, Loss: 0.3761995860\n",
      "Epoch 1301, Loss: 0.3792220988\n",
      "Epoch 1302, Loss: 0.3314860593\n",
      "Epoch 1303, Loss: 0.4263794176\n",
      "Epoch 1304, Loss: 0.4059795512\n",
      "Epoch 1305, Loss: 0.3305045807\n",
      "Epoch 1306, Loss: 0.6015191676\n",
      "Epoch 1307, Loss: 0.3199364998\n",
      "Epoch 1308, Loss: 0.3101805340\n",
      "Epoch 1309, Loss: 0.2577630267\n",
      "Epoch 1310, Loss: 0.3157381136\n",
      "Epoch 1311, Loss: 0.3026118185\n",
      "Epoch 1312, Loss: 0.3075277010\n",
      "Epoch 1313, Loss: 0.3493574794\n",
      "Epoch 1314, Loss: 0.3256621130\n",
      "Epoch 1315, Loss: 0.3337938573\n",
      "Epoch 1316, Loss: 0.4026530690\n",
      "Epoch 1317, Loss: 0.4163755257\n",
      "Epoch 1318, Loss: 0.4042782499\n",
      "Epoch 1319, Loss: 0.3192791897\n",
      "Epoch 1320, Loss: 0.2481101265\n",
      "Epoch 1321, Loss: 0.3153325643\n",
      "Epoch 1322, Loss: 0.3432788866\n",
      "Epoch 1323, Loss: 0.3923824651\n",
      "Epoch 1324, Loss: 0.5750458814\n",
      "Epoch 1325, Loss: 0.4189654731\n",
      "Epoch 1326, Loss: 0.5011422489\n",
      "Epoch 1327, Loss: 0.2647829808\n",
      "Epoch 1328, Loss: 0.3375957125\n",
      "Epoch 1329, Loss: 0.4313685025\n",
      "Epoch 1330, Loss: 0.2354132804\n",
      "Epoch 1331, Loss: 0.3502032684\n",
      "Epoch 1332, Loss: 0.4323788427\n",
      "Epoch 1333, Loss: 0.3318320939\n",
      "Epoch 1334, Loss: 0.2716192810\n",
      "Epoch 1335, Loss: 0.4141102611\n",
      "Epoch 1336, Loss: 0.2691353305\n",
      "Epoch 1337, Loss: 0.1990078008\n",
      "Epoch 1338, Loss: 0.4265345354\n",
      "Epoch 1339, Loss: 0.3669187308\n",
      "Epoch 1340, Loss: 0.2948931796\n",
      "Epoch 1341, Loss: 0.4093129883\n",
      "Epoch 1342, Loss: 0.5021944989\n",
      "Epoch 1343, Loss: 0.4110726534\n",
      "Epoch 1344, Loss: 0.3226048626\n",
      "Epoch 1345, Loss: 0.2749110653\n",
      "Epoch 1346, Loss: 0.5976103489\n",
      "Epoch 1347, Loss: 0.4230138621\n",
      "Epoch 1348, Loss: 0.3468281978\n",
      "Epoch 1349, Loss: 0.5598235595\n",
      "Epoch 1350, Loss: 0.4023115032\n",
      "Epoch 1351, Loss: 0.3467292612\n",
      "Epoch 1352, Loss: 0.2816848016\n",
      "Epoch 1353, Loss: 0.4765853590\n",
      "Epoch 1354, Loss: 0.3143924323\n",
      "Epoch 1355, Loss: 0.4370545893\n",
      "Epoch 1356, Loss: 0.3811757906\n",
      "Epoch 1357, Loss: 0.3761372786\n",
      "Epoch 1358, Loss: 0.2984556845\n",
      "Epoch 1359, Loss: 0.2872648208\n",
      "Epoch 1360, Loss: 0.3703583488\n",
      "Epoch 1361, Loss: 0.4871171646\n",
      "Epoch 1362, Loss: 0.3755014007\n",
      "Epoch 1363, Loss: 0.4089135559\n",
      "Epoch 1364, Loss: 0.3154161622\n",
      "Epoch 1365, Loss: 0.3539225955\n",
      "Epoch 1366, Loss: 0.3963092577\n",
      "Epoch 1367, Loss: 0.3158387496\n",
      "Epoch 1368, Loss: 0.3039158899\n",
      "Epoch 1369, Loss: 0.3601546561\n",
      "Epoch 1370, Loss: 0.4700062647\n",
      "Epoch 1371, Loss: 0.4236462587\n",
      "Epoch 1372, Loss: 0.4232077349\n",
      "Epoch 1373, Loss: 0.3964626347\n",
      "Epoch 1374, Loss: 0.3064795397\n",
      "Epoch 1375, Loss: 0.3210391121\n",
      "Epoch 1376, Loss: 0.3905072620\n",
      "Epoch 1377, Loss: 0.4441671321\n",
      "Epoch 1378, Loss: 0.4360532739\n",
      "Epoch 1379, Loss: 0.5466358414\n",
      "Epoch 1380, Loss: 0.5158972619\n",
      "Epoch 1381, Loss: 0.3632573776\n",
      "Epoch 1382, Loss: 0.4088327531\n",
      "Epoch 1383, Loss: 0.6191152570\n",
      "Epoch 1384, Loss: 0.5967804851\n",
      "Epoch 1385, Loss: 0.4238537768\n",
      "Epoch 1386, Loss: 0.3765250426\n",
      "Epoch 1387, Loss: 0.4654681727\n",
      "Epoch 1388, Loss: 0.4240659441\n",
      "Epoch 1389, Loss: 0.3258983527\n",
      "Epoch 1390, Loss: 0.3764670492\n",
      "Epoch 1391, Loss: 0.4257404150\n",
      "Epoch 1392, Loss: 0.3829005765\n",
      "Epoch 1393, Loss: 0.3413285608\n",
      "Epoch 1394, Loss: 0.4406152574\n",
      "Epoch 1395, Loss: 0.4447954220\n",
      "Epoch 1396, Loss: 0.5147099259\n",
      "Epoch 1397, Loss: 0.4951147509\n",
      "Epoch 1398, Loss: 0.4094687777\n",
      "Epoch 1399, Loss: 0.3204913774\n",
      "Epoch 1400, Loss: 0.4661665702\n",
      "Epoch 1401, Loss: 0.5586406271\n",
      "Epoch 1402, Loss: 0.6034538818\n",
      "Epoch 1403, Loss: 0.6908608520\n",
      "Epoch 1404, Loss: 0.4578149639\n",
      "Epoch 1405, Loss: 0.7277713822\n",
      "Epoch 1406, Loss: 0.4149933202\n",
      "Epoch 1407, Loss: 0.5672265181\n",
      "Epoch 1408, Loss: 0.3445357016\n",
      "Epoch 1409, Loss: 0.3887967494\n",
      "Epoch 1410, Loss: 0.4977484532\n",
      "Epoch 1411, Loss: 0.3216176029\n",
      "Epoch 1412, Loss: 0.5677037260\n",
      "Epoch 1413, Loss: 0.4463512715\n",
      "Epoch 1414, Loss: 0.5954402678\n",
      "Epoch 1415, Loss: 1.1387368675\n",
      "Epoch 1416, Loss: 0.7664327746\n",
      "Epoch 1417, Loss: 0.4827243579\n",
      "Epoch 1418, Loss: 0.7725451941\n",
      "Epoch 1419, Loss: 0.5325767777\n",
      "Epoch 1420, Loss: 0.4563111579\n",
      "Epoch 1421, Loss: 0.6265327943\n",
      "Epoch 1422, Loss: 0.4128727071\n",
      "Epoch 1423, Loss: 0.4744444368\n",
      "Epoch 1424, Loss: 0.5241665292\n",
      "Epoch 1425, Loss: 0.3468287919\n",
      "Epoch 1426, Loss: 0.5532009974\n",
      "Epoch 1427, Loss: 0.4615436939\n",
      "Epoch 1428, Loss: 0.4098155398\n",
      "Epoch 1429, Loss: 0.4002065868\n",
      "Epoch 1430, Loss: 0.3428689359\n",
      "Epoch 1431, Loss: 0.4167763010\n",
      "Epoch 1432, Loss: 0.4354128873\n",
      "Epoch 1433, Loss: 0.4779625107\n",
      "Epoch 1434, Loss: 0.4694753132\n",
      "Epoch 1435, Loss: 0.4623111176\n",
      "Epoch 1436, Loss: 0.6494867534\n",
      "Epoch 1437, Loss: 0.6916033721\n",
      "Epoch 1438, Loss: 0.8498140709\n",
      "Epoch 1439, Loss: 0.7111504347\n",
      "Epoch 1440, Loss: 0.5828651202\n",
      "Epoch 1441, Loss: 0.4559094432\n",
      "Epoch 1442, Loss: 0.7224115603\n",
      "Epoch 1443, Loss: 0.4936315146\n",
      "Epoch 1444, Loss: 0.4881213878\n",
      "Epoch 1445, Loss: 0.4157255811\n",
      "Epoch 1446, Loss: 0.5023020324\n",
      "Epoch 1447, Loss: 0.3969085092\n",
      "Epoch 1448, Loss: 0.5190006172\n",
      "Epoch 1449, Loss: 0.3707388037\n",
      "Epoch 1450, Loss: 0.5446363620\n",
      "Epoch 1451, Loss: 0.3812604835\n",
      "Epoch 1452, Loss: 0.4016159692\n",
      "Epoch 1453, Loss: 0.5417656864\n",
      "Epoch 1454, Loss: 0.4217279435\n",
      "Epoch 1455, Loss: 0.4955331518\n",
      "Epoch 1456, Loss: 0.5369514903\n",
      "Epoch 1457, Loss: 0.4063120040\n",
      "Epoch 1458, Loss: 0.4429695206\n",
      "Epoch 1459, Loss: 0.3891808862\n",
      "Epoch 1460, Loss: 0.3254516735\n",
      "Epoch 1461, Loss: 0.7706146352\n",
      "Epoch 1462, Loss: 0.5203141022\n",
      "Epoch 1463, Loss: 0.4865589262\n",
      "Epoch 1464, Loss: 0.4170511273\n",
      "Epoch 1465, Loss: 0.4131877805\n",
      "Epoch 1466, Loss: 0.3635079808\n",
      "Epoch 1467, Loss: 0.3288894493\n",
      "Epoch 1468, Loss: 0.4448831645\n",
      "Epoch 1469, Loss: 0.4546457181\n",
      "Epoch 1470, Loss: 0.3862098388\n",
      "Epoch 1471, Loss: 0.3484076032\n",
      "Epoch 1472, Loss: 0.3926919189\n",
      "Epoch 1473, Loss: 0.2978149949\n",
      "Epoch 1474, Loss: 0.2877506005\n",
      "Epoch 1475, Loss: 0.3027959913\n",
      "Epoch 1476, Loss: 0.3958984523\n",
      "Epoch 1477, Loss: 0.3694617239\n",
      "Epoch 1478, Loss: 0.3211768546\n",
      "Epoch 1479, Loss: 0.3893625415\n",
      "Epoch 1480, Loss: 0.4723266298\n",
      "Epoch 1481, Loss: 0.3848420587\n",
      "Epoch 1482, Loss: 0.3780390853\n",
      "Epoch 1483, Loss: 0.3945723583\n",
      "Epoch 1484, Loss: 0.4685204079\n",
      "Epoch 1485, Loss: 0.4527697415\n",
      "Epoch 1486, Loss: 0.5122619274\n",
      "Epoch 1487, Loss: 0.3331389805\n",
      "Epoch 1488, Loss: 0.2823862133\n",
      "Epoch 1489, Loss: 0.2893793501\n",
      "Epoch 1490, Loss: 0.3273343195\n",
      "Epoch 1491, Loss: 0.3710579375\n",
      "Epoch 1492, Loss: 0.4376090914\n",
      "Epoch 1493, Loss: 0.3642368353\n",
      "Epoch 1494, Loss: 0.3141440233\n",
      "Epoch 1495, Loss: 0.2798124712\n",
      "Epoch 1496, Loss: 0.2528523230\n",
      "Epoch 1497, Loss: 0.3006888663\n",
      "Epoch 1498, Loss: 0.2979508735\n",
      "Epoch 1499, Loss: 0.5637903306\n",
      "Epoch 1500, Loss: 0.4184122232\n",
      "Epoch 1501, Loss: 0.4461640713\n",
      "Epoch 1502, Loss: 0.3504078505\n",
      "Epoch 1503, Loss: 0.3152834090\n",
      "Epoch 1504, Loss: 0.3716355969\n",
      "Epoch 1505, Loss: 0.2813414501\n",
      "Epoch 1506, Loss: 0.5011473121\n",
      "Epoch 1507, Loss: 0.4350160692\n",
      "Epoch 1508, Loss: 0.3638592602\n",
      "Epoch 1509, Loss: 0.4193475491\n",
      "Epoch 1510, Loss: 0.5345647632\n",
      "Epoch 1511, Loss: 0.5634426590\n",
      "Epoch 1512, Loss: 0.4878293839\n",
      "Epoch 1513, Loss: 0.5546908400\n",
      "Epoch 1514, Loss: 0.3570660600\n",
      "Epoch 1515, Loss: 0.3441994260\n",
      "Epoch 1516, Loss: 0.2860027018\n",
      "Epoch 1517, Loss: 0.2958199315\n",
      "Epoch 1518, Loss: 0.2991242547\n",
      "Epoch 1519, Loss: 0.4195358893\n",
      "Epoch 1520, Loss: 0.4256666871\n",
      "Epoch 1521, Loss: 0.3475654642\n",
      "Epoch 1522, Loss: 0.2290959120\n",
      "Epoch 1523, Loss: 0.2569721078\n",
      "Epoch 1524, Loss: 0.3433055136\n",
      "Epoch 1525, Loss: 0.3592761351\n",
      "Epoch 1526, Loss: 0.3357194909\n",
      "Epoch 1527, Loss: 0.1984235070\n",
      "Epoch 1528, Loss: 0.2161545175\n",
      "Epoch 1529, Loss: 0.2411588468\n",
      "Epoch 1530, Loss: 0.3043330151\n",
      "Epoch 1531, Loss: 0.3922120510\n",
      "Epoch 1532, Loss: 0.4200705563\n",
      "Epoch 1533, Loss: 0.4982322899\n",
      "Epoch 1534, Loss: 0.4947539085\n",
      "Epoch 1535, Loss: 0.3211975732\n",
      "Epoch 1536, Loss: 0.3459428740\n",
      "Epoch 1537, Loss: 0.2008442503\n",
      "Epoch 1538, Loss: 0.2605804342\n",
      "Epoch 1539, Loss: 0.3174838075\n",
      "Epoch 1540, Loss: 0.4356718244\n",
      "Epoch 1541, Loss: 0.2633370182\n",
      "Epoch 1542, Loss: 0.2832539381\n",
      "Epoch 1543, Loss: 0.2707966221\n",
      "Epoch 1544, Loss: 0.2287456035\n",
      "Epoch 1545, Loss: 0.3082658014\n",
      "Epoch 1546, Loss: 0.4203651775\n",
      "Epoch 1547, Loss: 0.4222412562\n",
      "Epoch 1548, Loss: 0.3921544650\n",
      "Epoch 1549, Loss: 0.3993445747\n",
      "Epoch 1550, Loss: 0.2630784973\n",
      "Epoch 1551, Loss: 0.2196188195\n",
      "Epoch 1552, Loss: 0.2143108900\n",
      "Epoch 1553, Loss: 0.2635020943\n",
      "Epoch 1554, Loss: 0.4457346884\n",
      "Epoch 1555, Loss: 0.5690569434\n",
      "Epoch 1556, Loss: 0.5348808533\n",
      "Epoch 1557, Loss: 0.4352071028\n",
      "Epoch 1558, Loss: 0.3253730436\n",
      "Epoch 1559, Loss: 0.3040449863\n",
      "Epoch 1560, Loss: 0.2930932438\n",
      "Epoch 1561, Loss: 0.2165416702\n",
      "Epoch 1562, Loss: 0.2241816257\n",
      "Epoch 1563, Loss: 0.3257902442\n",
      "Epoch 1564, Loss: 0.3036920044\n",
      "Epoch 1565, Loss: 0.2449018116\n",
      "Epoch 1566, Loss: 0.2177379615\n",
      "Epoch 1567, Loss: 0.2105068760\n",
      "Epoch 1568, Loss: 0.1980290278\n",
      "Epoch 1569, Loss: 0.1874675215\n",
      "Epoch 1570, Loss: 0.2513152180\n",
      "Epoch 1571, Loss: 0.2930384927\n",
      "Epoch 1572, Loss: 0.2866489061\n",
      "Epoch 1573, Loss: 0.2859923822\n",
      "Epoch 1574, Loss: 0.3313089135\n",
      "Epoch 1575, Loss: 0.3971529020\n",
      "Epoch 1576, Loss: 0.3508622653\n",
      "Epoch 1577, Loss: 0.2937989632\n",
      "Epoch 1578, Loss: 0.2987465585\n",
      "Epoch 1579, Loss: 0.3659216535\n",
      "Epoch 1580, Loss: 0.2543726771\n",
      "Epoch 1581, Loss: 0.1959247444\n",
      "Epoch 1582, Loss: 0.1832390609\n",
      "Epoch 1583, Loss: 0.1917139632\n",
      "Epoch 1584, Loss: 0.2571301492\n",
      "Epoch 1585, Loss: 0.2647826083\n",
      "Epoch 1586, Loss: 0.3419266023\n",
      "Epoch 1587, Loss: 0.3906101694\n",
      "Epoch 1588, Loss: 0.2804481768\n",
      "Epoch 1589, Loss: 0.3996881900\n",
      "Epoch 1590, Loss: 0.3947405758\n",
      "Epoch 1591, Loss: 0.3377472482\n",
      "Epoch 1592, Loss: 0.3817003846\n",
      "Epoch 1593, Loss: 0.2551461836\n",
      "Epoch 1594, Loss: 0.2415389463\n",
      "Epoch 1595, Loss: 0.2829997217\n",
      "Epoch 1596, Loss: 0.3839217351\n",
      "Epoch 1597, Loss: 0.3265858869\n",
      "Epoch 1598, Loss: 0.2133610909\n",
      "Epoch 1599, Loss: 0.2626121744\n",
      "Epoch 1600, Loss: 0.2034705234\n",
      "Epoch 1601, Loss: 0.2631528690\n",
      "Epoch 1602, Loss: 0.3100634077\n",
      "Epoch 1603, Loss: 0.2437370677\n",
      "Epoch 1604, Loss: 0.2086900813\n",
      "Epoch 1605, Loss: 0.2645725660\n",
      "Epoch 1606, Loss: 0.2788032195\n",
      "Epoch 1607, Loss: 0.4000920498\n",
      "Epoch 1608, Loss: 0.3643636986\n",
      "Epoch 1609, Loss: 0.3916695844\n",
      "Epoch 1610, Loss: 0.4136754387\n",
      "Epoch 1611, Loss: 0.5657688039\n",
      "Epoch 1612, Loss: 0.3484848217\n",
      "Epoch 1613, Loss: 0.2644399274\n",
      "Epoch 1614, Loss: 0.2577765606\n",
      "Epoch 1615, Loss: 0.2720398733\n",
      "Epoch 1616, Loss: 0.1952805619\n",
      "Epoch 1617, Loss: 0.4975828960\n",
      "Epoch 1618, Loss: 0.2462639520\n",
      "Epoch 1619, Loss: 0.4450822849\n",
      "Epoch 1620, Loss: 0.4172527034\n",
      "Epoch 1621, Loss: 0.3223611369\n",
      "Epoch 1622, Loss: 0.4101501057\n",
      "Epoch 1623, Loss: 0.3250857347\n",
      "Epoch 1624, Loss: 0.3619511748\n",
      "Epoch 1625, Loss: 0.6314435613\n",
      "Epoch 1626, Loss: 0.5541116366\n",
      "Epoch 1627, Loss: 0.5215392386\n",
      "Epoch 1628, Loss: 0.4812759079\n",
      "Epoch 1629, Loss: 0.3680169368\n",
      "Epoch 1630, Loss: 0.8887433759\n",
      "Epoch 1631, Loss: 0.3027991983\n",
      "Epoch 1632, Loss: 0.4679176355\n",
      "Epoch 1633, Loss: 0.2967486506\n",
      "Epoch 1634, Loss: 0.2482162802\n",
      "Epoch 1635, Loss: 0.3125035670\n",
      "Epoch 1636, Loss: 0.2805822196\n",
      "Epoch 1637, Loss: 0.2922274176\n",
      "Epoch 1638, Loss: 0.2848597286\n",
      "Epoch 1639, Loss: 0.2986863263\n",
      "Epoch 1640, Loss: 0.2755880255\n",
      "Epoch 1641, Loss: 0.2411105378\n",
      "Epoch 1642, Loss: 0.3658562195\n",
      "Epoch 1643, Loss: 0.6311560414\n",
      "Epoch 1644, Loss: 0.3687827076\n",
      "Epoch 1645, Loss: 0.2436752116\n",
      "Epoch 1646, Loss: 0.3409414055\n",
      "Epoch 1647, Loss: 0.3942916672\n",
      "Epoch 1648, Loss: 0.4384432592\n",
      "Epoch 1649, Loss: 0.2974099878\n",
      "Epoch 1650, Loss: 0.2920059351\n",
      "Epoch 1651, Loss: 0.2359485169\n",
      "Epoch 1652, Loss: 0.2177688919\n",
      "Epoch 1653, Loss: 0.2990132949\n",
      "Epoch 1654, Loss: 0.3492176586\n",
      "Epoch 1655, Loss: 0.3985314028\n",
      "Epoch 1656, Loss: 0.3195597171\n",
      "Epoch 1657, Loss: 0.3756136716\n",
      "Epoch 1658, Loss: 0.3339863366\n",
      "Epoch 1659, Loss: 0.3724979441\n",
      "Epoch 1660, Loss: 0.3490668140\n",
      "Epoch 1661, Loss: 0.2080822072\n",
      "Epoch 1662, Loss: 0.3551675273\n",
      "Epoch 1663, Loss: 0.2634208866\n",
      "Epoch 1664, Loss: 0.3083962093\n",
      "Epoch 1665, Loss: 0.2520154168\n",
      "Epoch 1666, Loss: 0.1861855689\n",
      "Epoch 1667, Loss: 0.2919536149\n",
      "Epoch 1668, Loss: 0.1697914604\n",
      "Epoch 1669, Loss: 0.1968609447\n",
      "Epoch 1670, Loss: 0.3127073183\n",
      "Epoch 1671, Loss: 0.3077718623\n",
      "Epoch 1672, Loss: 0.2562064246\n",
      "Epoch 1673, Loss: 0.3806480866\n",
      "Epoch 1674, Loss: 0.2101473581\n",
      "Epoch 1675, Loss: 0.3417193902\n",
      "Epoch 1676, Loss: 0.3694674458\n",
      "Epoch 1677, Loss: 0.4419600904\n",
      "Epoch 1678, Loss: 0.2609644856\n",
      "Epoch 1679, Loss: 0.2865369157\n",
      "Epoch 1680, Loss: 0.2323496913\n",
      "Epoch 1681, Loss: 0.3927970549\n",
      "Epoch 1682, Loss: 0.2817776843\n",
      "Epoch 1683, Loss: 0.3373481925\n",
      "Epoch 1684, Loss: 0.3359447135\n",
      "Epoch 1685, Loss: 0.2609670365\n",
      "Epoch 1686, Loss: 0.2519171566\n",
      "Epoch 1687, Loss: 0.3148914292\n",
      "Epoch 1688, Loss: 0.2833254670\n",
      "Epoch 1689, Loss: 0.3590357120\n",
      "Epoch 1690, Loss: 0.3402418133\n",
      "Epoch 1691, Loss: 0.2681865132\n",
      "Epoch 1692, Loss: 0.2303677354\n",
      "Epoch 1693, Loss: 0.2397524257\n",
      "Epoch 1694, Loss: 0.2302533059\n",
      "Epoch 1695, Loss: 0.2738332365\n",
      "Epoch 1696, Loss: 0.2451284053\n",
      "Epoch 1697, Loss: 0.3852463919\n",
      "Epoch 1698, Loss: 0.6385566941\n",
      "Epoch 1699, Loss: 0.4017248266\n",
      "Epoch 1700, Loss: 0.6656551109\n",
      "Epoch 1701, Loss: 0.3657747732\n",
      "Epoch 1702, Loss: 0.2279240521\n",
      "Epoch 1703, Loss: 0.3951854280\n",
      "Epoch 1704, Loss: 0.2337640660\n",
      "Epoch 1705, Loss: 0.3471294289\n",
      "Epoch 1706, Loss: 0.2303971277\n",
      "Epoch 1707, Loss: 0.4188482303\n",
      "Epoch 1708, Loss: 0.2139536882\n",
      "Epoch 1709, Loss: 0.2962137816\n",
      "Epoch 1710, Loss: 0.3750227563\n",
      "Epoch 1711, Loss: 0.2732477528\n",
      "Epoch 1712, Loss: 0.3026229407\n",
      "Epoch 1713, Loss: 0.2785041061\n",
      "Epoch 1714, Loss: 0.3451698413\n",
      "Epoch 1715, Loss: 0.2881281313\n",
      "Epoch 1716, Loss: 0.3412000235\n",
      "Epoch 1717, Loss: 0.4811072708\n",
      "Epoch 1718, Loss: 0.5671306983\n",
      "Epoch 1719, Loss: 0.3360464782\n",
      "Epoch 1720, Loss: 0.3814466029\n",
      "Epoch 1721, Loss: 0.3724917826\n",
      "Epoch 1722, Loss: 0.3078027362\n",
      "Epoch 1723, Loss: 0.4823214434\n",
      "Epoch 1724, Loss: 0.4118807755\n",
      "Epoch 1725, Loss: 0.3526560735\n",
      "Epoch 1726, Loss: 0.3681116692\n",
      "Epoch 1727, Loss: 0.2185519167\n",
      "Epoch 1728, Loss: 0.3713507099\n",
      "Epoch 1729, Loss: 0.3536945932\n",
      "Epoch 1730, Loss: 0.3225559139\n",
      "Epoch 1731, Loss: 0.2309820786\n",
      "Epoch 1732, Loss: 0.3932306787\n",
      "Epoch 1733, Loss: 0.2666253016\n",
      "Epoch 1734, Loss: 0.3889378572\n",
      "Epoch 1735, Loss: 0.2545434663\n",
      "Epoch 1736, Loss: 0.4077055815\n",
      "Epoch 1737, Loss: 0.3281292115\n",
      "Epoch 1738, Loss: 0.4855294280\n",
      "Epoch 1739, Loss: 0.3124766078\n",
      "Epoch 1740, Loss: 0.5085607210\n",
      "Epoch 1741, Loss: 0.2977485846\n",
      "Epoch 1742, Loss: 0.3111796784\n",
      "Epoch 1743, Loss: 0.2842983700\n",
      "Epoch 1744, Loss: 0.2840434807\n",
      "Epoch 1745, Loss: 0.3710095108\n",
      "Epoch 1746, Loss: 0.3415343192\n",
      "Epoch 1747, Loss: 0.2396760030\n",
      "Epoch 1748, Loss: 0.2104310317\n",
      "Epoch 1749, Loss: 0.3133716674\n",
      "Epoch 1750, Loss: 0.2886557810\n",
      "Epoch 1751, Loss: 0.3395706675\n",
      "Epoch 1752, Loss: 0.2472716794\n",
      "Epoch 1753, Loss: 0.2932758758\n",
      "Epoch 1754, Loss: 0.4733829087\n",
      "Epoch 1755, Loss: 0.2595621906\n",
      "Epoch 1756, Loss: 0.3151665090\n",
      "Epoch 1757, Loss: 0.2586897844\n",
      "Epoch 1758, Loss: 0.3235935330\n",
      "Epoch 1759, Loss: 0.3035503741\n",
      "Epoch 1760, Loss: 0.4048703242\n",
      "Epoch 1761, Loss: 0.3508107705\n",
      "Epoch 1762, Loss: 0.4214232043\n",
      "Epoch 1763, Loss: 0.2842960934\n",
      "Epoch 1764, Loss: 0.2493514250\n",
      "Epoch 1765, Loss: 0.2993988657\n",
      "Epoch 1766, Loss: 0.3434537689\n",
      "Epoch 1767, Loss: 0.4108113944\n",
      "Epoch 1768, Loss: 0.8415322402\n",
      "Epoch 1769, Loss: 0.3404567900\n",
      "Epoch 1770, Loss: 0.2862561481\n",
      "Epoch 1771, Loss: 0.3582415824\n",
      "Epoch 1772, Loss: 0.4606798115\n",
      "Epoch 1773, Loss: 0.5023866897\n",
      "Epoch 1774, Loss: 0.3226070738\n",
      "Epoch 1775, Loss: 0.3019789364\n",
      "Epoch 1776, Loss: 0.2127707742\n",
      "Epoch 1777, Loss: 0.2480281149\n",
      "Epoch 1778, Loss: 0.3806710083\n",
      "Epoch 1779, Loss: 0.5263666461\n",
      "Epoch 1780, Loss: 0.5658022412\n",
      "Epoch 1781, Loss: 0.2766107100\n",
      "Epoch 1782, Loss: 0.3798756436\n",
      "Epoch 1783, Loss: 0.3148603773\n",
      "Epoch 1784, Loss: 0.3019176950\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_weights_init = []\n",
    "best_biases_init = []\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "# best_seed = 0\n",
    "\n",
    "for _ in range (1):\n",
    "    nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8, beta1=0.95, beta2=0.999)\n",
    "    nn.train(batches, 14, 0.0015)\n",
    "    if nn.get_best_loss() < best_loss:\n",
    "        best_loss = nn.get_best_loss()\n",
    "        best_weights = nn.get_best_weights()\n",
    "        best_biases = nn.get_best_biases()\n",
    "        # best_seed = nn.get_best_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16979146037132098\n",
      "0.16979146037132098\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "# print(best_seed)\n",
    "print(nn.get_best_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Number of layers in the Neural Network\n",
    "N = 5  # Example value, replace with the actual number of layers\n",
    "\n",
    "# Initialize the dictionary\n",
    "weights_dict = {\n",
    "    'weights': {},\n",
    "    'bias': {}\n",
    "}\n",
    "\n",
    "weights = best_weights\n",
    "biases = best_biases\n",
    "\n",
    "# Populate the weights and bias dictionaries\n",
    "for i in range(N):\n",
    "    weights_dict['weights'][f'fc{i+1}'] = weights[i]\n",
    "    weights_dict['bias'][f'fc{i+1}'] = biases[i].flatten()\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "N = 5\n",
    "# load the weights from the pickle file\n",
    "with open('weights.pkl', 'rb') as f:\n",
    "    weights_dict = pickle.load(f)\n",
    "\n",
    "# Retrieve the weights and bias from the dictionary\n",
    "weights = []\n",
    "biases = []\n",
    "for i in range(N):\n",
    "    weights.append(weights_dict['weights'][f'fc{i+1}'])\n",
    "    biases.append(weights_dict['bias'][f'fc{i+1}'])\n",
    "\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(256, 625)\n",
      "13 2\n",
      "(256, 8)\n",
      "(128, 8)\n"
     ]
    }
   ],
   "source": [
    "print(type(batches[0][0]))\n",
    "print(batches[0][0].shape)\n",
    "print(len(batches), len(batches[0]))\n",
    "print(batches[0][1].shape)\n",
    "print(batches[12][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a total of 13 batches. concatenate them\n",
    "X = np.concatenate([batch[0] for batch in batches])\n",
    "y = np.concatenate([batch[1] for batch in batches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 625)\n",
      "(3200, 8)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543.332673188227\n",
      "0.16979146037132092\n"
     ]
    }
   ],
   "source": [
    "def forward(X):\n",
    "    activations = [X]\n",
    "    pre_activations = []\n",
    "\n",
    "    # Pass through each layer except the output layer\n",
    "    for i in range(len(weights) - 1):\n",
    "        z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "        pre_activations.append(z)\n",
    "        a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "        activations.append(a)\n",
    "\n",
    "    # Pass through the output layer with softmax\n",
    "    z = np.dot(activations[-1], weights[-1]) + biases[-1]\n",
    "    pre_activations.append(z)\n",
    "    a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "    activations.append(a)\n",
    "\n",
    "    return activations, pre_activations\n",
    "\n",
    "act, pre_act = forward(X)\n",
    "pred = act[-1]\n",
    "loss = cross_entropy_loss(y, pred)\n",
    "print(loss)\n",
    "loss /= len(y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200,)\n",
      "[5 3 1 7 2 2 1 7 4 2]\n"
     ]
    }
   ],
   "source": [
    "# get argmax of the predictions\n",
    "y_pred = np.argmax(pred, axis=1)\n",
    "\n",
    "print(y_pred.shape)\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save y_pred in pickle file\n",
    "with open('predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
