{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Remember to import \"numpy_transforms\" functions if you wish to import these two classes in a different script.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "        label = row[\"class\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image), label\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image, label = self.dataset[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "        batch_labels = np.array(labels)\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(128, 625)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    print(labels.shape)  # Should be [batch_size]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For baseline purpose, running part B code in the same script\n",
    "\n",
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# # Sigmoid activation and its derivative\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def sigmoid_derivative(x):\n",
    "#     s = sigmoid(x)\n",
    "#     return s * (1 - s)\n",
    "\n",
    "# def softmax(x, axis=None):\n",
    "#     exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "#     return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# # Cross-entropy loss\n",
    "# def cross_entropy_loss(y_true, y_pred):\n",
    "#     y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "#     return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# # Neural Network Class with Softmax in the Output Layer and Sigmoid in Hidden Layers\n",
    "# class NeuralNetwork_Baseline:\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size):\n",
    "#         np.random.seed(0)\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "\n",
    "#         layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "#         # Initialize weights and biases\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "#             self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         pre_activations = []\n",
    "\n",
    "#         # Pass through each layer except the output layer\n",
    "#         for i in range(len(self.weights) - 1):\n",
    "#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             pre_activations.append(z)\n",
    "#             a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "#             activations.append(a)\n",
    "\n",
    "#         # Pass through the output layer with softmax\n",
    "#         z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         pre_activations.append(z)\n",
    "#         a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "#         activations.append(a)\n",
    "\n",
    "#         return activations, pre_activations\n",
    "\n",
    "#     def backward(self, X, y, activations, pre_activations):\n",
    "#         grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "#         grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "#         # Start with output layer error\n",
    "#         delta = activations[-1] - y\n",
    "\n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "#             grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "#             if i > 0:\n",
    "#                 delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "\n",
    "#         return grad_w, grad_b\n",
    "\n",
    "#     def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "#         for i in range(len(self.weights)):\n",
    "#             self.weights[i] -= learning_rate * grad_w[i]\n",
    "#             self.biases[i] -= learning_rate * grad_b[i]\n",
    "\n",
    "#     def train(self, batches, learning_rate):\n",
    "#         start_time = time.time()\n",
    "#         epoch = 0\n",
    "#         while (True):\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 activations, pre_activations = self.forward(X_batch)\n",
    "#                 grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "#                 self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "#             # Calculate average loss over batches\n",
    "#             loss = 0\n",
    "#             z = 0\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 y_pred, _ = self.forward(X_batch)\n",
    "#                 loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "#                 z += len(y_pred[-1])\n",
    "#             loss /= z\n",
    "#             print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "#             epoch += 1\n",
    "#             if (time.time() - start_time) > 60:\n",
    "#                 break\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "#     def get_weights(self):\n",
    "#         return self.weights\n",
    "\n",
    "#     def get_biases(self):\n",
    "#         return self.biases\n",
    "\n",
    "# # Example usage:\n",
    "# nn = NeuralNetwork_Baseline(625, [512, 256, 128, 32], 8)\n",
    "# nn.train(batches, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Sigmoid activation and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Neural Network Class with Softmax in the Output Layer and Sigmoid in Hidden Layers\n",
    "class NeuralNetwork_Adam:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, init_weights = None, init_biases = None, init_seed = None, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        if (init_seed is None):\n",
    "            self.best_seed = int(time.time())\n",
    "            np.random.seed(self.best_seed)\n",
    "        else:\n",
    "            np.random.seed(init_seed)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.m_w = []\n",
    "        self.v_w = []\n",
    "        self.m_b = []\n",
    "        self.v_b = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step for Adam\n",
    "        self.best_weights = []\n",
    "        self.best_biases = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if (init_weights is not None) and (init_biases is not None):\n",
    "                self.weights.append(init_weights[i])\n",
    "                self.biases.append(init_biases[i])\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "                self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "            self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.best_weights = self.weights\n",
    "            self.best_biases = self.biases\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each layer except the output layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        self.t += 1  # Increment time step for Adam\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def train(self, batches, time_of_running, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while(True):\n",
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
    "            loss /= z\n",
    "            \n",
    "            if (loss < self.best_loss):\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = self.weights\n",
    "                self.best_biases = self.biases\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            epoch += 1\n",
    "            if time.time() - start_time > 60*time_of_running:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    \n",
    "    def get_best_biases(self):\n",
    "        return self.best_biases\n",
    "    \n",
    "    def get_best_loss(self):\n",
    "        return self.best_loss\n",
    "    \n",
    "    def get_best_seed(self):\n",
    "        return self.best_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0847705717\n",
      "Epoch 2, Loss: 2.0830641002\n",
      "Epoch 3, Loss: 2.0792866890\n",
      "Epoch 4, Loss: 2.0792339175\n",
      "Epoch 5, Loss: 2.0787628911\n",
      "Epoch 6, Loss: 2.0782313542\n",
      "Epoch 7, Loss: 2.0774237375\n",
      "Epoch 8, Loss: 2.0760058061\n",
      "Epoch 9, Loss: 2.0735814832\n",
      "Epoch 10, Loss: 2.0695028404\n",
      "Epoch 11, Loss: 2.0626650579\n",
      "Epoch 12, Loss: 2.0537338466\n",
      "Epoch 13, Loss: 2.0330557539\n",
      "Epoch 14, Loss: 2.0154985262\n",
      "Epoch 15, Loss: 1.9754347935\n",
      "Epoch 16, Loss: 1.9045834434\n",
      "Epoch 17, Loss: 1.8933598837\n",
      "Epoch 18, Loss: 1.8182076249\n",
      "Epoch 19, Loss: 1.7618549651\n",
      "Epoch 20, Loss: 1.7787823379\n",
      "Epoch 21, Loss: 1.7478924979\n",
      "Epoch 22, Loss: 1.6277276463\n",
      "Epoch 23, Loss: 1.6109096523\n",
      "Epoch 24, Loss: 1.6393730024\n",
      "Epoch 25, Loss: 1.5452481495\n",
      "Epoch 26, Loss: 1.5358609731\n",
      "Epoch 27, Loss: 1.5432493248\n",
      "Epoch 28, Loss: 1.4753527946\n",
      "Epoch 29, Loss: 1.4891893637\n",
      "Epoch 30, Loss: 1.4798243307\n",
      "Epoch 31, Loss: 1.4376026307\n",
      "Epoch 32, Loss: 1.4541129370\n",
      "Epoch 33, Loss: 1.4383403142\n",
      "Epoch 34, Loss: 1.4161704775\n",
      "Epoch 35, Loss: 1.4375272322\n",
      "Epoch 36, Loss: 1.4261325135\n",
      "Epoch 37, Loss: 1.4132025789\n",
      "Epoch 38, Loss: 1.4216809357\n",
      "Epoch 39, Loss: 1.3880664619\n",
      "Epoch 40, Loss: 1.3738365124\n",
      "Epoch 41, Loss: 1.3804324329\n",
      "Epoch 42, Loss: 1.3693360721\n",
      "Epoch 43, Loss: 1.3614758417\n",
      "Epoch 44, Loss: 1.3775021991\n",
      "Epoch 45, Loss: 1.3681311189\n",
      "Epoch 46, Loss: 1.3491608838\n",
      "Epoch 47, Loss: 1.3652759013\n",
      "Epoch 48, Loss: 1.3552845999\n",
      "Epoch 49, Loss: 1.3281476923\n",
      "Epoch 50, Loss: 1.3473412211\n",
      "Epoch 51, Loss: 1.3438165468\n",
      "Epoch 52, Loss: 1.3104040582\n",
      "Epoch 53, Loss: 1.3327602172\n",
      "Epoch 54, Loss: 1.3305665861\n",
      "Epoch 55, Loss: 1.2950157666\n",
      "Epoch 56, Loss: 1.3152219251\n",
      "Epoch 57, Loss: 1.3223578211\n",
      "Epoch 58, Loss: 1.2799110508\n",
      "Epoch 59, Loss: 1.2916285377\n",
      "Epoch 60, Loss: 1.3151358218\n",
      "Epoch 61, Loss: 1.2666830264\n",
      "Epoch 62, Loss: 1.2586792335\n",
      "Epoch 63, Loss: 1.3029580689\n",
      "Epoch 64, Loss: 1.3118009487\n",
      "Epoch 65, Loss: 1.2442869088\n",
      "Epoch 66, Loss: 1.2828065423\n",
      "Epoch 67, Loss: 1.3650614053\n",
      "Epoch 68, Loss: 1.2930641761\n",
      "Epoch 69, Loss: 1.2820361578\n",
      "Epoch 70, Loss: 1.3685484264\n",
      "Epoch 71, Loss: 1.2441519108\n",
      "Epoch 72, Loss: 1.2722569299\n",
      "Epoch 73, Loss: 1.2705514874\n",
      "Epoch 74, Loss: 1.2101379378\n",
      "Epoch 75, Loss: 1.2059922144\n",
      "Epoch 76, Loss: 1.2242017277\n",
      "Epoch 77, Loss: 1.2668351114\n",
      "Epoch 78, Loss: 1.2021168263\n",
      "Epoch 79, Loss: 1.2033701058\n",
      "Epoch 80, Loss: 1.2284219954\n",
      "Epoch 81, Loss: 1.2474586931\n",
      "Epoch 82, Loss: 1.2392163780\n",
      "Epoch 83, Loss: 1.1952619181\n",
      "Epoch 84, Loss: 1.1813476181\n",
      "Epoch 85, Loss: 1.1788870975\n",
      "Epoch 86, Loss: 1.1811088789\n",
      "Epoch 87, Loss: 1.1846229040\n",
      "Epoch 88, Loss: 1.1820057757\n",
      "Epoch 89, Loss: 1.1714364202\n",
      "Epoch 90, Loss: 1.1623665252\n",
      "Epoch 91, Loss: 1.1633536541\n",
      "Epoch 92, Loss: 1.1823510922\n",
      "Epoch 93, Loss: 1.1741205777\n",
      "Epoch 94, Loss: 1.1366047326\n",
      "Epoch 95, Loss: 1.1323395251\n",
      "Epoch 96, Loss: 1.1298491435\n",
      "Epoch 97, Loss: 1.1259712005\n",
      "Epoch 98, Loss: 1.1220362335\n",
      "Epoch 99, Loss: 1.1153218420\n",
      "Epoch 100, Loss: 1.1089672838\n",
      "Epoch 101, Loss: 1.1047084614\n",
      "Epoch 102, Loss: 1.1088987541\n",
      "Epoch 103, Loss: 1.1154310508\n",
      "Epoch 104, Loss: 1.1148972070\n",
      "Epoch 105, Loss: 1.1177730065\n",
      "Epoch 106, Loss: 1.1414776671\n",
      "Epoch 107, Loss: 1.1744314294\n",
      "Epoch 108, Loss: 1.0972839529\n",
      "Epoch 109, Loss: 1.0718990835\n",
      "Epoch 110, Loss: 1.0753523822\n",
      "Epoch 111, Loss: 1.0910700106\n",
      "Epoch 112, Loss: 1.0986846375\n",
      "Epoch 113, Loss: 1.1126861607\n",
      "Epoch 114, Loss: 1.1213736798\n",
      "Epoch 115, Loss: 1.0852726258\n",
      "Epoch 116, Loss: 1.0578800812\n",
      "Epoch 117, Loss: 1.0560275381\n",
      "Epoch 118, Loss: 1.0621216217\n",
      "Epoch 119, Loss: 1.0720223347\n",
      "Epoch 120, Loss: 1.0731968819\n",
      "Epoch 121, Loss: 1.0787864992\n",
      "Epoch 122, Loss: 1.0806500128\n",
      "Epoch 123, Loss: 1.0775438442\n",
      "Epoch 124, Loss: 1.0729376719\n",
      "Epoch 125, Loss: 1.0699887429\n",
      "Epoch 126, Loss: 1.0681242317\n",
      "Epoch 127, Loss: 1.0679984818\n",
      "Epoch 128, Loss: 1.0676507454\n",
      "Epoch 129, Loss: 1.0674587912\n",
      "Epoch 130, Loss: 1.0654988477\n",
      "Epoch 131, Loss: 1.0615763202\n",
      "Epoch 132, Loss: 1.0530809065\n",
      "Epoch 133, Loss: 1.0413168705\n",
      "Epoch 134, Loss: 1.0299479785\n",
      "Epoch 135, Loss: 1.0260649482\n",
      "Epoch 136, Loss: 1.0329186425\n",
      "Epoch 137, Loss: 1.0690686148\n",
      "Epoch 138, Loss: 1.2729703253\n",
      "Epoch 139, Loss: 1.1918690658\n",
      "Epoch 140, Loss: 1.0592570993\n",
      "Epoch 141, Loss: 1.0449677372\n",
      "Epoch 142, Loss: 1.0845300536\n",
      "Epoch 143, Loss: 1.1368990069\n",
      "Epoch 144, Loss: 1.2540837190\n",
      "Epoch 145, Loss: 1.0766540564\n",
      "Epoch 146, Loss: 1.0193703407\n",
      "Epoch 147, Loss: 1.0689391291\n",
      "Epoch 148, Loss: 1.0100736071\n",
      "Epoch 149, Loss: 1.0363146457\n",
      "Epoch 150, Loss: 1.0170109967\n",
      "Epoch 151, Loss: 1.0031852618\n",
      "Epoch 152, Loss: 1.0053978408\n",
      "Epoch 153, Loss: 1.0164629260\n",
      "Epoch 154, Loss: 1.0145527874\n",
      "Epoch 155, Loss: 1.0003421613\n",
      "Epoch 156, Loss: 0.9962919726\n",
      "Epoch 157, Loss: 1.0102552157\n",
      "Epoch 158, Loss: 1.0158284789\n",
      "Epoch 159, Loss: 0.9993397837\n",
      "Epoch 160, Loss: 0.9882379691\n",
      "Epoch 161, Loss: 0.9975953717\n",
      "Epoch 162, Loss: 1.0146377538\n",
      "Epoch 163, Loss: 1.0017702182\n",
      "Epoch 164, Loss: 0.9887771682\n",
      "Epoch 165, Loss: 0.9869650459\n",
      "Epoch 166, Loss: 1.0096301318\n",
      "Epoch 167, Loss: 0.9915529223\n",
      "Epoch 168, Loss: 0.9852679995\n",
      "Epoch 169, Loss: 0.9793458094\n",
      "Epoch 170, Loss: 1.0089056978\n",
      "Epoch 171, Loss: 0.9890545172\n",
      "Epoch 172, Loss: 0.9849912605\n",
      "Epoch 173, Loss: 0.9854416139\n",
      "Epoch 174, Loss: 1.0114199373\n",
      "Epoch 175, Loss: 0.9786752092\n",
      "Epoch 176, Loss: 0.9790552637\n",
      "Epoch 177, Loss: 1.0042974826\n",
      "Epoch 178, Loss: 1.0065845094\n",
      "Epoch 179, Loss: 0.9744222685\n",
      "Epoch 180, Loss: 0.9676570870\n",
      "Epoch 181, Loss: 1.0055009312\n",
      "Epoch 182, Loss: 0.9979768206\n",
      "Epoch 183, Loss: 0.9691156221\n",
      "Epoch 184, Loss: 0.9577778965\n",
      "Epoch 185, Loss: 0.9886819352\n",
      "Epoch 186, Loss: 0.9922622894\n",
      "Epoch 187, Loss: 0.9677227729\n",
      "Epoch 188, Loss: 0.9599203731\n",
      "Epoch 189, Loss: 0.9926796465\n",
      "Epoch 190, Loss: 0.9981852974\n",
      "Epoch 191, Loss: 0.9670308273\n",
      "Epoch 192, Loss: 0.9483807699\n",
      "Epoch 193, Loss: 0.9481505753\n",
      "Epoch 194, Loss: 0.9685826059\n",
      "Epoch 195, Loss: 0.9579240615\n",
      "Epoch 196, Loss: 0.9644903899\n",
      "Epoch 197, Loss: 0.9570728636\n",
      "Epoch 198, Loss: 0.9508948016\n",
      "Epoch 199, Loss: 0.9783812082\n",
      "Epoch 200, Loss: 0.9538443032\n",
      "Epoch 201, Loss: 0.9600837544\n",
      "Epoch 202, Loss: 0.9709923529\n",
      "Epoch 203, Loss: 0.9811328808\n",
      "Epoch 204, Loss: 0.9462840580\n",
      "Epoch 205, Loss: 0.9506561588\n",
      "Epoch 206, Loss: 0.9936981330\n",
      "Epoch 207, Loss: 1.0053241320\n",
      "Epoch 208, Loss: 0.9826355598\n",
      "Epoch 209, Loss: 0.9672623685\n",
      "Epoch 210, Loss: 0.9739123375\n",
      "Epoch 211, Loss: 0.9584084484\n",
      "Epoch 212, Loss: 1.0095901176\n",
      "Epoch 213, Loss: 0.9367209409\n",
      "Epoch 214, Loss: 0.9985160660\n",
      "Epoch 215, Loss: 0.9277359485\n",
      "Epoch 216, Loss: 0.9218900896\n",
      "Epoch 217, Loss: 0.9087603263\n",
      "Epoch 218, Loss: 0.9219789400\n",
      "Epoch 219, Loss: 0.9149600502\n",
      "Epoch 220, Loss: 0.9141392291\n",
      "Epoch 221, Loss: 0.9163633148\n",
      "Epoch 222, Loss: 0.9068705070\n",
      "Epoch 223, Loss: 0.9040046075\n",
      "Epoch 224, Loss: 0.9058346709\n",
      "Epoch 225, Loss: 0.9053898574\n",
      "Epoch 226, Loss: 0.9064218504\n",
      "Epoch 227, Loss: 0.9095123173\n",
      "Epoch 228, Loss: 0.9109909494\n",
      "Epoch 229, Loss: 0.9182311733\n",
      "Epoch 230, Loss: 0.9301173333\n",
      "Epoch 231, Loss: 0.9704437641\n",
      "Epoch 232, Loss: 0.9281563662\n",
      "Epoch 233, Loss: 0.9529868798\n",
      "Epoch 234, Loss: 0.9374101157\n",
      "Epoch 235, Loss: 0.9654772627\n",
      "Epoch 236, Loss: 0.9312795006\n",
      "Epoch 237, Loss: 0.9138681640\n",
      "Epoch 238, Loss: 0.8883495473\n",
      "Epoch 239, Loss: 0.8850755929\n",
      "Epoch 240, Loss: 0.8813341229\n",
      "Epoch 241, Loss: 0.8813278038\n",
      "Epoch 242, Loss: 0.8824095955\n",
      "Epoch 243, Loss: 0.8812797019\n",
      "Epoch 244, Loss: 0.8800001014\n",
      "Epoch 245, Loss: 0.8814394253\n",
      "Epoch 246, Loss: 0.8828265533\n",
      "Epoch 247, Loss: 0.8835046396\n",
      "Epoch 248, Loss: 0.8807385984\n",
      "Epoch 249, Loss: 0.8964537538\n",
      "Epoch 250, Loss: 0.8972989474\n",
      "Epoch 251, Loss: 0.8811760713\n",
      "Epoch 252, Loss: 0.8798502809\n",
      "Epoch 253, Loss: 0.8748422390\n",
      "Epoch 254, Loss: 0.8726723952\n",
      "Epoch 255, Loss: 0.8690754968\n",
      "Epoch 256, Loss: 0.8781169460\n",
      "Epoch 257, Loss: 0.8742997287\n",
      "Epoch 258, Loss: 0.8696702930\n",
      "Epoch 259, Loss: 0.8706755316\n",
      "Epoch 260, Loss: 0.8863751089\n",
      "Epoch 261, Loss: 0.8975834429\n",
      "Epoch 262, Loss: 0.8879855620\n",
      "Epoch 263, Loss: 0.8902346247\n",
      "Epoch 264, Loss: 0.9078096048\n",
      "Epoch 265, Loss: 0.9222581047\n",
      "Epoch 266, Loss: 0.9151149018\n",
      "Epoch 267, Loss: 0.8914861051\n",
      "Epoch 268, Loss: 0.9038054067\n",
      "Epoch 269, Loss: 0.8979766998\n",
      "Epoch 270, Loss: 0.9380735704\n",
      "Epoch 271, Loss: 0.9346098996\n",
      "Epoch 272, Loss: 0.9085779407\n",
      "Epoch 273, Loss: 0.9051041560\n",
      "Epoch 274, Loss: 0.9199634785\n",
      "Epoch 275, Loss: 0.9249715400\n",
      "Epoch 276, Loss: 0.9649282182\n",
      "Epoch 277, Loss: 0.8679457819\n",
      "Epoch 278, Loss: 0.8775624552\n",
      "Epoch 279, Loss: 0.8602262443\n",
      "Epoch 280, Loss: 0.8525845955\n",
      "Epoch 281, Loss: 0.8602298477\n",
      "Epoch 282, Loss: 0.8693659342\n",
      "Epoch 283, Loss: 0.8699192613\n",
      "Epoch 284, Loss: 0.8623910523\n",
      "Epoch 285, Loss: 0.8554081079\n",
      "Epoch 286, Loss: 0.8555628436\n",
      "Epoch 287, Loss: 0.8724412831\n",
      "Epoch 288, Loss: 0.8737693521\n",
      "Epoch 289, Loss: 0.8689619146\n",
      "Epoch 290, Loss: 0.8635640050\n",
      "Epoch 291, Loss: 0.9183163383\n",
      "Epoch 292, Loss: 0.9122739657\n",
      "Epoch 293, Loss: 0.9132331500\n",
      "Epoch 294, Loss: 0.9276868684\n",
      "Epoch 295, Loss: 0.8912462815\n",
      "Epoch 296, Loss: 0.9142409909\n",
      "Epoch 297, Loss: 0.8368566569\n",
      "Epoch 298, Loss: 0.8558427970\n",
      "Epoch 299, Loss: 0.8891897091\n",
      "Epoch 300, Loss: 0.8959588592\n",
      "Epoch 301, Loss: 0.8943164760\n",
      "Epoch 302, Loss: 0.8800176961\n",
      "Epoch 303, Loss: 0.8599530512\n",
      "Epoch 304, Loss: 0.8625537309\n",
      "Epoch 305, Loss: 0.8722683197\n",
      "Epoch 306, Loss: 0.8786573617\n",
      "Epoch 307, Loss: 0.8818809168\n",
      "Epoch 308, Loss: 0.8766834664\n",
      "Epoch 309, Loss: 0.8620839137\n",
      "Epoch 310, Loss: 0.8451186975\n",
      "Epoch 311, Loss: 0.8375778137\n",
      "Epoch 312, Loss: 0.8367170291\n",
      "Epoch 313, Loss: 0.8414495805\n",
      "Epoch 314, Loss: 0.8470726870\n",
      "Epoch 315, Loss: 0.8543714202\n",
      "Epoch 316, Loss: 0.8530601034\n",
      "Epoch 317, Loss: 0.8620085170\n",
      "Epoch 318, Loss: 0.8913491790\n",
      "Epoch 319, Loss: 0.8266575500\n",
      "Epoch 320, Loss: 0.8099358024\n",
      "Epoch 321, Loss: 0.8746075055\n",
      "Epoch 322, Loss: 0.8440248246\n",
      "Epoch 323, Loss: 0.8087710673\n",
      "Epoch 324, Loss: 0.8137490263\n",
      "Epoch 325, Loss: 0.8471004083\n",
      "Epoch 326, Loss: 0.8249642095\n",
      "Epoch 327, Loss: 0.8073319584\n",
      "Epoch 328, Loss: 0.7979019519\n",
      "Epoch 329, Loss: 0.7970533608\n",
      "Epoch 330, Loss: 0.8018648322\n",
      "Epoch 331, Loss: 0.8241990783\n",
      "Epoch 332, Loss: 0.8733587254\n",
      "Epoch 333, Loss: 0.8313561166\n",
      "Epoch 334, Loss: 0.8077255992\n",
      "Epoch 335, Loss: 0.8010333434\n",
      "Epoch 336, Loss: 0.7993174331\n",
      "Epoch 337, Loss: 0.8070029799\n",
      "Epoch 338, Loss: 0.8180785517\n",
      "Epoch 339, Loss: 0.8309378379\n",
      "Epoch 340, Loss: 0.8356287853\n",
      "Epoch 341, Loss: 0.8915347248\n",
      "Epoch 342, Loss: 0.8301004885\n",
      "Epoch 343, Loss: 0.8263124298\n",
      "Epoch 344, Loss: 0.8296872996\n",
      "Epoch 345, Loss: 0.8379025857\n",
      "Epoch 346, Loss: 0.8742561442\n",
      "Epoch 347, Loss: 0.8623366375\n",
      "Epoch 348, Loss: 0.8642699900\n",
      "Epoch 349, Loss: 0.8566482282\n",
      "Epoch 350, Loss: 0.8953690463\n",
      "Epoch 351, Loss: 0.8172075243\n",
      "Epoch 352, Loss: 0.8919525241\n",
      "Epoch 353, Loss: 0.8952608297\n",
      "Epoch 354, Loss: 0.7959591990\n",
      "Epoch 355, Loss: 0.8824515521\n",
      "Epoch 356, Loss: 0.8277282610\n",
      "Epoch 357, Loss: 0.7973103891\n",
      "Epoch 358, Loss: 0.8219426695\n",
      "Epoch 359, Loss: 0.8502447056\n",
      "Epoch 360, Loss: 0.8588906221\n",
      "Epoch 361, Loss: 0.8715611763\n",
      "Epoch 362, Loss: 0.8981753944\n",
      "Epoch 363, Loss: 0.9336032301\n",
      "Epoch 364, Loss: 0.9107487874\n",
      "Epoch 365, Loss: 0.8048841933\n",
      "Epoch 366, Loss: 0.7743050565\n",
      "Epoch 367, Loss: 0.7904488521\n",
      "Epoch 368, Loss: 0.7928987167\n",
      "Epoch 369, Loss: 0.7868616929\n",
      "Epoch 370, Loss: 0.7934222173\n",
      "Epoch 371, Loss: 0.7839177343\n",
      "Epoch 372, Loss: 0.8523247419\n",
      "Epoch 373, Loss: 0.8422894446\n",
      "Epoch 374, Loss: 0.8753739250\n",
      "Epoch 375, Loss: 0.7950240834\n",
      "Epoch 376, Loss: 0.8126308808\n",
      "Epoch 377, Loss: 0.8220589195\n",
      "Epoch 378, Loss: 0.7848963617\n",
      "Epoch 379, Loss: 0.7718398732\n",
      "Epoch 380, Loss: 0.7796624491\n",
      "Epoch 381, Loss: 0.7730918814\n",
      "Epoch 382, Loss: 0.7648024408\n",
      "Epoch 383, Loss: 0.7669437784\n",
      "Epoch 384, Loss: 0.7737511687\n",
      "Epoch 385, Loss: 0.7862743858\n",
      "Epoch 386, Loss: 0.7929948000\n",
      "Epoch 387, Loss: 0.7944726668\n",
      "Epoch 388, Loss: 0.7917018232\n",
      "Epoch 389, Loss: 0.7931050829\n",
      "Epoch 390, Loss: 0.7880263670\n",
      "Epoch 391, Loss: 0.8391927691\n",
      "Epoch 392, Loss: 0.8586661734\n",
      "Epoch 393, Loss: 0.8202025798\n",
      "Epoch 394, Loss: 0.8094551718\n",
      "Epoch 395, Loss: 0.8196874181\n",
      "Epoch 396, Loss: 0.8085233100\n",
      "Epoch 397, Loss: 0.8302150817\n",
      "Epoch 398, Loss: 0.8475187941\n",
      "Epoch 399, Loss: 0.8191037324\n",
      "Epoch 400, Loss: 0.8428778078\n",
      "Epoch 401, Loss: 0.8557918405\n",
      "Epoch 402, Loss: 0.8341216555\n",
      "Epoch 403, Loss: 0.8477356955\n",
      "Epoch 404, Loss: 0.9250784723\n",
      "Epoch 405, Loss: 0.8477973802\n",
      "Epoch 406, Loss: 0.9483230838\n",
      "Epoch 407, Loss: 0.9081047201\n",
      "Epoch 408, Loss: 0.7607579089\n",
      "Epoch 409, Loss: 0.7610952027\n",
      "Epoch 410, Loss: 0.7813825186\n",
      "Epoch 411, Loss: 0.7783902176\n",
      "Epoch 412, Loss: 0.8082277505\n",
      "Epoch 413, Loss: 0.8009725407\n",
      "Epoch 414, Loss: 0.8628037893\n",
      "Epoch 415, Loss: 0.8338981558\n",
      "Epoch 416, Loss: 0.7827191119\n",
      "Epoch 417, Loss: 0.9280774398\n",
      "Epoch 418, Loss: 0.7768781373\n",
      "Epoch 419, Loss: 0.7883538202\n",
      "Epoch 420, Loss: 0.7996177046\n",
      "Epoch 421, Loss: 0.8613398881\n",
      "Epoch 422, Loss: 0.7880639304\n",
      "Epoch 423, Loss: 0.7439902682\n",
      "Epoch 424, Loss: 0.7392607471\n",
      "Epoch 425, Loss: 0.7655723985\n",
      "Epoch 426, Loss: 0.8596868848\n",
      "Epoch 427, Loss: 0.7791333403\n",
      "Epoch 428, Loss: 0.7496533310\n",
      "Epoch 429, Loss: 0.7584810982\n",
      "Epoch 430, Loss: 0.7661688330\n",
      "Epoch 431, Loss: 0.7678166719\n",
      "Epoch 432, Loss: 0.7548887837\n",
      "Epoch 433, Loss: 0.7552088717\n",
      "Epoch 434, Loss: 0.7679180453\n",
      "Epoch 435, Loss: 0.7833977323\n",
      "Epoch 436, Loss: 0.7654449543\n",
      "Epoch 437, Loss: 0.7475477382\n",
      "Epoch 438, Loss: 0.8109341459\n",
      "Epoch 439, Loss: 0.8149159476\n",
      "Epoch 440, Loss: 0.7836764516\n",
      "Epoch 441, Loss: 0.7466671662\n",
      "Epoch 442, Loss: 0.8540088693\n",
      "Epoch 443, Loss: 0.9941745889\n",
      "Epoch 444, Loss: 0.9101689678\n",
      "Epoch 445, Loss: 1.0460859814\n",
      "Epoch 446, Loss: 0.7600584604\n",
      "Epoch 447, Loss: 0.7802717177\n",
      "Epoch 448, Loss: 0.7867902281\n",
      "Epoch 449, Loss: 0.7546172187\n",
      "Epoch 450, Loss: 0.7383577077\n",
      "Epoch 451, Loss: 0.7434237982\n",
      "Epoch 452, Loss: 0.7959500889\n",
      "Epoch 453, Loss: 0.8511156264\n",
      "Epoch 454, Loss: 0.7828747900\n",
      "Epoch 455, Loss: 0.8260458503\n",
      "Epoch 456, Loss: 0.8100410123\n",
      "Epoch 457, Loss: 0.7786647253\n",
      "Epoch 458, Loss: 0.7562351428\n",
      "Epoch 459, Loss: 0.8367837269\n",
      "Epoch 460, Loss: 0.7734456104\n",
      "Epoch 461, Loss: 0.7556499340\n",
      "Epoch 462, Loss: 0.7526932564\n",
      "Epoch 463, Loss: 0.7781303259\n",
      "Epoch 464, Loss: 0.7513094701\n",
      "Epoch 465, Loss: 0.7744445484\n",
      "Epoch 466, Loss: 0.9637496657\n",
      "Epoch 467, Loss: 0.7734841876\n",
      "Epoch 468, Loss: 1.0109775427\n",
      "Epoch 469, Loss: 0.9803161751\n",
      "Epoch 470, Loss: 0.7648802509\n",
      "Epoch 471, Loss: 0.7319547225\n",
      "Epoch 472, Loss: 0.8302001845\n",
      "Epoch 473, Loss: 1.2869056023\n",
      "Epoch 474, Loss: 0.8648180359\n",
      "Epoch 475, Loss: 0.9719585560\n",
      "Epoch 476, Loss: 0.7592422582\n",
      "Epoch 477, Loss: 0.8668294200\n",
      "Epoch 478, Loss: 0.9429594624\n",
      "Epoch 479, Loss: 1.1063503285\n",
      "Epoch 480, Loss: 0.7507081907\n",
      "Epoch 481, Loss: 0.7697432009\n",
      "Epoch 482, Loss: 0.9161281261\n",
      "Epoch 483, Loss: 1.0212974901\n",
      "Epoch 484, Loss: 0.8288003860\n",
      "Epoch 485, Loss: 0.8154911346\n",
      "Epoch 486, Loss: 0.8177078571\n",
      "Epoch 487, Loss: 0.8335363226\n",
      "Epoch 488, Loss: 0.8678591986\n",
      "Epoch 489, Loss: 0.9384428355\n",
      "Epoch 490, Loss: 1.0453842372\n",
      "Epoch 491, Loss: 0.7985741226\n",
      "Epoch 492, Loss: 0.7509345294\n",
      "Epoch 493, Loss: 0.8565426296\n",
      "Epoch 494, Loss: 0.8667957328\n",
      "Epoch 495, Loss: 0.8392225722\n",
      "Epoch 496, Loss: 0.8735127195\n",
      "Epoch 497, Loss: 0.9182130618\n",
      "Epoch 498, Loss: 1.0463080900\n",
      "Epoch 499, Loss: 0.7843933928\n",
      "Epoch 500, Loss: 0.7476978568\n",
      "Epoch 501, Loss: 0.7881429515\n",
      "Epoch 502, Loss: 0.8783290991\n",
      "Epoch 503, Loss: 0.8876096052\n",
      "Epoch 504, Loss: 0.8007414886\n",
      "Epoch 505, Loss: 0.8250893605\n",
      "Epoch 506, Loss: 0.8447650939\n",
      "Epoch 507, Loss: 0.9376348681\n",
      "Epoch 508, Loss: 1.0571907826\n",
      "Epoch 509, Loss: 0.7958673449\n",
      "Epoch 510, Loss: 0.7466093563\n",
      "Epoch 511, Loss: 0.7735551952\n",
      "Epoch 512, Loss: 0.8006994417\n",
      "Epoch 513, Loss: 0.8469083682\n",
      "Epoch 514, Loss: 0.8702642141\n",
      "Epoch 515, Loss: 0.8154271234\n",
      "Epoch 516, Loss: 0.7862092556\n",
      "Epoch 517, Loss: 0.7941929984\n",
      "Epoch 518, Loss: 0.7989190224\n",
      "Epoch 519, Loss: 0.8313405142\n",
      "Epoch 520, Loss: 0.8141071702\n",
      "Epoch 521, Loss: 0.8117556011\n",
      "Epoch 522, Loss: 0.7663605848\n",
      "Epoch 523, Loss: 0.7380102437\n",
      "Epoch 524, Loss: 0.7316183441\n",
      "Epoch 525, Loss: 0.7428377901\n",
      "Epoch 526, Loss: 0.7642435748\n",
      "Epoch 527, Loss: 0.7779391483\n",
      "Epoch 528, Loss: 0.7769372678\n",
      "Epoch 529, Loss: 0.7762095158\n",
      "Epoch 530, Loss: 0.7792153922\n",
      "Epoch 531, Loss: 0.7876233510\n",
      "Epoch 532, Loss: 0.8063717827\n",
      "Epoch 533, Loss: 0.8407904697\n",
      "Epoch 534, Loss: 0.8751149836\n",
      "Epoch 535, Loss: 0.8738839361\n",
      "Epoch 536, Loss: 0.8755113163\n",
      "Epoch 537, Loss: 0.8389492454\n",
      "Epoch 538, Loss: 0.8651784824\n",
      "Epoch 539, Loss: 0.7772430885\n",
      "Epoch 540, Loss: 0.7035996227\n",
      "Epoch 541, Loss: 0.7195739388\n",
      "Epoch 542, Loss: 0.7490498531\n",
      "Epoch 543, Loss: 0.7628266496\n",
      "Epoch 544, Loss: 0.7931175972\n",
      "Epoch 545, Loss: 0.7726513473\n",
      "Epoch 546, Loss: 0.7964695141\n",
      "Epoch 547, Loss: 0.7927042720\n",
      "Epoch 548, Loss: 0.7850374533\n",
      "Epoch 549, Loss: 0.7855417015\n",
      "Epoch 550, Loss: 0.7859036724\n",
      "Epoch 551, Loss: 0.7831357189\n",
      "Epoch 552, Loss: 0.7953045312\n",
      "Epoch 553, Loss: 0.8019807034\n",
      "Epoch 554, Loss: 0.8121540258\n",
      "Epoch 555, Loss: 0.6735262040\n",
      "Epoch 556, Loss: 0.7655408560\n",
      "Epoch 557, Loss: 0.8026588442\n",
      "Epoch 558, Loss: 0.9882239600\n",
      "Epoch 559, Loss: 0.9166053953\n",
      "Epoch 560, Loss: 0.8998008668\n",
      "Epoch 561, Loss: 0.8662897250\n",
      "Epoch 562, Loss: 0.7786714753\n",
      "Epoch 563, Loss: 0.7178478270\n",
      "Epoch 564, Loss: 0.7351210557\n",
      "Epoch 565, Loss: 0.7491159432\n",
      "Epoch 566, Loss: 0.9837529291\n",
      "Epoch 567, Loss: 0.7584138924\n",
      "Epoch 568, Loss: 0.7722217138\n",
      "Epoch 569, Loss: 0.7067685647\n",
      "Epoch 570, Loss: 0.6961237978\n",
      "Epoch 571, Loss: 0.7872469431\n",
      "Epoch 572, Loss: 0.9596472583\n",
      "Epoch 573, Loss: 0.7348472170\n",
      "Epoch 574, Loss: 0.7581201797\n",
      "Epoch 575, Loss: 0.6973074034\n",
      "Epoch 576, Loss: 0.6841809385\n",
      "Epoch 577, Loss: 0.6981191335\n",
      "Epoch 578, Loss: 0.7458698905\n",
      "Epoch 579, Loss: 0.7659534865\n",
      "Epoch 580, Loss: 0.8536924761\n",
      "Epoch 581, Loss: 1.0413750320\n",
      "Epoch 582, Loss: 0.7596665351\n",
      "Epoch 583, Loss: 0.7204707154\n",
      "Epoch 584, Loss: 0.7484492850\n",
      "Epoch 585, Loss: 0.7207460419\n",
      "Epoch 586, Loss: 0.7049962305\n",
      "Epoch 587, Loss: 0.7368676073\n",
      "Epoch 588, Loss: 0.8013419303\n",
      "Epoch 589, Loss: 0.8324109016\n",
      "Epoch 590, Loss: 0.7325990399\n",
      "Epoch 591, Loss: 0.7884703875\n",
      "Epoch 592, Loss: 0.7209070514\n",
      "Epoch 593, Loss: 0.7071190333\n",
      "Epoch 594, Loss: 0.7267588057\n",
      "Epoch 595, Loss: 0.7583662158\n",
      "Epoch 596, Loss: 0.7920157630\n",
      "Epoch 597, Loss: 0.8109878779\n",
      "Epoch 598, Loss: 0.8199701417\n",
      "Epoch 599, Loss: 0.8287720032\n",
      "Epoch 600, Loss: 0.8314894577\n",
      "Epoch 601, Loss: 0.8331166615\n",
      "Epoch 602, Loss: 0.7559953254\n",
      "Epoch 603, Loss: 0.6932425190\n",
      "Epoch 604, Loss: 0.7339639845\n",
      "Epoch 605, Loss: 0.7272641278\n",
      "Epoch 606, Loss: 0.7052739006\n",
      "Epoch 607, Loss: 0.7396980988\n",
      "Epoch 608, Loss: 0.7951150642\n",
      "Epoch 609, Loss: 0.7459107715\n",
      "Epoch 610, Loss: 0.7658440843\n",
      "Epoch 611, Loss: 0.7784544221\n",
      "Epoch 612, Loss: 0.6856110866\n",
      "Epoch 613, Loss: 0.6865344372\n",
      "Epoch 614, Loss: 0.7169349864\n",
      "Epoch 615, Loss: 0.7389358333\n",
      "Epoch 616, Loss: 0.7322318236\n",
      "Epoch 617, Loss: 0.7370097054\n",
      "Epoch 618, Loss: 0.7564521516\n",
      "Epoch 619, Loss: 0.7797580447\n",
      "Epoch 620, Loss: 0.8324038336\n",
      "Epoch 621, Loss: 0.9505760167\n",
      "Epoch 622, Loss: 0.7523833429\n",
      "Epoch 623, Loss: 0.6983788435\n",
      "Epoch 624, Loss: 0.7082476979\n",
      "Epoch 625, Loss: 0.6826742442\n",
      "Epoch 626, Loss: 0.7158993510\n",
      "Epoch 627, Loss: 0.7600508285\n",
      "Epoch 628, Loss: 0.7537928467\n",
      "Epoch 629, Loss: 0.7297531493\n",
      "Epoch 630, Loss: 0.7250756454\n",
      "Epoch 631, Loss: 0.7285224623\n",
      "Epoch 632, Loss: 0.7211224297\n",
      "Epoch 633, Loss: 0.7095430289\n",
      "Epoch 634, Loss: 0.7512982578\n",
      "Epoch 635, Loss: 0.8310950726\n",
      "Epoch 636, Loss: 0.9526005231\n",
      "Epoch 637, Loss: 0.7313218073\n",
      "Epoch 638, Loss: 0.7325108993\n",
      "Epoch 639, Loss: 0.7312749503\n",
      "Epoch 640, Loss: 0.7278703712\n",
      "Epoch 641, Loss: 0.7167900065\n",
      "Epoch 642, Loss: 0.7432397400\n",
      "Epoch 643, Loss: 0.7460536392\n",
      "Epoch 644, Loss: 0.7958549391\n",
      "Epoch 645, Loss: 1.2447025276\n",
      "Epoch 646, Loss: 0.8677542939\n",
      "Epoch 647, Loss: 0.7495085449\n",
      "Epoch 648, Loss: 0.7229158098\n",
      "Epoch 649, Loss: 0.8331391138\n",
      "Epoch 650, Loss: 0.7731402789\n",
      "Epoch 651, Loss: 0.8606857062\n",
      "Epoch 652, Loss: 0.7762141392\n",
      "Epoch 653, Loss: 0.7194265290\n",
      "Epoch 654, Loss: 0.8044548352\n",
      "Epoch 655, Loss: 0.8786876159\n",
      "Epoch 656, Loss: 0.7841547211\n",
      "Epoch 657, Loss: 0.7445927688\n",
      "Epoch 658, Loss: 0.7977068862\n",
      "Epoch 659, Loss: 0.7698737672\n",
      "Epoch 660, Loss: 0.7049603204\n",
      "Epoch 661, Loss: 0.7711024201\n",
      "Epoch 662, Loss: 0.9114479054\n",
      "Epoch 663, Loss: 0.9354221994\n",
      "Epoch 664, Loss: 0.7503727542\n",
      "Epoch 665, Loss: 0.7125739151\n",
      "Epoch 666, Loss: 0.8560427149\n",
      "Epoch 667, Loss: 0.7599375477\n",
      "Epoch 668, Loss: 0.7000123068\n",
      "Epoch 669, Loss: 0.7751607023\n",
      "Epoch 670, Loss: 0.8282214219\n",
      "Epoch 671, Loss: 0.7945170769\n",
      "Epoch 672, Loss: 0.8029295514\n",
      "Epoch 673, Loss: 0.8624293846\n",
      "Epoch 674, Loss: 0.8619538729\n",
      "Epoch 675, Loss: 0.6968652056\n",
      "Epoch 676, Loss: 0.6667478422\n",
      "Epoch 677, Loss: 0.6917118278\n",
      "Epoch 678, Loss: 0.7635123705\n",
      "Epoch 679, Loss: 0.8580314254\n",
      "Epoch 680, Loss: 0.8984193772\n",
      "Epoch 681, Loss: 0.9484238438\n",
      "Epoch 682, Loss: 0.7380004495\n",
      "Epoch 683, Loss: 0.6640128627\n",
      "Epoch 684, Loss: 0.6782566233\n",
      "Epoch 685, Loss: 0.7248653624\n",
      "Epoch 686, Loss: 0.8245293606\n",
      "Epoch 687, Loss: 0.8512139162\n",
      "Epoch 688, Loss: 0.9149290395\n",
      "Epoch 689, Loss: 0.8767860569\n",
      "Epoch 690, Loss: 0.6673294313\n",
      "Epoch 691, Loss: 0.6556059657\n",
      "Epoch 692, Loss: 0.7073877497\n",
      "Epoch 693, Loss: 0.8319698951\n",
      "Epoch 694, Loss: 0.8780581864\n",
      "Epoch 695, Loss: 0.8555536576\n",
      "Epoch 696, Loss: 0.8093056320\n",
      "Epoch 697, Loss: 0.6746131718\n",
      "Epoch 698, Loss: 0.6840948848\n",
      "Epoch 699, Loss: 0.7566134755\n",
      "Epoch 700, Loss: 0.8342649628\n",
      "Epoch 701, Loss: 0.8315614215\n",
      "Epoch 702, Loss: 0.8596110612\n",
      "Epoch 703, Loss: 0.8726516151\n",
      "Epoch 704, Loss: 0.7342187222\n",
      "Epoch 705, Loss: 0.6539210039\n",
      "Epoch 706, Loss: 0.6530719474\n",
      "Epoch 707, Loss: 0.7038373844\n",
      "Epoch 708, Loss: 0.8224845529\n",
      "Epoch 709, Loss: 0.8606487530\n",
      "Epoch 710, Loss: 0.8274030132\n",
      "Epoch 711, Loss: 0.8775953955\n",
      "Epoch 712, Loss: 0.8193622561\n",
      "Epoch 713, Loss: 0.7004723519\n",
      "Epoch 714, Loss: 0.6492542389\n",
      "Epoch 715, Loss: 0.6412803513\n",
      "Epoch 716, Loss: 0.6499305561\n",
      "Epoch 717, Loss: 0.6708653598\n",
      "Epoch 718, Loss: 0.7212527124\n",
      "Epoch 719, Loss: 0.7532266809\n",
      "Epoch 720, Loss: 0.7447594649\n",
      "Epoch 721, Loss: 0.7420307835\n",
      "Epoch 722, Loss: 0.8264505185\n",
      "Epoch 723, Loss: 0.9524630154\n",
      "Epoch 724, Loss: 0.8672098086\n",
      "Epoch 725, Loss: 0.6685511514\n",
      "Epoch 726, Loss: 0.6813223940\n",
      "Epoch 727, Loss: 0.7319917567\n",
      "Epoch 728, Loss: 0.8588132688\n",
      "Epoch 729, Loss: 0.8926956720\n",
      "Epoch 730, Loss: 0.8325631523\n",
      "Epoch 731, Loss: 0.7878797107\n",
      "Epoch 732, Loss: 0.8582491650\n",
      "Epoch 733, Loss: 0.7436482790\n",
      "Epoch 734, Loss: 0.6681178867\n",
      "Epoch 735, Loss: 0.6596346760\n",
      "Epoch 736, Loss: 0.6272714166\n",
      "Epoch 737, Loss: 0.6503557603\n",
      "Epoch 738, Loss: 0.6414688740\n",
      "Epoch 739, Loss: 0.6675375799\n",
      "Epoch 740, Loss: 0.7316977283\n",
      "Epoch 741, Loss: 0.7709560036\n",
      "Epoch 742, Loss: 0.7494326344\n",
      "Epoch 743, Loss: 0.7196412438\n",
      "Epoch 744, Loss: 0.7227623758\n",
      "Epoch 745, Loss: 0.7456842259\n",
      "Epoch 746, Loss: 0.7353375988\n",
      "Epoch 747, Loss: 0.7212611923\n",
      "Epoch 748, Loss: 0.7514481284\n",
      "Epoch 749, Loss: 0.7781201091\n",
      "Epoch 750, Loss: 0.8160698779\n",
      "Epoch 751, Loss: 0.9071464297\n",
      "Epoch 752, Loss: 1.0358250535\n",
      "Epoch 753, Loss: 0.8687284129\n",
      "Epoch 754, Loss: 0.6797510346\n",
      "Epoch 755, Loss: 0.7068355910\n",
      "Epoch 756, Loss: 0.6483103786\n",
      "Epoch 757, Loss: 0.6180309936\n",
      "Epoch 758, Loss: 0.6254449074\n",
      "Epoch 759, Loss: 0.6471672380\n",
      "Epoch 760, Loss: 0.6776785866\n",
      "Epoch 761, Loss: 0.7166956456\n",
      "Epoch 762, Loss: 0.7587107532\n",
      "Epoch 763, Loss: 0.7805133207\n",
      "Epoch 764, Loss: 0.8223637856\n",
      "Epoch 765, Loss: 0.8419992203\n",
      "Epoch 766, Loss: 0.8095832377\n",
      "Epoch 767, Loss: 0.7843588265\n",
      "Epoch 768, Loss: 0.7446881918\n",
      "Epoch 769, Loss: 0.8491003750\n",
      "Epoch 770, Loss: 0.9807106497\n",
      "Epoch 771, Loss: 0.9571553347\n",
      "Epoch 772, Loss: 0.6728768839\n",
      "Epoch 773, Loss: 0.6957512890\n",
      "Epoch 774, Loss: 0.6694352794\n",
      "Epoch 775, Loss: 0.6147684887\n",
      "Epoch 776, Loss: 0.6291874468\n",
      "Epoch 777, Loss: 0.6296363134\n",
      "Epoch 778, Loss: 0.6823444215\n",
      "Epoch 779, Loss: 0.7033533329\n",
      "Epoch 780, Loss: 0.7223438785\n",
      "Epoch 781, Loss: 0.7477856959\n",
      "Epoch 782, Loss: 0.7442317018\n",
      "Epoch 783, Loss: 0.7208626731\n",
      "Epoch 784, Loss: 0.7430652516\n",
      "Epoch 785, Loss: 0.7236786879\n",
      "Epoch 786, Loss: 0.7475236786\n",
      "Epoch 787, Loss: 0.7640313899\n",
      "Epoch 788, Loss: 0.7868716243\n",
      "Epoch 789, Loss: 0.7648532131\n",
      "Epoch 790, Loss: 0.7045297088\n",
      "Epoch 791, Loss: 0.7104288691\n",
      "Epoch 792, Loss: 0.7474553014\n",
      "Epoch 793, Loss: 0.7254232626\n",
      "Epoch 794, Loss: 0.7803114086\n",
      "Epoch 795, Loss: 0.7677735704\n",
      "Epoch 796, Loss: 0.7757757086\n",
      "Epoch 797, Loss: 0.7539010620\n",
      "Epoch 798, Loss: 0.7223436504\n",
      "Epoch 799, Loss: 0.7100884966\n",
      "Epoch 800, Loss: 0.7291211116\n",
      "Epoch 801, Loss: 0.7201540920\n",
      "Epoch 802, Loss: 0.7312622066\n",
      "Epoch 803, Loss: 0.7274127614\n",
      "Epoch 804, Loss: 0.7033535377\n",
      "Epoch 805, Loss: 0.7087538103\n",
      "Epoch 806, Loss: 0.7104416204\n",
      "Epoch 807, Loss: 0.7187230105\n",
      "Epoch 808, Loss: 0.7451963691\n",
      "Epoch 809, Loss: 0.7146695686\n",
      "Epoch 810, Loss: 0.7207062056\n",
      "Epoch 811, Loss: 0.7137416435\n",
      "Epoch 812, Loss: 0.7124270995\n",
      "Epoch 813, Loss: 0.7074170719\n",
      "Epoch 814, Loss: 0.6948246527\n",
      "Epoch 815, Loss: 0.7458534285\n",
      "Epoch 816, Loss: 0.7002350610\n",
      "Epoch 817, Loss: 0.7314849507\n",
      "Epoch 818, Loss: 0.6861155094\n",
      "Epoch 819, Loss: 0.7279140805\n",
      "Epoch 820, Loss: 0.6823595672\n",
      "Epoch 821, Loss: 0.7327171699\n",
      "Epoch 822, Loss: 0.7422981151\n",
      "Epoch 823, Loss: 0.7497092365\n",
      "Epoch 824, Loss: 0.7508651139\n",
      "Epoch 825, Loss: 0.7635785488\n",
      "Epoch 826, Loss: 0.7760815401\n",
      "Epoch 827, Loss: 0.7429207283\n",
      "Epoch 828, Loss: 0.6944124315\n",
      "Epoch 829, Loss: 0.6649250937\n",
      "Epoch 830, Loss: 0.6571538736\n",
      "Epoch 831, Loss: 0.7467839616\n",
      "Epoch 832, Loss: 0.7379653384\n",
      "Epoch 833, Loss: 0.7049809282\n",
      "Epoch 834, Loss: 0.6812134496\n",
      "Epoch 835, Loss: 0.6881488811\n",
      "Epoch 836, Loss: 0.7020200379\n",
      "Epoch 837, Loss: 0.7382084652\n",
      "Epoch 838, Loss: 0.7738606986\n",
      "Epoch 839, Loss: 0.6743426210\n",
      "Epoch 840, Loss: 0.6829911082\n",
      "Epoch 841, Loss: 0.6808264812\n",
      "Epoch 842, Loss: 0.6827229728\n",
      "Epoch 843, Loss: 0.7050743136\n",
      "Epoch 844, Loss: 0.7572298519\n",
      "Epoch 845, Loss: 0.7820810640\n",
      "Epoch 846, Loss: 0.7465828436\n",
      "Epoch 847, Loss: 0.7149181726\n",
      "Epoch 848, Loss: 0.7121967925\n",
      "Epoch 849, Loss: 0.7691350946\n",
      "Epoch 850, Loss: 0.8851033614\n",
      "Epoch 851, Loss: 0.6383100103\n",
      "Epoch 852, Loss: 0.7462640588\n",
      "Epoch 853, Loss: 0.6827233808\n",
      "Epoch 854, Loss: 0.7135253531\n",
      "Epoch 855, Loss: 0.7830911050\n",
      "Epoch 856, Loss: 0.7852440451\n",
      "Epoch 857, Loss: 0.7525840944\n",
      "Epoch 858, Loss: 0.7513026166\n",
      "Epoch 859, Loss: 0.6909473528\n",
      "Epoch 860, Loss: 0.6574492118\n",
      "Epoch 861, Loss: 0.7246392192\n",
      "Epoch 862, Loss: 0.7584914742\n",
      "Epoch 863, Loss: 0.7394981614\n",
      "Epoch 864, Loss: 0.7127981006\n",
      "Epoch 865, Loss: 0.6936304331\n",
      "Epoch 866, Loss: 0.7517864590\n",
      "Epoch 867, Loss: 0.8040912405\n",
      "Epoch 868, Loss: 0.7975006501\n",
      "Epoch 869, Loss: 0.7614514538\n",
      "Epoch 870, Loss: 0.7562247819\n",
      "Epoch 871, Loss: 0.7909062736\n",
      "Epoch 872, Loss: 0.7927843774\n",
      "Epoch 873, Loss: 0.7965399815\n",
      "Epoch 874, Loss: 0.7722320878\n",
      "Epoch 875, Loss: 0.7753851410\n",
      "Epoch 876, Loss: 0.8415849224\n",
      "Epoch 877, Loss: 0.9168382040\n",
      "Epoch 878, Loss: 0.9601976923\n",
      "Epoch 879, Loss: 1.2344774334\n",
      "Epoch 880, Loss: 0.6509900345\n",
      "Epoch 881, Loss: 0.6311290001\n",
      "Epoch 882, Loss: 0.5855875725\n",
      "Epoch 883, Loss: 0.5885652630\n",
      "Epoch 884, Loss: 0.6448403948\n",
      "Epoch 885, Loss: 0.6482759587\n",
      "Epoch 886, Loss: 0.5740530642\n",
      "Epoch 887, Loss: 0.5738623963\n",
      "Epoch 888, Loss: 0.5788603679\n",
      "Epoch 889, Loss: 0.5634852996\n",
      "Epoch 890, Loss: 0.5767876381\n",
      "Epoch 891, Loss: 0.6143151958\n",
      "Epoch 892, Loss: 0.6063619105\n",
      "Epoch 893, Loss: 0.5677706720\n",
      "Epoch 894, Loss: 0.5606069588\n",
      "Epoch 895, Loss: 0.5730147199\n",
      "Epoch 896, Loss: 0.5809486331\n",
      "Epoch 897, Loss: 0.5745456486\n",
      "Epoch 898, Loss: 0.6114083776\n",
      "Epoch 899, Loss: 0.6115856041\n",
      "Epoch 900, Loss: 0.5919978597\n",
      "Epoch 901, Loss: 0.5933026452\n",
      "Epoch 902, Loss: 0.5950362773\n",
      "Epoch 903, Loss: 0.5706667655\n",
      "Epoch 904, Loss: 0.5692706607\n",
      "Epoch 905, Loss: 0.5909967320\n",
      "Epoch 906, Loss: 0.5636244866\n",
      "Epoch 907, Loss: 0.5796569052\n",
      "Epoch 908, Loss: 0.6155108807\n",
      "Epoch 909, Loss: 0.6283097317\n",
      "Epoch 910, Loss: 0.5697774321\n",
      "Epoch 911, Loss: 0.5733295763\n",
      "Epoch 912, Loss: 0.5729720205\n",
      "Epoch 913, Loss: 0.5863543306\n",
      "Epoch 914, Loss: 0.5741021551\n",
      "Epoch 915, Loss: 0.5952877546\n",
      "Epoch 916, Loss: 0.6009716832\n",
      "Epoch 917, Loss: 0.5814979202\n",
      "Epoch 918, Loss: 0.5508799867\n",
      "Epoch 919, Loss: 0.5637305686\n",
      "Epoch 920, Loss: 0.5725723129\n",
      "Epoch 921, Loss: 0.5724217046\n",
      "Epoch 922, Loss: 0.5463024030\n",
      "Epoch 923, Loss: 0.5451077701\n",
      "Epoch 924, Loss: 0.6039148834\n",
      "Epoch 925, Loss: 0.6135827353\n",
      "Epoch 926, Loss: 0.6831633735\n",
      "Epoch 927, Loss: 0.6959488037\n",
      "Epoch 928, Loss: 0.6724255252\n",
      "Epoch 929, Loss: 0.6842845785\n",
      "Epoch 930, Loss: 0.6544085083\n",
      "Epoch 931, Loss: 0.6042150815\n",
      "Epoch 932, Loss: 0.6572690736\n",
      "Epoch 933, Loss: 0.7221612497\n",
      "Epoch 934, Loss: 0.7391876383\n",
      "Epoch 935, Loss: 0.6612642964\n",
      "Epoch 936, Loss: 0.6031918668\n",
      "Epoch 937, Loss: 0.6792128850\n",
      "Epoch 938, Loss: 0.7032324106\n",
      "Epoch 939, Loss: 0.6670073278\n",
      "Epoch 940, Loss: 0.6133399265\n",
      "Epoch 941, Loss: 0.6227659419\n",
      "Epoch 942, Loss: 0.6648695352\n",
      "Epoch 943, Loss: 0.6015160926\n",
      "Epoch 944, Loss: 0.8007138455\n",
      "Epoch 945, Loss: 0.6690350649\n",
      "Epoch 946, Loss: 0.8128429022\n",
      "Epoch 947, Loss: 0.7473563841\n",
      "Epoch 948, Loss: 0.6376541207\n",
      "Epoch 949, Loss: 0.6899558041\n",
      "Epoch 950, Loss: 0.6794896683\n",
      "Epoch 951, Loss: 0.6597417810\n",
      "Epoch 952, Loss: 0.6087537685\n",
      "Epoch 953, Loss: 0.6087616795\n",
      "Epoch 954, Loss: 0.6931963154\n",
      "Epoch 955, Loss: 0.7067478284\n",
      "Epoch 956, Loss: 0.6001335822\n",
      "Epoch 957, Loss: 0.6820174404\n",
      "Epoch 958, Loss: 0.7216975042\n",
      "Epoch 959, Loss: 0.6491638493\n",
      "Epoch 960, Loss: 0.6087383597\n",
      "Epoch 961, Loss: 0.7150420397\n",
      "Epoch 962, Loss: 0.6678162671\n",
      "Epoch 963, Loss: 0.6019750902\n",
      "Epoch 964, Loss: 0.7256573946\n",
      "Epoch 965, Loss: 0.6721777052\n",
      "Epoch 966, Loss: 0.7670358115\n",
      "Epoch 967, Loss: 0.7034499115\n",
      "Epoch 968, Loss: 0.7155692833\n",
      "Epoch 969, Loss: 0.7425789620\n",
      "Epoch 970, Loss: 0.6903109503\n",
      "Epoch 971, Loss: 0.6898210289\n",
      "Epoch 972, Loss: 0.6913187279\n",
      "Epoch 973, Loss: 0.6102656079\n",
      "Epoch 974, Loss: 0.6433669944\n",
      "Epoch 975, Loss: 0.6639805051\n",
      "Epoch 976, Loss: 0.6276065553\n",
      "Epoch 977, Loss: 0.6806192022\n",
      "Epoch 978, Loss: 0.7068371782\n",
      "Epoch 979, Loss: 0.5961870212\n",
      "Epoch 980, Loss: 0.6099425816\n",
      "Epoch 981, Loss: 0.7197751847\n",
      "Epoch 982, Loss: 0.5983579247\n",
      "Epoch 983, Loss: 0.6704534586\n",
      "Epoch 984, Loss: 0.6630793392\n",
      "Epoch 985, Loss: 0.6514449351\n",
      "Epoch 986, Loss: 0.5852749769\n",
      "Epoch 987, Loss: 0.5956677422\n",
      "Epoch 988, Loss: 0.7054267609\n",
      "Epoch 989, Loss: 0.6136586283\n",
      "Epoch 990, Loss: 0.6180825081\n",
      "Epoch 991, Loss: 0.6772252543\n",
      "Epoch 992, Loss: 0.6351089981\n",
      "Epoch 993, Loss: 0.6912382704\n",
      "Epoch 994, Loss: 0.7714612267\n",
      "Epoch 995, Loss: 0.6802553834\n",
      "Epoch 996, Loss: 0.5816758109\n",
      "Epoch 997, Loss: 0.6621679190\n",
      "Epoch 998, Loss: 0.6309196323\n",
      "Epoch 999, Loss: 0.6337421855\n",
      "Epoch 1000, Loss: 0.6144694054\n",
      "Epoch 1001, Loss: 0.6184063229\n",
      "Epoch 1002, Loss: 0.6382845441\n",
      "Epoch 1003, Loss: 0.6415845105\n",
      "Epoch 1004, Loss: 0.7162383585\n",
      "Epoch 1005, Loss: 0.6902919028\n",
      "Epoch 1006, Loss: 0.5857669001\n",
      "Epoch 1007, Loss: 0.6244903221\n",
      "Epoch 1008, Loss: 0.6386321533\n",
      "Epoch 1009, Loss: 0.6224766342\n",
      "Epoch 1010, Loss: 0.6767340674\n",
      "Epoch 1011, Loss: 0.6760588004\n",
      "Epoch 1012, Loss: 0.5980505521\n",
      "Epoch 1013, Loss: 0.5902833391\n",
      "Epoch 1014, Loss: 0.6977712339\n",
      "Epoch 1015, Loss: 0.7846177773\n",
      "Epoch 1016, Loss: 0.6557161650\n",
      "Epoch 1017, Loss: 0.6205894080\n",
      "Epoch 1018, Loss: 0.7526343423\n",
      "Epoch 1019, Loss: 0.6966332951\n",
      "Epoch 1020, Loss: 0.5608306415\n",
      "Epoch 1021, Loss: 0.6071958477\n",
      "Epoch 1022, Loss: 0.6923073227\n",
      "Epoch 1023, Loss: 0.6864086483\n",
      "Epoch 1024, Loss: 0.6083018298\n",
      "Epoch 1025, Loss: 0.6427821530\n",
      "Epoch 1026, Loss: 0.6792853423\n",
      "Epoch 1027, Loss: 0.5680840813\n",
      "Epoch 1028, Loss: 0.5746462768\n",
      "Epoch 1029, Loss: 0.8964231331\n",
      "Epoch 1030, Loss: 0.5889090276\n",
      "Epoch 1031, Loss: 0.6802782657\n",
      "Epoch 1032, Loss: 0.6027124872\n",
      "Epoch 1033, Loss: 0.6500162913\n",
      "Epoch 1034, Loss: 0.7055898455\n",
      "Epoch 1035, Loss: 0.6439838780\n",
      "Epoch 1036, Loss: 0.6240724533\n",
      "Epoch 1037, Loss: 0.7015389389\n",
      "Epoch 1038, Loss: 0.8072724286\n",
      "Epoch 1039, Loss: 0.7177575761\n",
      "Epoch 1040, Loss: 0.5691504767\n",
      "Epoch 1041, Loss: 0.5714668642\n",
      "Epoch 1042, Loss: 0.6356929484\n",
      "Epoch 1043, Loss: 0.5964850518\n",
      "Epoch 1044, Loss: 0.7972880813\n",
      "Epoch 1045, Loss: 0.7192478187\n",
      "Epoch 1046, Loss: 0.6844186483\n",
      "Epoch 1047, Loss: 0.6830524497\n",
      "Epoch 1048, Loss: 0.6750780865\n",
      "Epoch 1049, Loss: 0.6741759803\n",
      "Epoch 1050, Loss: 0.7002980568\n",
      "Epoch 1051, Loss: 0.7199349518\n",
      "Epoch 1052, Loss: 0.7300646903\n",
      "Epoch 1053, Loss: 0.5926882658\n",
      "Epoch 1054, Loss: 0.5514197766\n",
      "Epoch 1055, Loss: 0.6428731735\n",
      "Epoch 1056, Loss: 0.7192449448\n",
      "Epoch 1057, Loss: 0.6841872156\n",
      "Epoch 1058, Loss: 0.6803027769\n",
      "Epoch 1059, Loss: 0.6453125029\n",
      "Epoch 1060, Loss: 0.6940064282\n",
      "Epoch 1061, Loss: 0.7293163772\n",
      "Epoch 1062, Loss: 0.6890647357\n",
      "Epoch 1063, Loss: 0.6939201599\n",
      "Epoch 1064, Loss: 0.6841466988\n",
      "Epoch 1065, Loss: 0.6451909780\n",
      "Epoch 1066, Loss: 0.6562166286\n",
      "Epoch 1067, Loss: 0.6918762803\n",
      "Epoch 1068, Loss: 0.6958051005\n",
      "Epoch 1069, Loss: 0.6785919564\n",
      "Epoch 1070, Loss: 0.6565105941\n",
      "Epoch 1071, Loss: 0.6670453335\n",
      "Epoch 1072, Loss: 0.7271008739\n",
      "Epoch 1073, Loss: 0.7623969824\n",
      "Epoch 1074, Loss: 0.6981389130\n",
      "Epoch 1075, Loss: 0.6293572164\n",
      "Epoch 1076, Loss: 0.6177575488\n",
      "Epoch 1077, Loss: 0.6323116997\n",
      "Epoch 1078, Loss: 0.6718516267\n",
      "Epoch 1079, Loss: 0.7558435702\n",
      "Epoch 1080, Loss: 0.9062910166\n",
      "Epoch 1081, Loss: 0.8523912683\n",
      "Epoch 1082, Loss: 0.5574926351\n",
      "Epoch 1083, Loss: 0.6125254055\n",
      "Epoch 1084, Loss: 0.7534602286\n",
      "Epoch 1085, Loss: 0.5886845171\n",
      "Epoch 1086, Loss: 0.6260046652\n",
      "Epoch 1087, Loss: 0.6554369387\n",
      "Epoch 1088, Loss: 0.7874331793\n",
      "Epoch 1089, Loss: 0.6648512581\n",
      "Epoch 1090, Loss: 0.7464663236\n",
      "Epoch 1091, Loss: 0.7207330405\n",
      "Epoch 1092, Loss: 0.7188481938\n",
      "Epoch 1093, Loss: 0.7176117865\n",
      "Epoch 1094, Loss: 0.7569109325\n",
      "Epoch 1095, Loss: 0.7962474921\n",
      "Epoch 1096, Loss: 0.7657562165\n",
      "Epoch 1097, Loss: 0.8038348521\n",
      "Epoch 1098, Loss: 0.8478713708\n",
      "Epoch 1099, Loss: 0.8665088631\n",
      "Epoch 1100, Loss: 0.8950449465\n",
      "Epoch 1101, Loss: 0.9979538944\n",
      "Epoch 1102, Loss: 1.2252899850\n",
      "Epoch 1103, Loss: 0.5683355271\n",
      "Epoch 1104, Loss: 0.5614833229\n",
      "Epoch 1105, Loss: 0.6474902732\n",
      "Epoch 1106, Loss: 0.5783600056\n",
      "Epoch 1107, Loss: 0.4990163655\n",
      "Epoch 1108, Loss: 0.5380040681\n",
      "Epoch 1109, Loss: 0.5665660106\n",
      "Epoch 1110, Loss: 0.5913919523\n",
      "Epoch 1111, Loss: 0.5212640814\n",
      "Epoch 1112, Loss: 0.5176992256\n",
      "Epoch 1113, Loss: 0.5145157646\n",
      "Epoch 1114, Loss: 0.5099228752\n",
      "Epoch 1115, Loss: 0.5269980298\n",
      "Epoch 1116, Loss: 0.5919589833\n",
      "Epoch 1117, Loss: 0.5943173312\n",
      "Epoch 1118, Loss: 0.5666207470\n",
      "Epoch 1119, Loss: 0.4995729139\n",
      "Epoch 1120, Loss: 0.5251472754\n",
      "Epoch 1121, Loss: 0.6007322967\n",
      "Epoch 1122, Loss: 0.5443111132\n",
      "Epoch 1123, Loss: 0.5074564853\n",
      "Epoch 1124, Loss: 0.4967836207\n",
      "Epoch 1125, Loss: 0.5473824709\n",
      "Epoch 1126, Loss: 0.5075835718\n",
      "Epoch 1127, Loss: 0.5392589505\n",
      "Epoch 1128, Loss: 0.5457111062\n",
      "Epoch 1129, Loss: 0.5901801283\n",
      "Epoch 1130, Loss: 0.5864108993\n",
      "Epoch 1131, Loss: 0.4818939834\n",
      "Epoch 1132, Loss: 0.4924453324\n",
      "Epoch 1133, Loss: 0.5058241297\n",
      "Epoch 1134, Loss: 0.5952839838\n",
      "Epoch 1135, Loss: 0.6758755843\n",
      "Epoch 1136, Loss: 0.5864170919\n",
      "Epoch 1137, Loss: 0.6294178771\n",
      "Epoch 1138, Loss: 0.5777001868\n",
      "Epoch 1139, Loss: 0.8655920414\n",
      "Epoch 1140, Loss: 0.8048909371\n",
      "Epoch 1141, Loss: 0.5659634282\n",
      "Epoch 1142, Loss: 0.5428631697\n",
      "Epoch 1143, Loss: 0.6017900642\n",
      "Epoch 1144, Loss: 0.4913484797\n",
      "Epoch 1145, Loss: 0.5243777808\n",
      "Epoch 1146, Loss: 0.5258319883\n",
      "Epoch 1147, Loss: 0.5597446182\n",
      "Epoch 1148, Loss: 0.5470629679\n",
      "Epoch 1149, Loss: 0.5821036973\n",
      "Epoch 1150, Loss: 0.6580713750\n",
      "Epoch 1151, Loss: 0.7449653151\n",
      "Epoch 1152, Loss: 0.7006727683\n",
      "Epoch 1153, Loss: 0.5762834954\n",
      "Epoch 1154, Loss: 0.4921685408\n",
      "Epoch 1155, Loss: 0.5741234432\n",
      "Epoch 1156, Loss: 0.5954631983\n",
      "Epoch 1157, Loss: 0.5489216284\n",
      "Epoch 1158, Loss: 0.5183167332\n",
      "Epoch 1159, Loss: 0.4858859619\n",
      "Epoch 1160, Loss: 0.5181538470\n",
      "Epoch 1161, Loss: 0.6484939665\n",
      "Epoch 1162, Loss: 0.6827260629\n",
      "Epoch 1163, Loss: 0.5492705938\n",
      "Epoch 1164, Loss: 0.5926743352\n",
      "Epoch 1165, Loss: 0.5793877283\n",
      "Epoch 1166, Loss: 0.6258707643\n",
      "Epoch 1167, Loss: 0.6305423278\n",
      "Epoch 1168, Loss: 0.7925366036\n",
      "Epoch 1169, Loss: 0.9138283694\n",
      "Epoch 1170, Loss: 0.5134052323\n",
      "Epoch 1171, Loss: 0.5134378947\n",
      "Epoch 1172, Loss: 0.5311057318\n",
      "Epoch 1173, Loss: 0.5012264420\n",
      "Epoch 1174, Loss: 0.4930203602\n",
      "Epoch 1175, Loss: 0.5127771612\n",
      "Epoch 1176, Loss: 0.4970253626\n",
      "Epoch 1177, Loss: 0.4723536771\n",
      "Epoch 1178, Loss: 0.5254125000\n",
      "Epoch 1179, Loss: 0.4722524092\n",
      "Epoch 1180, Loss: 0.5448750704\n",
      "Epoch 1181, Loss: 0.4880924875\n",
      "Epoch 1182, Loss: 0.4988465859\n",
      "Epoch 1183, Loss: 0.5298975002\n",
      "Epoch 1184, Loss: 0.5146675010\n",
      "Epoch 1185, Loss: 0.5173430827\n",
      "Epoch 1186, Loss: 0.5686585117\n",
      "Epoch 1187, Loss: 0.5662014602\n",
      "Epoch 1188, Loss: 0.5764043079\n",
      "Epoch 1189, Loss: 0.5922781431\n",
      "Epoch 1190, Loss: 0.5286337268\n",
      "Epoch 1191, Loss: 0.5130926117\n",
      "Epoch 1192, Loss: 0.5774851299\n",
      "Epoch 1193, Loss: 0.6701570824\n",
      "Epoch 1194, Loss: 0.5092542500\n",
      "Epoch 1195, Loss: 0.5869416119\n",
      "Epoch 1196, Loss: 0.5983138788\n",
      "Epoch 1197, Loss: 0.5589639846\n",
      "Epoch 1198, Loss: 0.5592541663\n",
      "Epoch 1199, Loss: 0.5511007975\n",
      "Epoch 1200, Loss: 0.5339954966\n",
      "Epoch 1201, Loss: 0.5474413161\n",
      "Epoch 1202, Loss: 0.5141392696\n",
      "Epoch 1203, Loss: 0.5329765454\n",
      "Epoch 1204, Loss: 0.5787120004\n",
      "Epoch 1205, Loss: 0.6247827338\n",
      "Epoch 1206, Loss: 0.6477711215\n",
      "Epoch 1207, Loss: 0.6254683183\n",
      "Epoch 1208, Loss: 0.6029300878\n",
      "Epoch 1209, Loss: 0.5760086650\n",
      "Epoch 1210, Loss: 0.5676138983\n",
      "Epoch 1211, Loss: 0.5834571700\n",
      "Epoch 1212, Loss: 0.5823651643\n",
      "Epoch 1213, Loss: 0.5948372288\n",
      "Epoch 1214, Loss: 0.5935710048\n",
      "Epoch 1215, Loss: 0.5774341650\n",
      "Epoch 1216, Loss: 0.5647454071\n",
      "Epoch 1217, Loss: 0.5356441980\n",
      "Epoch 1218, Loss: 0.5257559503\n",
      "Epoch 1219, Loss: 0.5112072383\n",
      "Epoch 1220, Loss: 0.5196121641\n",
      "Epoch 1221, Loss: 0.5240026725\n",
      "Epoch 1222, Loss: 0.5368659686\n",
      "Epoch 1223, Loss: 0.5507747230\n",
      "Epoch 1224, Loss: 0.5507888740\n",
      "Epoch 1225, Loss: 0.5414092148\n",
      "Epoch 1226, Loss: 0.5491334017\n",
      "Epoch 1227, Loss: 0.5651782345\n",
      "Epoch 1228, Loss: 0.5559127150\n",
      "Epoch 1229, Loss: 0.5251213391\n",
      "Epoch 1230, Loss: 0.4988205276\n",
      "Epoch 1231, Loss: 0.4970706877\n",
      "Epoch 1232, Loss: 0.5006898703\n",
      "Epoch 1233, Loss: 0.4965805936\n",
      "Epoch 1234, Loss: 0.4890902701\n",
      "Epoch 1235, Loss: 0.4999547722\n",
      "Epoch 1236, Loss: 0.5022127877\n",
      "Epoch 1237, Loss: 0.4183067852\n",
      "Epoch 1238, Loss: 0.4397196701\n",
      "Epoch 1239, Loss: 0.7607213402\n",
      "Epoch 1240, Loss: 0.5658101409\n",
      "Epoch 1241, Loss: 0.5655437435\n",
      "Epoch 1242, Loss: 0.5987393734\n",
      "Epoch 1243, Loss: 0.7245862290\n",
      "Epoch 1244, Loss: 0.7129411882\n",
      "Epoch 1245, Loss: 0.5403420555\n",
      "Epoch 1246, Loss: 0.4946723463\n",
      "Epoch 1247, Loss: 0.5028163055\n",
      "Epoch 1248, Loss: 0.5190308333\n",
      "Epoch 1249, Loss: 0.5632504423\n",
      "Epoch 1250, Loss: 0.6415331246\n",
      "Epoch 1251, Loss: 0.6453597231\n",
      "Epoch 1252, Loss: 0.5320647245\n",
      "Epoch 1253, Loss: 0.5393365226\n",
      "Epoch 1254, Loss: 0.5209387806\n",
      "Epoch 1255, Loss: 0.4443416915\n",
      "Epoch 1256, Loss: 0.4635086563\n",
      "Epoch 1257, Loss: 0.5547871582\n",
      "Epoch 1258, Loss: 0.7134089189\n",
      "Epoch 1259, Loss: 0.6914030122\n",
      "Epoch 1260, Loss: 0.7266840568\n",
      "Epoch 1261, Loss: 0.7190905663\n",
      "Epoch 1262, Loss: 0.5157742600\n",
      "Epoch 1263, Loss: 0.5104836562\n",
      "Epoch 1264, Loss: 0.5370785108\n",
      "Epoch 1265, Loss: 0.6987621860\n",
      "Epoch 1266, Loss: 0.6887498421\n",
      "Epoch 1267, Loss: 0.6844193417\n",
      "Epoch 1268, Loss: 0.5040949992\n",
      "Epoch 1269, Loss: 0.4856281952\n",
      "Epoch 1270, Loss: 0.4999561588\n",
      "Epoch 1271, Loss: 0.6019679003\n",
      "Epoch 1272, Loss: 0.7361703639\n",
      "Epoch 1273, Loss: 0.5825875241\n",
      "Epoch 1274, Loss: 0.5628013474\n",
      "Epoch 1275, Loss: 0.5911669762\n",
      "Epoch 1276, Loss: 0.5999864229\n",
      "Epoch 1277, Loss: 0.6607886410\n",
      "Epoch 1278, Loss: 0.5724069046\n",
      "Epoch 1279, Loss: 0.5348898821\n",
      "Epoch 1280, Loss: 0.4872328472\n",
      "Epoch 1281, Loss: 0.6862836734\n",
      "Epoch 1282, Loss: 0.5702715740\n",
      "Epoch 1283, Loss: 0.4914680348\n",
      "Epoch 1284, Loss: 0.5537061862\n",
      "Epoch 1285, Loss: 0.5641857688\n",
      "Epoch 1286, Loss: 0.5344626808\n",
      "Epoch 1287, Loss: 0.6171545049\n",
      "Epoch 1288, Loss: 0.6359925316\n",
      "Epoch 1289, Loss: 0.5540296363\n",
      "Epoch 1290, Loss: 0.5168739904\n",
      "Epoch 1291, Loss: 0.4722756095\n",
      "Epoch 1292, Loss: 0.4538284107\n",
      "Epoch 1293, Loss: 0.4610613972\n",
      "Epoch 1294, Loss: 0.5238702147\n",
      "Epoch 1295, Loss: 0.5264483470\n",
      "Epoch 1296, Loss: 0.5561052117\n",
      "Epoch 1297, Loss: 0.5100283574\n",
      "Epoch 1298, Loss: 0.4664497517\n",
      "Epoch 1299, Loss: 0.6139708512\n",
      "Epoch 1300, Loss: 0.5069786201\n",
      "Epoch 1301, Loss: 0.4868442805\n",
      "Epoch 1302, Loss: 0.4131083329\n",
      "Epoch 1303, Loss: 0.4369029912\n",
      "Epoch 1304, Loss: 0.4589215954\n",
      "Epoch 1305, Loss: 0.4977540295\n",
      "Epoch 1306, Loss: 0.4601004125\n",
      "Epoch 1307, Loss: 0.4537783822\n",
      "Epoch 1308, Loss: 0.5371495219\n",
      "Epoch 1309, Loss: 0.5032066879\n",
      "Epoch 1310, Loss: 0.6062378058\n",
      "Epoch 1311, Loss: 0.4873939893\n",
      "Epoch 1312, Loss: 0.4907977394\n",
      "Epoch 1313, Loss: 0.5098733102\n",
      "Epoch 1314, Loss: 0.5151945040\n",
      "Epoch 1315, Loss: 0.4816334156\n",
      "Epoch 1316, Loss: 0.4605681889\n",
      "Epoch 1317, Loss: 0.4742267097\n",
      "Epoch 1318, Loss: 0.5132075216\n",
      "Epoch 1319, Loss: 0.5124724845\n",
      "Epoch 1320, Loss: 0.4620576459\n",
      "Epoch 1321, Loss: 0.5368813343\n",
      "Epoch 1322, Loss: 0.5199002218\n",
      "Epoch 1323, Loss: 0.4788695379\n",
      "Epoch 1324, Loss: 0.4632705772\n",
      "Epoch 1325, Loss: 0.4866183450\n",
      "Epoch 1326, Loss: 0.5365111543\n",
      "Epoch 1327, Loss: 0.5003143106\n",
      "Epoch 1328, Loss: 0.4704697148\n",
      "Epoch 1329, Loss: 0.5117375304\n",
      "Epoch 1330, Loss: 0.5507168304\n",
      "Epoch 1331, Loss: 0.5676089655\n",
      "Epoch 1332, Loss: 0.4684455028\n",
      "Epoch 1333, Loss: 0.4254219595\n",
      "Epoch 1334, Loss: 0.4245818521\n",
      "Epoch 1335, Loss: 0.4309863234\n",
      "Epoch 1336, Loss: 0.4935723294\n",
      "Epoch 1337, Loss: 0.5612193854\n",
      "Epoch 1338, Loss: 0.6914085510\n",
      "Epoch 1339, Loss: 0.5199434509\n",
      "Epoch 1340, Loss: 0.5130418676\n",
      "Epoch 1341, Loss: 0.5187578296\n",
      "Epoch 1342, Loss: 0.5443163260\n",
      "Epoch 1343, Loss: 0.5078352378\n",
      "Epoch 1344, Loss: 0.4271616258\n",
      "Epoch 1345, Loss: 0.4028039489\n",
      "Epoch 1346, Loss: 0.4396863450\n",
      "Epoch 1347, Loss: 0.4395039400\n",
      "Epoch 1348, Loss: 0.4584418160\n",
      "Epoch 1349, Loss: 0.4893156041\n",
      "Epoch 1350, Loss: 0.4650812184\n",
      "Epoch 1351, Loss: 0.4333651742\n",
      "Epoch 1352, Loss: 0.4196304307\n",
      "Epoch 1353, Loss: 0.4089129854\n",
      "Epoch 1354, Loss: 0.4052615772\n",
      "Epoch 1355, Loss: 0.4158158276\n",
      "Epoch 1356, Loss: 0.4510450082\n",
      "Epoch 1357, Loss: 0.4748821843\n",
      "Epoch 1358, Loss: 0.4908398761\n",
      "Epoch 1359, Loss: 0.5197768649\n",
      "Epoch 1360, Loss: 0.4653675056\n",
      "Epoch 1361, Loss: 0.3954846232\n",
      "Epoch 1362, Loss: 0.4711721430\n",
      "Epoch 1363, Loss: 0.4077611058\n",
      "Epoch 1364, Loss: 0.4180869432\n",
      "Epoch 1365, Loss: 0.3849175840\n",
      "Epoch 1366, Loss: 0.3913825705\n",
      "Epoch 1367, Loss: 0.3882012748\n",
      "Epoch 1368, Loss: 0.3897423518\n",
      "Epoch 1369, Loss: 0.3894716041\n",
      "Epoch 1370, Loss: 0.3903452219\n",
      "Epoch 1371, Loss: 0.3869048899\n",
      "Epoch 1372, Loss: 0.3859676911\n",
      "Epoch 1373, Loss: 0.3886856077\n",
      "Epoch 1374, Loss: 0.3979911653\n",
      "Epoch 1375, Loss: 0.4275522612\n",
      "Epoch 1376, Loss: 0.4948610535\n",
      "Epoch 1377, Loss: 0.5878669394\n",
      "Epoch 1378, Loss: 0.6119401218\n",
      "Epoch 1379, Loss: 0.5364674741\n",
      "Epoch 1380, Loss: 0.5292511337\n",
      "Epoch 1381, Loss: 0.5518571704\n",
      "Epoch 1382, Loss: 0.4770736753\n",
      "Epoch 1383, Loss: 0.4480034890\n",
      "Epoch 1384, Loss: 0.4433991531\n",
      "Epoch 1385, Loss: 0.4539472485\n",
      "Epoch 1386, Loss: 0.4575565100\n",
      "Epoch 1387, Loss: 0.4584758979\n",
      "Epoch 1388, Loss: 0.4453759136\n",
      "Epoch 1389, Loss: 0.3883285538\n",
      "Epoch 1390, Loss: 0.4095355712\n",
      "Epoch 1391, Loss: 0.3781590977\n",
      "Epoch 1392, Loss: 0.3933769561\n",
      "Epoch 1393, Loss: 0.4728325878\n",
      "Epoch 1394, Loss: 0.4784292875\n",
      "Epoch 1395, Loss: 0.4661962250\n",
      "Epoch 1396, Loss: 0.4476852572\n",
      "Epoch 1397, Loss: 0.4260706926\n",
      "Epoch 1398, Loss: 0.4112242854\n",
      "Epoch 1399, Loss: 0.3877096185\n",
      "Epoch 1400, Loss: 0.3640248758\n",
      "Epoch 1401, Loss: 0.3908722302\n",
      "Epoch 1402, Loss: 0.3884732843\n",
      "Epoch 1403, Loss: 0.5439037130\n",
      "Epoch 1404, Loss: 0.5495078777\n",
      "Epoch 1405, Loss: 0.4337750135\n",
      "Epoch 1406, Loss: 0.4361147932\n",
      "Epoch 1407, Loss: 0.3955149908\n",
      "Epoch 1408, Loss: 0.4269486101\n",
      "Epoch 1409, Loss: 0.4545011789\n",
      "Epoch 1410, Loss: 0.5924087434\n",
      "Epoch 1411, Loss: 0.4700517458\n",
      "Epoch 1412, Loss: 0.5023993286\n",
      "Epoch 1413, Loss: 0.4031872488\n",
      "Epoch 1414, Loss: 0.4201982308\n",
      "Epoch 1415, Loss: 0.3929325046\n",
      "Epoch 1416, Loss: 0.4331154486\n",
      "Epoch 1417, Loss: 0.4154440344\n",
      "Epoch 1418, Loss: 0.4732543318\n",
      "Epoch 1419, Loss: 0.4302711759\n",
      "Epoch 1420, Loss: 0.4040447503\n",
      "Epoch 1421, Loss: 0.3821818143\n",
      "Epoch 1422, Loss: 0.5966793964\n",
      "Epoch 1423, Loss: 0.5953750534\n",
      "Epoch 1424, Loss: 0.5000299385\n",
      "Epoch 1425, Loss: 0.4577996640\n",
      "Epoch 1426, Loss: 0.4882052698\n",
      "Epoch 1427, Loss: 0.6255459105\n",
      "Epoch 1428, Loss: 0.5741060779\n",
      "Epoch 1429, Loss: 0.4275514271\n",
      "Epoch 1430, Loss: 0.5403562804\n",
      "Epoch 1431, Loss: 0.4530526612\n",
      "Epoch 1432, Loss: 0.6089700448\n",
      "Epoch 1433, Loss: 0.5400595502\n",
      "Epoch 1434, Loss: 0.5336891818\n",
      "Epoch 1435, Loss: 0.4785713364\n",
      "Epoch 1436, Loss: 0.5284459196\n",
      "Epoch 1437, Loss: 0.6399830971\n",
      "Epoch 1438, Loss: 0.5575472089\n",
      "Epoch 1439, Loss: 0.5897591089\n",
      "Epoch 1440, Loss: 0.5991170345\n",
      "Epoch 1441, Loss: 0.6610150181\n",
      "Epoch 1442, Loss: 0.5391444974\n",
      "Epoch 1443, Loss: 0.4413450930\n",
      "Epoch 1444, Loss: 0.5582224723\n",
      "Epoch 1445, Loss: 0.4963194975\n",
      "Epoch 1446, Loss: 0.6237468039\n",
      "Epoch 1447, Loss: 0.6137608403\n",
      "Epoch 1448, Loss: 0.6088276520\n",
      "Epoch 1449, Loss: 0.5689201576\n",
      "Epoch 1450, Loss: 0.5613754726\n",
      "Epoch 1451, Loss: 0.6242796146\n",
      "Epoch 1452, Loss: 0.9136497445\n",
      "Epoch 1453, Loss: 0.4800132272\n",
      "Epoch 1454, Loss: 0.4370820580\n",
      "Epoch 1455, Loss: 0.5589959899\n",
      "Epoch 1456, Loss: 0.4013776428\n",
      "Epoch 1457, Loss: 0.4269632688\n",
      "Epoch 1458, Loss: 0.5478451822\n",
      "Epoch 1459, Loss: 0.7173999512\n",
      "Epoch 1460, Loss: 0.5011076168\n",
      "Epoch 1461, Loss: 0.4537461307\n",
      "Epoch 1462, Loss: 0.4929290833\n",
      "Epoch 1463, Loss: 0.5727663088\n",
      "Epoch 1464, Loss: 0.5830196680\n",
      "Epoch 1465, Loss: 0.6330526545\n",
      "Epoch 1466, Loss: 0.4713299820\n",
      "Epoch 1467, Loss: 0.4727643740\n",
      "Epoch 1468, Loss: 0.5008691240\n",
      "Epoch 1469, Loss: 0.5244178416\n",
      "Epoch 1470, Loss: 0.5173503939\n",
      "Epoch 1471, Loss: 0.4463908477\n",
      "Epoch 1472, Loss: 0.4878629182\n",
      "Epoch 1473, Loss: 0.4601081019\n",
      "Epoch 1474, Loss: 0.4579299026\n",
      "Epoch 1475, Loss: 0.4573483882\n",
      "Epoch 1476, Loss: 0.4647934150\n",
      "Epoch 1477, Loss: 0.4552745861\n",
      "Epoch 1478, Loss: 0.4042735215\n",
      "Epoch 1479, Loss: 0.5351250114\n",
      "Epoch 1480, Loss: 0.5274352726\n",
      "Epoch 1481, Loss: 0.4578606523\n",
      "Epoch 1482, Loss: 0.4637517841\n",
      "Epoch 1483, Loss: 0.4507829265\n",
      "Epoch 1484, Loss: 0.4840748809\n",
      "Epoch 1485, Loss: 0.4636481848\n",
      "Epoch 1486, Loss: 0.4329828883\n",
      "Epoch 1487, Loss: 0.4132622496\n",
      "Epoch 1488, Loss: 0.4075420598\n",
      "Epoch 1489, Loss: 0.4402537826\n",
      "Epoch 1490, Loss: 0.5333511449\n",
      "Epoch 1491, Loss: 0.6288587651\n",
      "Epoch 1492, Loss: 0.4037531054\n",
      "Epoch 1493, Loss: 0.3335015386\n",
      "Epoch 1494, Loss: 0.3326635201\n",
      "Epoch 1495, Loss: 0.3464053218\n",
      "Epoch 1496, Loss: 0.3717867967\n",
      "Epoch 1497, Loss: 0.4295872816\n",
      "Epoch 1498, Loss: 0.4942589181\n",
      "Epoch 1499, Loss: 0.4333209274\n",
      "Epoch 1500, Loss: 0.4854533974\n",
      "Epoch 1501, Loss: 0.5020542505\n",
      "Epoch 1502, Loss: 0.3488584012\n",
      "Epoch 1503, Loss: 0.3652844723\n",
      "Epoch 1504, Loss: 0.4586484935\n",
      "Epoch 1505, Loss: 0.5913890585\n",
      "Epoch 1506, Loss: 0.7509006640\n",
      "Epoch 1507, Loss: 0.8404266018\n",
      "Epoch 1508, Loss: 0.5575881228\n",
      "Epoch 1509, Loss: 0.4912610883\n",
      "Epoch 1510, Loss: 0.4810067515\n",
      "Epoch 1511, Loss: 0.4397908734\n",
      "Epoch 1512, Loss: 0.3551395253\n",
      "Epoch 1513, Loss: 0.5091821596\n",
      "Epoch 1514, Loss: 0.4305267787\n",
      "Epoch 1515, Loss: 0.4019926665\n",
      "Epoch 1516, Loss: 0.3934819507\n",
      "Epoch 1517, Loss: 0.3545524735\n",
      "Epoch 1518, Loss: 0.3775834274\n",
      "Epoch 1519, Loss: 0.4418297584\n",
      "Epoch 1520, Loss: 0.5289150580\n",
      "Epoch 1521, Loss: 0.5975016235\n",
      "Epoch 1522, Loss: 0.5831045107\n",
      "Epoch 1523, Loss: 0.4921847910\n",
      "Epoch 1524, Loss: 0.3959459986\n",
      "Epoch 1525, Loss: 0.3608830456\n",
      "Epoch 1526, Loss: 0.3786158069\n",
      "Epoch 1527, Loss: 0.4317266803\n",
      "Epoch 1528, Loss: 0.5069659644\n",
      "Epoch 1529, Loss: 0.5607487314\n",
      "Epoch 1530, Loss: 0.5983900646\n",
      "Epoch 1531, Loss: 0.6062963621\n",
      "Epoch 1532, Loss: 0.4985375970\n",
      "Epoch 1533, Loss: 0.4479266461\n",
      "Epoch 1534, Loss: 0.3982784563\n",
      "Epoch 1535, Loss: 0.3909004060\n",
      "Epoch 1536, Loss: 0.3990850478\n",
      "Epoch 1537, Loss: 0.5939840349\n",
      "Epoch 1538, Loss: 0.3730978325\n",
      "Epoch 1539, Loss: 0.3582124355\n",
      "Epoch 1540, Loss: 0.5164883974\n",
      "Epoch 1541, Loss: 0.6735477522\n",
      "Epoch 1542, Loss: 0.3909136086\n",
      "Epoch 1543, Loss: 0.4099911792\n",
      "Epoch 1544, Loss: 0.5257092534\n",
      "Epoch 1545, Loss: 0.5487604934\n",
      "Epoch 1546, Loss: 0.4563823470\n",
      "Epoch 1547, Loss: 0.4581084165\n",
      "Epoch 1548, Loss: 0.5891389570\n",
      "Epoch 1549, Loss: 0.8486636336\n",
      "Epoch 1550, Loss: 0.9488415380\n",
      "Epoch 1551, Loss: 0.3459859815\n",
      "Epoch 1552, Loss: 0.4232708092\n",
      "Epoch 1553, Loss: 0.5487962688\n",
      "Epoch 1554, Loss: 0.5995864752\n",
      "Epoch 1555, Loss: 0.4532985469\n",
      "Epoch 1556, Loss: 0.3968914467\n",
      "Epoch 1557, Loss: 0.4323206817\n",
      "Epoch 1558, Loss: 0.5051501360\n",
      "Epoch 1559, Loss: 0.5546085518\n",
      "Epoch 1560, Loss: 0.4589163196\n",
      "Epoch 1561, Loss: 0.3784556163\n",
      "Epoch 1562, Loss: 0.4862591666\n",
      "Epoch 1563, Loss: 0.5115974128\n",
      "Epoch 1564, Loss: 0.5512218775\n",
      "Epoch 1565, Loss: 0.3996903893\n",
      "Epoch 1566, Loss: 0.3714515891\n",
      "Epoch 1567, Loss: 0.3967658935\n",
      "Epoch 1568, Loss: 0.3990601515\n",
      "Epoch 1569, Loss: 0.4540946575\n",
      "Epoch 1570, Loss: 0.4090787409\n",
      "Epoch 1571, Loss: 0.3499006095\n",
      "Epoch 1572, Loss: 0.3599149579\n",
      "Epoch 1573, Loss: 0.4035386494\n",
      "Epoch 1574, Loss: 0.5183218748\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_weights_init = []\n",
    "best_biases_init = []\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "best_seed = 0\n",
    "\n",
    "for _ in range (1):\n",
    "    nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8)\n",
    "    nn.train(batches, 14, 0.001)\n",
    "    if nn.get_best_loss() < best_loss:\n",
    "        best_loss = nn.get_best_loss()\n",
    "        best_weights = nn.get_best_weights()\n",
    "        best_biases = nn.get_best_biases()\n",
    "        best_seed = nn.get_best_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3326635201129378\n",
      "1726930603\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "print(best_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8, init_weights=best_weights, init_biases=best_biases, init_seed=best_seed)\n",
    "# nn.train(batches, 3, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.get_best_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Number of layers in the Neural Network\n",
    "N = 5  # Example value, replace with the actual number of layers\n",
    "\n",
    "# Initialize the dictionary\n",
    "weights_dict = {\n",
    "    'weights': {},\n",
    "    'bias': {}\n",
    "}\n",
    "\n",
    "weights = best_weights\n",
    "biases = best_biases\n",
    "\n",
    "# Populate the weights and bias dictionaries\n",
    "for i in range(N):\n",
    "    weights_dict['weights'][f'fc{i+1}'] = weights[i]\n",
    "    weights_dict['bias'][f'fc{i+1}'] = biases[i].flatten()\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "N = 5\n",
    "# load the weights from the pickle file\n",
    "with open('weights.pkl', 'rb') as f:\n",
    "    weights_dict = pickle.load(f)\n",
    "\n",
    "# Retrieve the weights and bias from the dictionary\n",
    "weights = []\n",
    "biases = []\n",
    "for i in range(N):\n",
    "    weights.append(weights_dict['weights'][f'fc{i+1}'])\n",
    "    biases.append(weights_dict['bias'][f'fc{i+1}'])\n",
    "\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(256, 625)\n",
      "13 2\n",
      "(256, 8)\n",
      "(128, 8)\n"
     ]
    }
   ],
   "source": [
    "print(type(batches[0][0]))\n",
    "print(batches[0][0].shape)\n",
    "print(len(batches), len(batches[0]))\n",
    "print(batches[0][1].shape)\n",
    "print(batches[12][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a total of 13 batches. concatenate them\n",
    "X = np.concatenate([batch[0] for batch in batches])\n",
    "y = np.concatenate([batch[1] for batch in batches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 625)\n",
      "(3200, 8)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    activations = [X]\n",
    "    pre_activations = []\n",
    "\n",
    "    # Pass through each layer except the output layer\n",
    "    for i in range(len(weights) - 1):\n",
    "        z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "        pre_activations.append(z)\n",
    "        a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "        activations.append(a)\n",
    "\n",
    "    # Pass through the output layer with softmax\n",
    "    z = np.dot(activations[-1], weights[-1]) + biases[-1]\n",
    "    pre_activations.append(z)\n",
    "    a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "    activations.append(a)\n",
    "\n",
    "    return activations, pre_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "act, pre_act = forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = act[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 8)\n",
      "[[1.25807308e-02 2.23260489e-05 1.45437094e-04 1.05072683e-05\n",
      "  8.79186312e-05 9.86907233e-01 2.45792154e-04 5.51017447e-08]\n",
      " [3.77941673e-06 6.47621973e-04 8.56439057e-04 9.12429616e-01\n",
      "  2.34237125e-04 1.19265585e-04 8.56677928e-02 4.12481022e-05]\n",
      " [1.14951456e-04 7.30366364e-01 1.05931635e-03 1.22562976e-01\n",
      "  9.94911331e-04 3.16930506e-05 1.28995518e-01 1.58742694e-02]\n",
      " [1.40654734e-05 2.18715552e-05 2.72690842e-03 5.35444382e-10\n",
      "  8.56883147e-05 4.14888508e-09 1.15546503e-04 9.97035915e-01]\n",
      " [3.52339890e-03 1.20059281e-07 8.25379545e-01 1.81509849e-09\n",
      "  1.58714232e-01 6.53428579e-07 3.02977375e-03 9.35227472e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(pred.shape)\n",
    "print(pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1658.6299993724401\n",
      "0.5183218748038876\n"
     ]
    }
   ],
   "source": [
    "loss = cross_entropy_loss(y, pred)\n",
    "print(loss)\n",
    "loss /= len(y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200,)\n",
      "[5 3 1 7 2 2 1 2 4 2]\n"
     ]
    }
   ],
   "source": [
    "# get argmax of the predictions\n",
    "y_pred = np.argmax(pred, axis=1)\n",
    "\n",
    "print(y_pred.shape)\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save y_pred in pickle file\n",
    "with open('predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
