{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Remember to import \"numpy_transforms\" functions if you wish to import these two classes in a different script.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "        label = row[\"class\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image), label\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image, label = self.dataset[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "        batch_labels = np.array(labels)\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(128, 625)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    print(labels.shape)  # Should be [batch_size]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For baseline purpose, running part B code in the same script\n",
    "\n",
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# # Sigmoid activation and its derivative\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# def sigmoid_derivative(x):\n",
    "#     s = sigmoid(x)\n",
    "#     return s * (1 - s)\n",
    "\n",
    "# def softmax(x, axis=None):\n",
    "#     exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "#     return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# # Cross-entropy loss\n",
    "# def cross_entropy_loss(y_true, y_pred):\n",
    "#     y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "#     return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# # Neural Network Class with Softmax in the Output Layer and Sigmoid in Hidden Layers\n",
    "# class NeuralNetwork_Baseline:\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size):\n",
    "#         np.random.seed(0)\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "\n",
    "#         layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "#         # Initialize weights and biases\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "#             self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         pre_activations = []\n",
    "\n",
    "#         # Pass through each layer except the output layer\n",
    "#         for i in range(len(self.weights) - 1):\n",
    "#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             pre_activations.append(z)\n",
    "#             a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "#             activations.append(a)\n",
    "\n",
    "#         # Pass through the output layer with softmax\n",
    "#         z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         pre_activations.append(z)\n",
    "#         a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "#         activations.append(a)\n",
    "\n",
    "#         return activations, pre_activations\n",
    "\n",
    "#     def backward(self, X, y, activations, pre_activations):\n",
    "#         grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "#         grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "#         # Start with output layer error\n",
    "#         delta = activations[-1] - y\n",
    "\n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "#             grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "#             if i > 0:\n",
    "#                 delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "\n",
    "#         return grad_w, grad_b\n",
    "\n",
    "#     def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "#         for i in range(len(self.weights)):\n",
    "#             self.weights[i] -= learning_rate * grad_w[i]\n",
    "#             self.biases[i] -= learning_rate * grad_b[i]\n",
    "\n",
    "#     def train(self, batches, learning_rate):\n",
    "#         start_time = time.time()\n",
    "#         epoch = 0\n",
    "#         while (True):\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 activations, pre_activations = self.forward(X_batch)\n",
    "#                 grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "#                 self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "#             # Calculate average loss over batches\n",
    "#             loss = 0\n",
    "#             z = 0\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 y_pred, _ = self.forward(X_batch)\n",
    "#                 loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "#                 z += len(y_pred[-1])\n",
    "#             loss /= z\n",
    "#             print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "#             epoch += 1\n",
    "#             if (time.time() - start_time) > 60:\n",
    "#                 break\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "#     def get_weights(self):\n",
    "#         return self.weights\n",
    "\n",
    "#     def get_biases(self):\n",
    "#         return self.biases\n",
    "\n",
    "# # Example usage:\n",
    "# nn = NeuralNetwork_Baseline(625, [512, 256, 128, 32], 8)\n",
    "# nn.train(batches, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "# import copy\n",
    "\n",
    "# Sigmoid activation and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Neural Network Class with Softmax in the Output Layer and Sigmoid in Hidden Layers\n",
    "class NeuralNetwork_Adam:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, init_weights = None, init_biases = None, init_seed = None, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        if (init_seed is None):\n",
    "            self.best_seed = int(time.time())\n",
    "            np.random.seed(self.best_seed)\n",
    "        else:\n",
    "            np.random.seed(init_seed)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.m_w = []\n",
    "        self.v_w = []\n",
    "        self.m_b = []\n",
    "        self.v_b = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step for Adam\n",
    "        self.best_weights = []\n",
    "        self.best_biases = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if (init_weights is not None) and (init_biases is not None):\n",
    "                self.weights.append(init_weights[i])\n",
    "                self.biases.append(init_biases[i])\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "                self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "            self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "            # self.best_weights = self.weights\n",
    "            # self.best_biases = self.biases\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each layer except the output layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        self.t += 1  # Increment time step for Adam\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def train(self, batches, time_of_running, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while(epoch<1880):\n",
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
    "            loss /= z\n",
    "            \n",
    "            if (loss < self.best_loss):\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = [np.copy(w) for w in self.weights]  # Use np.copy to create independent copies\n",
    "                self.best_biases = [np.copy(b) for b in self.biases]  # Use np.copy for biases\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            epoch += 1\n",
    "            if time.time() - start_time > 60*time_of_running:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    \n",
    "    def get_best_biases(self):\n",
    "        return self.best_biases\n",
    "    \n",
    "    def get_best_loss(self):\n",
    "        return self.best_loss\n",
    "    \n",
    "    def get_best_seed(self):\n",
    "        return self.best_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0862184558\n",
      "Epoch 2, Loss: 2.0903854791\n",
      "Epoch 3, Loss: 2.0809142824\n",
      "Epoch 4, Loss: 2.0797374437\n",
      "Epoch 5, Loss: 2.0800167394\n",
      "Epoch 6, Loss: 2.0796976038\n",
      "Epoch 7, Loss: 2.0794163806\n",
      "Epoch 8, Loss: 2.0793904514\n",
      "Epoch 9, Loss: 2.0793493257\n",
      "Epoch 10, Loss: 2.0791755929\n",
      "Epoch 11, Loss: 2.0789260915\n",
      "Epoch 12, Loss: 2.0785691959\n",
      "Epoch 13, Loss: 2.0780045940\n",
      "Epoch 14, Loss: 2.0769535169\n",
      "Epoch 15, Loss: 2.0749422044\n",
      "Epoch 16, Loss: 2.0709886164\n",
      "Epoch 17, Loss: 2.0657370344\n",
      "Epoch 18, Loss: 2.0484315203\n",
      "Epoch 19, Loss: 2.0224080908\n",
      "Epoch 20, Loss: 1.9858755850\n",
      "Epoch 21, Loss: 1.9332012026\n",
      "Epoch 22, Loss: 1.8984396047\n",
      "Epoch 23, Loss: 1.8482503905\n",
      "Epoch 24, Loss: 1.8138642462\n",
      "Epoch 25, Loss: 1.7724631977\n",
      "Epoch 26, Loss: 1.7662871856\n",
      "Epoch 27, Loss: 1.7287791291\n",
      "Epoch 28, Loss: 1.7097649653\n",
      "Epoch 29, Loss: 1.6967509127\n",
      "Epoch 30, Loss: 1.7025204582\n",
      "Epoch 31, Loss: 1.6348110353\n",
      "Epoch 32, Loss: 1.6040756664\n",
      "Epoch 33, Loss: 1.6310866187\n",
      "Epoch 34, Loss: 1.5598641881\n",
      "Epoch 35, Loss: 1.5584930078\n",
      "Epoch 36, Loss: 1.5102995367\n",
      "Epoch 37, Loss: 1.5015218311\n",
      "Epoch 38, Loss: 1.5240229435\n",
      "Epoch 39, Loss: 1.5122739405\n",
      "Epoch 40, Loss: 1.4570548427\n",
      "Epoch 41, Loss: 1.4342452134\n",
      "Epoch 42, Loss: 1.4425081025\n",
      "Epoch 43, Loss: 1.4321000272\n",
      "Epoch 44, Loss: 1.4076311342\n",
      "Epoch 45, Loss: 1.4037058935\n",
      "Epoch 46, Loss: 1.3999500187\n",
      "Epoch 47, Loss: 1.4469891031\n",
      "Epoch 48, Loss: 1.4096907420\n",
      "Epoch 49, Loss: 1.3761405546\n",
      "Epoch 50, Loss: 1.3685770282\n",
      "Epoch 51, Loss: 1.3983239899\n",
      "Epoch 52, Loss: 1.3947529205\n",
      "Epoch 53, Loss: 1.3863052650\n",
      "Epoch 54, Loss: 1.3648171089\n",
      "Epoch 55, Loss: 1.3587900612\n",
      "Epoch 56, Loss: 1.3562932378\n",
      "Epoch 57, Loss: 1.3509790479\n",
      "Epoch 58, Loss: 1.3506932951\n",
      "Epoch 59, Loss: 1.3286944751\n",
      "Epoch 60, Loss: 1.3165430722\n",
      "Epoch 61, Loss: 1.3762763212\n",
      "Epoch 62, Loss: 1.3173895510\n",
      "Epoch 63, Loss: 1.3327821720\n",
      "Epoch 64, Loss: 1.3365351947\n",
      "Epoch 65, Loss: 1.3160338211\n",
      "Epoch 66, Loss: 1.3590277326\n",
      "Epoch 67, Loss: 1.3546859416\n",
      "Epoch 68, Loss: 1.3364223408\n",
      "Epoch 69, Loss: 1.3143942317\n",
      "Epoch 70, Loss: 1.2846688892\n",
      "Epoch 71, Loss: 1.3399060117\n",
      "Epoch 72, Loss: 1.3022712597\n",
      "Epoch 73, Loss: 1.2773157998\n",
      "Epoch 74, Loss: 1.2804325258\n",
      "Epoch 75, Loss: 1.2750426469\n",
      "Epoch 76, Loss: 1.2707963695\n",
      "Epoch 77, Loss: 1.3099203032\n",
      "Epoch 78, Loss: 1.2524328794\n",
      "Epoch 79, Loss: 1.2546036193\n",
      "Epoch 80, Loss: 1.2537338539\n",
      "Epoch 81, Loss: 1.3073228062\n",
      "Epoch 82, Loss: 1.2845859326\n",
      "Epoch 83, Loss: 1.2733587122\n",
      "Epoch 84, Loss: 1.2838326208\n",
      "Epoch 85, Loss: 1.2280452107\n",
      "Epoch 86, Loss: 1.1955329136\n",
      "Epoch 87, Loss: 1.2119388208\n",
      "Epoch 88, Loss: 1.1833112565\n",
      "Epoch 89, Loss: 1.3110727821\n",
      "Epoch 90, Loss: 1.1820750278\n",
      "Epoch 91, Loss: 1.1882117166\n",
      "Epoch 92, Loss: 1.1852437897\n",
      "Epoch 93, Loss: 1.1833700431\n",
      "Epoch 94, Loss: 1.1542616632\n",
      "Epoch 95, Loss: 1.1410730635\n",
      "Epoch 96, Loss: 1.2263274745\n",
      "Epoch 97, Loss: 1.2022359891\n",
      "Epoch 98, Loss: 1.2299589636\n",
      "Epoch 99, Loss: 1.2073696892\n",
      "Epoch 100, Loss: 1.1423463766\n",
      "Epoch 101, Loss: 1.1378001502\n",
      "Epoch 102, Loss: 1.1706179128\n",
      "Epoch 103, Loss: 1.1259533213\n",
      "Epoch 104, Loss: 1.1483613747\n",
      "Epoch 105, Loss: 1.1231929943\n",
      "Epoch 106, Loss: 1.1211684123\n",
      "Epoch 107, Loss: 1.1012378719\n",
      "Epoch 108, Loss: 1.1191787169\n",
      "Epoch 109, Loss: 1.0945320076\n",
      "Epoch 110, Loss: 1.0843107039\n",
      "Epoch 111, Loss: 1.0765806128\n",
      "Epoch 112, Loss: 1.0850907754\n",
      "Epoch 113, Loss: 1.1148803691\n",
      "Epoch 114, Loss: 1.0900369695\n",
      "Epoch 115, Loss: 1.1128849539\n",
      "Epoch 116, Loss: 1.1268065373\n",
      "Epoch 117, Loss: 1.1630021342\n",
      "Epoch 118, Loss: 1.1152861561\n",
      "Epoch 119, Loss: 1.0502994678\n",
      "Epoch 120, Loss: 1.0422204345\n",
      "Epoch 121, Loss: 1.0406231373\n",
      "Epoch 122, Loss: 1.0343918024\n",
      "Epoch 123, Loss: 1.0361104799\n",
      "Epoch 124, Loss: 1.0205538890\n",
      "Epoch 125, Loss: 1.0155630063\n",
      "Epoch 126, Loss: 1.0194808520\n",
      "Epoch 127, Loss: 1.0113166489\n",
      "Epoch 128, Loss: 1.0109449060\n",
      "Epoch 129, Loss: 1.0043141286\n",
      "Epoch 130, Loss: 1.0145437152\n",
      "Epoch 131, Loss: 1.0072943595\n",
      "Epoch 132, Loss: 0.9962905844\n",
      "Epoch 133, Loss: 1.0208297530\n",
      "Epoch 134, Loss: 0.9895033894\n",
      "Epoch 135, Loss: 0.9891109347\n",
      "Epoch 136, Loss: 1.0330663135\n",
      "Epoch 137, Loss: 0.9829040914\n",
      "Epoch 138, Loss: 1.0087126088\n",
      "Epoch 139, Loss: 0.9802595223\n",
      "Epoch 140, Loss: 0.9873978207\n",
      "Epoch 141, Loss: 0.9795653269\n",
      "Epoch 142, Loss: 0.9771536866\n",
      "Epoch 143, Loss: 0.9777374634\n",
      "Epoch 144, Loss: 1.0501503360\n",
      "Epoch 145, Loss: 0.9916518399\n",
      "Epoch 146, Loss: 1.0277596104\n",
      "Epoch 147, Loss: 0.9673572199\n",
      "Epoch 148, Loss: 0.9786076635\n",
      "Epoch 149, Loss: 0.9892560824\n",
      "Epoch 150, Loss: 0.9538845896\n",
      "Epoch 151, Loss: 0.9504770216\n",
      "Epoch 152, Loss: 0.9847644018\n",
      "Epoch 153, Loss: 0.9855354363\n",
      "Epoch 154, Loss: 0.9584224858\n",
      "Epoch 155, Loss: 0.9653233444\n",
      "Epoch 156, Loss: 0.9438836298\n",
      "Epoch 157, Loss: 0.9564476285\n",
      "Epoch 158, Loss: 0.9561237581\n",
      "Epoch 159, Loss: 0.9851246243\n",
      "Epoch 160, Loss: 1.0450536646\n",
      "Epoch 161, Loss: 0.9681555041\n",
      "Epoch 162, Loss: 0.9574049534\n",
      "Epoch 163, Loss: 0.9382550159\n",
      "Epoch 164, Loss: 0.9827375289\n",
      "Epoch 165, Loss: 0.9702025926\n",
      "Epoch 166, Loss: 0.9537081549\n",
      "Epoch 167, Loss: 0.9223652952\n",
      "Epoch 168, Loss: 0.9275176840\n",
      "Epoch 169, Loss: 0.9342518189\n",
      "Epoch 170, Loss: 0.9310598894\n",
      "Epoch 171, Loss: 0.9432137911\n",
      "Epoch 172, Loss: 0.9934752085\n",
      "Epoch 173, Loss: 1.0491024576\n",
      "Epoch 174, Loss: 1.0358958253\n",
      "Epoch 175, Loss: 0.9664540814\n",
      "Epoch 176, Loss: 0.9366170559\n",
      "Epoch 177, Loss: 0.9133011748\n",
      "Epoch 178, Loss: 0.9458528564\n",
      "Epoch 179, Loss: 0.9236277986\n",
      "Epoch 180, Loss: 0.9398697705\n",
      "Epoch 181, Loss: 0.9323524638\n",
      "Epoch 182, Loss: 0.9372221702\n",
      "Epoch 183, Loss: 0.9379431371\n",
      "Epoch 184, Loss: 0.9387831554\n",
      "Epoch 185, Loss: 0.9410343114\n",
      "Epoch 186, Loss: 0.9407408828\n",
      "Epoch 187, Loss: 0.9352573585\n",
      "Epoch 188, Loss: 0.9526952054\n",
      "Epoch 189, Loss: 0.9266686419\n",
      "Epoch 190, Loss: 0.9167129764\n",
      "Epoch 191, Loss: 0.9283281500\n",
      "Epoch 192, Loss: 0.9263481463\n",
      "Epoch 193, Loss: 0.9188934147\n",
      "Epoch 194, Loss: 0.9584519580\n",
      "Epoch 195, Loss: 0.9740341421\n",
      "Epoch 196, Loss: 0.9057443429\n",
      "Epoch 197, Loss: 0.9434372052\n",
      "Epoch 198, Loss: 0.9887397735\n",
      "Epoch 199, Loss: 0.8774842896\n",
      "Epoch 200, Loss: 0.9269191376\n",
      "Epoch 201, Loss: 0.9087761880\n",
      "Epoch 202, Loss: 0.9090211184\n",
      "Epoch 203, Loss: 0.8789936985\n",
      "Epoch 204, Loss: 0.8992873969\n",
      "Epoch 205, Loss: 0.9184308313\n",
      "Epoch 206, Loss: 0.8939662368\n",
      "Epoch 207, Loss: 0.8832899095\n",
      "Epoch 208, Loss: 0.9121962022\n",
      "Epoch 209, Loss: 0.9429344532\n",
      "Epoch 210, Loss: 0.9545227380\n",
      "Epoch 211, Loss: 0.9122638020\n",
      "Epoch 212, Loss: 0.8593126553\n",
      "Epoch 213, Loss: 0.9059636439\n",
      "Epoch 214, Loss: 0.8960873705\n",
      "Epoch 215, Loss: 0.8681528557\n",
      "Epoch 216, Loss: 0.8975791655\n",
      "Epoch 217, Loss: 0.8944950860\n",
      "Epoch 218, Loss: 0.8963493048\n",
      "Epoch 219, Loss: 0.9750809280\n",
      "Epoch 220, Loss: 0.9211134607\n",
      "Epoch 221, Loss: 0.8972217597\n",
      "Epoch 222, Loss: 0.8702904657\n",
      "Epoch 223, Loss: 0.8999522917\n",
      "Epoch 224, Loss: 0.8841880888\n",
      "Epoch 225, Loss: 0.8608388218\n",
      "Epoch 226, Loss: 0.8724379822\n",
      "Epoch 227, Loss: 0.8812641508\n",
      "Epoch 228, Loss: 0.8593097255\n",
      "Epoch 229, Loss: 0.8621951293\n",
      "Epoch 230, Loss: 0.8570662246\n",
      "Epoch 231, Loss: 0.8616391734\n",
      "Epoch 232, Loss: 0.9437626459\n",
      "Epoch 233, Loss: 0.9329188522\n",
      "Epoch 234, Loss: 0.9504913869\n",
      "Epoch 235, Loss: 0.8936257684\n",
      "Epoch 236, Loss: 0.8632155324\n",
      "Epoch 237, Loss: 0.9396547491\n",
      "Epoch 238, Loss: 0.8385637168\n",
      "Epoch 239, Loss: 0.9239690034\n",
      "Epoch 240, Loss: 0.8925581381\n",
      "Epoch 241, Loss: 0.8367370044\n",
      "Epoch 242, Loss: 0.8766011741\n",
      "Epoch 243, Loss: 0.8570112360\n",
      "Epoch 244, Loss: 0.8466676353\n",
      "Epoch 245, Loss: 0.8766766695\n",
      "Epoch 246, Loss: 0.8588849491\n",
      "Epoch 247, Loss: 0.8369919557\n",
      "Epoch 248, Loss: 0.8581271097\n",
      "Epoch 249, Loss: 0.9005447155\n",
      "Epoch 250, Loss: 0.9514591844\n",
      "Epoch 251, Loss: 0.8192938399\n",
      "Epoch 252, Loss: 0.8608796107\n",
      "Epoch 253, Loss: 0.8569611330\n",
      "Epoch 254, Loss: 0.8335918947\n",
      "Epoch 255, Loss: 0.8813402800\n",
      "Epoch 256, Loss: 0.8780536294\n",
      "Epoch 257, Loss: 0.9025273478\n",
      "Epoch 258, Loss: 0.8692455788\n",
      "Epoch 259, Loss: 0.8543194048\n",
      "Epoch 260, Loss: 0.8390137805\n",
      "Epoch 261, Loss: 0.8909966829\n",
      "Epoch 262, Loss: 0.8360109557\n",
      "Epoch 263, Loss: 0.8779110213\n",
      "Epoch 264, Loss: 0.8086701725\n",
      "Epoch 265, Loss: 0.8760451446\n",
      "Epoch 266, Loss: 0.8222359876\n",
      "Epoch 267, Loss: 0.8663474252\n",
      "Epoch 268, Loss: 0.8215696200\n",
      "Epoch 269, Loss: 0.8594855601\n",
      "Epoch 270, Loss: 0.8457218124\n",
      "Epoch 271, Loss: 0.8637053839\n",
      "Epoch 272, Loss: 0.8320561714\n",
      "Epoch 273, Loss: 0.8646366872\n",
      "Epoch 274, Loss: 0.9763968558\n",
      "Epoch 275, Loss: 0.9261041286\n",
      "Epoch 276, Loss: 0.9403270451\n",
      "Epoch 277, Loss: 0.8524213243\n",
      "Epoch 278, Loss: 0.9915818348\n",
      "Epoch 279, Loss: 1.0177577476\n",
      "Epoch 280, Loss: 1.1088871337\n",
      "Epoch 281, Loss: 1.3164909910\n",
      "Epoch 282, Loss: 0.9848687678\n",
      "Epoch 283, Loss: 0.9996122718\n",
      "Epoch 284, Loss: 1.0390475132\n",
      "Epoch 285, Loss: 1.0330029806\n",
      "Epoch 286, Loss: 0.8749571581\n",
      "Epoch 287, Loss: 0.8667902975\n",
      "Epoch 288, Loss: 0.8161145101\n",
      "Epoch 289, Loss: 0.9466299234\n",
      "Epoch 290, Loss: 0.8000236362\n",
      "Epoch 291, Loss: 0.8468729215\n",
      "Epoch 292, Loss: 0.7941458798\n",
      "Epoch 293, Loss: 0.8413786758\n",
      "Epoch 294, Loss: 0.8330265279\n",
      "Epoch 295, Loss: 0.7958635540\n",
      "Epoch 296, Loss: 0.8798768292\n",
      "Epoch 297, Loss: 0.9869870358\n",
      "Epoch 298, Loss: 0.9298153726\n",
      "Epoch 299, Loss: 0.8479521169\n",
      "Epoch 300, Loss: 0.8498962011\n",
      "Epoch 301, Loss: 0.8347600159\n",
      "Epoch 302, Loss: 0.8626152243\n",
      "Epoch 303, Loss: 0.7796459333\n",
      "Epoch 304, Loss: 0.7936517293\n",
      "Epoch 305, Loss: 0.8480199285\n",
      "Epoch 306, Loss: 0.9378718273\n",
      "Epoch 307, Loss: 0.7855901605\n",
      "Epoch 308, Loss: 0.7669450785\n",
      "Epoch 309, Loss: 0.8327078388\n",
      "Epoch 310, Loss: 0.8110941765\n",
      "Epoch 311, Loss: 0.7824383548\n",
      "Epoch 312, Loss: 0.8054957744\n",
      "Epoch 313, Loss: 0.8609978662\n",
      "Epoch 314, Loss: 0.9064311564\n",
      "Epoch 315, Loss: 0.9540495370\n",
      "Epoch 316, Loss: 0.7829641348\n",
      "Epoch 317, Loss: 0.8649706192\n",
      "Epoch 318, Loss: 0.7793510789\n",
      "Epoch 319, Loss: 0.8203930492\n",
      "Epoch 320, Loss: 0.7871072730\n",
      "Epoch 321, Loss: 0.7729077502\n",
      "Epoch 322, Loss: 0.7823892909\n",
      "Epoch 323, Loss: 0.8307904956\n",
      "Epoch 324, Loss: 0.8609323014\n",
      "Epoch 325, Loss: 0.8005650971\n",
      "Epoch 326, Loss: 0.7409710025\n",
      "Epoch 327, Loss: 0.7438536926\n",
      "Epoch 328, Loss: 0.7714708020\n",
      "Epoch 329, Loss: 0.8092197327\n",
      "Epoch 330, Loss: 0.8004892684\n",
      "Epoch 331, Loss: 0.7948203620\n",
      "Epoch 332, Loss: 0.8226816777\n",
      "Epoch 333, Loss: 0.9975537793\n",
      "Epoch 334, Loss: 0.7514506538\n",
      "Epoch 335, Loss: 0.8244143509\n",
      "Epoch 336, Loss: 0.7514245029\n",
      "Epoch 337, Loss: 0.7437582567\n",
      "Epoch 338, Loss: 0.7681661992\n",
      "Epoch 339, Loss: 0.7652630237\n",
      "Epoch 340, Loss: 0.7562742812\n",
      "Epoch 341, Loss: 0.7678036482\n",
      "Epoch 342, Loss: 0.7576825045\n",
      "Epoch 343, Loss: 0.7689201609\n",
      "Epoch 344, Loss: 0.7902504036\n",
      "Epoch 345, Loss: 0.8504110411\n",
      "Epoch 346, Loss: 0.9623518700\n",
      "Epoch 347, Loss: 0.7516906572\n",
      "Epoch 348, Loss: 0.8001621995\n",
      "Epoch 349, Loss: 0.7463928807\n",
      "Epoch 350, Loss: 0.7452460840\n",
      "Epoch 351, Loss: 0.8078966732\n",
      "Epoch 352, Loss: 0.8322480500\n",
      "Epoch 353, Loss: 0.7527392970\n",
      "Epoch 354, Loss: 0.7318015251\n",
      "Epoch 355, Loss: 0.7730360391\n",
      "Epoch 356, Loss: 0.8129828789\n",
      "Epoch 357, Loss: 0.8529166621\n",
      "Epoch 358, Loss: 0.8279263357\n",
      "Epoch 359, Loss: 0.7243987639\n",
      "Epoch 360, Loss: 0.7756203037\n",
      "Epoch 361, Loss: 0.7412961582\n",
      "Epoch 362, Loss: 0.7794701269\n",
      "Epoch 363, Loss: 0.8383502716\n",
      "Epoch 364, Loss: 0.9153543917\n",
      "Epoch 365, Loss: 0.7171196873\n",
      "Epoch 366, Loss: 0.7130397315\n",
      "Epoch 367, Loss: 0.7423350297\n",
      "Epoch 368, Loss: 0.7863567719\n",
      "Epoch 369, Loss: 0.8370758237\n",
      "Epoch 370, Loss: 0.8703152016\n",
      "Epoch 371, Loss: 0.7257793161\n",
      "Epoch 372, Loss: 0.7274977431\n",
      "Epoch 373, Loss: 0.7236241848\n",
      "Epoch 374, Loss: 0.7636968406\n",
      "Epoch 375, Loss: 0.7930269277\n",
      "Epoch 376, Loss: 0.8666685050\n",
      "Epoch 377, Loss: 0.8745960356\n",
      "Epoch 378, Loss: 0.7279970377\n",
      "Epoch 379, Loss: 0.7629763297\n",
      "Epoch 380, Loss: 0.7540231266\n",
      "Epoch 381, Loss: 0.7174230668\n",
      "Epoch 382, Loss: 0.7276714105\n",
      "Epoch 383, Loss: 0.7882707264\n",
      "Epoch 384, Loss: 0.8413840114\n",
      "Epoch 385, Loss: 0.7081336512\n",
      "Epoch 386, Loss: 0.6945080887\n",
      "Epoch 387, Loss: 0.7050032147\n",
      "Epoch 388, Loss: 0.7331377309\n",
      "Epoch 389, Loss: 0.7480026882\n",
      "Epoch 390, Loss: 0.7602081207\n",
      "Epoch 391, Loss: 0.7595408817\n",
      "Epoch 392, Loss: 0.8343703752\n",
      "Epoch 393, Loss: 0.9106835576\n",
      "Epoch 394, Loss: 0.7045810221\n",
      "Epoch 395, Loss: 0.7706831247\n",
      "Epoch 396, Loss: 0.7168604446\n",
      "Epoch 397, Loss: 0.7053393292\n",
      "Epoch 398, Loss: 0.7191939394\n",
      "Epoch 399, Loss: 0.8056224847\n",
      "Epoch 400, Loss: 0.7484164940\n",
      "Epoch 401, Loss: 0.6801236425\n",
      "Epoch 402, Loss: 0.7075595400\n",
      "Epoch 403, Loss: 0.7017256232\n",
      "Epoch 404, Loss: 0.7551653388\n",
      "Epoch 405, Loss: 0.7502073807\n",
      "Epoch 406, Loss: 0.7117731604\n",
      "Epoch 407, Loss: 0.6803254164\n",
      "Epoch 408, Loss: 0.7324478980\n",
      "Epoch 409, Loss: 0.7463191889\n",
      "Epoch 410, Loss: 0.7434736924\n",
      "Epoch 411, Loss: 0.7562852691\n",
      "Epoch 412, Loss: 0.7582086058\n",
      "Epoch 413, Loss: 0.7532392125\n",
      "Epoch 414, Loss: 0.8368435156\n",
      "Epoch 415, Loss: 0.9275348345\n",
      "Epoch 416, Loss: 0.6972242603\n",
      "Epoch 417, Loss: 0.7224185311\n",
      "Epoch 418, Loss: 0.6634262752\n",
      "Epoch 419, Loss: 0.6749124635\n",
      "Epoch 420, Loss: 0.6838339749\n",
      "Epoch 421, Loss: 0.7320127473\n",
      "Epoch 422, Loss: 0.7188461804\n",
      "Epoch 423, Loss: 0.6742230528\n",
      "Epoch 424, Loss: 0.6785098195\n",
      "Epoch 425, Loss: 0.7182197568\n",
      "Epoch 426, Loss: 0.6934576488\n",
      "Epoch 427, Loss: 0.6563807383\n",
      "Epoch 428, Loss: 0.6583694126\n",
      "Epoch 429, Loss: 0.7275699774\n",
      "Epoch 430, Loss: 0.7111279029\n",
      "Epoch 431, Loss: 0.7714862665\n",
      "Epoch 432, Loss: 0.6998138496\n",
      "Epoch 433, Loss: 0.6658739424\n",
      "Epoch 434, Loss: 0.6470017308\n",
      "Epoch 435, Loss: 0.6827775853\n",
      "Epoch 436, Loss: 0.6547188767\n",
      "Epoch 437, Loss: 0.6354823134\n",
      "Epoch 438, Loss: 0.6605214058\n",
      "Epoch 439, Loss: 0.6943059464\n",
      "Epoch 440, Loss: 0.6980206686\n",
      "Epoch 441, Loss: 0.6446389612\n",
      "Epoch 442, Loss: 0.6435365940\n",
      "Epoch 443, Loss: 0.6549549958\n",
      "Epoch 444, Loss: 0.6437249305\n",
      "Epoch 445, Loss: 0.6588680773\n",
      "Epoch 446, Loss: 0.6881219554\n",
      "Epoch 447, Loss: 0.6686373486\n",
      "Epoch 448, Loss: 0.6926806188\n",
      "Epoch 449, Loss: 0.6370528243\n",
      "Epoch 450, Loss: 0.6405226511\n",
      "Epoch 451, Loss: 0.6271613284\n",
      "Epoch 452, Loss: 0.6906360624\n",
      "Epoch 453, Loss: 0.6564870549\n",
      "Epoch 454, Loss: 0.7282648717\n",
      "Epoch 455, Loss: 0.6375062939\n",
      "Epoch 456, Loss: 0.6896095289\n",
      "Epoch 457, Loss: 0.6808235968\n",
      "Epoch 458, Loss: 0.6165414293\n",
      "Epoch 459, Loss: 0.6914176536\n",
      "Epoch 460, Loss: 0.6353463770\n",
      "Epoch 461, Loss: 0.6669320422\n",
      "Epoch 462, Loss: 0.6210961546\n",
      "Epoch 463, Loss: 0.6230291067\n",
      "Epoch 464, Loss: 0.6690672584\n",
      "Epoch 465, Loss: 0.7031230291\n",
      "Epoch 466, Loss: 0.6340612746\n",
      "Epoch 467, Loss: 0.6014847346\n",
      "Epoch 468, Loss: 0.6357353466\n",
      "Epoch 469, Loss: 0.6336837808\n",
      "Epoch 470, Loss: 0.6537268324\n",
      "Epoch 471, Loss: 0.6275009503\n",
      "Epoch 472, Loss: 0.6018854997\n",
      "Epoch 473, Loss: 0.6680970333\n",
      "Epoch 474, Loss: 0.6150769859\n",
      "Epoch 475, Loss: 0.6451078337\n",
      "Epoch 476, Loss: 0.6266688517\n",
      "Epoch 477, Loss: 0.6306879153\n",
      "Epoch 478, Loss: 0.6848692157\n",
      "Epoch 479, Loss: 0.6335117361\n",
      "Epoch 480, Loss: 0.6947361961\n",
      "Epoch 481, Loss: 0.6280575873\n",
      "Epoch 482, Loss: 0.6244420478\n",
      "Epoch 483, Loss: 0.6571446599\n",
      "Epoch 484, Loss: 0.6224776248\n",
      "Epoch 485, Loss: 0.6107628988\n",
      "Epoch 486, Loss: 0.6128440939\n",
      "Epoch 487, Loss: 0.6746289919\n",
      "Epoch 488, Loss: 0.8073519668\n",
      "Epoch 489, Loss: 1.0912235236\n",
      "Epoch 490, Loss: 0.6628435967\n",
      "Epoch 491, Loss: 0.6316583771\n",
      "Epoch 492, Loss: 0.8825551710\n",
      "Epoch 493, Loss: 0.6577528863\n",
      "Epoch 494, Loss: 0.6456862592\n",
      "Epoch 495, Loss: 0.6040185326\n",
      "Epoch 496, Loss: 0.5926859142\n",
      "Epoch 497, Loss: 0.5842266661\n",
      "Epoch 498, Loss: 0.7094739817\n",
      "Epoch 499, Loss: 0.8082027743\n",
      "Epoch 500, Loss: 0.8152971066\n",
      "Epoch 501, Loss: 0.5952117106\n",
      "Epoch 502, Loss: 0.6365536496\n",
      "Epoch 503, Loss: 0.8052529784\n",
      "Epoch 504, Loss: 0.6703384112\n",
      "Epoch 505, Loss: 0.6466729904\n",
      "Epoch 506, Loss: 0.6635493799\n",
      "Epoch 507, Loss: 0.5878549977\n",
      "Epoch 508, Loss: 0.5742633496\n",
      "Epoch 509, Loss: 0.6064759703\n",
      "Epoch 510, Loss: 0.6438964881\n",
      "Epoch 511, Loss: 0.7155075948\n",
      "Epoch 512, Loss: 0.7222564725\n",
      "Epoch 513, Loss: 0.7170022029\n",
      "Epoch 514, Loss: 0.7114199573\n",
      "Epoch 515, Loss: 0.5804986599\n",
      "Epoch 516, Loss: 0.6364033570\n",
      "Epoch 517, Loss: 0.7131041359\n",
      "Epoch 518, Loss: 0.5823941295\n",
      "Epoch 519, Loss: 0.5852673848\n",
      "Epoch 520, Loss: 0.6136041625\n",
      "Epoch 521, Loss: 0.6288819644\n",
      "Epoch 522, Loss: 0.6097433679\n",
      "Epoch 523, Loss: 0.6889014673\n",
      "Epoch 524, Loss: 0.6718648547\n",
      "Epoch 525, Loss: 0.7320098369\n",
      "Epoch 526, Loss: 0.6976004172\n",
      "Epoch 527, Loss: 0.5778234749\n",
      "Epoch 528, Loss: 0.6297120715\n",
      "Epoch 529, Loss: 0.6292939303\n",
      "Epoch 530, Loss: 0.6365890283\n",
      "Epoch 531, Loss: 0.6163945213\n",
      "Epoch 532, Loss: 0.6336208318\n",
      "Epoch 533, Loss: 0.6156657732\n",
      "Epoch 534, Loss: 0.5731290086\n",
      "Epoch 535, Loss: 0.6217008247\n",
      "Epoch 536, Loss: 0.5789609721\n",
      "Epoch 537, Loss: 0.6677374930\n",
      "Epoch 538, Loss: 0.6052973500\n",
      "Epoch 539, Loss: 0.6110154354\n",
      "Epoch 540, Loss: 0.5822581430\n",
      "Epoch 541, Loss: 0.5666687904\n",
      "Epoch 542, Loss: 0.5886946137\n",
      "Epoch 543, Loss: 0.6168484462\n",
      "Epoch 544, Loss: 0.7213406305\n",
      "Epoch 545, Loss: 0.7328606327\n",
      "Epoch 546, Loss: 0.5674205806\n",
      "Epoch 547, Loss: 0.6227404785\n",
      "Epoch 548, Loss: 0.5952749789\n",
      "Epoch 549, Loss: 0.6370829299\n",
      "Epoch 550, Loss: 0.5988992117\n",
      "Epoch 551, Loss: 0.5567065373\n",
      "Epoch 552, Loss: 0.5756435871\n",
      "Epoch 553, Loss: 0.5842238679\n",
      "Epoch 554, Loss: 0.5825853084\n",
      "Epoch 555, Loss: 0.6349507977\n",
      "Epoch 556, Loss: 0.5775647153\n",
      "Epoch 557, Loss: 0.6722956445\n",
      "Epoch 558, Loss: 0.6250804397\n",
      "Epoch 559, Loss: 0.6364508458\n",
      "Epoch 560, Loss: 0.6141181351\n",
      "Epoch 561, Loss: 0.8957990746\n",
      "Epoch 562, Loss: 0.7617594415\n",
      "Epoch 563, Loss: 0.6793170098\n",
      "Epoch 564, Loss: 0.6767225888\n",
      "Epoch 565, Loss: 0.6429452186\n",
      "Epoch 566, Loss: 0.6940068851\n",
      "Epoch 567, Loss: 0.6178880334\n",
      "Epoch 568, Loss: 0.6651000428\n",
      "Epoch 569, Loss: 0.5816889641\n",
      "Epoch 570, Loss: 0.6073292214\n",
      "Epoch 571, Loss: 0.5907803361\n",
      "Epoch 572, Loss: 0.5433475844\n",
      "Epoch 573, Loss: 0.5342568359\n",
      "Epoch 574, Loss: 0.5642617140\n",
      "Epoch 575, Loss: 0.6291923267\n",
      "Epoch 576, Loss: 0.5624337743\n",
      "Epoch 577, Loss: 0.6275908482\n",
      "Epoch 578, Loss: 0.5851410881\n",
      "Epoch 579, Loss: 0.5549281273\n",
      "Epoch 580, Loss: 0.5457115603\n",
      "Epoch 581, Loss: 0.5588616613\n",
      "Epoch 582, Loss: 0.5333266869\n",
      "Epoch 583, Loss: 0.5743618147\n",
      "Epoch 584, Loss: 0.5249883549\n",
      "Epoch 585, Loss: 0.5278698406\n",
      "Epoch 586, Loss: 0.5935729679\n",
      "Epoch 587, Loss: 0.6635804898\n",
      "Epoch 588, Loss: 0.7020639298\n",
      "Epoch 589, Loss: 0.7788570820\n",
      "Epoch 590, Loss: 0.6547953537\n",
      "Epoch 591, Loss: 0.5466312833\n",
      "Epoch 592, Loss: 0.6424280235\n",
      "Epoch 593, Loss: 0.5220266441\n",
      "Epoch 594, Loss: 0.6235558543\n",
      "Epoch 595, Loss: 0.5348204819\n",
      "Epoch 596, Loss: 0.5119601786\n",
      "Epoch 597, Loss: 0.5356448659\n",
      "Epoch 598, Loss: 0.5369908426\n",
      "Epoch 599, Loss: 0.5758268765\n",
      "Epoch 600, Loss: 0.5576791747\n",
      "Epoch 601, Loss: 0.5874324658\n",
      "Epoch 602, Loss: 0.5785243241\n",
      "Epoch 603, Loss: 0.5618335872\n",
      "Epoch 604, Loss: 0.5260070884\n",
      "Epoch 605, Loss: 0.5120075931\n",
      "Epoch 606, Loss: 0.5292179603\n",
      "Epoch 607, Loss: 0.5075468675\n",
      "Epoch 608, Loss: 0.5273205291\n",
      "Epoch 609, Loss: 0.4867668129\n",
      "Epoch 610, Loss: 0.5304049295\n",
      "Epoch 611, Loss: 0.4933176490\n",
      "Epoch 612, Loss: 0.5410980098\n",
      "Epoch 613, Loss: 0.4980176366\n",
      "Epoch 614, Loss: 0.5366559487\n",
      "Epoch 615, Loss: 0.4921156167\n",
      "Epoch 616, Loss: 0.5239848916\n",
      "Epoch 617, Loss: 0.5308097865\n",
      "Epoch 618, Loss: 0.5444848521\n",
      "Epoch 619, Loss: 0.5335306401\n",
      "Epoch 620, Loss: 0.6169198954\n",
      "Epoch 621, Loss: 0.5822856552\n",
      "Epoch 622, Loss: 0.6074939347\n",
      "Epoch 623, Loss: 0.6401517094\n",
      "Epoch 624, Loss: 0.6196281914\n",
      "Epoch 625, Loss: 0.5505511825\n",
      "Epoch 626, Loss: 0.5012249114\n",
      "Epoch 627, Loss: 0.6068340696\n",
      "Epoch 628, Loss: 0.7328320174\n",
      "Epoch 629, Loss: 0.8450880544\n",
      "Epoch 630, Loss: 0.8048187163\n",
      "Epoch 631, Loss: 0.6734289994\n",
      "Epoch 632, Loss: 0.6198850848\n",
      "Epoch 633, Loss: 0.7942302047\n",
      "Epoch 634, Loss: 0.9808442553\n",
      "Epoch 635, Loss: 0.8576517731\n",
      "Epoch 636, Loss: 0.6442297775\n",
      "Epoch 637, Loss: 0.6564591017\n",
      "Epoch 638, Loss: 0.7059302782\n",
      "Epoch 639, Loss: 0.6616251524\n",
      "Epoch 640, Loss: 0.5604767200\n",
      "Epoch 641, Loss: 0.5147718102\n",
      "Epoch 642, Loss: 0.5527986993\n",
      "Epoch 643, Loss: 0.6715076809\n",
      "Epoch 644, Loss: 0.7558777665\n",
      "Epoch 645, Loss: 0.7289650327\n",
      "Epoch 646, Loss: 0.6322328404\n",
      "Epoch 647, Loss: 0.5308617849\n",
      "Epoch 648, Loss: 0.6323195425\n",
      "Epoch 649, Loss: 0.8565751156\n",
      "Epoch 650, Loss: 0.6223197924\n",
      "Epoch 651, Loss: 0.5750484634\n",
      "Epoch 652, Loss: 0.5654849655\n",
      "Epoch 653, Loss: 0.5119556082\n",
      "Epoch 654, Loss: 0.5645118998\n",
      "Epoch 655, Loss: 0.6413497448\n",
      "Epoch 656, Loss: 0.6324415164\n",
      "Epoch 657, Loss: 0.5469094559\n",
      "Epoch 658, Loss: 0.4953731181\n",
      "Epoch 659, Loss: 0.4989781696\n",
      "Epoch 660, Loss: 0.4900708114\n",
      "Epoch 661, Loss: 0.5532238814\n",
      "Epoch 662, Loss: 0.7485942660\n",
      "Epoch 663, Loss: 0.6904155265\n",
      "Epoch 664, Loss: 0.6090489987\n",
      "Epoch 665, Loss: 0.5016869825\n",
      "Epoch 666, Loss: 0.5119311728\n",
      "Epoch 667, Loss: 0.5162537341\n",
      "Epoch 668, Loss: 0.5597457823\n",
      "Epoch 669, Loss: 0.7421898775\n",
      "Epoch 670, Loss: 1.0210969299\n",
      "Epoch 671, Loss: 0.5845894241\n",
      "Epoch 672, Loss: 0.5398397441\n",
      "Epoch 673, Loss: 0.5457009644\n",
      "Epoch 674, Loss: 0.6101345622\n",
      "Epoch 675, Loss: 0.5241050810\n",
      "Epoch 676, Loss: 0.5018566945\n",
      "Epoch 677, Loss: 0.5159033576\n",
      "Epoch 678, Loss: 0.5474079532\n",
      "Epoch 679, Loss: 0.5190773915\n",
      "Epoch 680, Loss: 0.5451784194\n",
      "Epoch 681, Loss: 0.5956216798\n",
      "Epoch 682, Loss: 0.6254986418\n",
      "Epoch 683, Loss: 0.5226935128\n",
      "Epoch 684, Loss: 0.4899632564\n",
      "Epoch 685, Loss: 0.7203210822\n",
      "Epoch 686, Loss: 0.7323542167\n",
      "Epoch 687, Loss: 0.6382514891\n",
      "Epoch 688, Loss: 0.8429292921\n",
      "Epoch 689, Loss: 0.4795704660\n",
      "Epoch 690, Loss: 0.5324609203\n",
      "Epoch 691, Loss: 0.6821557762\n",
      "Epoch 692, Loss: 0.5078355778\n",
      "Epoch 693, Loss: 0.5015584572\n",
      "Epoch 694, Loss: 0.4928263002\n",
      "Epoch 695, Loss: 0.4755568080\n",
      "Epoch 696, Loss: 0.4487442908\n",
      "Epoch 697, Loss: 0.4825717878\n",
      "Epoch 698, Loss: 0.5551600351\n",
      "Epoch 699, Loss: 0.7036967339\n",
      "Epoch 700, Loss: 0.7682693576\n",
      "Epoch 701, Loss: 0.5839274424\n",
      "Epoch 702, Loss: 0.5476030973\n",
      "Epoch 703, Loss: 0.7436734603\n",
      "Epoch 704, Loss: 0.5489123271\n",
      "Epoch 705, Loss: 0.5771443735\n",
      "Epoch 706, Loss: 0.4805434721\n",
      "Epoch 707, Loss: 0.5433200063\n",
      "Epoch 708, Loss: 0.4737747578\n",
      "Epoch 709, Loss: 0.5612247399\n",
      "Epoch 710, Loss: 0.4837314250\n",
      "Epoch 711, Loss: 0.5342073531\n",
      "Epoch 712, Loss: 0.5766210950\n",
      "Epoch 713, Loss: 0.5546343488\n",
      "Epoch 714, Loss: 0.4528982459\n",
      "Epoch 715, Loss: 0.4977820372\n",
      "Epoch 716, Loss: 0.4924282786\n",
      "Epoch 717, Loss: 0.5547478126\n",
      "Epoch 718, Loss: 0.6045148701\n",
      "Epoch 719, Loss: 0.5735381821\n",
      "Epoch 720, Loss: 0.5367094054\n",
      "Epoch 721, Loss: 0.4433926829\n",
      "Epoch 722, Loss: 0.4532739615\n",
      "Epoch 723, Loss: 0.4896559993\n",
      "Epoch 724, Loss: 0.6863374583\n",
      "Epoch 725, Loss: 0.7054849136\n",
      "Epoch 726, Loss: 0.5216339497\n",
      "Epoch 727, Loss: 0.5315387005\n",
      "Epoch 728, Loss: 0.4528698566\n",
      "Epoch 729, Loss: 0.6249982187\n",
      "Epoch 730, Loss: 0.4653952995\n",
      "Epoch 731, Loss: 0.4550781542\n",
      "Epoch 732, Loss: 0.4887125499\n",
      "Epoch 733, Loss: 0.5388387220\n",
      "Epoch 734, Loss: 0.5720806298\n",
      "Epoch 735, Loss: 0.4935818034\n",
      "Epoch 736, Loss: 0.4338048627\n",
      "Epoch 737, Loss: 0.4535806167\n",
      "Epoch 738, Loss: 0.4439389859\n",
      "Epoch 739, Loss: 0.5237075372\n",
      "Epoch 740, Loss: 0.6755904819\n",
      "Epoch 741, Loss: 0.4754957584\n",
      "Epoch 742, Loss: 0.4629800841\n",
      "Epoch 743, Loss: 0.4873913814\n",
      "Epoch 744, Loss: 0.5302384431\n",
      "Epoch 745, Loss: 0.7976369989\n",
      "Epoch 746, Loss: 0.9116686476\n",
      "Epoch 747, Loss: 0.6057497829\n",
      "Epoch 748, Loss: 0.6938423357\n",
      "Epoch 749, Loss: 0.4833245875\n",
      "Epoch 750, Loss: 0.4603953731\n",
      "Epoch 751, Loss: 0.5560778473\n",
      "Epoch 752, Loss: 0.5881576825\n",
      "Epoch 753, Loss: 0.5584145586\n",
      "Epoch 754, Loss: 0.4904636828\n",
      "Epoch 755, Loss: 0.5135096767\n",
      "Epoch 756, Loss: 0.4682580463\n",
      "Epoch 757, Loss: 0.4399878944\n",
      "Epoch 758, Loss: 0.4197621546\n",
      "Epoch 759, Loss: 0.4295427515\n",
      "Epoch 760, Loss: 0.4839865717\n",
      "Epoch 761, Loss: 0.6053305336\n",
      "Epoch 762, Loss: 0.5864869188\n",
      "Epoch 763, Loss: 0.4845422501\n",
      "Epoch 764, Loss: 0.5543505569\n",
      "Epoch 765, Loss: 0.4944949860\n",
      "Epoch 766, Loss: 0.5292524937\n",
      "Epoch 767, Loss: 0.4809269707\n",
      "Epoch 768, Loss: 0.4994025903\n",
      "Epoch 769, Loss: 0.4965815923\n",
      "Epoch 770, Loss: 0.5738515857\n",
      "Epoch 771, Loss: 0.8215274590\n",
      "Epoch 772, Loss: 0.5396651937\n",
      "Epoch 773, Loss: 0.4907603857\n",
      "Epoch 774, Loss: 0.5090837689\n",
      "Epoch 775, Loss: 0.4905323888\n",
      "Epoch 776, Loss: 0.4431562938\n",
      "Epoch 777, Loss: 0.4576248702\n",
      "Epoch 778, Loss: 0.4654521058\n",
      "Epoch 779, Loss: 0.4890194239\n",
      "Epoch 780, Loss: 0.5205535630\n",
      "Epoch 781, Loss: 0.5157663112\n",
      "Epoch 782, Loss: 0.5597410276\n",
      "Epoch 783, Loss: 0.6652098128\n",
      "Epoch 784, Loss: 0.5659175626\n",
      "Epoch 785, Loss: 0.4548456380\n",
      "Epoch 786, Loss: 0.4513527496\n",
      "Epoch 787, Loss: 0.5141438316\n",
      "Epoch 788, Loss: 0.4848508218\n",
      "Epoch 789, Loss: 0.4582490613\n",
      "Epoch 790, Loss: 0.5126320133\n",
      "Epoch 791, Loss: 0.4624198272\n",
      "Epoch 792, Loss: 0.4298384265\n",
      "Epoch 793, Loss: 0.4391790534\n",
      "Epoch 794, Loss: 0.4481429116\n",
      "Epoch 795, Loss: 0.4988121072\n",
      "Epoch 796, Loss: 0.5353427594\n",
      "Epoch 797, Loss: 0.6590444971\n",
      "Epoch 798, Loss: 0.6098084624\n",
      "Epoch 799, Loss: 0.4819244219\n",
      "Epoch 800, Loss: 0.5640100761\n",
      "Epoch 801, Loss: 0.4211927190\n",
      "Epoch 802, Loss: 0.4496442458\n",
      "Epoch 803, Loss: 0.4840164322\n",
      "Epoch 804, Loss: 0.4635258681\n",
      "Epoch 805, Loss: 0.5116834826\n",
      "Epoch 806, Loss: 0.6666108985\n",
      "Epoch 807, Loss: 0.6107246735\n",
      "Epoch 808, Loss: 0.4539475235\n",
      "Epoch 809, Loss: 0.4060958030\n",
      "Epoch 810, Loss: 0.5242674157\n",
      "Epoch 811, Loss: 0.4087614404\n",
      "Epoch 812, Loss: 0.4869127193\n",
      "Epoch 813, Loss: 0.4075019227\n",
      "Epoch 814, Loss: 0.3929783731\n",
      "Epoch 815, Loss: 0.4186769288\n",
      "Epoch 816, Loss: 0.5204289030\n",
      "Epoch 817, Loss: 0.6372337172\n",
      "Epoch 818, Loss: 0.7959265169\n",
      "Epoch 819, Loss: 0.5329491321\n",
      "Epoch 820, Loss: 0.5037239828\n",
      "Epoch 821, Loss: 0.5358299403\n",
      "Epoch 822, Loss: 0.5637034218\n",
      "Epoch 823, Loss: 0.4336799035\n",
      "Epoch 824, Loss: 0.4365739755\n",
      "Epoch 825, Loss: 0.4383028497\n",
      "Epoch 826, Loss: 0.4804596186\n",
      "Epoch 827, Loss: 0.4471821367\n",
      "Epoch 828, Loss: 0.3987362839\n",
      "Epoch 829, Loss: 0.5342832648\n",
      "Epoch 830, Loss: 0.4995018272\n",
      "Epoch 831, Loss: 0.5864579565\n",
      "Epoch 832, Loss: 0.6902428405\n",
      "Epoch 833, Loss: 0.5155251799\n",
      "Epoch 834, Loss: 0.4556710463\n",
      "Epoch 835, Loss: 0.4500848979\n",
      "Epoch 836, Loss: 0.4508801436\n",
      "Epoch 837, Loss: 0.4763518429\n",
      "Epoch 838, Loss: 0.4770081075\n",
      "Epoch 839, Loss: 0.4701449755\n",
      "Epoch 840, Loss: 0.5733420771\n",
      "Epoch 841, Loss: 0.5942234819\n",
      "Epoch 842, Loss: 0.5133894245\n",
      "Epoch 843, Loss: 0.4486944843\n",
      "Epoch 844, Loss: 0.4222441959\n",
      "Epoch 845, Loss: 0.3990196759\n",
      "Epoch 846, Loss: 0.4253348386\n",
      "Epoch 847, Loss: 0.3903587509\n",
      "Epoch 848, Loss: 0.3899844910\n",
      "Epoch 849, Loss: 0.4447566494\n",
      "Epoch 850, Loss: 0.4745786025\n",
      "Epoch 851, Loss: 0.6101824497\n",
      "Epoch 852, Loss: 0.5818062096\n",
      "Epoch 853, Loss: 0.5512095863\n",
      "Epoch 854, Loss: 0.3966570503\n",
      "Epoch 855, Loss: 0.4459297072\n",
      "Epoch 856, Loss: 0.4283488779\n",
      "Epoch 857, Loss: 0.5653372328\n",
      "Epoch 858, Loss: 0.4470198744\n",
      "Epoch 859, Loss: 0.4046745385\n",
      "Epoch 860, Loss: 0.5954444144\n",
      "Epoch 861, Loss: 0.5353482232\n",
      "Epoch 862, Loss: 0.4423216287\n",
      "Epoch 863, Loss: 0.5053855401\n",
      "Epoch 864, Loss: 0.5225496971\n",
      "Epoch 865, Loss: 0.4090567702\n",
      "Epoch 866, Loss: 0.5676841730\n",
      "Epoch 867, Loss: 0.5596215089\n",
      "Epoch 868, Loss: 0.6887056656\n",
      "Epoch 869, Loss: 0.7638500753\n",
      "Epoch 870, Loss: 0.4855678436\n",
      "Epoch 871, Loss: 0.4193218681\n",
      "Epoch 872, Loss: 0.4568344830\n",
      "Epoch 873, Loss: 0.5228770290\n",
      "Epoch 874, Loss: 0.4737506391\n",
      "Epoch 875, Loss: 0.5127532848\n",
      "Epoch 876, Loss: 0.4743301376\n",
      "Epoch 877, Loss: 0.4102925411\n",
      "Epoch 878, Loss: 0.4292058477\n",
      "Epoch 879, Loss: 0.4256656474\n",
      "Epoch 880, Loss: 0.4217700049\n",
      "Epoch 881, Loss: 0.3955420250\n",
      "Epoch 882, Loss: 0.4940238362\n",
      "Epoch 883, Loss: 0.5033024815\n",
      "Epoch 884, Loss: 0.5596143650\n",
      "Epoch 885, Loss: 0.5188518274\n",
      "Epoch 886, Loss: 0.4578525458\n",
      "Epoch 887, Loss: 0.3946129857\n",
      "Epoch 888, Loss: 0.4432281752\n",
      "Epoch 889, Loss: 0.3958691344\n",
      "Epoch 890, Loss: 0.5150248809\n",
      "Epoch 891, Loss: 0.5818164370\n",
      "Epoch 892, Loss: 0.5999509697\n",
      "Epoch 893, Loss: 0.6277433594\n",
      "Epoch 894, Loss: 0.4141770768\n",
      "Epoch 895, Loss: 0.4884548653\n",
      "Epoch 896, Loss: 0.4635159869\n",
      "Epoch 897, Loss: 0.5727715487\n",
      "Epoch 898, Loss: 0.5129713363\n",
      "Epoch 899, Loss: 0.4247284095\n",
      "Epoch 900, Loss: 0.5085785886\n",
      "Epoch 901, Loss: 0.5163325291\n",
      "Epoch 902, Loss: 0.4871428186\n",
      "Epoch 903, Loss: 0.4618168528\n",
      "Epoch 904, Loss: 0.5058805119\n",
      "Epoch 905, Loss: 0.4728046079\n",
      "Epoch 906, Loss: 0.9200175345\n",
      "Epoch 907, Loss: 0.6381257062\n",
      "Epoch 908, Loss: 0.6002310618\n",
      "Epoch 909, Loss: 0.7671714225\n",
      "Epoch 910, Loss: 0.4731202953\n",
      "Epoch 911, Loss: 0.5107315941\n",
      "Epoch 912, Loss: 0.6789429928\n",
      "Epoch 913, Loss: 0.5175602964\n",
      "Epoch 914, Loss: 0.6921482503\n",
      "Epoch 915, Loss: 0.5627707975\n",
      "Epoch 916, Loss: 0.4321977912\n",
      "Epoch 917, Loss: 0.5740524697\n",
      "Epoch 918, Loss: 0.4917540981\n",
      "Epoch 919, Loss: 0.5060699829\n",
      "Epoch 920, Loss: 0.5314986205\n",
      "Epoch 921, Loss: 0.3873348490\n",
      "Epoch 922, Loss: 0.5686186513\n",
      "Epoch 923, Loss: 0.5358014998\n",
      "Epoch 924, Loss: 0.3758698523\n",
      "Epoch 925, Loss: 0.3717041271\n",
      "Epoch 926, Loss: 0.4235805868\n",
      "Epoch 927, Loss: 0.4783812582\n",
      "Epoch 928, Loss: 0.4219086424\n",
      "Epoch 929, Loss: 0.4400360820\n",
      "Epoch 930, Loss: 0.4212733836\n",
      "Epoch 931, Loss: 0.4484019875\n",
      "Epoch 932, Loss: 0.7471112329\n",
      "Epoch 933, Loss: 0.4880102183\n",
      "Epoch 934, Loss: 0.4819279467\n",
      "Epoch 935, Loss: 0.4774336762\n",
      "Epoch 936, Loss: 0.4010873427\n",
      "Epoch 937, Loss: 0.4970125401\n",
      "Epoch 938, Loss: 0.4330520527\n",
      "Epoch 939, Loss: 0.3505304550\n",
      "Epoch 940, Loss: 0.6056101921\n",
      "Epoch 941, Loss: 0.4704655848\n",
      "Epoch 942, Loss: 0.4782867040\n",
      "Epoch 943, Loss: 0.6916718316\n",
      "Epoch 944, Loss: 0.5334781972\n",
      "Epoch 945, Loss: 0.4539009612\n",
      "Epoch 946, Loss: 0.5232652083\n",
      "Epoch 947, Loss: 0.5704526024\n",
      "Epoch 948, Loss: 0.4032343085\n",
      "Epoch 949, Loss: 0.3958625779\n",
      "Epoch 950, Loss: 0.5734866365\n",
      "Epoch 951, Loss: 0.6960519245\n",
      "Epoch 952, Loss: 0.4794646606\n",
      "Epoch 953, Loss: 0.5767339164\n",
      "Epoch 954, Loss: 0.6376605577\n",
      "Epoch 955, Loss: 0.4664901359\n",
      "Epoch 956, Loss: 0.4864704664\n",
      "Epoch 957, Loss: 0.4333397693\n",
      "Epoch 958, Loss: 0.3611242822\n",
      "Epoch 959, Loss: 0.4219322972\n",
      "Epoch 960, Loss: 0.5260255131\n",
      "Epoch 961, Loss: 0.4605914124\n",
      "Epoch 962, Loss: 0.4037217742\n",
      "Epoch 963, Loss: 0.4776876725\n",
      "Epoch 964, Loss: 0.6240430641\n",
      "Epoch 965, Loss: 0.5642017792\n",
      "Epoch 966, Loss: 0.5493007449\n",
      "Epoch 967, Loss: 0.3650178480\n",
      "Epoch 968, Loss: 0.4236750931\n",
      "Epoch 969, Loss: 0.5016822946\n",
      "Epoch 970, Loss: 0.3918865710\n",
      "Epoch 971, Loss: 0.6175897400\n",
      "Epoch 972, Loss: 0.4372348784\n",
      "Epoch 973, Loss: 0.3609035565\n",
      "Epoch 974, Loss: 0.6425010623\n",
      "Epoch 975, Loss: 0.6353848344\n",
      "Epoch 976, Loss: 0.4381849347\n",
      "Epoch 977, Loss: 0.5739107160\n",
      "Epoch 978, Loss: 0.4099755688\n",
      "Epoch 979, Loss: 0.4824546687\n",
      "Epoch 980, Loss: 0.4487427571\n",
      "Epoch 981, Loss: 0.8582116602\n",
      "Epoch 982, Loss: 0.3896359003\n",
      "Epoch 983, Loss: 0.5334269050\n",
      "Epoch 984, Loss: 0.4116841457\n",
      "Epoch 985, Loss: 0.5439368168\n",
      "Epoch 986, Loss: 0.4128504259\n",
      "Epoch 987, Loss: 0.5440417723\n",
      "Epoch 988, Loss: 0.4068400166\n",
      "Epoch 989, Loss: 0.5028104517\n",
      "Epoch 990, Loss: 0.3642636710\n",
      "Epoch 991, Loss: 0.5548165950\n",
      "Epoch 992, Loss: 0.4317238979\n",
      "Epoch 993, Loss: 0.5260586272\n",
      "Epoch 994, Loss: 0.5277445195\n",
      "Epoch 995, Loss: 0.4597801484\n",
      "Epoch 996, Loss: 0.4409346720\n",
      "Epoch 997, Loss: 0.4239552121\n",
      "Epoch 998, Loss: 0.5990829038\n",
      "Epoch 999, Loss: 0.4062677529\n",
      "Epoch 1000, Loss: 0.3106598466\n",
      "Epoch 1001, Loss: 0.3623783105\n",
      "Epoch 1002, Loss: 0.5247810936\n",
      "Epoch 1003, Loss: 0.6536979443\n",
      "Epoch 1004, Loss: 0.5038102090\n",
      "Epoch 1005, Loss: 0.5017849592\n",
      "Epoch 1006, Loss: 0.5335419384\n",
      "Epoch 1007, Loss: 0.4581960324\n",
      "Epoch 1008, Loss: 0.3381446700\n",
      "Epoch 1009, Loss: 0.5163300678\n",
      "Epoch 1010, Loss: 0.6690081408\n",
      "Epoch 1011, Loss: 0.6976668592\n",
      "Epoch 1012, Loss: 0.6411168338\n",
      "Epoch 1013, Loss: 0.5956542830\n",
      "Epoch 1014, Loss: 0.4925478905\n",
      "Epoch 1015, Loss: 0.6338393260\n",
      "Epoch 1016, Loss: 0.3717554577\n",
      "Epoch 1017, Loss: 0.6904284517\n",
      "Epoch 1018, Loss: 0.5952589000\n",
      "Epoch 1019, Loss: 0.5338652154\n",
      "Epoch 1020, Loss: 0.5044744246\n",
      "Epoch 1021, Loss: 0.6348444144\n",
      "Epoch 1022, Loss: 0.4862847402\n",
      "Epoch 1023, Loss: 0.6714573020\n",
      "Epoch 1024, Loss: 0.4037806438\n",
      "Epoch 1025, Loss: 0.9744720294\n",
      "Epoch 1026, Loss: 0.3741349130\n",
      "Epoch 1027, Loss: 0.8607533421\n",
      "Epoch 1028, Loss: 0.5295611514\n",
      "Epoch 1029, Loss: 0.7784724600\n",
      "Epoch 1030, Loss: 0.5271793292\n",
      "Epoch 1031, Loss: 0.4440758747\n",
      "Epoch 1032, Loss: 0.6425770844\n",
      "Epoch 1033, Loss: 0.7080098791\n",
      "Epoch 1034, Loss: 0.7478769023\n",
      "Epoch 1035, Loss: 0.8747632972\n",
      "Epoch 1036, Loss: 0.5871287052\n",
      "Epoch 1037, Loss: 0.5752132126\n",
      "Epoch 1038, Loss: 0.7653791639\n",
      "Epoch 1039, Loss: 0.6877588209\n",
      "Epoch 1040, Loss: 0.8595158660\n",
      "Epoch 1041, Loss: 0.6216191846\n",
      "Epoch 1042, Loss: 0.6526283264\n",
      "Epoch 1043, Loss: 0.7055469734\n",
      "Epoch 1044, Loss: 0.6811101816\n",
      "Epoch 1045, Loss: 0.4918302011\n",
      "Epoch 1046, Loss: 0.5698233756\n",
      "Epoch 1047, Loss: 0.4209362852\n",
      "Epoch 1048, Loss: 0.4740203472\n",
      "Epoch 1049, Loss: 0.5112823117\n",
      "Epoch 1050, Loss: 0.5023672464\n",
      "Epoch 1051, Loss: 0.4010455413\n",
      "Epoch 1052, Loss: 0.4144658233\n",
      "Epoch 1053, Loss: 0.4527573475\n",
      "Epoch 1054, Loss: 0.5691386731\n",
      "Epoch 1055, Loss: 0.4754569190\n",
      "Epoch 1056, Loss: 0.3923414045\n",
      "Epoch 1057, Loss: 0.5007500844\n",
      "Epoch 1058, Loss: 0.5610819208\n",
      "Epoch 1059, Loss: 0.4223307621\n",
      "Epoch 1060, Loss: 0.3288481378\n",
      "Epoch 1061, Loss: 0.4119250508\n",
      "Epoch 1062, Loss: 0.3677116610\n",
      "Epoch 1063, Loss: 0.4150884575\n",
      "Epoch 1064, Loss: 0.5577205042\n",
      "Epoch 1065, Loss: 0.4206114995\n",
      "Epoch 1066, Loss: 0.3587865299\n",
      "Epoch 1067, Loss: 0.4052745962\n",
      "Epoch 1068, Loss: 0.5301120018\n",
      "Epoch 1069, Loss: 0.3995372049\n",
      "Epoch 1070, Loss: 0.3508485136\n",
      "Epoch 1071, Loss: 0.3722395406\n",
      "Epoch 1072, Loss: 0.3806431882\n",
      "Epoch 1073, Loss: 0.4822861507\n",
      "Epoch 1074, Loss: 0.5139673946\n",
      "Epoch 1075, Loss: 0.6385655733\n",
      "Epoch 1076, Loss: 0.3707937778\n",
      "Epoch 1077, Loss: 0.3304557159\n",
      "Epoch 1078, Loss: 0.4254620388\n",
      "Epoch 1079, Loss: 0.4231746956\n",
      "Epoch 1080, Loss: 0.4855129694\n",
      "Epoch 1081, Loss: 0.4490225083\n",
      "Epoch 1082, Loss: 0.3437876696\n",
      "Epoch 1083, Loss: 0.3614639644\n",
      "Epoch 1084, Loss: 0.4393668360\n",
      "Epoch 1085, Loss: 0.4506906526\n",
      "Epoch 1086, Loss: 0.4923585398\n",
      "Epoch 1087, Loss: 0.4037085139\n",
      "Epoch 1088, Loss: 0.4090112547\n",
      "Epoch 1089, Loss: 0.4410931015\n",
      "Epoch 1090, Loss: 0.4667909640\n",
      "Epoch 1091, Loss: 0.3883933437\n",
      "Epoch 1092, Loss: 0.3705258686\n",
      "Epoch 1093, Loss: 0.3507931739\n",
      "Epoch 1094, Loss: 0.3648163223\n",
      "Epoch 1095, Loss: 0.4045560208\n",
      "Epoch 1096, Loss: 0.3919714906\n",
      "Epoch 1097, Loss: 0.3404455563\n",
      "Epoch 1098, Loss: 0.3763395632\n",
      "Epoch 1099, Loss: 0.4078798004\n",
      "Epoch 1100, Loss: 0.4582875805\n",
      "Epoch 1101, Loss: 0.3686873476\n",
      "Epoch 1102, Loss: 0.4792468647\n",
      "Epoch 1103, Loss: 0.5540424751\n",
      "Epoch 1104, Loss: 0.3712436117\n",
      "Epoch 1105, Loss: 0.3693387416\n",
      "Epoch 1106, Loss: 0.4537063752\n",
      "Epoch 1107, Loss: 0.4198167200\n",
      "Epoch 1108, Loss: 0.6446253420\n",
      "Epoch 1109, Loss: 0.4478883332\n",
      "Epoch 1110, Loss: 0.4846232587\n",
      "Epoch 1111, Loss: 0.4340819996\n",
      "Epoch 1112, Loss: 0.6640408938\n",
      "Epoch 1113, Loss: 0.3935571973\n",
      "Epoch 1114, Loss: 0.5654302057\n",
      "Epoch 1115, Loss: 0.4703204014\n",
      "Epoch 1116, Loss: 0.4342196970\n",
      "Epoch 1117, Loss: 0.4218599276\n",
      "Epoch 1118, Loss: 0.4708158638\n",
      "Epoch 1119, Loss: 0.3702968563\n",
      "Epoch 1120, Loss: 0.3811269124\n",
      "Epoch 1121, Loss: 0.3264236710\n",
      "Epoch 1122, Loss: 0.4051968406\n",
      "Epoch 1123, Loss: 0.5171025651\n",
      "Epoch 1124, Loss: 0.4933795628\n",
      "Epoch 1125, Loss: 0.5176373515\n",
      "Epoch 1126, Loss: 0.4879687834\n",
      "Epoch 1127, Loss: 0.6699765127\n",
      "Epoch 1128, Loss: 0.3502821101\n",
      "Epoch 1129, Loss: 0.5284794188\n",
      "Epoch 1130, Loss: 0.4124031827\n",
      "Epoch 1131, Loss: 0.3658069004\n",
      "Epoch 1132, Loss: 0.3701483743\n",
      "Epoch 1133, Loss: 0.3213201686\n",
      "Epoch 1134, Loss: 0.3548285280\n",
      "Epoch 1135, Loss: 0.4172138731\n",
      "Epoch 1136, Loss: 0.4558368723\n",
      "Epoch 1137, Loss: 0.5268141718\n",
      "Epoch 1138, Loss: 0.6431371223\n",
      "Epoch 1139, Loss: 0.6656273100\n",
      "Epoch 1140, Loss: 0.3763297806\n",
      "Epoch 1141, Loss: 0.3720547575\n",
      "Epoch 1142, Loss: 0.5391037063\n",
      "Epoch 1143, Loss: 0.3232409738\n",
      "Epoch 1144, Loss: 0.4233690741\n",
      "Epoch 1145, Loss: 0.4891886449\n",
      "Epoch 1146, Loss: 0.3856676929\n",
      "Epoch 1147, Loss: 0.5023613167\n",
      "Epoch 1148, Loss: 0.4186923632\n",
      "Epoch 1149, Loss: 0.3250701022\n",
      "Epoch 1150, Loss: 0.4190109699\n",
      "Epoch 1151, Loss: 0.3520834690\n",
      "Epoch 1152, Loss: 0.3417796322\n",
      "Epoch 1153, Loss: 0.3924275415\n",
      "Epoch 1154, Loss: 0.5070661552\n",
      "Epoch 1155, Loss: 0.4593329663\n",
      "Epoch 1156, Loss: 0.5128361328\n",
      "Epoch 1157, Loss: 0.4295794097\n",
      "Epoch 1158, Loss: 0.4993662293\n",
      "Epoch 1159, Loss: 0.4349457438\n",
      "Epoch 1160, Loss: 0.3592105361\n",
      "Epoch 1161, Loss: 0.4675227319\n",
      "Epoch 1162, Loss: 0.3359860394\n",
      "Epoch 1163, Loss: 0.3962441208\n",
      "Epoch 1164, Loss: 0.4100080364\n",
      "Epoch 1165, Loss: 0.3183750763\n",
      "Epoch 1166, Loss: 0.4106014592\n",
      "Epoch 1167, Loss: 0.3402928778\n",
      "Epoch 1168, Loss: 0.3432034992\n",
      "Epoch 1169, Loss: 0.3803862431\n",
      "Epoch 1170, Loss: 0.3169313713\n",
      "Epoch 1171, Loss: 0.3071272725\n",
      "Epoch 1172, Loss: 0.3206069140\n",
      "Epoch 1173, Loss: 0.3632486183\n",
      "Epoch 1174, Loss: 0.3657488189\n",
      "Epoch 1175, Loss: 0.3616766153\n",
      "Epoch 1176, Loss: 0.4128008665\n",
      "Epoch 1177, Loss: 0.3674909573\n",
      "Epoch 1178, Loss: 0.3574777932\n",
      "Epoch 1179, Loss: 0.4417093037\n",
      "Epoch 1180, Loss: 0.4279146734\n",
      "Epoch 1181, Loss: 0.4123994575\n",
      "Epoch 1182, Loss: 0.5360796526\n",
      "Epoch 1183, Loss: 0.3697019220\n",
      "Epoch 1184, Loss: 0.3445557423\n",
      "Epoch 1185, Loss: 0.4444570618\n",
      "Epoch 1186, Loss: 0.3935213505\n",
      "Epoch 1187, Loss: 0.3533449607\n",
      "Epoch 1188, Loss: 0.3025777924\n",
      "Epoch 1189, Loss: 0.3182799140\n",
      "Epoch 1190, Loss: 0.2990828661\n",
      "Epoch 1191, Loss: 0.3558220315\n",
      "Epoch 1192, Loss: 0.3301128501\n",
      "Epoch 1193, Loss: 0.3462238901\n",
      "Epoch 1194, Loss: 0.3328794629\n",
      "Epoch 1195, Loss: 0.3912531266\n",
      "Epoch 1196, Loss: 0.4302248050\n",
      "Epoch 1197, Loss: 0.3719138899\n",
      "Epoch 1198, Loss: 0.3851703743\n",
      "Epoch 1199, Loss: 0.2620978545\n",
      "Epoch 1200, Loss: 0.4204435955\n",
      "Epoch 1201, Loss: 0.3338436524\n",
      "Epoch 1202, Loss: 0.3527726361\n",
      "Epoch 1203, Loss: 0.2583356346\n",
      "Epoch 1204, Loss: 0.3894466166\n",
      "Epoch 1205, Loss: 0.2907255875\n",
      "Epoch 1206, Loss: 0.4475944807\n",
      "Epoch 1207, Loss: 0.3702478302\n",
      "Epoch 1208, Loss: 0.3919153829\n",
      "Epoch 1209, Loss: 0.3310138976\n",
      "Epoch 1210, Loss: 0.3320526263\n",
      "Epoch 1211, Loss: 0.4145031944\n",
      "Epoch 1212, Loss: 0.4668672361\n",
      "Epoch 1213, Loss: 0.4490013325\n",
      "Epoch 1214, Loss: 0.3643384335\n",
      "Epoch 1215, Loss: 0.4523615364\n",
      "Epoch 1216, Loss: 0.4705260737\n",
      "Epoch 1217, Loss: 0.5147889637\n",
      "Epoch 1218, Loss: 0.4044165975\n",
      "Epoch 1219, Loss: 0.3633303215\n",
      "Epoch 1220, Loss: 0.3785910752\n",
      "Epoch 1221, Loss: 0.3539587337\n",
      "Epoch 1222, Loss: 0.4404944695\n",
      "Epoch 1223, Loss: 0.4747865329\n",
      "Epoch 1224, Loss: 0.4887438348\n",
      "Epoch 1225, Loss: 0.3508724641\n",
      "Epoch 1226, Loss: 0.2590444168\n",
      "Epoch 1227, Loss: 0.4318745112\n",
      "Epoch 1228, Loss: 0.3531184834\n",
      "Epoch 1229, Loss: 0.3376443967\n",
      "Epoch 1230, Loss: 0.3321806503\n",
      "Epoch 1231, Loss: 0.3649035899\n",
      "Epoch 1232, Loss: 0.3927999426\n",
      "Epoch 1233, Loss: 0.2748796902\n",
      "Epoch 1234, Loss: 0.3859557502\n",
      "Epoch 1235, Loss: 0.3081545705\n",
      "Epoch 1236, Loss: 0.3269500667\n",
      "Epoch 1237, Loss: 0.3314034118\n",
      "Epoch 1238, Loss: 0.3840341967\n",
      "Epoch 1239, Loss: 0.4025099319\n",
      "Epoch 1240, Loss: 0.4135805702\n",
      "Epoch 1241, Loss: 0.4048321588\n",
      "Epoch 1242, Loss: 0.3867705612\n",
      "Epoch 1243, Loss: 0.3691374796\n",
      "Epoch 1244, Loss: 0.3087836798\n",
      "Epoch 1245, Loss: 0.3152608371\n",
      "Epoch 1246, Loss: 0.4206240948\n",
      "Epoch 1247, Loss: 0.4378099680\n",
      "Epoch 1248, Loss: 0.6446511235\n",
      "Epoch 1249, Loss: 0.2690308114\n",
      "Epoch 1250, Loss: 0.3942325182\n",
      "Epoch 1251, Loss: 0.4663336153\n",
      "Epoch 1252, Loss: 0.3915283920\n",
      "Epoch 1253, Loss: 0.3062947915\n",
      "Epoch 1254, Loss: 0.3131850598\n",
      "Epoch 1255, Loss: 0.3993909406\n",
      "Epoch 1256, Loss: 0.5603409451\n",
      "Epoch 1257, Loss: 0.5163362055\n",
      "Epoch 1258, Loss: 0.4827996206\n",
      "Epoch 1259, Loss: 0.2785246591\n",
      "Epoch 1260, Loss: 0.3522290619\n",
      "Epoch 1261, Loss: 0.4059043282\n",
      "Epoch 1262, Loss: 0.4097025365\n",
      "Epoch 1263, Loss: 0.3145964606\n",
      "Epoch 1264, Loss: 0.3812535247\n",
      "Epoch 1265, Loss: 0.3039862654\n",
      "Epoch 1266, Loss: 0.3771375389\n",
      "Epoch 1267, Loss: 0.2930102509\n",
      "Epoch 1268, Loss: 0.3415569284\n",
      "Epoch 1269, Loss: 0.3063040662\n",
      "Epoch 1270, Loss: 0.4620035263\n",
      "Epoch 1271, Loss: 0.3317149504\n",
      "Epoch 1272, Loss: 0.3899764228\n",
      "Epoch 1273, Loss: 0.4814388066\n",
      "Epoch 1274, Loss: 0.4971423532\n",
      "Epoch 1275, Loss: 0.8713674207\n",
      "Epoch 1276, Loss: 0.3615222467\n",
      "Epoch 1277, Loss: 0.3269361904\n",
      "Epoch 1278, Loss: 0.4589941504\n",
      "Epoch 1279, Loss: 0.3367536041\n",
      "Epoch 1280, Loss: 0.4275455318\n",
      "Epoch 1281, Loss: 0.3970760381\n",
      "Epoch 1282, Loss: 0.3735025191\n",
      "Epoch 1283, Loss: 0.5867960530\n",
      "Epoch 1284, Loss: 0.5113856959\n",
      "Epoch 1285, Loss: 0.5415756132\n",
      "Epoch 1286, Loss: 0.5741593974\n",
      "Epoch 1287, Loss: 0.5660332367\n",
      "Epoch 1288, Loss: 0.5845667348\n",
      "Epoch 1289, Loss: 0.4177981149\n",
      "Epoch 1290, Loss: 0.4036827848\n",
      "Epoch 1291, Loss: 0.3736553219\n",
      "Epoch 1292, Loss: 0.4909151235\n",
      "Epoch 1293, Loss: 0.3320481372\n",
      "Epoch 1294, Loss: 0.4355975274\n",
      "Epoch 1295, Loss: 0.3909359973\n",
      "Epoch 1296, Loss: 0.3371134744\n",
      "Epoch 1297, Loss: 0.4071758759\n",
      "Epoch 1298, Loss: 0.4487789658\n",
      "Epoch 1299, Loss: 0.4559491978\n",
      "Epoch 1300, Loss: 0.3631688234\n",
      "Epoch 1301, Loss: 0.4767661748\n",
      "Epoch 1302, Loss: 0.3765878383\n",
      "Epoch 1303, Loss: 0.6369507624\n",
      "Epoch 1304, Loss: 0.3934266183\n",
      "Epoch 1305, Loss: 0.4668984973\n",
      "Epoch 1306, Loss: 0.4422178984\n",
      "Epoch 1307, Loss: 0.4208581007\n",
      "Epoch 1308, Loss: 0.3663525220\n",
      "Epoch 1309, Loss: 0.4535916926\n",
      "Epoch 1310, Loss: 0.5063225637\n",
      "Epoch 1311, Loss: 0.4167760084\n",
      "Epoch 1312, Loss: 0.3750730400\n",
      "Epoch 1313, Loss: 0.4308291128\n",
      "Epoch 1314, Loss: 0.5113878928\n",
      "Epoch 1315, Loss: 0.5162501076\n",
      "Epoch 1316, Loss: 0.5371590013\n",
      "Epoch 1317, Loss: 0.3943681273\n",
      "Epoch 1318, Loss: 0.5265037413\n",
      "Epoch 1319, Loss: 0.5424695451\n",
      "Epoch 1320, Loss: 0.3263291117\n",
      "Epoch 1321, Loss: 0.7145780276\n",
      "Epoch 1322, Loss: 0.6407788800\n",
      "Epoch 1323, Loss: 0.4584104784\n",
      "Epoch 1324, Loss: 0.4918861721\n",
      "Epoch 1325, Loss: 0.4875395248\n",
      "Epoch 1326, Loss: 0.4774158839\n",
      "Epoch 1327, Loss: 0.4743732267\n",
      "Epoch 1328, Loss: 0.5527535783\n",
      "Epoch 1329, Loss: 0.5470539762\n",
      "Epoch 1330, Loss: 0.6065817630\n",
      "Epoch 1331, Loss: 0.5290474108\n",
      "Epoch 1332, Loss: 0.6448093167\n",
      "Epoch 1333, Loss: 0.8405100250\n",
      "Epoch 1334, Loss: 0.8402084202\n",
      "Epoch 1335, Loss: 0.6827109066\n",
      "Epoch 1336, Loss: 0.8083266751\n",
      "Epoch 1337, Loss: 0.4446874553\n",
      "Epoch 1338, Loss: 0.7579874088\n",
      "Epoch 1339, Loss: 0.4857561808\n",
      "Epoch 1340, Loss: 0.4221582370\n",
      "Epoch 1341, Loss: 0.7827365840\n",
      "Epoch 1342, Loss: 0.3889901828\n",
      "Epoch 1343, Loss: 0.5569542220\n",
      "Epoch 1344, Loss: 0.3318099159\n",
      "Epoch 1345, Loss: 0.5067606627\n",
      "Epoch 1346, Loss: 0.3057588666\n",
      "Epoch 1347, Loss: 0.3661505155\n",
      "Epoch 1348, Loss: 0.3297010313\n",
      "Epoch 1349, Loss: 0.4146891249\n",
      "Epoch 1350, Loss: 0.5765927441\n",
      "Epoch 1351, Loss: 0.5127609591\n",
      "Epoch 1352, Loss: 0.3968929421\n",
      "Epoch 1353, Loss: 0.5322417747\n",
      "Epoch 1354, Loss: 0.3115772530\n",
      "Epoch 1355, Loss: 0.2962504850\n",
      "Epoch 1356, Loss: 0.4779759126\n",
      "Epoch 1357, Loss: 0.4897273954\n",
      "Epoch 1358, Loss: 0.4559346907\n",
      "Epoch 1359, Loss: 0.4100152716\n",
      "Epoch 1360, Loss: 0.3911632511\n",
      "Epoch 1361, Loss: 0.3837477829\n",
      "Epoch 1362, Loss: 0.4761763284\n",
      "Epoch 1363, Loss: 0.6842359134\n",
      "Epoch 1364, Loss: 0.5049180267\n",
      "Epoch 1365, Loss: 0.7607479105\n",
      "Epoch 1366, Loss: 0.4797502113\n",
      "Epoch 1367, Loss: 0.3652131246\n",
      "Epoch 1368, Loss: 0.4644570741\n",
      "Epoch 1369, Loss: 0.4537395600\n",
      "Epoch 1370, Loss: 0.5136297388\n",
      "Epoch 1371, Loss: 0.5371272212\n",
      "Epoch 1372, Loss: 0.7802749154\n",
      "Epoch 1373, Loss: 0.5935298155\n",
      "Epoch 1374, Loss: 0.6365359453\n",
      "Epoch 1375, Loss: 0.5693817250\n",
      "Epoch 1376, Loss: 0.8214087270\n",
      "Epoch 1377, Loss: 0.7049395929\n",
      "Epoch 1378, Loss: 0.4878990303\n",
      "Epoch 1379, Loss: 0.6108683321\n",
      "Epoch 1380, Loss: 0.4768111473\n",
      "Epoch 1381, Loss: 0.5715077108\n",
      "Epoch 1382, Loss: 0.4913293560\n",
      "Epoch 1383, Loss: 0.5437328689\n",
      "Epoch 1384, Loss: 0.4998397274\n",
      "Epoch 1385, Loss: 0.5339404046\n",
      "Epoch 1386, Loss: 0.4356928264\n",
      "Epoch 1387, Loss: 0.5361759617\n",
      "Epoch 1388, Loss: 0.6182399504\n",
      "Epoch 1389, Loss: 0.6493768600\n",
      "Epoch 1390, Loss: 0.5629840808\n",
      "Epoch 1391, Loss: 0.4704803864\n",
      "Epoch 1392, Loss: 0.4885424718\n",
      "Epoch 1393, Loss: 0.4461577928\n",
      "Epoch 1394, Loss: 0.5132327102\n",
      "Epoch 1395, Loss: 0.3745349339\n",
      "Epoch 1396, Loss: 0.3244977233\n",
      "Epoch 1397, Loss: 0.3396247554\n",
      "Epoch 1398, Loss: 0.3612471864\n",
      "Epoch 1399, Loss: 0.4899896788\n",
      "Epoch 1400, Loss: 0.5222608259\n",
      "Epoch 1401, Loss: 0.5218536562\n",
      "Epoch 1402, Loss: 0.3574074913\n",
      "Epoch 1403, Loss: 0.3865706960\n",
      "Epoch 1404, Loss: 0.4143543618\n",
      "Epoch 1405, Loss: 0.3868186174\n",
      "Epoch 1406, Loss: 0.3807389121\n",
      "Epoch 1407, Loss: 0.4009755975\n",
      "Epoch 1408, Loss: 0.4133564223\n",
      "Epoch 1409, Loss: 0.3467796151\n",
      "Epoch 1410, Loss: 0.3473340313\n",
      "Epoch 1411, Loss: 0.3361015491\n",
      "Epoch 1412, Loss: 0.4061195203\n",
      "Epoch 1413, Loss: 0.3491457664\n",
      "Epoch 1414, Loss: 0.3963539321\n",
      "Epoch 1415, Loss: 0.4123615118\n",
      "Epoch 1416, Loss: 0.4016210735\n",
      "Epoch 1417, Loss: 0.4569317282\n",
      "Epoch 1418, Loss: 0.4830561302\n",
      "Epoch 1419, Loss: 0.4997750570\n",
      "Epoch 1420, Loss: 0.5608050140\n",
      "Epoch 1421, Loss: 0.6872755452\n",
      "Epoch 1422, Loss: 0.7354334267\n",
      "Epoch 1423, Loss: 0.4109398963\n",
      "Epoch 1424, Loss: 0.5856613448\n",
      "Epoch 1425, Loss: 0.4135263985\n",
      "Epoch 1426, Loss: 0.4716063809\n",
      "Epoch 1427, Loss: 0.4173367404\n",
      "Epoch 1428, Loss: 0.4669137668\n",
      "Epoch 1429, Loss: 0.5549338446\n",
      "Epoch 1430, Loss: 0.4485576639\n",
      "Epoch 1431, Loss: 0.4691246193\n",
      "Epoch 1432, Loss: 0.2882130043\n",
      "Epoch 1433, Loss: 0.4489980996\n",
      "Epoch 1434, Loss: 0.3406856080\n",
      "Epoch 1435, Loss: 0.5331113077\n",
      "Epoch 1436, Loss: 0.4198777449\n",
      "Epoch 1437, Loss: 0.5993052872\n",
      "Epoch 1438, Loss: 0.3487003935\n",
      "Epoch 1439, Loss: 0.3469912226\n",
      "Epoch 1440, Loss: 0.4212432810\n",
      "Epoch 1441, Loss: 0.4528069847\n",
      "Epoch 1442, Loss: 0.5406928573\n",
      "Epoch 1443, Loss: 0.4655049276\n",
      "Epoch 1444, Loss: 0.4381961881\n",
      "Epoch 1445, Loss: 0.4709220182\n",
      "Epoch 1446, Loss: 0.4296004646\n",
      "Epoch 1447, Loss: 0.4945151663\n",
      "Epoch 1448, Loss: 0.3702232370\n",
      "Epoch 1449, Loss: 0.6249404624\n",
      "Epoch 1450, Loss: 0.4942551729\n",
      "Epoch 1451, Loss: 0.4697550120\n",
      "Epoch 1452, Loss: 0.3202997689\n",
      "Epoch 1453, Loss: 0.4775377165\n",
      "Epoch 1454, Loss: 0.4426049973\n",
      "Epoch 1455, Loss: 0.4806380342\n",
      "Epoch 1456, Loss: 0.3768471489\n",
      "Epoch 1457, Loss: 0.4376662747\n",
      "Epoch 1458, Loss: 0.3927613234\n",
      "Epoch 1459, Loss: 0.3279567566\n",
      "Epoch 1460, Loss: 0.3454087096\n",
      "Epoch 1461, Loss: 0.5017677561\n",
      "Epoch 1462, Loss: 0.3105120722\n",
      "Epoch 1463, Loss: 0.3040326512\n",
      "Epoch 1464, Loss: 0.3484414744\n",
      "Epoch 1465, Loss: 0.3416759493\n",
      "Epoch 1466, Loss: 0.3717365490\n",
      "Epoch 1467, Loss: 0.3180579624\n",
      "Epoch 1468, Loss: 0.4037218116\n",
      "Epoch 1469, Loss: 0.3017902786\n",
      "Epoch 1470, Loss: 0.3304898688\n",
      "Epoch 1471, Loss: 0.3715021911\n",
      "Epoch 1472, Loss: 0.3617450122\n",
      "Epoch 1473, Loss: 0.3770320942\n",
      "Epoch 1474, Loss: 0.3008695010\n",
      "Epoch 1475, Loss: 0.3554891765\n",
      "Epoch 1476, Loss: 0.2993484280\n",
      "Epoch 1477, Loss: 0.3168234878\n",
      "Epoch 1478, Loss: 0.4188375409\n",
      "Epoch 1479, Loss: 0.4195231784\n",
      "Epoch 1480, Loss: 0.4362882148\n",
      "Epoch 1481, Loss: 0.3539372320\n",
      "Epoch 1482, Loss: 0.3964751030\n",
      "Epoch 1483, Loss: 0.3417721942\n",
      "Epoch 1484, Loss: 0.3054682896\n",
      "Epoch 1485, Loss: 0.2870872201\n",
      "Epoch 1486, Loss: 0.3504700641\n",
      "Epoch 1487, Loss: 0.3618735733\n",
      "Epoch 1488, Loss: 0.3024730528\n",
      "Epoch 1489, Loss: 0.2830086066\n",
      "Epoch 1490, Loss: 0.3671523837\n",
      "Epoch 1491, Loss: 0.3129423880\n",
      "Epoch 1492, Loss: 0.4133198591\n",
      "Epoch 1493, Loss: 0.3992822324\n",
      "Epoch 1494, Loss: 0.4084572680\n",
      "Epoch 1495, Loss: 0.4207026223\n",
      "Epoch 1496, Loss: 0.4777400224\n",
      "Epoch 1497, Loss: 0.6195384231\n",
      "Epoch 1498, Loss: 0.4631161456\n",
      "Epoch 1499, Loss: 0.3226747678\n",
      "Epoch 1500, Loss: 0.3087641656\n",
      "Epoch 1501, Loss: 0.3366401599\n",
      "Epoch 1502, Loss: 0.3318296663\n",
      "Epoch 1503, Loss: 0.3842637973\n",
      "Epoch 1504, Loss: 0.3681636518\n",
      "Epoch 1505, Loss: 0.3471568891\n",
      "Epoch 1506, Loss: 0.4495200249\n",
      "Epoch 1507, Loss: 0.5345413524\n",
      "Epoch 1508, Loss: 0.6860112920\n",
      "Epoch 1509, Loss: 0.3002167149\n",
      "Epoch 1510, Loss: 0.4959934207\n",
      "Epoch 1511, Loss: 0.3790418005\n",
      "Epoch 1512, Loss: 0.3290859770\n",
      "Epoch 1513, Loss: 0.4549913993\n",
      "Epoch 1514, Loss: 0.4384113195\n",
      "Epoch 1515, Loss: 0.4157311420\n",
      "Epoch 1516, Loss: 0.3598231936\n",
      "Epoch 1517, Loss: 0.3878388200\n",
      "Epoch 1518, Loss: 0.3544895052\n",
      "Epoch 1519, Loss: 0.3847173215\n",
      "Epoch 1520, Loss: 0.4093315200\n",
      "Epoch 1521, Loss: 0.5010749746\n",
      "Epoch 1522, Loss: 0.4998697160\n",
      "Epoch 1523, Loss: 0.4837639601\n",
      "Epoch 1524, Loss: 0.4383152410\n",
      "Epoch 1525, Loss: 0.4149308108\n",
      "Epoch 1526, Loss: 0.5783270693\n",
      "Epoch 1527, Loss: 0.2906798123\n",
      "Epoch 1528, Loss: 0.4249330407\n",
      "Epoch 1529, Loss: 0.4654992167\n",
      "Epoch 1530, Loss: 0.2870018331\n",
      "Epoch 1531, Loss: 0.4635062148\n",
      "Epoch 1532, Loss: 0.5120484352\n",
      "Epoch 1533, Loss: 0.5638748661\n",
      "Epoch 1534, Loss: 0.5002525231\n",
      "Epoch 1535, Loss: 0.4124633640\n",
      "Epoch 1536, Loss: 0.2419495423\n",
      "Epoch 1537, Loss: 0.3668384358\n",
      "Epoch 1538, Loss: 0.5488919638\n",
      "Epoch 1539, Loss: 0.3930689780\n",
      "Epoch 1540, Loss: 0.2847648519\n",
      "Epoch 1541, Loss: 0.4100023712\n",
      "Epoch 1542, Loss: 0.5208133550\n",
      "Epoch 1543, Loss: 0.5340360636\n",
      "Epoch 1544, Loss: 0.9413973327\n",
      "Epoch 1545, Loss: 0.7397886185\n",
      "Epoch 1546, Loss: 0.5250595966\n",
      "Epoch 1547, Loss: 0.5962512985\n",
      "Epoch 1548, Loss: 0.3749700014\n",
      "Epoch 1549, Loss: 0.4803779868\n",
      "Epoch 1550, Loss: 0.6148664750\n",
      "Epoch 1551, Loss: 0.4087128906\n",
      "Epoch 1552, Loss: 0.5150474072\n",
      "Epoch 1553, Loss: 0.2772041536\n",
      "Epoch 1554, Loss: 0.3933012131\n",
      "Epoch 1555, Loss: 0.3285285325\n",
      "Epoch 1556, Loss: 0.5194973453\n",
      "Epoch 1557, Loss: 0.3433982387\n",
      "Epoch 1558, Loss: 0.3806864343\n",
      "Epoch 1559, Loss: 0.4749855009\n",
      "Epoch 1560, Loss: 0.5646893001\n",
      "Epoch 1561, Loss: 0.5707956301\n",
      "Epoch 1562, Loss: 0.7408088233\n",
      "Epoch 1563, Loss: 0.3145324790\n",
      "Epoch 1564, Loss: 0.4933301973\n",
      "Epoch 1565, Loss: 0.4102887520\n",
      "Epoch 1566, Loss: 0.5324753501\n",
      "Epoch 1567, Loss: 0.3397573532\n",
      "Epoch 1568, Loss: 0.6458847692\n",
      "Epoch 1569, Loss: 0.2868118878\n",
      "Epoch 1570, Loss: 0.3838970393\n",
      "Epoch 1571, Loss: 0.3607459255\n",
      "Epoch 1572, Loss: 0.3694285580\n",
      "Epoch 1573, Loss: 0.3465306647\n",
      "Epoch 1574, Loss: 0.3726789861\n",
      "Epoch 1575, Loss: 0.3810123210\n",
      "Epoch 1576, Loss: 0.3489804768\n",
      "Epoch 1577, Loss: 0.4799572660\n",
      "Epoch 1578, Loss: 0.4988899486\n",
      "Epoch 1579, Loss: 0.3889076144\n",
      "Epoch 1580, Loss: 0.3329744268\n",
      "Epoch 1581, Loss: 0.5975353631\n",
      "Epoch 1582, Loss: 0.3940245649\n",
      "Epoch 1583, Loss: 0.6919865555\n",
      "Epoch 1584, Loss: 0.3459057437\n",
      "Epoch 1585, Loss: 0.4244800190\n",
      "Epoch 1586, Loss: 0.3817906059\n",
      "Epoch 1587, Loss: 0.2985772469\n",
      "Epoch 1588, Loss: 0.3472417561\n",
      "Epoch 1589, Loss: 0.2872714724\n",
      "Epoch 1590, Loss: 0.3143789010\n",
      "Epoch 1591, Loss: 0.3111531226\n",
      "Epoch 1592, Loss: 0.4836789794\n",
      "Epoch 1593, Loss: 0.5236224931\n",
      "Epoch 1594, Loss: 0.5725059545\n",
      "Epoch 1595, Loss: 0.5209446558\n",
      "Epoch 1596, Loss: 0.5230199542\n",
      "Epoch 1597, Loss: 0.6093966187\n",
      "Epoch 1598, Loss: 0.8042762443\n",
      "Epoch 1599, Loss: 0.5441617011\n",
      "Epoch 1600, Loss: 0.4384199961\n",
      "Epoch 1601, Loss: 0.4196128788\n",
      "Epoch 1602, Loss: 0.4003879393\n",
      "Epoch 1603, Loss: 0.3253549120\n",
      "Epoch 1604, Loss: 0.3857759636\n",
      "Epoch 1605, Loss: 0.3603536078\n",
      "Epoch 1606, Loss: 0.5476407343\n",
      "Epoch 1607, Loss: 0.6146815924\n",
      "Epoch 1608, Loss: 0.4621623759\n",
      "Epoch 1609, Loss: 0.4040349300\n",
      "Epoch 1610, Loss: 0.5309999334\n",
      "Epoch 1611, Loss: 0.4496852649\n",
      "Epoch 1612, Loss: 0.4548659009\n",
      "Epoch 1613, Loss: 0.5482550751\n",
      "Epoch 1614, Loss: 0.3767768822\n",
      "Epoch 1615, Loss: 0.4062150632\n",
      "Epoch 1616, Loss: 0.5155046877\n",
      "Epoch 1617, Loss: 0.4441975616\n",
      "Epoch 1618, Loss: 0.4332835982\n",
      "Epoch 1619, Loss: 0.5070273258\n",
      "Epoch 1620, Loss: 0.4120455613\n",
      "Epoch 1621, Loss: 0.3690472250\n",
      "Epoch 1622, Loss: 0.3269760627\n",
      "Epoch 1623, Loss: 0.4080062236\n",
      "Epoch 1624, Loss: 0.3498630990\n",
      "Epoch 1625, Loss: 0.3836893661\n",
      "Epoch 1626, Loss: 0.3566879045\n",
      "Epoch 1627, Loss: 0.3397015757\n",
      "Epoch 1628, Loss: 0.3648992164\n",
      "Epoch 1629, Loss: 0.3761741051\n",
      "Epoch 1630, Loss: 0.3604628895\n",
      "Epoch 1631, Loss: 0.4190449841\n",
      "Epoch 1632, Loss: 0.4066542823\n",
      "Epoch 1633, Loss: 0.4972917989\n",
      "Epoch 1634, Loss: 0.5118114442\n",
      "Epoch 1635, Loss: 0.5466587407\n",
      "Epoch 1636, Loss: 0.6500709520\n",
      "Epoch 1637, Loss: 0.8256796709\n",
      "Epoch 1638, Loss: 0.4493542528\n",
      "Epoch 1639, Loss: 0.5917837784\n",
      "Epoch 1640, Loss: 0.2951119047\n",
      "Epoch 1641, Loss: 0.4404911445\n",
      "Epoch 1642, Loss: 0.3883005664\n",
      "Epoch 1643, Loss: 0.3985648100\n",
      "Epoch 1644, Loss: 0.3291105482\n",
      "Epoch 1645, Loss: 0.4174619899\n",
      "Epoch 1646, Loss: 0.4143727506\n",
      "Epoch 1647, Loss: 0.3220622833\n",
      "Epoch 1648, Loss: 0.3458569396\n",
      "Epoch 1649, Loss: 0.3365493355\n",
      "Epoch 1650, Loss: 0.4400726102\n",
      "Epoch 1651, Loss: 0.4946142710\n",
      "Epoch 1652, Loss: 0.5404200647\n",
      "Epoch 1653, Loss: 0.5067679892\n",
      "Epoch 1654, Loss: 0.5134962185\n",
      "Epoch 1655, Loss: 0.3753482437\n",
      "Epoch 1656, Loss: 0.3206072612\n",
      "Epoch 1657, Loss: 0.3858333917\n",
      "Epoch 1658, Loss: 0.3631331397\n",
      "Epoch 1659, Loss: 0.3787323328\n",
      "Epoch 1660, Loss: 0.3850157468\n",
      "Epoch 1661, Loss: 0.5776352693\n",
      "Epoch 1662, Loss: 0.3560003021\n",
      "Epoch 1663, Loss: 0.4805259559\n",
      "Epoch 1664, Loss: 0.3343268591\n",
      "Epoch 1665, Loss: 0.4105022444\n",
      "Epoch 1666, Loss: 0.2564519533\n",
      "Epoch 1667, Loss: 0.4516914849\n",
      "Epoch 1668, Loss: 0.4887549170\n",
      "Epoch 1669, Loss: 0.5759946235\n",
      "Epoch 1670, Loss: 0.5281880247\n",
      "Epoch 1671, Loss: 0.4278983545\n",
      "Epoch 1672, Loss: 0.3919101528\n",
      "Epoch 1673, Loss: 0.3588084900\n",
      "Epoch 1674, Loss: 0.4032774992\n",
      "Epoch 1675, Loss: 0.3492486527\n",
      "Epoch 1676, Loss: 0.4364806538\n",
      "Epoch 1677, Loss: 0.3962892572\n",
      "Epoch 1678, Loss: 0.3767526273\n",
      "Epoch 1679, Loss: 0.4716490411\n",
      "Epoch 1680, Loss: 0.4343893899\n",
      "Epoch 1681, Loss: 0.4155774072\n",
      "Epoch 1682, Loss: 0.4456418501\n",
      "Epoch 1683, Loss: 0.4977033556\n",
      "Epoch 1684, Loss: 0.3859485356\n",
      "Epoch 1685, Loss: 0.3893413318\n",
      "Epoch 1686, Loss: 0.4657344319\n",
      "Epoch 1687, Loss: 0.4354405668\n",
      "Epoch 1688, Loss: 0.4315125950\n",
      "Epoch 1689, Loss: 0.6667274761\n",
      "Epoch 1690, Loss: 0.4595375102\n",
      "Epoch 1691, Loss: 0.4471697501\n",
      "Epoch 1692, Loss: 0.2786349590\n",
      "Epoch 1693, Loss: 0.4145496652\n",
      "Epoch 1694, Loss: 0.5878472375\n",
      "Epoch 1695, Loss: 0.6239310898\n",
      "Epoch 1696, Loss: 0.4078628682\n",
      "Epoch 1697, Loss: 0.3219979855\n",
      "Epoch 1698, Loss: 0.3364504005\n",
      "Epoch 1699, Loss: 0.3106510847\n",
      "Epoch 1700, Loss: 0.6652332543\n",
      "Epoch 1701, Loss: 0.4465187179\n",
      "Epoch 1702, Loss: 0.5639731529\n",
      "Epoch 1703, Loss: 0.5336491351\n",
      "Epoch 1704, Loss: 0.4161671713\n",
      "Epoch 1705, Loss: 0.3492167098\n",
      "Epoch 1706, Loss: 0.3638633027\n",
      "Epoch 1707, Loss: 0.4927914585\n",
      "Epoch 1708, Loss: 0.4596352832\n",
      "Epoch 1709, Loss: 0.4517223238\n",
      "Epoch 1710, Loss: 0.4514219702\n",
      "Epoch 1711, Loss: 0.3734121809\n",
      "Epoch 1712, Loss: 0.3989733210\n",
      "Epoch 1713, Loss: 0.4704536875\n",
      "Epoch 1714, Loss: 0.4633702581\n",
      "Epoch 1715, Loss: 0.4488840479\n",
      "Epoch 1716, Loss: 0.4227601793\n",
      "Epoch 1717, Loss: 0.4306777065\n",
      "Epoch 1718, Loss: 0.3376825291\n",
      "Epoch 1719, Loss: 0.2799799184\n",
      "Epoch 1720, Loss: 0.3427081264\n",
      "Epoch 1721, Loss: 0.2356305092\n",
      "Epoch 1722, Loss: 0.2316494998\n",
      "Epoch 1723, Loss: 0.2560236882\n",
      "Epoch 1724, Loss: 0.5563118490\n",
      "Epoch 1725, Loss: 0.4450585435\n",
      "Epoch 1726, Loss: 0.5332939605\n",
      "Epoch 1727, Loss: 0.4148911577\n",
      "Epoch 1728, Loss: 0.3835807501\n",
      "Epoch 1729, Loss: 0.2741601518\n",
      "Epoch 1730, Loss: 0.4219496010\n",
      "Epoch 1731, Loss: 0.4192905357\n",
      "Epoch 1732, Loss: 0.2684113274\n",
      "Epoch 1733, Loss: 0.3345426381\n",
      "Epoch 1734, Loss: 0.3567987178\n",
      "Epoch 1735, Loss: 0.4378593215\n",
      "Epoch 1736, Loss: 0.4855857001\n",
      "Epoch 1737, Loss: 0.4809877250\n",
      "Epoch 1738, Loss: 0.2705619656\n",
      "Epoch 1739, Loss: 0.2506829804\n",
      "Epoch 1740, Loss: 0.2063429248\n",
      "Epoch 1741, Loss: 0.2283215373\n",
      "Epoch 1742, Loss: 0.2756906799\n",
      "Epoch 1743, Loss: 0.3282846301\n",
      "Epoch 1744, Loss: 0.4166890575\n",
      "Epoch 1745, Loss: 0.3452945803\n",
      "Epoch 1746, Loss: 0.2941519011\n",
      "Epoch 1747, Loss: 0.2454031905\n",
      "Epoch 1748, Loss: 0.2307731643\n",
      "Epoch 1749, Loss: 0.2414706343\n",
      "Epoch 1750, Loss: 0.2638066298\n",
      "Epoch 1751, Loss: 0.3442203663\n",
      "Epoch 1752, Loss: 0.3590695019\n",
      "Epoch 1753, Loss: 0.3060672841\n",
      "Epoch 1754, Loss: 0.4620316173\n",
      "Epoch 1755, Loss: 0.2157622136\n",
      "Epoch 1756, Loss: 0.2388390965\n",
      "Epoch 1757, Loss: 0.1921079521\n",
      "Epoch 1758, Loss: 0.2400662363\n",
      "Epoch 1759, Loss: 0.2220985429\n",
      "Epoch 1760, Loss: 0.2292821979\n",
      "Epoch 1761, Loss: 0.2631339978\n",
      "Epoch 1762, Loss: 0.2742025063\n",
      "Epoch 1763, Loss: 0.2692536599\n",
      "Epoch 1764, Loss: 0.2180870134\n",
      "Epoch 1765, Loss: 0.2340724807\n",
      "Epoch 1766, Loss: 0.2920300142\n",
      "Epoch 1767, Loss: 0.4423788819\n",
      "Epoch 1768, Loss: 0.2648271909\n",
      "Epoch 1769, Loss: 0.2346851905\n",
      "Epoch 1770, Loss: 0.2535764519\n",
      "Epoch 1771, Loss: 0.2349530378\n",
      "Epoch 1772, Loss: 0.2934955139\n",
      "Epoch 1773, Loss: 0.3689482297\n",
      "Epoch 1774, Loss: 0.3173541070\n",
      "Epoch 1775, Loss: 0.3797311101\n",
      "Epoch 1776, Loss: 0.3318140933\n",
      "Epoch 1777, Loss: 0.2270804243\n",
      "Epoch 1778, Loss: 0.4013127310\n",
      "Epoch 1779, Loss: 0.2881216085\n",
      "Epoch 1780, Loss: 0.1906984832\n",
      "Epoch 1781, Loss: 0.1926170331\n",
      "Epoch 1782, Loss: 0.3075623237\n",
      "Epoch 1783, Loss: 0.2997531808\n",
      "Epoch 1784, Loss: 0.3017327248\n",
      "Epoch 1785, Loss: 0.4673395262\n",
      "Epoch 1786, Loss: 0.3837462848\n",
      "Epoch 1787, Loss: 0.4187482685\n",
      "Epoch 1788, Loss: 0.3799051399\n",
      "Epoch 1789, Loss: 0.3052508540\n",
      "Epoch 1790, Loss: 0.2853426115\n",
      "Epoch 1791, Loss: 0.4897496647\n",
      "Epoch 1792, Loss: 0.5821025393\n",
      "Epoch 1793, Loss: 0.3878219488\n",
      "Epoch 1794, Loss: 0.2137160116\n",
      "Epoch 1795, Loss: 0.2832419582\n",
      "Epoch 1796, Loss: 0.2607798080\n",
      "Epoch 1797, Loss: 0.3364517064\n",
      "Epoch 1798, Loss: 0.2079291872\n",
      "Epoch 1799, Loss: 0.2156428868\n",
      "Epoch 1800, Loss: 0.3253841233\n",
      "Epoch 1801, Loss: 0.3633677401\n",
      "Epoch 1802, Loss: 0.5786232152\n",
      "Epoch 1803, Loss: 0.4384846338\n",
      "Epoch 1804, Loss: 0.3090052804\n",
      "Epoch 1805, Loss: 0.2538905901\n",
      "Epoch 1806, Loss: 0.3182842222\n",
      "Epoch 1807, Loss: 0.5150549497\n",
      "Epoch 1808, Loss: 0.4938261536\n",
      "Epoch 1809, Loss: 0.3827445966\n",
      "Epoch 1810, Loss: 0.3655732412\n",
      "Epoch 1811, Loss: 0.3362544978\n",
      "Epoch 1812, Loss: 0.4858908988\n",
      "Epoch 1813, Loss: 0.4793188255\n",
      "Epoch 1814, Loss: 0.7006128104\n",
      "Epoch 1815, Loss: 0.5563411981\n",
      "Epoch 1816, Loss: 0.4255467313\n",
      "Epoch 1817, Loss: 0.3586938297\n",
      "Epoch 1818, Loss: 0.4288646161\n",
      "Epoch 1819, Loss: 0.3408307437\n",
      "Epoch 1820, Loss: 0.3992143962\n",
      "Epoch 1821, Loss: 0.5237184770\n",
      "Epoch 1822, Loss: 0.3277917915\n",
      "Epoch 1823, Loss: 0.4027318789\n",
      "Epoch 1824, Loss: 0.6661080201\n",
      "Epoch 1825, Loss: 0.5164171836\n",
      "Epoch 1826, Loss: 0.4657408118\n",
      "Epoch 1827, Loss: 0.3467570946\n",
      "Epoch 1828, Loss: 0.2634374039\n",
      "Epoch 1829, Loss: 0.2546400462\n",
      "Epoch 1830, Loss: 0.1890722635\n",
      "Epoch 1831, Loss: 0.3025128950\n",
      "Epoch 1832, Loss: 0.5294391192\n",
      "Epoch 1833, Loss: 0.4659767474\n",
      "Epoch 1834, Loss: 0.3597880727\n",
      "Epoch 1835, Loss: 0.2414557351\n",
      "Epoch 1836, Loss: 0.2349642604\n",
      "Epoch 1837, Loss: 0.2880724581\n",
      "Epoch 1838, Loss: 0.3561720047\n",
      "Epoch 1839, Loss: 0.3050637526\n",
      "Epoch 1840, Loss: 0.2797268693\n",
      "Epoch 1841, Loss: 0.4751060934\n",
      "Epoch 1842, Loss: 0.1870425343\n",
      "Epoch 1843, Loss: 0.3297310977\n",
      "Epoch 1844, Loss: 0.2576921240\n",
      "Epoch 1845, Loss: 0.3814501463\n",
      "Epoch 1846, Loss: 0.2453488584\n",
      "Epoch 1847, Loss: 0.5556842526\n",
      "Epoch 1848, Loss: 0.4179232120\n",
      "Epoch 1849, Loss: 0.3025307951\n",
      "Epoch 1850, Loss: 0.1892958484\n",
      "Epoch 1851, Loss: 0.2444111017\n",
      "Epoch 1852, Loss: 0.2534192335\n",
      "Epoch 1853, Loss: 0.4623633323\n",
      "Epoch 1854, Loss: 0.4376451491\n",
      "Epoch 1855, Loss: 0.4480209586\n",
      "Epoch 1856, Loss: 0.2864737171\n",
      "Epoch 1857, Loss: 0.3642771644\n",
      "Epoch 1858, Loss: 0.1966557784\n",
      "Epoch 1859, Loss: 0.3494926246\n",
      "Epoch 1860, Loss: 0.2771129659\n",
      "Epoch 1861, Loss: 0.3075062009\n",
      "Epoch 1862, Loss: 0.3806124524\n",
      "Epoch 1863, Loss: 0.3995323790\n",
      "Epoch 1864, Loss: 0.3531935764\n",
      "Epoch 1865, Loss: 0.4156083337\n",
      "Epoch 1866, Loss: 0.5815858346\n",
      "Epoch 1867, Loss: 0.5000439159\n",
      "Epoch 1868, Loss: 0.4707916351\n",
      "Epoch 1869, Loss: 0.4420433817\n",
      "Epoch 1870, Loss: 0.3772289575\n",
      "Epoch 1871, Loss: 0.3441986684\n",
      "Epoch 1872, Loss: 0.3008049444\n",
      "Epoch 1873, Loss: 0.2576661738\n",
      "Epoch 1874, Loss: 0.2786088515\n",
      "Epoch 1875, Loss: 0.3135270664\n",
      "Epoch 1876, Loss: 0.2049036048\n",
      "Epoch 1877, Loss: 0.2062447098\n",
      "Epoch 1878, Loss: 0.2770920584\n",
      "Epoch 1879, Loss: 0.2881925469\n",
      "Epoch 1880, Loss: 0.2676530310\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_weights_init = []\n",
    "best_biases_init = []\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "best_seed = 0\n",
    "\n",
    "for _ in range (1):\n",
    "    nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8, beta1=0.95, beta2=0.999)\n",
    "    nn.train(batches, 15, 0.0015)\n",
    "    if nn.get_best_loss() < best_loss:\n",
    "        best_loss = nn.get_best_loss()\n",
    "        best_weights = nn.get_best_weights()\n",
    "        best_biases = nn.get_best_biases()\n",
    "        best_seed = nn.get_best_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18704253429702888\n",
      "1726936013\n",
      "0.18704253429702888\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "print(best_seed)\n",
    "print(nn.get_best_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Number of layers in the Neural Network\n",
    "N = 5  # Example value, replace with the actual number of layers\n",
    "\n",
    "# Initialize the dictionary\n",
    "weights_dict = {\n",
    "    'weights': {},\n",
    "    'bias': {}\n",
    "}\n",
    "\n",
    "weights = best_weights\n",
    "biases = best_biases\n",
    "\n",
    "# Populate the weights and bias dictionaries\n",
    "for i in range(N):\n",
    "    weights_dict['weights'][f'fc{i+1}'] = weights[i]\n",
    "    weights_dict['bias'][f'fc{i+1}'] = biases[i].flatten()\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "N = 5\n",
    "# load the weights from the pickle file\n",
    "with open('weights.pkl', 'rb') as f:\n",
    "    weights_dict = pickle.load(f)\n",
    "\n",
    "# Retrieve the weights and bias from the dictionary\n",
    "weights = []\n",
    "biases = []\n",
    "for i in range(N):\n",
    "    weights.append(weights_dict['weights'][f'fc{i+1}'])\n",
    "    biases.append(weights_dict['bias'][f'fc{i+1}'])\n",
    "\n",
    "# print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(256, 625)\n",
      "13 2\n",
      "(256, 8)\n",
      "(128, 8)\n"
     ]
    }
   ],
   "source": [
    "print(type(batches[0][0]))\n",
    "print(batches[0][0].shape)\n",
    "print(len(batches), len(batches[0]))\n",
    "print(batches[0][1].shape)\n",
    "print(batches[12][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are a total of 13 batches. concatenate them\n",
    "X = np.concatenate([batch[0] for batch in batches])\n",
    "y = np.concatenate([batch[1] for batch in batches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 625)\n",
      "(3200, 8)\n",
      "[0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598.5361097504923\n",
      "0.18704253429702886\n"
     ]
    }
   ],
   "source": [
    "def forward(X):\n",
    "    activations = [X]\n",
    "    pre_activations = []\n",
    "\n",
    "    # Pass through each layer except the output layer\n",
    "    for i in range(len(weights) - 1):\n",
    "        z = np.dot(activations[-1], weights[i]) + biases[i]\n",
    "        pre_activations.append(z)\n",
    "        a = sigmoid(z)  # Sigmoid for hidden layers\n",
    "        activations.append(a)\n",
    "\n",
    "    # Pass through the output layer with softmax\n",
    "    z = np.dot(activations[-1], weights[-1]) + biases[-1]\n",
    "    pre_activations.append(z)\n",
    "    a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "    activations.append(a)\n",
    "\n",
    "    return activations, pre_activations\n",
    "\n",
    "act, pre_act = forward(X)\n",
    "pred = act[-1]\n",
    "loss = cross_entropy_loss(y, pred)\n",
    "print(loss)\n",
    "loss /= len(y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200,)\n",
      "[5 3 1 7 2 2 1 7 4 2]\n"
     ]
    }
   ],
   "source": [
    "# get argmax of the predictions\n",
    "y_pred = np.argmax(pred, axis=1)\n",
    "\n",
    "print(y_pred.shape)\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save y_pred in pickle file\n",
    "with open('predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(y_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
