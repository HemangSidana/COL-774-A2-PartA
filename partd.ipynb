{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Remember to import \"numpy_transforms\" functions if you wish to import these two classes in a different script.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "        label = row[\"class\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image), label\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image, label = self.dataset[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "        batch_labels = np.array(labels)\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(128, 625)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    print(labels.shape)  # Should be [batch_size]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# # ReLU activation and its derivative\n",
    "# def relu(x):\n",
    "#     return np.maximum(0, x)\n",
    "\n",
    "# def relu_derivative(x):\n",
    "#     return np.where(x > 0, 1, 0)\n",
    "\n",
    "# # Mean Squared Error loss\n",
    "# def mean_squared_error(y_true, y_pred):\n",
    "#     return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "# # Neural Network Class with ReLU in the Output Layer and Hidden Layers\n",
    "# class NeuralNetwork_Adam:\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "#         if init_seed is None:\n",
    "#             self.best_seed = int(time.time())\n",
    "#             np.random.seed(self.best_seed)\n",
    "#         else:\n",
    "#             np.random.seed(init_seed)\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "#         self.m_w = []\n",
    "#         self.v_w = []\n",
    "#         self.m_b = []\n",
    "#         self.v_b = []\n",
    "#         self.beta1 = beta1\n",
    "#         self.beta2 = beta2\n",
    "#         self.epsilon = epsilon\n",
    "#         self.t = 0  # Time step for Adam\n",
    "#         self.best_weights = []\n",
    "#         self.best_biases = []\n",
    "#         self.best_loss = float(\"inf\")\n",
    "\n",
    "#         layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "#         # Initialize weights, biases, and Adam parameters (m, v)\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             if (init_weights is not None) and (init_biases is not None):\n",
    "#                 self.weights.append(init_weights[i])\n",
    "#                 self.biases.append(init_biases[i])\n",
    "#             else:\n",
    "#                 self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "#                 self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "#             self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.best_weights = self.weights\n",
    "#             self.best_biases = self.biases\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         pre_activations = []\n",
    "\n",
    "#         # Pass through each layer except the output layer\n",
    "#         for i in range(len(self.weights) - 1):\n",
    "#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             pre_activations.append(z)\n",
    "#             a = relu(z)  # ReLU for hidden layers\n",
    "#             activations.append(a)\n",
    "\n",
    "#         # Pass through the output layer with ReLU\n",
    "#         z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         pre_activations.append(z)\n",
    "#         a = relu(z)  # ReLU for the output layer\n",
    "#         activations.append(a)\n",
    "\n",
    "#         return activations, pre_activations\n",
    "\n",
    "#     def backward(self, X, y, activations, pre_activations):\n",
    "#         grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "#         grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "#         # Start with output layer error\n",
    "#         delta = activations[-1] - y\n",
    "#         delta *= relu_derivative(pre_activations[-1])  # ReLU derivative for the output layer\n",
    "\n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "#             grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "#             if i > 0:\n",
    "#                 delta = np.dot(delta, self.weights[i].T) * relu_derivative(pre_activations[i - 1])\n",
    "\n",
    "#         return grad_w, grad_b\n",
    "\n",
    "#     def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "#         self.t += 1  # Increment time step for Adam\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             # Update biased first moment estimate\n",
    "#             self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "#             self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "#             # Update biased second moment estimate\n",
    "#             self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "#             self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "#             # Compute bias-corrected first moment estimate\n",
    "#             m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "#             m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "#             # Compute bias-corrected second moment estimate\n",
    "#             v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "#             v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "#             # Update weights and biases\n",
    "#             self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "#             self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "#     def train(self, batches, time_of_running, learning_rate):\n",
    "#         start_time = time.time()\n",
    "#         epoch = 0\n",
    "#         while True:\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 activations, pre_activations = self.forward(X_batch)\n",
    "#                 grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "#                 self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "#             # Calculate average loss over batches\n",
    "#             loss = 0\n",
    "#             z = 0\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 y_pred, _ = self.forward(X_batch)\n",
    "#                 loss += mean_squared_error(y_batch, y_pred[-1])\n",
    "#                 z += len(y_pred[-1])\n",
    "#             loss /= z\n",
    "            \n",
    "#             if loss < self.best_loss:\n",
    "#                 self.best_loss = loss\n",
    "#                 self.best_weights = self.weights\n",
    "#                 self.best_biases = self.biases\n",
    "#             print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "#             epoch += 1\n",
    "#             # if time elapsed is greater than 1 minute, break the loop\n",
    "#             if time.time() - start_time > 60 * time_of_running:\n",
    "#                 break\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "#     def get_best_weights(self):\n",
    "#         return self.best_weights\n",
    "\n",
    "#     def get_best_biases(self):\n",
    "#         return self.best_biases\n",
    "\n",
    "#     def get_best_loss(self):\n",
    "#         return self.best_loss\n",
    "\n",
    "#     def get_best_seed(self):\n",
    "#         return self.best_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'val' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset_val = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "# Create the DataLoader\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=len(dataset_val))\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches_val=[]\n",
    "for images,labels in dataloader_val:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches_val.append((images,one_hot_labels))\n",
    "\n",
    "def get_stat():\n",
    "    for X_val, Y_val in batches_val:\n",
    "        Y_pred= nn.predict(X_val)\n",
    "        # print(cross_entropy_loss(Y_val,Y_pred)/len(dataset))\n",
    "        print(accuracy(Y_val, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Leaky ReLU activation and its derivative\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Neural Network Class with Leaky ReLU and Adam Optimizer\n",
    "class NeuralNetwork_Adam:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, alpha=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        if init_seed is None:\n",
    "            self.best_seed = int(time.time())\n",
    "            np.random.seed(self.best_seed)\n",
    "        else:\n",
    "            np.random.seed(init_seed)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.m_w = []\n",
    "        self.v_w = []\n",
    "        self.m_b = []\n",
    "        self.v_b = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step for Adam\n",
    "        self.alpha = alpha  # Leaky ReLU parameter\n",
    "        self.best_weights = []\n",
    "        self.best_biases = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if init_weights is not None and init_biases is not None:\n",
    "                self.weights.append(init_weights[i])\n",
    "                self.biases.append(init_biases[i])\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "                self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "            self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "        self.best_weights = self.weights\n",
    "        self.best_biases = self.biases\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each hidden layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            # a = leaky_relu(z, alpha=self.alpha)  # Leaky ReLU for hidden layers\n",
    "            a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def forward_pred(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each hidden layer\n",
    "        for i in range(len(self.best_weights) - 1):\n",
    "            z = np.dot(activations[-1], self.best_weights[i]) + self.best_biases[i]\n",
    "            pre_activations.append(z)\n",
    "            # a = leaky_relu(z, alpha=self.alpha)  # Leaky ReLU for hidden layers\n",
    "            a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.best_weights[-1]) + self.best_biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                # delta = np.dot(delta, self.weights[i].T) * leaky_relu_derivative(pre_activations[i - 1], alpha=self.alpha)\n",
    "                delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        self.t += 1  # Increment time step for Adam\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def train(self, batches, time_of_running, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while epoch<2000:\n",
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
    "            loss /= z\n",
    "            \n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = self.weights\n",
    "                self.best_biases = self.biases\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            get_stat()\n",
    "            epoch += 1\n",
    "            # if time elapsed is greater than 1 minute, break the loop\n",
    "            if time.time() - start_time > 60 * time_of_running:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_pred(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    \n",
    "    def get_best_biases(self):\n",
    "        return self.best_biases\n",
    "    \n",
    "    def get_best_loss(self):\n",
    "        return self.best_loss\n",
    "    \n",
    "    def get_best_seed(self):\n",
    "        return self.best_seed\n",
    "\n",
    "# Example usage:\n",
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8)\n",
    "# nn.train(batches, 1, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0987236950\n",
      "0.125\n",
      "Epoch 2, Loss: 2.0787225370\n",
      "0.1975\n",
      "Epoch 3, Loss: 2.0771810012\n",
      "0.125\n",
      "Epoch 4, Loss: 2.0744224936\n",
      "0.155\n",
      "Epoch 5, Loss: 2.0697406080\n",
      "0.19375\n",
      "Epoch 6, Loss: 2.0644354625\n",
      "0.12625\n",
      "Epoch 7, Loss: 2.0514811374\n",
      "0.16875\n",
      "Epoch 8, Loss: 2.0306749020\n",
      "0.37\n",
      "Epoch 9, Loss: 2.0094116291\n",
      "0.2175\n",
      "Epoch 10, Loss: 1.9553816159\n",
      "0.24875\n",
      "Epoch 11, Loss: 1.8978289082\n",
      "0.285\n",
      "Epoch 12, Loss: 1.8515446552\n",
      "0.2825\n",
      "Epoch 13, Loss: 1.8136455009\n",
      "0.3275\n",
      "Epoch 14, Loss: 1.7801253373\n",
      "0.3725\n",
      "Epoch 15, Loss: 1.7506464013\n",
      "0.37875\n",
      "Epoch 16, Loss: 1.7302218676\n",
      "0.33125\n",
      "Epoch 17, Loss: 1.7177804360\n",
      "0.31375\n",
      "Epoch 18, Loss: 1.6930764030\n",
      "0.3725\n",
      "Epoch 19, Loss: 1.6485668729\n",
      "0.40375\n",
      "Epoch 20, Loss: 1.6308692323\n",
      "0.40125\n",
      "Epoch 21, Loss: 1.6043055591\n",
      "0.375\n",
      "Epoch 22, Loss: 1.5616101779\n",
      "0.41625\n",
      "Epoch 23, Loss: 1.5273657350\n",
      "0.4275\n",
      "Epoch 24, Loss: 1.5163584128\n",
      "0.40625\n",
      "Epoch 25, Loss: 1.4833450261\n",
      "0.44125\n",
      "Epoch 26, Loss: 1.4641542912\n",
      "0.4425\n",
      "Epoch 27, Loss: 1.4709800677\n",
      "0.415\n",
      "Epoch 28, Loss: 1.4440630192\n",
      "0.46375\n",
      "Epoch 29, Loss: 1.4262468614\n",
      "0.4425\n",
      "Epoch 30, Loss: 1.4442954803\n",
      "0.43125\n",
      "Epoch 31, Loss: 1.4162026766\n",
      "0.4725\n",
      "Epoch 32, Loss: 1.4028470074\n",
      "0.455\n",
      "Epoch 33, Loss: 1.4125826509\n",
      "0.44375\n",
      "Epoch 34, Loss: 1.3863957997\n",
      "0.47625\n",
      "Epoch 35, Loss: 1.3831533790\n",
      "0.455\n",
      "Epoch 36, Loss: 1.3860620558\n",
      "0.45625\n",
      "Epoch 37, Loss: 1.3651467536\n",
      "0.48375\n",
      "Epoch 38, Loss: 1.3583515548\n",
      "0.47125\n",
      "Epoch 39, Loss: 1.3681970569\n",
      "0.465\n",
      "Epoch 40, Loss: 1.3437250560\n",
      "0.49625\n",
      "Epoch 41, Loss: 1.3332553524\n",
      "0.495\n",
      "Epoch 42, Loss: 1.3460279878\n",
      "0.4725\n",
      "Epoch 43, Loss: 1.3229943652\n",
      "0.50375\n",
      "Epoch 44, Loss: 1.3119035122\n",
      "0.5075\n",
      "Epoch 45, Loss: 1.3113625849\n",
      "0.49125\n",
      "Epoch 46, Loss: 1.3137295927\n",
      "0.47625\n",
      "Epoch 47, Loss: 1.2962520912\n",
      "0.50625\n",
      "Epoch 48, Loss: 1.2851616754\n",
      "0.505\n",
      "Epoch 49, Loss: 1.2925601097\n",
      "0.48375\n",
      "Epoch 50, Loss: 1.2896438669\n",
      "0.48625\n",
      "Epoch 51, Loss: 1.2806876105\n",
      "0.49375\n",
      "Epoch 52, Loss: 1.2727970138\n",
      "0.4875\n",
      "Epoch 53, Loss: 1.2790956805\n",
      "0.485\n",
      "Epoch 54, Loss: 1.2780467956\n",
      "0.48125\n",
      "Epoch 55, Loss: 1.2661555653\n",
      "0.48875\n",
      "Epoch 56, Loss: 1.2586542771\n",
      "0.48875\n",
      "Epoch 57, Loss: 1.2591875330\n",
      "0.495\n",
      "Epoch 58, Loss: 1.2601419099\n",
      "0.49375\n",
      "Epoch 59, Loss: 1.2448795880\n",
      "0.5025\n",
      "Epoch 60, Loss: 1.2392284804\n",
      "0.5025\n",
      "Epoch 61, Loss: 1.2295039059\n",
      "0.5025\n",
      "Epoch 62, Loss: 1.2264791082\n",
      "0.515\n",
      "Epoch 63, Loss: 1.2013123548\n",
      "0.53\n",
      "Epoch 64, Loss: 1.1837479724\n",
      "0.5325\n",
      "Epoch 65, Loss: 1.1705632022\n",
      "0.53625\n",
      "Epoch 66, Loss: 1.1675147747\n",
      "0.53625\n",
      "Epoch 67, Loss: 1.1735109941\n",
      "0.53\n",
      "Epoch 68, Loss: 1.1558056518\n",
      "0.54125\n",
      "Epoch 69, Loss: 1.1438634034\n",
      "0.5425\n",
      "Epoch 70, Loss: 1.1354458315\n",
      "0.54125\n",
      "Epoch 71, Loss: 1.1269164715\n",
      "0.545\n",
      "Epoch 72, Loss: 1.1254163059\n",
      "0.545\n",
      "Epoch 73, Loss: 1.1241784768\n",
      "0.53875\n",
      "Epoch 74, Loss: 1.1081227022\n",
      "0.54625\n",
      "Epoch 75, Loss: 1.0981759246\n",
      "0.5425\n",
      "Epoch 76, Loss: 1.0924197974\n",
      "0.5425\n",
      "Epoch 77, Loss: 1.0847028169\n",
      "0.5475\n",
      "Epoch 78, Loss: 1.0792569645\n",
      "0.55\n",
      "Epoch 79, Loss: 1.0786235238\n",
      "0.55375\n",
      "Epoch 80, Loss: 1.0727124386\n",
      "0.5525\n",
      "Epoch 81, Loss: 1.0618888014\n",
      "0.55375\n",
      "Epoch 82, Loss: 1.0559964403\n",
      "0.5525\n",
      "Epoch 83, Loss: 1.0525698257\n",
      "0.55375\n",
      "Epoch 84, Loss: 1.0490708752\n",
      "0.55875\n",
      "Epoch 85, Loss: 1.0447827152\n",
      "0.55875\n",
      "Epoch 86, Loss: 1.0415903304\n",
      "0.5575\n",
      "Epoch 87, Loss: 1.0376947693\n",
      "0.5575\n",
      "Epoch 88, Loss: 1.0320308723\n",
      "0.56125\n",
      "Epoch 89, Loss: 1.0265609246\n",
      "0.56125\n",
      "Epoch 90, Loss: 1.0229337213\n",
      "0.56\n",
      "Epoch 91, Loss: 1.0213166269\n",
      "0.56\n",
      "Epoch 92, Loss: 1.0208221943\n",
      "0.55625\n",
      "Epoch 93, Loss: 1.0201663312\n",
      "0.56375\n",
      "Epoch 94, Loss: 1.0172962886\n",
      "0.56375\n",
      "Epoch 95, Loss: 1.0118218936\n",
      "0.56375\n",
      "Epoch 96, Loss: 1.0061710853\n",
      "0.56625\n",
      "Epoch 97, Loss: 1.0022803159\n",
      "0.5675\n",
      "Epoch 98, Loss: 1.0001378027\n",
      "0.56625\n",
      "Epoch 99, Loss: 0.9991422442\n",
      "0.565\n",
      "Epoch 100, Loss: 0.9982666397\n",
      "0.56375\n",
      "Epoch 101, Loss: 0.9960986212\n",
      "0.5625\n",
      "Epoch 102, Loss: 0.9919636793\n",
      "0.565\n",
      "Epoch 103, Loss: 0.9869849035\n",
      "0.56875\n",
      "Epoch 104, Loss: 0.9827470569\n",
      "0.57\n",
      "Epoch 105, Loss: 0.9797328012\n",
      "0.57375\n",
      "Epoch 106, Loss: 0.9778068528\n",
      "0.57125\n",
      "Epoch 107, Loss: 0.9765774341\n",
      "0.57\n",
      "Epoch 108, Loss: 0.9750016563\n",
      "0.57\n",
      "Epoch 109, Loss: 0.9734623745\n",
      "0.57\n",
      "Epoch 110, Loss: 0.9706437265\n",
      "0.56875\n",
      "Epoch 111, Loss: 0.9675273444\n",
      "0.56875\n",
      "Epoch 112, Loss: 0.9639305595\n",
      "0.57\n",
      "Epoch 113, Loss: 0.9606209393\n",
      "0.57\n",
      "Epoch 114, Loss: 0.9577461212\n",
      "0.57125\n",
      "Epoch 115, Loss: 0.9553987653\n",
      "0.57375\n",
      "Epoch 116, Loss: 0.9536035695\n",
      "0.5725\n",
      "Epoch 117, Loss: 0.9522650598\n",
      "0.57\n",
      "Epoch 118, Loss: 0.9513235612\n",
      "0.56875\n",
      "Epoch 119, Loss: 0.9506262694\n",
      "0.5725\n",
      "Epoch 120, Loss: 0.9499651975\n",
      "0.57625\n",
      "Epoch 121, Loss: 0.9490865760\n",
      "0.57625\n",
      "Epoch 122, Loss: 0.9477024727\n",
      "0.57625\n",
      "Epoch 123, Loss: 0.9455933263\n",
      "0.5775\n",
      "Epoch 124, Loss: 0.9427163200\n",
      "0.575\n",
      "Epoch 125, Loss: 0.9393593002\n",
      "0.57625\n",
      "Epoch 126, Loss: 0.9358134317\n",
      "0.5775\n",
      "Epoch 127, Loss: 0.9326836749\n",
      "0.57875\n",
      "Epoch 128, Loss: 0.9299608701\n",
      "0.57875\n",
      "Epoch 129, Loss: 0.9275501597\n",
      "0.5775\n",
      "Epoch 130, Loss: 0.9257167066\n",
      "0.58\n",
      "Epoch 131, Loss: 0.9233840663\n",
      "0.5775\n",
      "Epoch 132, Loss: 0.9220498210\n",
      "0.5775\n",
      "Epoch 133, Loss: 0.9209200176\n",
      "0.5825\n",
      "Epoch 134, Loss: 0.9212413738\n",
      "0.58125\n",
      "Epoch 135, Loss: 0.9229956240\n",
      "0.5825\n",
      "Epoch 136, Loss: 0.9293964029\n",
      "0.57875\n",
      "Epoch 137, Loss: 0.9389499062\n",
      "0.57625\n",
      "Epoch 138, Loss: 0.9453279060\n",
      "0.57625\n",
      "Epoch 139, Loss: 0.9369768674\n",
      "0.57125\n",
      "Epoch 140, Loss: 0.9205518052\n",
      "0.58125\n",
      "Epoch 141, Loss: 0.9087411931\n",
      "0.58625\n",
      "Epoch 142, Loss: 0.9032797971\n",
      "0.5875\n",
      "Epoch 143, Loss: 0.9020687066\n",
      "0.585\n",
      "Epoch 144, Loss: 0.9041913116\n",
      "0.58375\n",
      "Epoch 145, Loss: 0.9087891754\n",
      "0.58125\n",
      "Epoch 146, Loss: 0.9113560620\n",
      "0.58375\n",
      "Epoch 147, Loss: 0.9086777546\n",
      "0.58625\n",
      "Epoch 148, Loss: 0.9045254685\n",
      "0.58375\n",
      "Epoch 149, Loss: 0.9003510640\n",
      "0.585\n",
      "Epoch 150, Loss: 0.8972630752\n",
      "0.5875\n",
      "Epoch 151, Loss: 0.8963217646\n",
      "0.5875\n",
      "Epoch 152, Loss: 0.8981670132\n",
      "0.58375\n",
      "Epoch 153, Loss: 0.9060740828\n",
      "0.58375\n",
      "Epoch 154, Loss: 0.9298130658\n",
      "0.57875\n",
      "Epoch 155, Loss: 0.9668954147\n",
      "0.565\n",
      "Epoch 156, Loss: 0.9226926088\n",
      "0.5725\n",
      "Epoch 157, Loss: 0.9279120884\n",
      "0.5675\n",
      "Epoch 158, Loss: 0.9008358850\n",
      "0.59\n",
      "Epoch 159, Loss: 0.8829678786\n",
      "0.58375\n",
      "Epoch 160, Loss: 0.8830415267\n",
      "0.5875\n",
      "Epoch 161, Loss: 0.8875986235\n",
      "0.58\n",
      "Epoch 162, Loss: 0.8851033019\n",
      "0.5825\n",
      "Epoch 163, Loss: 0.8855865211\n",
      "0.57875\n",
      "Epoch 164, Loss: 0.8850930789\n",
      "0.58625\n",
      "Epoch 165, Loss: 0.8809900497\n",
      "0.58125\n",
      "Epoch 166, Loss: 0.8802437817\n",
      "0.58625\n",
      "Epoch 167, Loss: 0.8777504390\n",
      "0.58125\n",
      "Epoch 168, Loss: 0.8780427663\n",
      "0.58625\n",
      "Epoch 169, Loss: 0.8772812029\n",
      "0.58375\n",
      "Epoch 170, Loss: 0.8785136773\n",
      "0.58375\n",
      "Epoch 171, Loss: 0.8789032671\n",
      "0.585\n",
      "Epoch 172, Loss: 0.8798734127\n",
      "0.5825\n",
      "Epoch 173, Loss: 0.8798744308\n",
      "0.585\n",
      "Epoch 174, Loss: 0.8789168821\n",
      "0.58375\n",
      "Epoch 175, Loss: 0.8774698431\n",
      "0.585\n",
      "Epoch 176, Loss: 0.8756588510\n",
      "0.58625\n",
      "Epoch 177, Loss: 0.8736512328\n",
      "0.585\n",
      "Epoch 178, Loss: 0.8715582468\n",
      "0.5875\n",
      "Epoch 179, Loss: 0.8698636941\n",
      "0.5875\n",
      "Epoch 180, Loss: 0.8688083259\n",
      "0.585\n",
      "Epoch 181, Loss: 0.8683724495\n",
      "0.58375\n",
      "Epoch 182, Loss: 0.8683388706\n",
      "0.585\n",
      "Epoch 183, Loss: 0.8688167564\n",
      "0.5825\n",
      "Epoch 184, Loss: 0.8699260840\n",
      "0.58125\n",
      "Epoch 185, Loss: 0.8712871271\n",
      "0.58\n",
      "Epoch 186, Loss: 0.8725460587\n",
      "0.58\n",
      "Epoch 187, Loss: 0.8733310321\n",
      "0.58\n",
      "Epoch 188, Loss: 0.8732187105\n",
      "0.58\n",
      "Epoch 189, Loss: 0.8718563652\n",
      "0.58\n",
      "Epoch 190, Loss: 0.8691030842\n",
      "0.58125\n",
      "Epoch 191, Loss: 0.8652755868\n",
      "0.58125\n",
      "Epoch 192, Loss: 0.8610612111\n",
      "0.58\n",
      "Epoch 193, Loss: 0.8571565618\n",
      "0.58125\n",
      "Epoch 194, Loss: 0.8541476110\n",
      "0.58\n",
      "Epoch 195, Loss: 0.8526175468\n",
      "0.5825\n",
      "Epoch 196, Loss: 0.8532108191\n",
      "0.5875\n",
      "Epoch 197, Loss: 0.8565227411\n",
      "0.58625\n",
      "Epoch 198, Loss: 0.8623627741\n",
      "0.58625\n",
      "Epoch 199, Loss: 0.8676558290\n",
      "0.59125\n",
      "Epoch 200, Loss: 0.8659172792\n",
      "0.58875\n",
      "Epoch 201, Loss: 0.8561978249\n",
      "0.59\n",
      "Epoch 202, Loss: 0.8465177609\n",
      "0.59125\n",
      "Epoch 203, Loss: 0.8410098244\n",
      "0.59125\n",
      "Epoch 204, Loss: 0.8382725088\n",
      "0.58875\n",
      "Epoch 205, Loss: 0.8368629801\n",
      "0.585\n",
      "Epoch 206, Loss: 0.8354555309\n",
      "0.585\n",
      "Epoch 207, Loss: 0.8337312076\n",
      "0.585\n",
      "Epoch 208, Loss: 0.8327795885\n",
      "0.58625\n",
      "Epoch 209, Loss: 0.8332699270\n",
      "0.585\n",
      "Epoch 210, Loss: 0.8347726735\n",
      "0.58375\n",
      "Epoch 211, Loss: 0.8361875010\n",
      "0.58375\n",
      "Epoch 212, Loss: 0.8367572313\n",
      "0.59125\n",
      "Epoch 213, Loss: 0.8366256018\n",
      "0.5925\n",
      "Epoch 214, Loss: 0.8361421689\n",
      "0.59125\n",
      "Epoch 215, Loss: 0.8351027765\n",
      "0.595\n",
      "Epoch 216, Loss: 0.8332963473\n",
      "0.59375\n",
      "Epoch 217, Loss: 0.8310031856\n",
      "0.59375\n",
      "Epoch 218, Loss: 0.8286420196\n",
      "0.5925\n",
      "Epoch 219, Loss: 0.8264518713\n",
      "0.5925\n",
      "Epoch 220, Loss: 0.8245798019\n",
      "0.59\n",
      "Epoch 221, Loss: 0.8231902200\n",
      "0.5875\n",
      "Epoch 222, Loss: 0.8223337099\n",
      "0.58625\n",
      "Epoch 223, Loss: 0.8213022363\n",
      "0.5875\n",
      "Epoch 224, Loss: 0.8182905826\n",
      "0.58375\n",
      "Epoch 225, Loss: 0.8137870534\n",
      "0.58375\n",
      "Epoch 226, Loss: 0.8108746434\n",
      "0.5825\n",
      "Epoch 227, Loss: 0.8099179126\n",
      "0.585\n",
      "Epoch 228, Loss: 0.8104037743\n",
      "0.58625\n",
      "Epoch 229, Loss: 0.8123718580\n",
      "0.5875\n",
      "Epoch 230, Loss: 0.8146454972\n",
      "0.585\n",
      "Epoch 231, Loss: 0.8141427999\n",
      "0.5825\n",
      "Epoch 232, Loss: 0.8096717820\n",
      "0.58875\n",
      "Epoch 233, Loss: 0.8037102412\n",
      "0.59\n",
      "Epoch 234, Loss: 0.7991405703\n",
      "0.59125\n",
      "Epoch 235, Loss: 0.7970264680\n",
      "0.59\n",
      "Epoch 236, Loss: 0.7973272512\n",
      "0.59\n",
      "Epoch 237, Loss: 0.8001799215\n",
      "0.59\n",
      "Epoch 238, Loss: 0.8067941076\n",
      "0.59\n",
      "Epoch 239, Loss: 0.8226090341\n",
      "0.58625\n",
      "Epoch 240, Loss: 0.8320253980\n",
      "0.58\n",
      "Epoch 241, Loss: 0.7943811384\n",
      "0.59125\n",
      "Epoch 242, Loss: 0.7891443915\n",
      "0.595\n",
      "Epoch 243, Loss: 0.7953028051\n",
      "0.585\n",
      "Epoch 244, Loss: 0.7978955801\n",
      "0.57875\n",
      "Epoch 245, Loss: 0.8037133836\n",
      "0.5825\n",
      "Epoch 246, Loss: 0.8130524007\n",
      "0.59\n",
      "Epoch 247, Loss: 0.8171947533\n",
      "0.59375\n",
      "Epoch 248, Loss: 0.8127575595\n",
      "0.5925\n",
      "Epoch 249, Loss: 0.8052479223\n",
      "0.58875\n",
      "Epoch 250, Loss: 0.8016752569\n",
      "0.585\n",
      "Epoch 251, Loss: 0.8072242499\n",
      "0.58875\n",
      "Epoch 252, Loss: 0.8208789738\n",
      "0.58625\n",
      "Epoch 253, Loss: 0.8188666374\n",
      "0.58625\n",
      "Epoch 254, Loss: 0.8216293862\n",
      "0.57875\n",
      "Epoch 255, Loss: 0.8288113088\n",
      "0.57375\n",
      "Epoch 256, Loss: 0.7806419747\n",
      "0.585\n",
      "Epoch 257, Loss: 0.7861310079\n",
      "0.5925\n",
      "Epoch 258, Loss: 0.7817397102\n",
      "0.60125\n",
      "Epoch 259, Loss: 0.7749618146\n",
      "0.59125\n",
      "Epoch 260, Loss: 0.7757695246\n",
      "0.59\n",
      "Epoch 261, Loss: 0.7788261052\n",
      "0.58375\n",
      "Epoch 262, Loss: 0.7807260432\n",
      "0.58625\n",
      "Epoch 263, Loss: 0.7871558592\n",
      "0.58375\n",
      "Epoch 264, Loss: 0.8162382894\n",
      "0.59\n",
      "Epoch 265, Loss: 0.8094702810\n",
      "0.58375\n",
      "Epoch 266, Loss: 0.7946013655\n",
      "0.58125\n",
      "Epoch 267, Loss: 0.7913803265\n",
      "0.59125\n",
      "Epoch 268, Loss: 0.7790104111\n",
      "0.59875\n",
      "Epoch 269, Loss: 0.7816578256\n",
      "0.5925\n",
      "Epoch 270, Loss: 0.7798284734\n",
      "0.5975\n",
      "Epoch 271, Loss: 0.7826783507\n",
      "0.6075\n",
      "Epoch 272, Loss: 0.7791540574\n",
      "0.5975\n",
      "Epoch 273, Loss: 0.7710523909\n",
      "0.595\n",
      "Epoch 274, Loss: 0.7688323588\n",
      "0.58875\n",
      "Epoch 275, Loss: 0.7734475095\n",
      "0.58125\n",
      "Epoch 276, Loss: 0.7878089965\n",
      "0.5925\n",
      "Epoch 277, Loss: 0.7917274325\n",
      "0.58625\n",
      "Epoch 278, Loss: 0.7880920176\n",
      "0.58625\n",
      "Epoch 279, Loss: 0.7731273666\n",
      "0.5925\n",
      "Epoch 280, Loss: 0.7564145631\n",
      "0.59125\n",
      "Epoch 281, Loss: 0.7661208464\n",
      "0.5975\n",
      "Epoch 282, Loss: 0.7893751082\n",
      "0.595\n",
      "Epoch 283, Loss: 0.7782894513\n",
      "0.6025\n",
      "Epoch 284, Loss: 0.7656814663\n",
      "0.60625\n",
      "Epoch 285, Loss: 0.7484859195\n",
      "0.60875\n",
      "Epoch 286, Loss: 0.7476345850\n",
      "0.60375\n",
      "Epoch 287, Loss: 0.7477634795\n",
      "0.60625\n",
      "Epoch 288, Loss: 0.7457012282\n",
      "0.6025\n",
      "Epoch 289, Loss: 0.7443346196\n",
      "0.6025\n",
      "Epoch 290, Loss: 0.7451823321\n",
      "0.59875\n",
      "Epoch 291, Loss: 0.7509983003\n",
      "0.595\n",
      "Epoch 292, Loss: 0.7565008819\n",
      "0.59875\n",
      "Epoch 293, Loss: 0.7612575379\n",
      "0.5975\n",
      "Epoch 294, Loss: 0.7864770484\n",
      "0.59\n",
      "Epoch 295, Loss: 0.8163905843\n",
      "0.5925\n",
      "Epoch 296, Loss: 0.7616122521\n",
      "0.5925\n",
      "Epoch 297, Loss: 0.7400405623\n",
      "0.61\n",
      "Epoch 298, Loss: 0.7587131144\n",
      "0.59625\n",
      "Epoch 299, Loss: 0.7636539415\n",
      "0.595\n",
      "Epoch 300, Loss: 0.7588759793\n",
      "0.595\n",
      "Epoch 301, Loss: 0.7586439026\n",
      "0.59125\n",
      "Epoch 302, Loss: 0.7593793023\n",
      "0.59625\n",
      "Epoch 303, Loss: 0.7582067795\n",
      "0.595\n",
      "Epoch 304, Loss: 0.7543495418\n",
      "0.595\n",
      "Epoch 305, Loss: 0.7490408901\n",
      "0.59375\n",
      "Epoch 306, Loss: 0.7440400251\n",
      "0.5925\n",
      "Epoch 307, Loss: 0.7403661621\n",
      "0.59625\n",
      "Epoch 308, Loss: 0.7383870689\n",
      "0.595\n",
      "Epoch 309, Loss: 0.7368337733\n",
      "0.5975\n",
      "Epoch 310, Loss: 0.7343655361\n",
      "0.5975\n",
      "Epoch 311, Loss: 0.7318263683\n",
      "0.59875\n",
      "Epoch 312, Loss: 0.7303571365\n",
      "0.6\n",
      "Epoch 313, Loss: 0.7299881161\n",
      "0.6\n",
      "Epoch 314, Loss: 0.7295164503\n",
      "0.59875\n",
      "Epoch 315, Loss: 0.7280820503\n",
      "0.59875\n",
      "Epoch 316, Loss: 0.7263405454\n",
      "0.5975\n",
      "Epoch 317, Loss: 0.7248150403\n",
      "0.5975\n",
      "Epoch 318, Loss: 0.7239278666\n",
      "0.5975\n",
      "Epoch 319, Loss: 0.7227793754\n",
      "0.6\n",
      "Epoch 320, Loss: 0.7229799216\n",
      "0.60375\n",
      "Epoch 321, Loss: 0.7233761165\n",
      "0.595\n",
      "Epoch 322, Loss: 0.7321342070\n",
      "0.605\n",
      "Epoch 323, Loss: 0.7282325907\n",
      "0.59625\n",
      "Epoch 324, Loss: 0.7131604064\n",
      "0.59875\n",
      "Epoch 325, Loss: 0.7096628898\n",
      "0.6025\n",
      "Epoch 326, Loss: 0.7150125641\n",
      "0.60375\n",
      "Epoch 327, Loss: 0.7146342951\n",
      "0.6\n",
      "Epoch 328, Loss: 0.7150306787\n",
      "0.6\n",
      "Epoch 329, Loss: 0.7153500848\n",
      "0.60125\n",
      "Epoch 330, Loss: 0.7118799150\n",
      "0.60125\n",
      "Epoch 331, Loss: 0.7109666176\n",
      "0.605\n",
      "Epoch 332, Loss: 0.7070329361\n",
      "0.6025\n",
      "Epoch 333, Loss: 0.7095403184\n",
      "0.60375\n",
      "Epoch 334, Loss: 0.7069634764\n",
      "0.60125\n",
      "Epoch 335, Loss: 0.7198958915\n",
      "0.6075\n",
      "Epoch 336, Loss: 0.7204899545\n",
      "0.595\n",
      "Epoch 337, Loss: 0.7073857430\n",
      "0.60375\n",
      "Epoch 338, Loss: 0.7095056442\n",
      "0.60125\n",
      "Epoch 339, Loss: 0.7115117337\n",
      "0.59875\n",
      "Epoch 340, Loss: 0.7233713124\n",
      "0.6075\n",
      "Epoch 341, Loss: 0.7307643969\n",
      "0.6025\n",
      "Epoch 342, Loss: 0.7592826521\n",
      "0.60125\n",
      "Epoch 343, Loss: 0.9449397382\n",
      "0.52625\n",
      "Epoch 344, Loss: 0.7846657203\n",
      "0.60125\n",
      "Epoch 345, Loss: 0.7856497992\n",
      "0.5775\n",
      "Epoch 346, Loss: 0.7639722195\n",
      "0.59375\n",
      "Epoch 347, Loss: 0.7898517400\n",
      "0.5825\n",
      "Epoch 348, Loss: 0.9104805650\n",
      "0.57625\n",
      "Epoch 349, Loss: 0.8510498361\n",
      "0.57125\n",
      "Epoch 350, Loss: 0.7559138513\n",
      "0.58375\n",
      "Epoch 351, Loss: 0.7575140246\n",
      "0.6\n",
      "Epoch 352, Loss: 0.7075585501\n",
      "0.60625\n",
      "Epoch 353, Loss: 0.7215350848\n",
      "0.60125\n",
      "Epoch 354, Loss: 0.7181655855\n",
      "0.60875\n",
      "Epoch 355, Loss: 0.7130569029\n",
      "0.6025\n",
      "Epoch 356, Loss: 0.7031474362\n",
      "0.6125\n",
      "Epoch 357, Loss: 0.7011225505\n",
      "0.60625\n",
      "Epoch 358, Loss: 0.7081865395\n",
      "0.6125\n",
      "Epoch 359, Loss: 0.7600584665\n",
      "0.58875\n",
      "Epoch 360, Loss: 0.8308606848\n",
      "0.565\n",
      "Epoch 361, Loss: 0.8896810440\n",
      "0.5375\n",
      "Epoch 362, Loss: 0.9626827399\n",
      "0.53625\n",
      "Epoch 363, Loss: 0.7974857823\n",
      "0.57875\n",
      "Epoch 364, Loss: 0.7866737591\n",
      "0.595\n",
      "Epoch 365, Loss: 0.8177448004\n",
      "0.565\n",
      "Epoch 366, Loss: 0.8345668251\n",
      "0.56875\n",
      "Epoch 367, Loss: 0.7422958761\n",
      "0.595\n",
      "Epoch 368, Loss: 0.7236545583\n",
      "0.6075\n",
      "Epoch 369, Loss: 0.7315793512\n",
      "0.5925\n",
      "Epoch 370, Loss: 0.7251092582\n",
      "0.585\n",
      "Epoch 371, Loss: 0.7386354478\n",
      "0.57875\n",
      "Epoch 372, Loss: 0.7101022960\n",
      "0.6025\n",
      "Epoch 373, Loss: 0.6941049490\n",
      "0.60875\n",
      "Epoch 374, Loss: 0.6960585888\n",
      "0.6075\n",
      "Epoch 375, Loss: 0.7112721603\n",
      "0.6\n",
      "Epoch 376, Loss: 0.7190197572\n",
      "0.59\n",
      "Epoch 377, Loss: 0.7183507082\n",
      "0.5925\n",
      "Epoch 378, Loss: 0.7015774972\n",
      "0.6075\n",
      "Epoch 379, Loss: 0.6945740402\n",
      "0.595\n",
      "Epoch 380, Loss: 0.6976931712\n",
      "0.595\n",
      "Epoch 381, Loss: 0.7066588398\n",
      "0.60125\n",
      "Epoch 382, Loss: 0.7134305416\n",
      "0.59\n",
      "Epoch 383, Loss: 0.7068590140\n",
      "0.60375\n",
      "Epoch 384, Loss: 0.6897570316\n",
      "0.61\n",
      "Epoch 385, Loss: 0.7111909993\n",
      "0.5925\n",
      "Epoch 386, Loss: 0.7196701215\n",
      "0.585\n",
      "Epoch 387, Loss: 0.7196456988\n",
      "0.58125\n",
      "Epoch 388, Loss: 0.7133800523\n",
      "0.585\n",
      "Epoch 389, Loss: 0.6927494230\n",
      "0.60125\n",
      "Epoch 390, Loss: 0.6784999595\n",
      "0.615\n",
      "Epoch 391, Loss: 0.6819963625\n",
      "0.61375\n",
      "Epoch 392, Loss: 0.6975507886\n",
      "0.6025\n",
      "Epoch 393, Loss: 0.7088489033\n",
      "0.59625\n",
      "Epoch 394, Loss: 0.7053530501\n",
      "0.595\n",
      "Epoch 395, Loss: 0.6918175360\n",
      "0.6025\n",
      "Epoch 396, Loss: 0.6850212064\n",
      "0.6\n",
      "Epoch 397, Loss: 0.7035273352\n",
      "0.59875\n",
      "Epoch 398, Loss: 0.7127726894\n",
      "0.5975\n",
      "Epoch 399, Loss: 0.7008270196\n",
      "0.60375\n",
      "Epoch 400, Loss: 0.7099876450\n",
      "0.585\n",
      "Epoch 401, Loss: 0.7281766533\n",
      "0.58375\n",
      "Epoch 402, Loss: 0.6867275029\n",
      "0.60125\n",
      "Epoch 403, Loss: 0.6860720309\n",
      "0.60375\n",
      "Epoch 404, Loss: 0.6783333723\n",
      "0.59625\n",
      "Epoch 405, Loss: 0.6833931127\n",
      "0.60125\n",
      "Epoch 406, Loss: 0.6948959331\n",
      "0.595\n",
      "Epoch 407, Loss: 0.7022635731\n",
      "0.59625\n",
      "Epoch 408, Loss: 0.6979757525\n",
      "0.605\n",
      "Epoch 409, Loss: 0.6717150582\n",
      "0.60875\n",
      "Epoch 410, Loss: 0.7155933337\n",
      "0.6025\n",
      "Epoch 411, Loss: 0.6976666467\n",
      "0.6025\n",
      "Epoch 412, Loss: 0.6993642905\n",
      "0.59125\n",
      "Epoch 413, Loss: 0.7290082687\n",
      "0.575\n",
      "Epoch 414, Loss: 0.7326211966\n",
      "0.58\n",
      "Epoch 415, Loss: 0.7170894949\n",
      "0.5925\n",
      "Epoch 416, Loss: 0.7083756848\n",
      "0.59625\n",
      "Epoch 417, Loss: 0.6989184720\n",
      "0.59625\n",
      "Epoch 418, Loss: 0.6944763639\n",
      "0.5975\n",
      "Epoch 419, Loss: 0.6872383639\n",
      "0.595\n",
      "Epoch 420, Loss: 0.6706606080\n",
      "0.60625\n",
      "Epoch 421, Loss: 0.6564263501\n",
      "0.62\n",
      "Epoch 422, Loss: 0.6610386638\n",
      "0.62\n",
      "Epoch 423, Loss: 0.6798120205\n",
      "0.6075\n",
      "Epoch 424, Loss: 0.6813571054\n",
      "0.595\n",
      "Epoch 425, Loss: 0.6594358995\n",
      "0.59875\n",
      "Epoch 426, Loss: 0.6921588581\n",
      "0.60375\n",
      "Epoch 427, Loss: 0.6876432898\n",
      "0.61125\n",
      "Epoch 428, Loss: 0.6620822177\n",
      "0.605\n",
      "Epoch 429, Loss: 0.6570253870\n",
      "0.60625\n",
      "Epoch 430, Loss: 0.6656007786\n",
      "0.595\n",
      "Epoch 431, Loss: 0.6672301187\n",
      "0.6\n",
      "Epoch 432, Loss: 0.6609311045\n",
      "0.6075\n",
      "Epoch 433, Loss: 0.6628620542\n",
      "0.6075\n",
      "Epoch 434, Loss: 0.6493072564\n",
      "0.605\n",
      "Epoch 435, Loss: 0.6818269846\n",
      "0.5975\n",
      "Epoch 436, Loss: 0.7028787442\n",
      "0.59125\n",
      "Epoch 437, Loss: 0.6754337603\n",
      "0.59\n",
      "Epoch 438, Loss: 0.6582423716\n",
      "0.59625\n",
      "Epoch 439, Loss: 0.6529309545\n",
      "0.60625\n",
      "Epoch 440, Loss: 0.6448213684\n",
      "0.60375\n",
      "Epoch 441, Loss: 0.6367744547\n",
      "0.6125\n",
      "Epoch 442, Loss: 0.6374103514\n",
      "0.60875\n",
      "Epoch 443, Loss: 0.6415174256\n",
      "0.6125\n",
      "Epoch 444, Loss: 0.6470071232\n",
      "0.6075\n",
      "Epoch 445, Loss: 0.6516091006\n",
      "0.6025\n",
      "Epoch 446, Loss: 0.6555305273\n",
      "0.58875\n",
      "Epoch 447, Loss: 0.6740418222\n",
      "0.585\n",
      "Epoch 448, Loss: 0.7003529908\n",
      "0.59875\n",
      "Epoch 449, Loss: 0.6912410819\n",
      "0.59125\n",
      "Epoch 450, Loss: 0.6460540149\n",
      "0.60875\n",
      "Epoch 451, Loss: 0.6420633509\n",
      "0.615\n",
      "Epoch 452, Loss: 0.6551185851\n",
      "0.5975\n",
      "Epoch 453, Loss: 0.6465263794\n",
      "0.605\n",
      "Epoch 454, Loss: 0.6560674568\n",
      "0.60625\n",
      "Epoch 455, Loss: 0.6580262780\n",
      "0.59625\n",
      "Epoch 456, Loss: 0.6396623563\n",
      "0.61\n",
      "Epoch 457, Loss: 0.6570784925\n",
      "0.6075\n",
      "Epoch 458, Loss: 0.6513293960\n",
      "0.615\n",
      "Epoch 459, Loss: 0.6419669292\n",
      "0.60625\n",
      "Epoch 460, Loss: 0.6366530624\n",
      "0.6025\n",
      "Epoch 461, Loss: 0.6292748733\n",
      "0.605\n",
      "Epoch 462, Loss: 0.6287329319\n",
      "0.61\n",
      "Epoch 463, Loss: 0.6447344620\n",
      "0.60625\n",
      "Epoch 464, Loss: 0.6468359292\n",
      "0.60625\n",
      "Epoch 465, Loss: 0.6371771867\n",
      "0.59875\n",
      "Epoch 466, Loss: 0.6459118826\n",
      "0.59625\n",
      "Epoch 467, Loss: 0.6522548945\n",
      "0.6025\n",
      "Epoch 468, Loss: 0.6630460530\n",
      "0.595\n",
      "Epoch 469, Loss: 0.6644911100\n",
      "0.59875\n",
      "Epoch 470, Loss: 0.6546720608\n",
      "0.60625\n",
      "Epoch 471, Loss: 0.6613781383\n",
      "0.60375\n",
      "Epoch 472, Loss: 0.6613531485\n",
      "0.615\n",
      "Epoch 473, Loss: 0.6674510316\n",
      "0.6\n",
      "Epoch 474, Loss: 0.6493466621\n",
      "0.5975\n",
      "Epoch 475, Loss: 0.6783880963\n",
      "0.59375\n",
      "Epoch 476, Loss: 0.7053930115\n",
      "0.56875\n",
      "Epoch 477, Loss: 0.6819614046\n",
      "0.57\n",
      "Epoch 478, Loss: 0.6222582329\n",
      "0.62375\n",
      "Epoch 479, Loss: 0.6381660700\n",
      "0.6125\n",
      "Epoch 480, Loss: 0.6239304351\n",
      "0.62\n",
      "Epoch 481, Loss: 0.6206852039\n",
      "0.605\n",
      "Epoch 482, Loss: 0.6157817015\n",
      "0.60875\n",
      "Epoch 483, Loss: 0.6168224775\n",
      "0.61375\n",
      "Epoch 484, Loss: 0.6239203539\n",
      "0.61\n",
      "Epoch 485, Loss: 0.6252123760\n",
      "0.60625\n",
      "Epoch 486, Loss: 0.6255428235\n",
      "0.6125\n",
      "Epoch 487, Loss: 0.6413893187\n",
      "0.61\n",
      "Epoch 488, Loss: 0.6487511623\n",
      "0.60875\n",
      "Epoch 489, Loss: 0.6442005150\n",
      "0.60625\n",
      "Epoch 490, Loss: 0.6442493668\n",
      "0.605\n",
      "Epoch 491, Loss: 0.6817484460\n",
      "0.59\n",
      "Epoch 492, Loss: 0.7107815077\n",
      "0.57375\n",
      "Epoch 493, Loss: 0.6339898803\n",
      "0.5925\n",
      "Epoch 494, Loss: 0.6221421569\n",
      "0.62\n",
      "Epoch 495, Loss: 0.6097556840\n",
      "0.625\n",
      "Epoch 496, Loss: 0.6018497016\n",
      "0.62125\n",
      "Epoch 497, Loss: 0.6024024818\n",
      "0.6175\n",
      "Epoch 498, Loss: 0.5993809602\n",
      "0.6175\n",
      "Epoch 499, Loss: 0.6006385460\n",
      "0.6175\n",
      "Epoch 500, Loss: 0.6046220696\n",
      "0.61625\n",
      "Epoch 501, Loss: 0.6015424915\n",
      "0.62\n",
      "Epoch 502, Loss: 0.6008759907\n",
      "0.62625\n",
      "Epoch 503, Loss: 0.6004053201\n",
      "0.62125\n",
      "Epoch 504, Loss: 0.5968023691\n",
      "0.62375\n",
      "Epoch 505, Loss: 0.5954995108\n",
      "0.6225\n",
      "Epoch 506, Loss: 0.5964047836\n",
      "0.62375\n",
      "Epoch 507, Loss: 0.5961555901\n",
      "0.62\n",
      "Epoch 508, Loss: 0.5950974465\n",
      "0.6225\n",
      "Epoch 509, Loss: 0.5941175727\n",
      "0.61875\n",
      "Epoch 510, Loss: 0.5929990830\n",
      "0.62125\n",
      "Epoch 511, Loss: 0.5921607718\n",
      "0.61625\n",
      "Epoch 512, Loss: 0.5915589056\n",
      "0.615\n",
      "Epoch 513, Loss: 0.5908800703\n",
      "0.61875\n",
      "Epoch 514, Loss: 0.5898474650\n",
      "0.6175\n",
      "Epoch 515, Loss: 0.5881676544\n",
      "0.61875\n",
      "Epoch 516, Loss: 0.5870225982\n",
      "0.62625\n",
      "Epoch 517, Loss: 0.5877000590\n",
      "0.61875\n",
      "Epoch 518, Loss: 0.5904187255\n",
      "0.61375\n",
      "Epoch 519, Loss: 0.5947028707\n",
      "0.6075\n",
      "Epoch 520, Loss: 0.6009792793\n",
      "0.605\n",
      "Epoch 521, Loss: 0.6111857144\n",
      "0.6\n",
      "Epoch 522, Loss: 0.6296852805\n",
      "0.59875\n",
      "Epoch 523, Loss: 0.7221468214\n",
      "0.5775\n",
      "Epoch 524, Loss: 0.7484223811\n",
      "0.59875\n",
      "Epoch 525, Loss: 0.6401304080\n",
      "0.615\n",
      "Epoch 526, Loss: 0.6760460020\n",
      "0.59125\n",
      "Epoch 527, Loss: 0.7232915497\n",
      "0.5625\n",
      "Epoch 528, Loss: 0.6068294862\n",
      "0.6025\n",
      "Epoch 529, Loss: 0.6563411277\n",
      "0.61125\n",
      "Epoch 530, Loss: 0.6159293509\n",
      "0.60625\n",
      "Epoch 531, Loss: 0.6194816540\n",
      "0.59875\n",
      "Epoch 532, Loss: 0.6142564492\n",
      "0.61\n",
      "Epoch 533, Loss: 0.6213914369\n",
      "0.61125\n",
      "Epoch 534, Loss: 0.6150630814\n",
      "0.61125\n",
      "Epoch 535, Loss: 0.6095661541\n",
      "0.6\n",
      "Epoch 536, Loss: 0.6214504522\n",
      "0.5975\n",
      "Epoch 537, Loss: 0.6834144568\n",
      "0.59\n",
      "Epoch 538, Loss: 0.8335010598\n",
      "0.5475\n",
      "Epoch 539, Loss: 0.6270285223\n",
      "0.605\n",
      "Epoch 540, Loss: 0.7082647471\n",
      "0.61375\n",
      "Epoch 541, Loss: 0.6827786629\n",
      "0.60875\n",
      "Epoch 542, Loss: 0.7102862235\n",
      "0.5975\n",
      "Epoch 543, Loss: 0.7700768631\n",
      "0.5875\n",
      "Epoch 544, Loss: 0.6360673541\n",
      "0.615\n",
      "Epoch 545, Loss: 0.7314126592\n",
      "0.5725\n",
      "Epoch 546, Loss: 0.6037305021\n",
      "0.62125\n",
      "Epoch 547, Loss: 0.6298649322\n",
      "0.61\n",
      "Epoch 548, Loss: 0.6043651894\n",
      "0.6125\n",
      "Epoch 549, Loss: 0.6504284709\n",
      "0.605\n",
      "Epoch 550, Loss: 0.6663720984\n",
      "0.59625\n",
      "Epoch 551, Loss: 0.6993456231\n",
      "0.59\n",
      "Epoch 552, Loss: 0.6595361749\n",
      "0.5925\n",
      "Epoch 553, Loss: 0.6277991216\n",
      "0.60125\n",
      "Epoch 554, Loss: 0.6083115789\n",
      "0.60625\n",
      "Epoch 555, Loss: 0.6018861419\n",
      "0.60875\n",
      "Epoch 556, Loss: 0.6165522308\n",
      "0.6075\n",
      "Epoch 557, Loss: 0.6338473174\n",
      "0.59875\n",
      "Epoch 558, Loss: 0.6100654281\n",
      "0.61625\n",
      "Epoch 559, Loss: 0.5926634497\n",
      "0.60625\n",
      "Epoch 560, Loss: 0.5942213353\n",
      "0.6125\n",
      "Epoch 561, Loss: 0.6816689327\n",
      "0.595\n",
      "Epoch 562, Loss: 0.7296339406\n",
      "0.58\n",
      "Epoch 563, Loss: 0.6064409242\n",
      "0.605\n",
      "Epoch 564, Loss: 0.5937859637\n",
      "0.61375\n",
      "Epoch 565, Loss: 0.5876498546\n",
      "0.62\n",
      "Epoch 566, Loss: 0.5946753762\n",
      "0.61\n",
      "Epoch 567, Loss: 0.6183825974\n",
      "0.6025\n",
      "Epoch 568, Loss: 0.6309246869\n",
      "0.5975\n",
      "Epoch 569, Loss: 0.5991050246\n",
      "0.62\n",
      "Epoch 570, Loss: 0.5762592131\n",
      "0.60625\n",
      "Epoch 571, Loss: 0.5716330105\n",
      "0.6125\n",
      "Epoch 572, Loss: 0.5722617482\n",
      "0.61125\n",
      "Epoch 573, Loss: 0.6126768335\n",
      "0.6125\n",
      "Epoch 574, Loss: 0.7314326979\n",
      "0.58375\n",
      "Epoch 575, Loss: 0.7278783583\n",
      "0.58375\n",
      "Epoch 576, Loss: 0.6187045696\n",
      "0.6075\n",
      "Epoch 577, Loss: 0.5951403517\n",
      "0.61125\n",
      "Epoch 578, Loss: 0.5948260990\n",
      "0.61125\n",
      "Epoch 579, Loss: 0.6236590686\n",
      "0.595\n",
      "Epoch 580, Loss: 0.6840204077\n",
      "0.5825\n",
      "Epoch 581, Loss: 0.6952853575\n",
      "0.5775\n",
      "Epoch 582, Loss: 0.6263699528\n",
      "0.59625\n",
      "Epoch 583, Loss: 0.5957934888\n",
      "0.60875\n",
      "Epoch 584, Loss: 0.5887242443\n",
      "0.6075\n",
      "Epoch 585, Loss: 0.6000411636\n",
      "0.6075\n",
      "Epoch 586, Loss: 0.6313288131\n",
      "0.60875\n",
      "Epoch 587, Loss: 0.6453615435\n",
      "0.6\n",
      "Epoch 588, Loss: 0.6085907473\n",
      "0.60625\n",
      "Epoch 589, Loss: 0.5968545175\n",
      "0.6075\n",
      "Epoch 590, Loss: 0.6070951326\n",
      "0.6175\n",
      "Epoch 591, Loss: 0.6051534989\n",
      "0.60375\n",
      "Epoch 592, Loss: 0.6654072072\n",
      "0.59625\n",
      "Epoch 593, Loss: 0.6615175610\n",
      "0.5925\n",
      "Epoch 594, Loss: 0.5921745081\n",
      "0.62\n",
      "Epoch 595, Loss: 0.5676670680\n",
      "0.6175\n",
      "Epoch 596, Loss: 0.5652321696\n",
      "0.61375\n",
      "Epoch 597, Loss: 0.6065873500\n",
      "0.6\n",
      "Epoch 598, Loss: 0.6993291333\n",
      "0.57625\n",
      "Epoch 599, Loss: 0.6785267619\n",
      "0.575\n",
      "Epoch 600, Loss: 0.5968001397\n",
      "0.59375\n",
      "Epoch 601, Loss: 0.5739304225\n",
      "0.5975\n",
      "Epoch 602, Loss: 0.5629980365\n",
      "0.60875\n",
      "Epoch 603, Loss: 0.5598242696\n",
      "0.61875\n",
      "Epoch 604, Loss: 0.5680587848\n",
      "0.6225\n",
      "Epoch 605, Loss: 0.5911436676\n",
      "0.60875\n",
      "Epoch 606, Loss: 0.6360550091\n",
      "0.6\n",
      "Epoch 607, Loss: 0.6763028177\n",
      "0.59125\n",
      "Epoch 608, Loss: 0.6547700059\n",
      "0.595\n",
      "Epoch 609, Loss: 0.5837764029\n",
      "0.6175\n",
      "Epoch 610, Loss: 0.5638045167\n",
      "0.6225\n",
      "Epoch 611, Loss: 0.5892406496\n",
      "0.61375\n",
      "Epoch 612, Loss: 0.6158703729\n",
      "0.5875\n",
      "Epoch 613, Loss: 0.5885467028\n",
      "0.5975\n",
      "Epoch 614, Loss: 0.6142699565\n",
      "0.595\n",
      "Epoch 615, Loss: 0.6819362818\n",
      "0.58\n",
      "Epoch 616, Loss: 0.6215815768\n",
      "0.60625\n",
      "Epoch 617, Loss: 0.5572180078\n",
      "0.62625\n",
      "Epoch 618, Loss: 0.5449817768\n",
      "0.6225\n",
      "Epoch 619, Loss: 0.5523255042\n",
      "0.6175\n",
      "Epoch 620, Loss: 0.5563282743\n",
      "0.61\n",
      "Epoch 621, Loss: 0.5451226779\n",
      "0.615\n",
      "Epoch 622, Loss: 0.5400116147\n",
      "0.62875\n",
      "Epoch 623, Loss: 0.5808822523\n",
      "0.62375\n",
      "Epoch 624, Loss: 0.6327267875\n",
      "0.61\n",
      "Epoch 625, Loss: 0.6396028161\n",
      "0.6075\n",
      "Epoch 626, Loss: 0.5927491624\n",
      "0.62125\n",
      "Epoch 627, Loss: 0.5487017244\n",
      "0.63625\n",
      "Epoch 628, Loss: 0.5369183944\n",
      "0.63625\n",
      "Epoch 629, Loss: 0.5424312189\n",
      "0.62\n",
      "Epoch 630, Loss: 0.5532526991\n",
      "0.61125\n",
      "Epoch 631, Loss: 0.5606958695\n",
      "0.615\n",
      "Epoch 632, Loss: 0.5558925253\n",
      "0.6125\n",
      "Epoch 633, Loss: 0.5369900698\n",
      "0.615\n",
      "Epoch 634, Loss: 0.5760808787\n",
      "0.61125\n",
      "Epoch 635, Loss: 0.6577465248\n",
      "0.59125\n",
      "Epoch 636, Loss: 0.6319480756\n",
      "0.59625\n",
      "Epoch 637, Loss: 0.6005808744\n",
      "0.6075\n",
      "Epoch 638, Loss: 0.5647839561\n",
      "0.61625\n",
      "Epoch 639, Loss: 0.5523533235\n",
      "0.625\n",
      "Epoch 640, Loss: 0.5422523227\n",
      "0.62\n",
      "Epoch 641, Loss: 0.5472479376\n",
      "0.61625\n",
      "Epoch 642, Loss: 0.5669511611\n",
      "0.6075\n",
      "Epoch 643, Loss: 0.5770557700\n",
      "0.6\n",
      "Epoch 644, Loss: 0.5558962152\n",
      "0.60875\n",
      "Epoch 645, Loss: 0.5509874800\n",
      "0.625\n",
      "Epoch 646, Loss: 0.5796639627\n",
      "0.61875\n",
      "Epoch 647, Loss: 0.5923003084\n",
      "0.61625\n",
      "Epoch 648, Loss: 0.5803612047\n",
      "0.6\n",
      "Epoch 649, Loss: 0.6042295675\n",
      "0.61\n",
      "Epoch 650, Loss: 0.6100664485\n",
      "0.595\n",
      "Epoch 651, Loss: 0.5870607959\n",
      "0.60875\n",
      "Epoch 652, Loss: 0.5639966254\n",
      "0.6175\n",
      "Epoch 653, Loss: 0.5455341830\n",
      "0.61875\n",
      "Epoch 654, Loss: 0.5322807715\n",
      "0.6225\n",
      "Epoch 655, Loss: 0.5263597886\n",
      "0.63625\n",
      "Epoch 656, Loss: 0.5311490975\n",
      "0.6325\n",
      "Epoch 657, Loss: 0.5449162419\n",
      "0.63625\n",
      "Epoch 658, Loss: 0.5551759977\n",
      "0.63\n",
      "Epoch 659, Loss: 0.5410232038\n",
      "0.62375\n",
      "Epoch 660, Loss: 0.5270748758\n",
      "0.62625\n",
      "Epoch 661, Loss: 0.5298181376\n",
      "0.6175\n",
      "Epoch 662, Loss: 0.5446502880\n",
      "0.6175\n",
      "Epoch 663, Loss: 0.6153130557\n",
      "0.6025\n",
      "Epoch 664, Loss: 0.6883264041\n",
      "0.585\n",
      "Epoch 665, Loss: 0.6128380046\n",
      "0.6025\n",
      "Epoch 666, Loss: 0.5853628118\n",
      "0.615\n",
      "Epoch 667, Loss: 0.6437548977\n",
      "0.615\n",
      "Epoch 668, Loss: 0.5920919701\n",
      "0.62\n",
      "Epoch 669, Loss: 0.5386320757\n",
      "0.61875\n",
      "Epoch 670, Loss: 0.6015478021\n",
      "0.6225\n",
      "Epoch 671, Loss: 0.6083172454\n",
      "0.63125\n",
      "Epoch 672, Loss: 0.5625951914\n",
      "0.635\n",
      "Epoch 673, Loss: 0.5905855842\n",
      "0.61875\n",
      "Epoch 674, Loss: 0.5960120393\n",
      "0.62125\n",
      "Epoch 675, Loss: 0.6083748574\n",
      "0.61125\n",
      "Epoch 676, Loss: 0.7845268646\n",
      "0.59375\n",
      "Epoch 677, Loss: 0.6838526717\n",
      "0.605\n",
      "Epoch 678, Loss: 0.7044239534\n",
      "0.58375\n",
      "Epoch 679, Loss: 0.6573591997\n",
      "0.615\n",
      "Epoch 680, Loss: 0.6251383386\n",
      "0.61875\n",
      "Epoch 681, Loss: 0.5697450868\n",
      "0.61875\n",
      "Epoch 682, Loss: 0.5674188718\n",
      "0.6075\n",
      "Epoch 683, Loss: 0.5749019087\n",
      "0.62125\n",
      "Epoch 684, Loss: 0.5714518692\n",
      "0.6225\n",
      "Epoch 685, Loss: 0.5741418698\n",
      "0.62125\n",
      "Epoch 686, Loss: 0.5756013715\n",
      "0.6175\n",
      "Epoch 687, Loss: 0.5915743190\n",
      "0.61375\n",
      "Epoch 688, Loss: 0.6110862298\n",
      "0.60375\n",
      "Epoch 689, Loss: 0.6006358460\n",
      "0.605\n",
      "Epoch 690, Loss: 0.5700414038\n",
      "0.605\n",
      "Epoch 691, Loss: 0.5496969962\n",
      "0.62\n",
      "Epoch 692, Loss: 0.5456057865\n",
      "0.61625\n",
      "Epoch 693, Loss: 0.5536458967\n",
      "0.6125\n",
      "Epoch 694, Loss: 0.5654298820\n",
      "0.60875\n",
      "Epoch 695, Loss: 0.5608547548\n",
      "0.60875\n",
      "Epoch 696, Loss: 0.5478676405\n",
      "0.60875\n",
      "Epoch 697, Loss: 0.5450894221\n",
      "0.6125\n",
      "Epoch 698, Loss: 0.5507071526\n",
      "0.615\n",
      "Epoch 699, Loss: 0.5676247289\n",
      "0.6175\n",
      "Epoch 700, Loss: 0.6124946406\n",
      "0.59\n",
      "Epoch 701, Loss: 0.6162798670\n",
      "0.5875\n",
      "Epoch 702, Loss: 0.5536966225\n",
      "0.61125\n",
      "Epoch 703, Loss: 0.5417222886\n",
      "0.60625\n",
      "Epoch 704, Loss: 0.5482934881\n",
      "0.60375\n",
      "Epoch 705, Loss: 0.5814213989\n",
      "0.60875\n",
      "Epoch 706, Loss: 0.6713916540\n",
      "0.585\n",
      "Epoch 707, Loss: 0.7713344362\n",
      "0.555\n",
      "Epoch 708, Loss: 0.5704871022\n",
      "0.5925\n",
      "Epoch 709, Loss: 0.5587982064\n",
      "0.61875\n",
      "Epoch 710, Loss: 0.6045479882\n",
      "0.60625\n",
      "Epoch 711, Loss: 0.6190832085\n",
      "0.60875\n",
      "Epoch 712, Loss: 0.5746745905\n",
      "0.62\n",
      "Epoch 713, Loss: 0.5377978828\n",
      "0.625\n",
      "Epoch 714, Loss: 0.5058746601\n",
      "0.63\n",
      "Epoch 715, Loss: 0.5063606584\n",
      "0.63375\n",
      "Epoch 716, Loss: 0.5231815862\n",
      "0.61875\n",
      "Epoch 717, Loss: 0.5494699730\n",
      "0.61375\n",
      "Epoch 718, Loss: 0.5791630486\n",
      "0.6075\n",
      "Epoch 719, Loss: 0.5985917464\n",
      "0.61125\n",
      "Epoch 720, Loss: 0.5871442697\n",
      "0.6075\n",
      "Epoch 721, Loss: 0.5258886662\n",
      "0.62375\n",
      "Epoch 722, Loss: 0.5605432639\n",
      "0.6125\n",
      "Epoch 723, Loss: 0.5148938176\n",
      "0.63\n",
      "Epoch 724, Loss: 0.5509730558\n",
      "0.6175\n",
      "Epoch 725, Loss: 0.5195766374\n",
      "0.6225\n",
      "Epoch 726, Loss: 0.5081085930\n",
      "0.6375\n",
      "Epoch 727, Loss: 0.5165141544\n",
      "0.6375\n",
      "Epoch 728, Loss: 0.5042299209\n",
      "0.64125\n",
      "Epoch 729, Loss: 0.5048792256\n",
      "0.6475\n",
      "Epoch 730, Loss: 0.5332979575\n",
      "0.63125\n",
      "Epoch 731, Loss: 0.5686501277\n",
      "0.63\n",
      "Epoch 732, Loss: 0.5574178736\n",
      "0.6275\n",
      "Epoch 733, Loss: 0.6106493512\n",
      "0.59875\n",
      "Epoch 734, Loss: 0.5662090772\n",
      "0.61875\n",
      "Epoch 735, Loss: 0.5617506929\n",
      "0.61125\n",
      "Epoch 736, Loss: 0.5954815550\n",
      "0.585\n",
      "Epoch 737, Loss: 0.6836998683\n",
      "0.565\n",
      "Epoch 738, Loss: 0.5473735563\n",
      "0.6075\n",
      "Epoch 739, Loss: 0.5066299350\n",
      "0.61875\n",
      "Epoch 740, Loss: 0.5125519602\n",
      "0.63\n",
      "Epoch 741, Loss: 0.5127278932\n",
      "0.62875\n",
      "Epoch 742, Loss: 0.5185831015\n",
      "0.6225\n",
      "Epoch 743, Loss: 0.5383102559\n",
      "0.62625\n",
      "Epoch 744, Loss: 0.5752367348\n",
      "0.60875\n",
      "Epoch 745, Loss: 0.5903535740\n",
      "0.605\n",
      "Epoch 746, Loss: 0.5259877410\n",
      "0.61125\n",
      "Epoch 747, Loss: 0.4843170099\n",
      "0.62375\n",
      "Epoch 748, Loss: 0.4827698211\n",
      "0.62375\n",
      "Epoch 749, Loss: 0.4944602210\n",
      "0.6225\n",
      "Epoch 750, Loss: 0.5204150665\n",
      "0.62125\n",
      "Epoch 751, Loss: 0.5637950180\n",
      "0.61\n",
      "Epoch 752, Loss: 0.5301111403\n",
      "0.60875\n",
      "Epoch 753, Loss: 0.5128754706\n",
      "0.63\n",
      "Epoch 754, Loss: 0.5681276146\n",
      "0.625\n",
      "Epoch 755, Loss: 0.5617819566\n",
      "0.62125\n",
      "Epoch 756, Loss: 0.5898941823\n",
      "0.60375\n",
      "Epoch 757, Loss: 0.6027468979\n",
      "0.6\n",
      "Epoch 758, Loss: 0.5024222629\n",
      "0.615\n",
      "Epoch 759, Loss: 0.5025014823\n",
      "0.61625\n",
      "Epoch 760, Loss: 0.5265648657\n",
      "0.61375\n",
      "Epoch 761, Loss: 0.5275357749\n",
      "0.62125\n",
      "Epoch 762, Loss: 0.5473287633\n",
      "0.61875\n",
      "Epoch 763, Loss: 0.5405883786\n",
      "0.61\n",
      "Epoch 764, Loss: 0.5477263079\n",
      "0.60125\n",
      "Epoch 765, Loss: 0.5897671406\n",
      "0.5975\n",
      "Epoch 766, Loss: 0.6106417944\n",
      "0.58\n",
      "Epoch 767, Loss: 0.6661430426\n",
      "0.59\n",
      "Epoch 768, Loss: 0.6884007025\n",
      "0.58\n",
      "Epoch 769, Loss: 0.6147045843\n",
      "0.59625\n",
      "Epoch 770, Loss: 0.5422291876\n",
      "0.6125\n",
      "Epoch 771, Loss: 0.5395245154\n",
      "0.60875\n",
      "Epoch 772, Loss: 0.5928870269\n",
      "0.5975\n",
      "Epoch 773, Loss: 0.6529405686\n",
      "0.58625\n",
      "Epoch 774, Loss: 0.5932502604\n",
      "0.595\n",
      "Epoch 775, Loss: 0.5195909665\n",
      "0.60875\n",
      "Epoch 776, Loss: 0.5277280588\n",
      "0.60125\n",
      "Epoch 777, Loss: 0.5646683399\n",
      "0.6\n",
      "Epoch 778, Loss: 0.6108167997\n",
      "0.60875\n",
      "Epoch 779, Loss: 0.6273217314\n",
      "0.6125\n",
      "Epoch 780, Loss: 0.5232639735\n",
      "0.62625\n",
      "Epoch 781, Loss: 0.4988545086\n",
      "0.61375\n",
      "Epoch 782, Loss: 0.5292477009\n",
      "0.61\n",
      "Epoch 783, Loss: 0.5716752208\n",
      "0.5975\n",
      "Epoch 784, Loss: 0.5840072698\n",
      "0.595\n",
      "Epoch 785, Loss: 0.5241328413\n",
      "0.6125\n",
      "Epoch 786, Loss: 0.4858570771\n",
      "0.62625\n",
      "Epoch 787, Loss: 0.4908847293\n",
      "0.63\n",
      "Epoch 788, Loss: 0.5257550944\n",
      "0.6375\n",
      "Epoch 789, Loss: 0.5387520321\n",
      "0.6325\n",
      "Epoch 790, Loss: 0.5428415594\n",
      "0.6225\n",
      "Epoch 791, Loss: 0.5539187879\n",
      "0.6125\n",
      "Epoch 792, Loss: 0.5901072097\n",
      "0.605\n",
      "Epoch 793, Loss: 0.6643804725\n",
      "0.585\n",
      "Epoch 794, Loss: 0.7091562687\n",
      "0.58375\n",
      "Epoch 795, Loss: 0.5604172098\n",
      "0.62\n",
      "Epoch 796, Loss: 0.4840481925\n",
      "0.62625\n",
      "Epoch 797, Loss: 0.4736103572\n",
      "0.62625\n",
      "Epoch 798, Loss: 0.4840408572\n",
      "0.6275\n",
      "Epoch 799, Loss: 0.4963422442\n",
      "0.6275\n",
      "Epoch 800, Loss: 0.5060297260\n",
      "0.62875\n",
      "Epoch 801, Loss: 0.5172669396\n",
      "0.625\n",
      "Epoch 802, Loss: 0.5320017251\n",
      "0.625\n",
      "Epoch 803, Loss: 0.5501884987\n",
      "0.61875\n",
      "Epoch 804, Loss: 0.5743466648\n",
      "0.60875\n",
      "Epoch 805, Loss: 0.6066597681\n",
      "0.59625\n",
      "Epoch 806, Loss: 0.6319639993\n",
      "0.58875\n",
      "Epoch 807, Loss: 0.5952507628\n",
      "0.60625\n",
      "Epoch 808, Loss: 0.4978193366\n",
      "0.62\n",
      "Epoch 809, Loss: 0.4560264627\n",
      "0.625\n",
      "Epoch 810, Loss: 0.4574813324\n",
      "0.6275\n",
      "Epoch 811, Loss: 0.4629903090\n",
      "0.6175\n",
      "Epoch 812, Loss: 0.4651182707\n",
      "0.61875\n",
      "Epoch 813, Loss: 0.4646786832\n",
      "0.62125\n",
      "Epoch 814, Loss: 0.4615895443\n",
      "0.61875\n",
      "Epoch 815, Loss: 0.4554037594\n",
      "0.6275\n",
      "Epoch 816, Loss: 0.4474858955\n",
      "0.63125\n",
      "Epoch 817, Loss: 0.4449456365\n",
      "0.6325\n",
      "Epoch 818, Loss: 0.4692226452\n",
      "0.635\n",
      "Epoch 819, Loss: 0.5180938418\n",
      "0.63\n",
      "Epoch 820, Loss: 0.4949000154\n",
      "0.61375\n",
      "Epoch 821, Loss: 0.4798901190\n",
      "0.62625\n",
      "Epoch 822, Loss: 0.4735408958\n",
      "0.62125\n",
      "Epoch 823, Loss: 0.4903897217\n",
      "0.615\n",
      "Epoch 824, Loss: 0.5240355148\n",
      "0.6025\n",
      "Epoch 825, Loss: 0.5832600921\n",
      "0.5975\n",
      "Epoch 826, Loss: 0.6543358799\n",
      "0.585\n",
      "Epoch 827, Loss: 0.6932198063\n",
      "0.575\n",
      "Epoch 828, Loss: 0.4622508973\n",
      "0.61375\n",
      "Epoch 829, Loss: 0.4571072509\n",
      "0.61875\n",
      "Epoch 830, Loss: 0.4853125438\n",
      "0.62\n",
      "Epoch 831, Loss: 0.4995652727\n",
      "0.62\n",
      "Epoch 832, Loss: 0.5074678828\n",
      "0.6225\n",
      "Epoch 833, Loss: 0.5161538054\n",
      "0.62125\n",
      "Epoch 834, Loss: 0.4670996379\n",
      "0.62375\n",
      "Epoch 835, Loss: 0.4600565889\n",
      "0.6075\n",
      "Epoch 836, Loss: 0.4824860556\n",
      "0.61125\n",
      "Epoch 837, Loss: 0.4922806789\n",
      "0.6075\n",
      "Epoch 838, Loss: 0.4800439376\n",
      "0.6125\n",
      "Epoch 839, Loss: 0.4516037674\n",
      "0.62\n",
      "Epoch 840, Loss: 0.4375450520\n",
      "0.625\n",
      "Epoch 841, Loss: 0.4403605455\n",
      "0.6225\n",
      "Epoch 842, Loss: 0.4661726313\n",
      "0.6275\n",
      "Epoch 843, Loss: 0.5622625570\n",
      "0.625\n",
      "Epoch 844, Loss: 0.5063018802\n",
      "0.625\n",
      "Epoch 845, Loss: 0.5086452197\n",
      "0.6025\n",
      "Epoch 846, Loss: 0.5334912307\n",
      "0.61375\n",
      "Epoch 847, Loss: 0.4848733053\n",
      "0.60875\n",
      "Epoch 848, Loss: 0.4696203530\n",
      "0.61875\n",
      "Epoch 849, Loss: 0.5072202729\n",
      "0.63\n",
      "Epoch 850, Loss: 0.4960461333\n",
      "0.62375\n",
      "Epoch 851, Loss: 0.4940471310\n",
      "0.61125\n",
      "Epoch 852, Loss: 0.5234569798\n",
      "0.59125\n",
      "Epoch 853, Loss: 0.4985920328\n",
      "0.61\n",
      "Epoch 854, Loss: 0.6749225142\n",
      "0.60125\n",
      "Epoch 855, Loss: 0.5553426606\n",
      "0.63125\n",
      "Epoch 856, Loss: 0.6075901818\n",
      "0.595\n",
      "Epoch 857, Loss: 0.6519721270\n",
      "0.605\n",
      "Epoch 858, Loss: 0.6387491188\n",
      "0.61125\n",
      "Epoch 859, Loss: 0.4745205339\n",
      "0.64875\n",
      "Epoch 860, Loss: 0.4989487902\n",
      "0.62375\n",
      "Epoch 861, Loss: 0.5417133175\n",
      "0.61625\n",
      "Epoch 862, Loss: 0.5666254930\n",
      "0.60125\n",
      "Epoch 863, Loss: 0.4692274303\n",
      "0.64125\n",
      "Epoch 864, Loss: 0.4600365185\n",
      "0.64125\n",
      "Epoch 865, Loss: 0.4555475875\n",
      "0.635\n",
      "Epoch 866, Loss: 0.4354972319\n",
      "0.63125\n",
      "Epoch 867, Loss: 0.4556767884\n",
      "0.62125\n",
      "Epoch 868, Loss: 0.5056913663\n",
      "0.62375\n",
      "Epoch 869, Loss: 0.5178425911\n",
      "0.62375\n",
      "Epoch 870, Loss: 0.4632127676\n",
      "0.64\n",
      "Epoch 871, Loss: 0.4629772797\n",
      "0.63875\n",
      "Epoch 872, Loss: 0.4535103248\n",
      "0.63\n",
      "Epoch 873, Loss: 0.4313172376\n",
      "0.64\n",
      "Epoch 874, Loss: 0.4262568669\n",
      "0.62625\n",
      "Epoch 875, Loss: 0.4191087206\n",
      "0.63375\n",
      "Epoch 876, Loss: 0.4907852125\n",
      "0.6275\n",
      "Epoch 877, Loss: 0.6475058219\n",
      "0.6125\n",
      "Epoch 878, Loss: 0.5716158814\n",
      "0.605\n",
      "Epoch 879, Loss: 0.5605395304\n",
      "0.605\n",
      "Epoch 880, Loss: 0.5481868134\n",
      "0.62125\n",
      "Epoch 881, Loss: 0.4656287446\n",
      "0.6325\n",
      "Epoch 882, Loss: 0.4400156808\n",
      "0.64\n",
      "Epoch 883, Loss: 0.4551795134\n",
      "0.64\n",
      "Epoch 884, Loss: 0.5109946934\n",
      "0.61875\n",
      "Epoch 885, Loss: 0.4726602313\n",
      "0.6325\n",
      "Epoch 886, Loss: 0.4656583846\n",
      "0.62375\n",
      "Epoch 887, Loss: 0.5110961564\n",
      "0.615\n",
      "Epoch 888, Loss: 0.4903421012\n",
      "0.6175\n",
      "Epoch 889, Loss: 0.5031672716\n",
      "0.625\n",
      "Epoch 890, Loss: 0.5140917950\n",
      "0.64125\n",
      "Epoch 891, Loss: 0.4569403728\n",
      "0.63625\n",
      "Epoch 892, Loss: 0.4317311765\n",
      "0.63875\n",
      "Epoch 893, Loss: 0.4330300154\n",
      "0.64375\n",
      "Epoch 894, Loss: 0.4353147657\n",
      "0.6325\n",
      "Epoch 895, Loss: 0.4356817820\n",
      "0.64125\n",
      "Epoch 896, Loss: 0.4413422447\n",
      "0.63625\n",
      "Epoch 897, Loss: 0.4572702179\n",
      "0.6325\n",
      "Epoch 898, Loss: 0.4804428446\n",
      "0.6325\n",
      "Epoch 899, Loss: 0.5039582055\n",
      "0.625\n",
      "Epoch 900, Loss: 0.5060683227\n",
      "0.6225\n",
      "Epoch 901, Loss: 0.4885037557\n",
      "0.62875\n",
      "Epoch 902, Loss: 0.4698713902\n",
      "0.62875\n",
      "Epoch 903, Loss: 0.4540627842\n",
      "0.63125\n",
      "Epoch 904, Loss: 0.4534775033\n",
      "0.64\n",
      "Epoch 905, Loss: 0.4315208025\n",
      "0.64125\n",
      "Epoch 906, Loss: 0.4506324448\n",
      "0.62\n",
      "Epoch 907, Loss: 0.4371703387\n",
      "0.62375\n",
      "Epoch 908, Loss: 0.4254078059\n",
      "0.63875\n",
      "Epoch 909, Loss: 0.4125706582\n",
      "0.62625\n",
      "Epoch 910, Loss: 0.4232055657\n",
      "0.62875\n",
      "Epoch 911, Loss: 0.4222743052\n",
      "0.6375\n",
      "Epoch 912, Loss: 0.4141175347\n",
      "0.635\n",
      "Epoch 913, Loss: 0.4242823712\n",
      "0.62875\n",
      "Epoch 914, Loss: 0.4338497863\n",
      "0.63125\n",
      "Epoch 915, Loss: 0.4356829443\n",
      "0.6375\n",
      "Epoch 916, Loss: 0.4472632734\n",
      "0.635\n",
      "Epoch 917, Loss: 0.4684985737\n",
      "0.64125\n",
      "Epoch 918, Loss: 0.4822957255\n",
      "0.6325\n",
      "Epoch 919, Loss: 0.4885110829\n",
      "0.63\n",
      "Epoch 920, Loss: 0.5055644659\n",
      "0.63125\n",
      "Epoch 921, Loss: 0.5192028300\n",
      "0.6325\n",
      "Epoch 922, Loss: 0.5051340210\n",
      "0.62125\n",
      "Epoch 923, Loss: 0.4707108668\n",
      "0.61875\n",
      "Epoch 924, Loss: 0.4760409656\n",
      "0.625\n",
      "Epoch 925, Loss: 0.4580421999\n",
      "0.62625\n",
      "Epoch 926, Loss: 0.4782381041\n",
      "0.61375\n",
      "Epoch 927, Loss: 0.4990693579\n",
      "0.61\n",
      "Epoch 928, Loss: 0.5126456694\n",
      "0.6125\n",
      "Epoch 929, Loss: 0.5084685299\n",
      "0.61625\n",
      "Epoch 930, Loss: 0.5392265105\n",
      "0.62125\n",
      "Epoch 931, Loss: 0.5926779714\n",
      "0.60625\n",
      "Epoch 932, Loss: 0.7221836359\n",
      "0.57875\n",
      "Epoch 933, Loss: 0.8893731136\n",
      "0.56625\n",
      "Epoch 934, Loss: 0.5693147657\n",
      "0.62875\n",
      "Epoch 935, Loss: 0.5214581389\n",
      "0.625\n",
      "Epoch 936, Loss: 0.4239789440\n",
      "0.64125\n",
      "Epoch 937, Loss: 0.3989549495\n",
      "0.64\n",
      "Epoch 938, Loss: 0.4093319284\n",
      "0.64375\n",
      "Epoch 939, Loss: 0.4280054099\n",
      "0.6475\n",
      "Epoch 940, Loss: 0.4401565520\n",
      "0.64375\n",
      "Epoch 941, Loss: 0.4634791761\n",
      "0.63625\n",
      "Epoch 942, Loss: 0.4945052241\n",
      "0.6275\n",
      "Epoch 943, Loss: 0.4891798459\n",
      "0.62625\n",
      "Epoch 944, Loss: 0.4288182373\n",
      "0.64125\n",
      "Epoch 945, Loss: 0.3957344220\n",
      "0.65\n",
      "Epoch 946, Loss: 0.5580148396\n",
      "0.5925\n",
      "Epoch 947, Loss: 0.4721370005\n",
      "0.62625\n",
      "Epoch 948, Loss: 0.4854865428\n",
      "0.625\n",
      "Epoch 949, Loss: 0.4811962184\n",
      "0.62875\n",
      "Epoch 950, Loss: 0.5151949284\n",
      "0.63625\n",
      "Epoch 951, Loss: 0.5663619874\n",
      "0.6275\n",
      "Epoch 952, Loss: 0.6623468694\n",
      "0.595\n",
      "Epoch 953, Loss: 0.4673759082\n",
      "0.62625\n",
      "Epoch 954, Loss: 0.3975939977\n",
      "0.635\n",
      "Epoch 955, Loss: 0.3914126623\n",
      "0.6425\n",
      "Epoch 956, Loss: 0.4103553606\n",
      "0.6425\n",
      "Epoch 957, Loss: 0.4450354037\n",
      "0.63625\n",
      "Epoch 958, Loss: 0.4622306060\n",
      "0.63125\n",
      "Epoch 959, Loss: 0.4522895703\n",
      "0.62375\n",
      "Epoch 960, Loss: 0.4066297687\n",
      "0.63125\n",
      "Epoch 961, Loss: 0.3770803153\n",
      "0.64375\n",
      "Epoch 962, Loss: 0.3947804087\n",
      "0.635\n",
      "Epoch 963, Loss: 0.3850166801\n",
      "0.63125\n",
      "Epoch 964, Loss: 0.3884163959\n",
      "0.64\n",
      "Epoch 965, Loss: 0.4695044683\n",
      "0.62875\n",
      "Epoch 966, Loss: 0.4532226885\n",
      "0.6275\n",
      "Epoch 967, Loss: 0.3997102808\n",
      "0.64125\n",
      "Epoch 968, Loss: 0.3787995220\n",
      "0.6375\n",
      "Epoch 969, Loss: 0.3772287407\n",
      "0.65\n",
      "Epoch 970, Loss: 0.3669307189\n",
      "0.6475\n",
      "Epoch 971, Loss: 0.3740374161\n",
      "0.64\n",
      "Epoch 972, Loss: 0.4133539785\n",
      "0.64125\n",
      "Epoch 973, Loss: 0.4515696348\n",
      "0.625\n",
      "Epoch 974, Loss: 0.4791353935\n",
      "0.6275\n",
      "Epoch 975, Loss: 0.4815081737\n",
      "0.6225\n",
      "Epoch 976, Loss: 0.4619484417\n",
      "0.62875\n",
      "Epoch 977, Loss: 0.4146776976\n",
      "0.63\n",
      "Epoch 978, Loss: 0.3709828467\n",
      "0.64125\n",
      "Epoch 979, Loss: 0.3631873955\n",
      "0.64875\n",
      "Epoch 980, Loss: 0.4211444866\n",
      "0.6225\n",
      "Epoch 981, Loss: 0.8205060951\n",
      "0.5375\n",
      "Epoch 982, Loss: 0.4761292045\n",
      "0.61\n",
      "Epoch 983, Loss: 0.4669366672\n",
      "0.6125\n",
      "Epoch 984, Loss: 0.4316377371\n",
      "0.645\n",
      "Epoch 985, Loss: 0.4518867835\n",
      "0.6275\n",
      "Epoch 986, Loss: 0.4716920688\n",
      "0.62875\n",
      "Epoch 987, Loss: 0.4969417243\n",
      "0.62125\n",
      "Epoch 988, Loss: 0.6404021776\n",
      "0.58625\n",
      "Epoch 989, Loss: 0.5037543850\n",
      "0.625\n",
      "Epoch 990, Loss: 0.4625011901\n",
      "0.63\n",
      "Epoch 991, Loss: 0.4541041971\n",
      "0.62125\n",
      "Epoch 992, Loss: 0.4866198153\n",
      "0.63125\n",
      "Epoch 993, Loss: 0.5010499591\n",
      "0.61625\n",
      "Epoch 994, Loss: 0.4802316981\n",
      "0.62625\n",
      "Epoch 995, Loss: 0.4876442612\n",
      "0.6275\n",
      "Epoch 996, Loss: 0.5319553929\n",
      "0.61875\n",
      "Epoch 997, Loss: 0.5814547958\n",
      "0.60875\n",
      "Epoch 998, Loss: 0.5733095072\n",
      "0.61625\n",
      "Epoch 999, Loss: 0.4818460062\n",
      "0.6375\n",
      "Epoch 1000, Loss: 0.4688737905\n",
      "0.645\n",
      "Epoch 1001, Loss: 0.5674842177\n",
      "0.63625\n",
      "Epoch 1002, Loss: 0.5117280935\n",
      "0.6375\n",
      "Epoch 1003, Loss: 0.4690587144\n",
      "0.63\n",
      "Epoch 1004, Loss: 0.4364514992\n",
      "0.63\n",
      "Epoch 1005, Loss: 0.5130565749\n",
      "0.62375\n",
      "Epoch 1006, Loss: 0.4868541584\n",
      "0.63625\n",
      "Epoch 1007, Loss: 0.4395276244\n",
      "0.6425\n",
      "Epoch 1008, Loss: 0.4344301518\n",
      "0.64125\n",
      "Epoch 1009, Loss: 0.4818620393\n",
      "0.63625\n",
      "Epoch 1010, Loss: 0.5459359582\n",
      "0.625\n",
      "Epoch 1011, Loss: 0.5453432811\n",
      "0.62625\n",
      "Epoch 1012, Loss: 0.4370057013\n",
      "0.64\n",
      "Epoch 1013, Loss: 0.3732934874\n",
      "0.64875\n",
      "Epoch 1014, Loss: 0.3702301164\n",
      "0.64625\n",
      "Epoch 1015, Loss: 0.3886387994\n",
      "0.655\n",
      "Epoch 1016, Loss: 0.4543817336\n",
      "0.64375\n",
      "Epoch 1017, Loss: 0.5080417654\n",
      "0.62625\n",
      "Epoch 1018, Loss: 0.4310766104\n",
      "0.6425\n",
      "Epoch 1019, Loss: 0.3649526559\n",
      "0.6525\n",
      "Epoch 1020, Loss: 0.3628390914\n",
      "0.6325\n",
      "Epoch 1021, Loss: 0.3779929359\n",
      "0.625\n",
      "Epoch 1022, Loss: 0.3774008274\n",
      "0.6325\n",
      "Epoch 1023, Loss: 0.3913695399\n",
      "0.63125\n",
      "Epoch 1024, Loss: 0.4406412492\n",
      "0.6425\n",
      "Epoch 1025, Loss: 0.5172945065\n",
      "0.63375\n",
      "Epoch 1026, Loss: 0.6021003798\n",
      "0.61875\n",
      "Epoch 1027, Loss: 0.4703477202\n",
      "0.64125\n",
      "Epoch 1028, Loss: 0.3753587415\n",
      "0.635\n",
      "Epoch 1029, Loss: 0.3768828637\n",
      "0.63125\n",
      "Epoch 1030, Loss: 0.3803337609\n",
      "0.6275\n",
      "Epoch 1031, Loss: 0.3573372034\n",
      "0.64125\n",
      "Epoch 1032, Loss: 0.4185763711\n",
      "0.6425\n",
      "Epoch 1033, Loss: 0.5708451047\n",
      "0.61875\n",
      "Epoch 1034, Loss: 0.5017270547\n",
      "0.625\n",
      "Epoch 1035, Loss: 0.3956276169\n",
      "0.62625\n",
      "Epoch 1036, Loss: 0.4106669720\n",
      "0.61625\n",
      "Epoch 1037, Loss: 0.3773387169\n",
      "0.63125\n",
      "Epoch 1038, Loss: 0.5777646575\n",
      "0.61625\n",
      "Epoch 1039, Loss: 0.7663644200\n",
      "0.625\n",
      "Epoch 1040, Loss: 0.5367161950\n",
      "0.61125\n",
      "Epoch 1041, Loss: 0.4961561102\n",
      "0.62\n",
      "Epoch 1042, Loss: 0.5424023683\n",
      "0.64625\n",
      "Epoch 1043, Loss: 0.4375084571\n",
      "0.65\n",
      "Epoch 1044, Loss: 0.4231420556\n",
      "0.6375\n",
      "Epoch 1045, Loss: 0.4579410137\n",
      "0.635\n",
      "Epoch 1046, Loss: 0.4820158515\n",
      "0.64\n",
      "Epoch 1047, Loss: 0.4704974555\n",
      "0.635\n",
      "Epoch 1048, Loss: 0.4510581252\n",
      "0.64125\n",
      "Epoch 1049, Loss: 0.4230215386\n",
      "0.64875\n",
      "Epoch 1050, Loss: 0.5304548480\n",
      "0.62375\n",
      "Epoch 1051, Loss: 0.4986532780\n",
      "0.63875\n",
      "Epoch 1052, Loss: 0.4478023663\n",
      "0.64875\n",
      "Epoch 1053, Loss: 0.4557856895\n",
      "0.63875\n",
      "Epoch 1054, Loss: 0.6264813681\n",
      "0.60875\n",
      "Epoch 1055, Loss: 0.6617113530\n",
      "0.595\n",
      "Epoch 1056, Loss: 0.5697143252\n",
      "0.59375\n",
      "Epoch 1057, Loss: 0.5050860348\n",
      "0.6025\n",
      "Epoch 1058, Loss: 0.4099534274\n",
      "0.63875\n",
      "Epoch 1059, Loss: 0.5308222934\n",
      "0.63\n",
      "Epoch 1060, Loss: 0.6364677100\n",
      "0.605\n",
      "Epoch 1061, Loss: 0.6542302542\n",
      "0.595\n",
      "Epoch 1062, Loss: 0.8171950024\n",
      "0.5725\n",
      "Epoch 1063, Loss: 0.5128586383\n",
      "0.6375\n",
      "Epoch 1064, Loss: 0.4204696423\n",
      "0.645\n",
      "Epoch 1065, Loss: 0.5605866748\n",
      "0.64125\n",
      "Epoch 1066, Loss: 0.4881597926\n",
      "0.6475\n",
      "Epoch 1067, Loss: 0.4545830644\n",
      "0.6475\n",
      "Epoch 1068, Loss: 0.4660331820\n",
      "0.64375\n",
      "Epoch 1069, Loss: 0.4996335502\n",
      "0.6425\n",
      "Epoch 1070, Loss: 0.5138947303\n",
      "0.63625\n",
      "Epoch 1071, Loss: 0.5406247475\n",
      "0.63\n",
      "Epoch 1072, Loss: 0.5562505503\n",
      "0.6375\n",
      "Epoch 1073, Loss: 0.5562885098\n",
      "0.64\n",
      "Epoch 1074, Loss: 0.5332072963\n",
      "0.62875\n",
      "Epoch 1075, Loss: 0.4694484472\n",
      "0.635\n",
      "Epoch 1076, Loss: 0.4067139850\n",
      "0.65\n",
      "Epoch 1077, Loss: 0.4196185200\n",
      "0.64375\n",
      "Epoch 1078, Loss: 0.4202108166\n",
      "0.655\n",
      "Epoch 1079, Loss: 0.4484205545\n",
      "0.65375\n",
      "Epoch 1080, Loss: 0.4958792451\n",
      "0.63375\n",
      "Epoch 1081, Loss: 0.5096144488\n",
      "0.6225\n",
      "Epoch 1082, Loss: 0.4227792030\n",
      "0.645\n",
      "Epoch 1083, Loss: 0.4323558003\n",
      "0.6425\n",
      "Epoch 1084, Loss: 0.4406432835\n",
      "0.64125\n",
      "Epoch 1085, Loss: 0.4161194272\n",
      "0.6575\n",
      "Epoch 1086, Loss: 0.4165739852\n",
      "0.655\n",
      "Epoch 1087, Loss: 0.4381706079\n",
      "0.6475\n",
      "Epoch 1088, Loss: 0.4553729573\n",
      "0.64875\n",
      "Epoch 1089, Loss: 0.5060776430\n",
      "0.63\n",
      "Epoch 1090, Loss: 0.5877003394\n",
      "0.6225\n",
      "Epoch 1091, Loss: 0.5971795526\n",
      "0.59125\n",
      "Epoch 1092, Loss: 0.4075319434\n",
      "0.63375\n",
      "Epoch 1093, Loss: 0.4746383382\n",
      "0.62625\n",
      "Epoch 1094, Loss: 0.5052717208\n",
      "0.64875\n",
      "Epoch 1095, Loss: 0.4588538014\n",
      "0.64\n",
      "Epoch 1096, Loss: 0.4068197821\n",
      "0.6425\n",
      "Epoch 1097, Loss: 0.4109771897\n",
      "0.64125\n",
      "Epoch 1098, Loss: 0.4344808380\n",
      "0.65125\n",
      "Epoch 1099, Loss: 0.4916085532\n",
      "0.63625\n",
      "Epoch 1100, Loss: 0.5493100714\n",
      "0.6225\n",
      "Epoch 1101, Loss: 0.5398671705\n",
      "0.62\n",
      "Epoch 1102, Loss: 0.4122538977\n",
      "0.65125\n",
      "Epoch 1103, Loss: 0.3275106133\n",
      "0.6425\n",
      "Epoch 1104, Loss: 0.3740522100\n",
      "0.6375\n",
      "Epoch 1105, Loss: 0.4806191199\n",
      "0.6275\n",
      "Epoch 1106, Loss: 0.6735677863\n",
      "0.60375\n",
      "Epoch 1107, Loss: 0.5832712563\n",
      "0.62875\n",
      "Epoch 1108, Loss: 0.3757000684\n",
      "0.66\n",
      "Epoch 1109, Loss: 0.3963703200\n",
      "0.6575\n",
      "Epoch 1110, Loss: 0.5014473083\n",
      "0.62625\n",
      "Epoch 1111, Loss: 0.4872505990\n",
      "0.62875\n",
      "Epoch 1112, Loss: 0.4410266866\n",
      "0.64125\n",
      "Epoch 1113, Loss: 0.4420100707\n",
      "0.64\n",
      "Epoch 1114, Loss: 0.4438887579\n",
      "0.63875\n",
      "Epoch 1115, Loss: 0.5188810423\n",
      "0.63\n",
      "Epoch 1116, Loss: 0.7322481136\n",
      "0.5875\n",
      "Epoch 1117, Loss: 0.8099600099\n",
      "0.6\n",
      "Epoch 1118, Loss: 0.3775705009\n",
      "0.64875\n",
      "Epoch 1119, Loss: 0.4867374419\n",
      "0.62875\n",
      "Epoch 1120, Loss: 0.6632196860\n",
      "0.5875\n",
      "Epoch 1121, Loss: 0.5123217563\n",
      "0.63\n",
      "Epoch 1122, Loss: 0.4313557450\n",
      "0.65125\n",
      "Epoch 1123, Loss: 0.6352800869\n",
      "0.6125\n",
      "Epoch 1124, Loss: 0.5475062083\n",
      "0.61875\n",
      "Epoch 1125, Loss: 0.4747006468\n",
      "0.625\n",
      "Epoch 1126, Loss: 0.5009345230\n",
      "0.63125\n",
      "Epoch 1127, Loss: 0.5272914439\n",
      "0.6375\n",
      "Epoch 1128, Loss: 0.6145379372\n",
      "0.62375\n",
      "Epoch 1129, Loss: 0.4863493270\n",
      "0.645\n",
      "Epoch 1130, Loss: 0.5459018321\n",
      "0.62625\n",
      "Epoch 1131, Loss: 0.4542506855\n",
      "0.6375\n",
      "Epoch 1132, Loss: 0.4304967108\n",
      "0.645\n",
      "Epoch 1133, Loss: 0.4230700378\n",
      "0.6475\n",
      "Epoch 1134, Loss: 0.4064616137\n",
      "0.64875\n",
      "Epoch 1135, Loss: 0.4925456553\n",
      "0.6375\n",
      "Epoch 1136, Loss: 0.5808977136\n",
      "0.63875\n",
      "Epoch 1137, Loss: 0.5427734687\n",
      "0.63625\n",
      "Epoch 1138, Loss: 0.4317341479\n",
      "0.65125\n",
      "Epoch 1139, Loss: 0.3934887541\n",
      "0.65875\n",
      "Epoch 1140, Loss: 0.4005847042\n",
      "0.655\n",
      "Epoch 1141, Loss: 0.4142653958\n",
      "0.6575\n",
      "Epoch 1142, Loss: 0.4225320499\n",
      "0.66\n",
      "Epoch 1143, Loss: 0.4328151475\n",
      "0.65\n",
      "Epoch 1144, Loss: 0.4448915714\n",
      "0.64875\n",
      "Epoch 1145, Loss: 0.4589513231\n",
      "0.64625\n",
      "Epoch 1146, Loss: 0.4878841216\n",
      "0.63375\n",
      "Epoch 1147, Loss: 0.5354527664\n",
      "0.6225\n",
      "Epoch 1148, Loss: 0.5187724970\n",
      "0.62625\n",
      "Epoch 1149, Loss: 0.3869113039\n",
      "0.6425\n",
      "Epoch 1150, Loss: 0.3849457673\n",
      "0.6475\n",
      "Epoch 1151, Loss: 0.4344817470\n",
      "0.65\n",
      "Epoch 1152, Loss: 0.6755972395\n",
      "0.61\n",
      "Epoch 1153, Loss: 0.6100654926\n",
      "0.615\n",
      "Epoch 1154, Loss: 0.5053679334\n",
      "0.63375\n",
      "Epoch 1155, Loss: 0.4677154054\n",
      "0.655\n",
      "Epoch 1156, Loss: 0.5198628649\n",
      "0.6525\n",
      "Epoch 1157, Loss: 0.3591016608\n",
      "0.65625\n",
      "Epoch 1158, Loss: 0.4298464644\n",
      "0.6425\n",
      "Epoch 1159, Loss: 0.4758078936\n",
      "0.6425\n",
      "Epoch 1160, Loss: 0.4313158914\n",
      "0.6475\n",
      "Epoch 1161, Loss: 0.4230412711\n",
      "0.6475\n",
      "Epoch 1162, Loss: 0.4374977832\n",
      "0.6425\n",
      "Epoch 1163, Loss: 0.4515103293\n",
      "0.65\n",
      "Epoch 1164, Loss: 0.4420800027\n",
      "0.65\n",
      "Epoch 1165, Loss: 0.4267771191\n",
      "0.6525\n",
      "Epoch 1166, Loss: 0.4286582892\n",
      "0.65625\n",
      "Epoch 1167, Loss: 0.4432511712\n",
      "0.65625\n",
      "Epoch 1168, Loss: 0.4451260419\n",
      "0.655\n",
      "Epoch 1169, Loss: 0.4316139734\n",
      "0.65375\n",
      "Epoch 1170, Loss: 0.4754670071\n",
      "0.6325\n",
      "Epoch 1171, Loss: 0.5221684558\n",
      "0.62625\n",
      "Epoch 1172, Loss: 0.4388863085\n",
      "0.6425\n",
      "Epoch 1173, Loss: 0.3907013165\n",
      "0.62875\n",
      "Epoch 1174, Loss: 0.3957080625\n",
      "0.6275\n",
      "Epoch 1175, Loss: 0.3764040674\n",
      "0.64625\n",
      "Epoch 1176, Loss: 0.3671417046\n",
      "0.65625\n",
      "Epoch 1177, Loss: 0.4240611320\n",
      "0.645\n",
      "Epoch 1178, Loss: 0.5059085622\n",
      "0.6275\n",
      "Epoch 1179, Loss: 0.4773342586\n",
      "0.61875\n",
      "Epoch 1180, Loss: 0.3711839106\n",
      "0.64875\n",
      "Epoch 1181, Loss: 0.3676888752\n",
      "0.64\n",
      "Epoch 1182, Loss: 0.3975230554\n",
      "0.6375\n",
      "Epoch 1183, Loss: 0.4180947337\n",
      "0.62375\n",
      "Epoch 1184, Loss: 0.3866749689\n",
      "0.64125\n",
      "Epoch 1185, Loss: 0.3386760686\n",
      "0.64375\n",
      "Epoch 1186, Loss: 0.3766848259\n",
      "0.635\n",
      "Epoch 1187, Loss: 0.4336367416\n",
      "0.62875\n",
      "Epoch 1188, Loss: 0.5091850681\n",
      "0.59875\n",
      "Epoch 1189, Loss: 0.5807653888\n",
      "0.57875\n",
      "Epoch 1190, Loss: 0.4383327296\n",
      "0.61125\n",
      "Epoch 1191, Loss: 0.5198452446\n",
      "0.5975\n",
      "Epoch 1192, Loss: 0.4698526149\n",
      "0.605\n",
      "Epoch 1193, Loss: 0.4079237391\n",
      "0.6375\n",
      "Epoch 1194, Loss: 0.4268500593\n",
      "0.61125\n",
      "Epoch 1195, Loss: 0.4000565665\n",
      "0.6475\n",
      "Epoch 1196, Loss: 0.6008205882\n",
      "0.61375\n",
      "Epoch 1197, Loss: 0.5435456696\n",
      "0.62\n",
      "Epoch 1198, Loss: 0.4512158913\n",
      "0.6525\n",
      "Epoch 1199, Loss: 0.4999800291\n",
      "0.6375\n",
      "Epoch 1200, Loss: 0.7517194664\n",
      "0.62\n",
      "Epoch 1201, Loss: 0.5303073861\n",
      "0.6175\n",
      "Epoch 1202, Loss: 0.4160816661\n",
      "0.61375\n",
      "Epoch 1203, Loss: 0.4192836948\n",
      "0.6375\n",
      "Epoch 1204, Loss: 0.4082171613\n",
      "0.65125\n",
      "Epoch 1205, Loss: 0.4560119979\n",
      "0.63875\n",
      "Epoch 1206, Loss: 0.4462622554\n",
      "0.64625\n",
      "Epoch 1207, Loss: 0.4533741893\n",
      "0.655\n",
      "Epoch 1208, Loss: 0.4636187676\n",
      "0.65875\n",
      "Epoch 1209, Loss: 0.5149216936\n",
      "0.64375\n",
      "Epoch 1210, Loss: 0.6850007712\n",
      "0.615\n",
      "Epoch 1211, Loss: 0.5540761858\n",
      "0.62875\n",
      "Epoch 1212, Loss: 0.3890395061\n",
      "0.64\n",
      "Epoch 1213, Loss: 0.4036194049\n",
      "0.635\n",
      "Epoch 1214, Loss: 0.3867791324\n",
      "0.63\n",
      "Epoch 1215, Loss: 0.4823130125\n",
      "0.64125\n",
      "Epoch 1216, Loss: 0.4734674811\n",
      "0.64\n",
      "Epoch 1217, Loss: 0.6471905979\n",
      "0.6275\n",
      "Epoch 1218, Loss: 0.4650782598\n",
      "0.6575\n",
      "Epoch 1219, Loss: 0.4805376937\n",
      "0.64375\n",
      "Epoch 1220, Loss: 0.6071587486\n",
      "0.61\n",
      "Epoch 1221, Loss: 0.6799611099\n",
      "0.6075\n",
      "Epoch 1222, Loss: 0.4888384251\n",
      "0.63625\n",
      "Epoch 1223, Loss: 0.3802681201\n",
      "0.64625\n",
      "Epoch 1224, Loss: 0.5128607083\n",
      "0.59375\n",
      "Epoch 1225, Loss: 0.4276852340\n",
      "0.6325\n",
      "Epoch 1226, Loss: 0.4589463063\n",
      "0.635\n",
      "Epoch 1227, Loss: 0.7155519045\n",
      "0.58875\n",
      "Epoch 1228, Loss: 0.4408566757\n",
      "0.62625\n",
      "Epoch 1229, Loss: 0.6474147444\n",
      "0.58125\n",
      "Epoch 1230, Loss: 0.4857405999\n",
      "0.63875\n",
      "Epoch 1231, Loss: 0.5605402340\n",
      "0.5875\n",
      "Epoch 1232, Loss: 0.5723831936\n",
      "0.62\n",
      "Epoch 1233, Loss: 0.5831742349\n",
      "0.61875\n",
      "Epoch 1234, Loss: 0.6467776158\n",
      "0.61125\n",
      "Epoch 1235, Loss: 0.7254789211\n",
      "0.57375\n",
      "Epoch 1236, Loss: 0.5549989643\n",
      "0.605\n",
      "Epoch 1237, Loss: 0.4755464163\n",
      "0.615\n",
      "Epoch 1238, Loss: 0.5814883435\n",
      "0.58125\n",
      "Epoch 1239, Loss: 0.8086791976\n",
      "0.55\n",
      "Epoch 1240, Loss: 0.6777782807\n",
      "0.58875\n",
      "Epoch 1241, Loss: 0.4878819358\n",
      "0.6375\n",
      "Epoch 1242, Loss: 0.4921909007\n",
      "0.61625\n",
      "Epoch 1243, Loss: 0.3973009296\n",
      "0.64125\n",
      "Epoch 1244, Loss: 0.4351336333\n",
      "0.62125\n",
      "Epoch 1245, Loss: 0.4932874053\n",
      "0.6175\n",
      "Epoch 1246, Loss: 0.4769401254\n",
      "0.6225\n",
      "Epoch 1247, Loss: 0.3887009090\n",
      "0.64125\n",
      "Epoch 1248, Loss: 0.3541695993\n",
      "0.6475\n",
      "Epoch 1249, Loss: 0.4377857435\n",
      "0.625\n",
      "Epoch 1250, Loss: 0.5865941247\n",
      "0.58\n",
      "Epoch 1251, Loss: 0.5096705853\n",
      "0.6175\n",
      "Epoch 1252, Loss: 0.3985968928\n",
      "0.64625\n",
      "Epoch 1253, Loss: 0.3981281889\n",
      "0.63125\n",
      "Epoch 1254, Loss: 0.4424993063\n",
      "0.63375\n",
      "Epoch 1255, Loss: 0.4311373232\n",
      "0.63125\n",
      "Epoch 1256, Loss: 0.4154757891\n",
      "0.63625\n",
      "Epoch 1257, Loss: 0.4177634280\n",
      "0.63375\n",
      "Epoch 1258, Loss: 0.4404962265\n",
      "0.61875\n",
      "Epoch 1259, Loss: 0.4240398921\n",
      "0.62125\n",
      "Epoch 1260, Loss: 0.3847537877\n",
      "0.6425\n",
      "Epoch 1261, Loss: 0.3717953965\n",
      "0.64\n",
      "Epoch 1262, Loss: 0.4286619751\n",
      "0.6175\n",
      "Epoch 1263, Loss: 0.4257463805\n",
      "0.62\n",
      "Epoch 1264, Loss: 0.3647033846\n",
      "0.64125\n",
      "Epoch 1265, Loss: 0.3736015228\n",
      "0.6475\n",
      "Epoch 1266, Loss: 0.4267433141\n",
      "0.6275\n",
      "Epoch 1267, Loss: 0.4823459951\n",
      "0.6075\n",
      "Epoch 1268, Loss: 0.4395101864\n",
      "0.61875\n",
      "Epoch 1269, Loss: 0.3840598289\n",
      "0.63375\n",
      "Epoch 1270, Loss: 0.3287046925\n",
      "0.65375\n",
      "Epoch 1271, Loss: 0.3322722092\n",
      "0.65\n",
      "Epoch 1272, Loss: 0.3800607619\n",
      "0.65\n",
      "Epoch 1273, Loss: 0.4684174523\n",
      "0.62\n",
      "Epoch 1274, Loss: 0.6351798010\n",
      "0.58875\n",
      "Epoch 1275, Loss: 0.5812275835\n",
      "0.575\n",
      "Epoch 1276, Loss: 0.4430107172\n",
      "0.635\n",
      "Epoch 1277, Loss: 0.4114255677\n",
      "0.63375\n",
      "Epoch 1278, Loss: 0.4294808882\n",
      "0.6575\n",
      "Epoch 1279, Loss: 0.3689860743\n",
      "0.645\n",
      "Epoch 1280, Loss: 0.3515538042\n",
      "0.6475\n",
      "Epoch 1281, Loss: 0.3372907920\n",
      "0.655\n",
      "Epoch 1282, Loss: 0.3665024523\n",
      "0.65625\n",
      "Epoch 1283, Loss: 0.4055841448\n",
      "0.64875\n",
      "Epoch 1284, Loss: 0.4883498449\n",
      "0.62\n",
      "Epoch 1285, Loss: 0.5511405998\n",
      "0.59625\n",
      "Epoch 1286, Loss: 0.4214670952\n",
      "0.6125\n",
      "Epoch 1287, Loss: 0.4122641143\n",
      "0.635\n",
      "Epoch 1288, Loss: 0.3622386096\n",
      "0.64\n",
      "Epoch 1289, Loss: 0.3873130276\n",
      "0.645\n",
      "Epoch 1290, Loss: 0.4184933176\n",
      "0.65125\n",
      "Epoch 1291, Loss: 0.4137231160\n",
      "0.6475\n",
      "Epoch 1292, Loss: 0.3959136652\n",
      "0.63625\n",
      "Epoch 1293, Loss: 0.3711295949\n",
      "0.65\n",
      "Epoch 1294, Loss: 0.4810375571\n",
      "0.6275\n",
      "Epoch 1295, Loss: 0.4846715496\n",
      "0.62\n",
      "Epoch 1296, Loss: 0.3547961972\n",
      "0.65125\n",
      "Epoch 1297, Loss: 0.4176396569\n",
      "0.6325\n",
      "Epoch 1298, Loss: 0.5211407944\n",
      "0.59875\n",
      "Epoch 1299, Loss: 0.3742278705\n",
      "0.63125\n",
      "Epoch 1300, Loss: 0.3248183872\n",
      "0.6425\n",
      "Epoch 1301, Loss: 0.4024438797\n",
      "0.62875\n",
      "Epoch 1302, Loss: 0.3616518565\n",
      "0.65125\n",
      "Epoch 1303, Loss: 0.3939367653\n",
      "0.65375\n",
      "Epoch 1304, Loss: 0.4176696576\n",
      "0.64125\n",
      "Epoch 1305, Loss: 0.4412355044\n",
      "0.63625\n",
      "Epoch 1306, Loss: 0.4152848682\n",
      "0.645\n",
      "Epoch 1307, Loss: 0.3773637366\n",
      "0.65125\n",
      "Epoch 1308, Loss: 0.3277577080\n",
      "0.66875\n",
      "Epoch 1309, Loss: 0.2943056110\n",
      "0.6575\n",
      "Epoch 1310, Loss: 0.3020101569\n",
      "0.6575\n",
      "Epoch 1311, Loss: 0.3254076079\n",
      "0.655\n",
      "Epoch 1312, Loss: 0.3493766874\n",
      "0.64125\n",
      "Epoch 1313, Loss: 0.3801334096\n",
      "0.64125\n",
      "Epoch 1314, Loss: 0.4698160015\n",
      "0.62375\n",
      "Epoch 1315, Loss: 0.6022052218\n",
      "0.61125\n",
      "Epoch 1316, Loss: 0.3515853510\n",
      "0.65875\n",
      "Epoch 1317, Loss: 0.3127448886\n",
      "0.6575\n",
      "Epoch 1318, Loss: 0.3171763907\n",
      "0.66125\n",
      "Epoch 1319, Loss: 0.2995768810\n",
      "0.67375\n",
      "Epoch 1320, Loss: 0.3107096881\n",
      "0.65625\n",
      "Epoch 1321, Loss: 0.3594259643\n",
      "0.65\n",
      "Epoch 1322, Loss: 0.4104684445\n",
      "0.63\n",
      "Epoch 1323, Loss: 0.4104041261\n",
      "0.63125\n",
      "Epoch 1324, Loss: 0.3926703027\n",
      "0.655\n",
      "Epoch 1325, Loss: 0.3415383955\n",
      "0.655\n",
      "Epoch 1326, Loss: 0.3540613203\n",
      "0.64625\n",
      "Epoch 1327, Loss: 0.3818842593\n",
      "0.6525\n",
      "Epoch 1328, Loss: 0.4079008179\n",
      "0.6425\n",
      "Epoch 1329, Loss: 0.4558552934\n",
      "0.6275\n",
      "Epoch 1330, Loss: 0.4680898551\n",
      "0.63\n",
      "Epoch 1331, Loss: 0.4260656481\n",
      "0.64625\n",
      "Epoch 1332, Loss: 0.3543092888\n",
      "0.6575\n",
      "Epoch 1333, Loss: 0.3713394971\n",
      "0.6375\n",
      "Epoch 1334, Loss: 0.3723886733\n",
      "0.6375\n",
      "Epoch 1335, Loss: 0.3399636048\n",
      "0.64875\n",
      "Epoch 1336, Loss: 0.4300341845\n",
      "0.635\n",
      "Epoch 1337, Loss: 0.3878316929\n",
      "0.635\n",
      "Epoch 1338, Loss: 0.3197878045\n",
      "0.6475\n",
      "Epoch 1339, Loss: 0.3360639911\n",
      "0.645\n",
      "Epoch 1340, Loss: 0.3994209431\n",
      "0.6225\n",
      "Epoch 1341, Loss: 0.4614142439\n",
      "0.6175\n",
      "Epoch 1342, Loss: 0.5251929332\n",
      "0.6025\n",
      "Epoch 1343, Loss: 0.4342242715\n",
      "0.6275\n",
      "Epoch 1344, Loss: 0.3481908651\n",
      "0.64625\n",
      "Epoch 1345, Loss: 0.3148701088\n",
      "0.6375\n",
      "Epoch 1346, Loss: 0.2876768512\n",
      "0.65\n",
      "Epoch 1347, Loss: 0.3105459170\n",
      "0.64\n",
      "Epoch 1348, Loss: 0.3715331337\n",
      "0.62125\n",
      "Epoch 1349, Loss: 0.4485428321\n",
      "0.615\n",
      "Epoch 1350, Loss: 0.4600214767\n",
      "0.6225\n",
      "Epoch 1351, Loss: 0.5738308826\n",
      "0.6075\n",
      "Epoch 1352, Loss: 0.4575257008\n",
      "0.6075\n",
      "Epoch 1353, Loss: 0.3995715604\n",
      "0.62875\n",
      "Epoch 1354, Loss: 0.4709663967\n",
      "0.61125\n",
      "Epoch 1355, Loss: 0.3522127986\n",
      "0.63625\n",
      "Epoch 1356, Loss: 0.4114071706\n",
      "0.61375\n",
      "Epoch 1357, Loss: 0.3303204359\n",
      "0.63625\n",
      "Epoch 1358, Loss: 0.3159098503\n",
      "0.65875\n",
      "Epoch 1359, Loss: 0.3912192216\n",
      "0.65\n",
      "Epoch 1360, Loss: 0.3936394178\n",
      "0.65125\n",
      "Epoch 1361, Loss: 0.4704773537\n",
      "0.62\n",
      "Epoch 1362, Loss: 0.4585831467\n",
      "0.61875\n",
      "Epoch 1363, Loss: 0.4318839348\n",
      "0.635\n",
      "Epoch 1364, Loss: 0.3307467900\n",
      "0.6475\n",
      "Epoch 1365, Loss: 0.3471154650\n",
      "0.63875\n",
      "Epoch 1366, Loss: 0.3549641388\n",
      "0.64\n",
      "Epoch 1367, Loss: 0.2818272728\n",
      "0.6425\n",
      "Epoch 1368, Loss: 0.2910883281\n",
      "0.645\n",
      "Epoch 1369, Loss: 0.2979523253\n",
      "0.65\n",
      "Epoch 1370, Loss: 0.3383101197\n",
      "0.65125\n",
      "Epoch 1371, Loss: 0.3703813965\n",
      "0.645\n",
      "Epoch 1372, Loss: 0.3843151558\n",
      "0.64625\n",
      "Epoch 1373, Loss: 0.3586632227\n",
      "0.65375\n",
      "Epoch 1374, Loss: 0.3247570578\n",
      "0.65625\n",
      "Epoch 1375, Loss: 0.3191162682\n",
      "0.64875\n",
      "Epoch 1376, Loss: 0.3460768935\n",
      "0.6425\n",
      "Epoch 1377, Loss: 0.4147395516\n",
      "0.63\n",
      "Epoch 1378, Loss: 0.4526228876\n",
      "0.61875\n",
      "Epoch 1379, Loss: 0.3114819508\n",
      "0.65375\n",
      "Epoch 1380, Loss: 0.5241756584\n",
      "0.61\n",
      "Epoch 1381, Loss: 0.3237202256\n",
      "0.64125\n",
      "Epoch 1382, Loss: 0.3481538098\n",
      "0.64\n",
      "Epoch 1383, Loss: 0.3185639155\n",
      "0.63625\n",
      "Epoch 1384, Loss: 0.3166183343\n",
      "0.66875\n",
      "Epoch 1385, Loss: 0.2942364690\n",
      "0.66875\n",
      "Epoch 1386, Loss: 0.3202052522\n",
      "0.65625\n",
      "Epoch 1387, Loss: 0.3289932276\n",
      "0.66\n",
      "Epoch 1388, Loss: 0.2972410903\n",
      "0.66\n",
      "Epoch 1389, Loss: 0.3471565250\n",
      "0.6575\n",
      "Epoch 1390, Loss: 0.3739475615\n",
      "0.65375\n",
      "Epoch 1391, Loss: 0.3234975265\n",
      "0.66\n",
      "Epoch 1392, Loss: 0.3392984794\n",
      "0.65125\n",
      "Epoch 1393, Loss: 0.3782844259\n",
      "0.65\n",
      "Epoch 1394, Loss: 0.4247998440\n",
      "0.6275\n",
      "Epoch 1395, Loss: 0.4731496940\n",
      "0.61375\n",
      "Epoch 1396, Loss: 0.4699809899\n",
      "0.61375\n",
      "Epoch 1397, Loss: 0.3158434439\n",
      "0.6525\n",
      "Epoch 1398, Loss: 0.3512561235\n",
      "0.63125\n",
      "Epoch 1399, Loss: 0.3312109207\n",
      "0.63875\n",
      "Epoch 1400, Loss: 0.3097733887\n",
      "0.655\n",
      "Epoch 1401, Loss: 0.2873258406\n",
      "0.64375\n",
      "Epoch 1402, Loss: 0.2577133758\n",
      "0.64625\n",
      "Epoch 1403, Loss: 0.2868155357\n",
      "0.66875\n",
      "Epoch 1404, Loss: 0.3148596162\n",
      "0.65375\n",
      "Epoch 1405, Loss: 0.2592495435\n",
      "0.655\n",
      "Epoch 1406, Loss: 0.2567958078\n",
      "0.66125\n",
      "Epoch 1407, Loss: 0.2674033897\n",
      "0.66125\n",
      "Epoch 1408, Loss: 0.2616830229\n",
      "0.655\n",
      "Epoch 1409, Loss: 0.2574176200\n",
      "0.65625\n",
      "Epoch 1410, Loss: 0.2605727404\n",
      "0.66125\n",
      "Epoch 1411, Loss: 0.2557082651\n",
      "0.6625\n",
      "Epoch 1412, Loss: 0.2492399319\n",
      "0.66125\n",
      "Epoch 1413, Loss: 0.2521128499\n",
      "0.66375\n",
      "Epoch 1414, Loss: 0.2527596049\n",
      "0.66125\n",
      "Epoch 1415, Loss: 0.2487924815\n",
      "0.65625\n",
      "Epoch 1416, Loss: 0.2450585849\n",
      "0.6575\n",
      "Epoch 1417, Loss: 0.2430511237\n",
      "0.6625\n",
      "Epoch 1418, Loss: 0.2414779298\n",
      "0.66375\n",
      "Epoch 1419, Loss: 0.2399368253\n",
      "0.6625\n",
      "Epoch 1420, Loss: 0.2417777978\n",
      "0.66375\n",
      "Epoch 1421, Loss: 0.2504754884\n",
      "0.65875\n",
      "Epoch 1422, Loss: 0.2716874158\n",
      "0.6425\n",
      "Epoch 1423, Loss: 0.3122066232\n",
      "0.62375\n",
      "Epoch 1424, Loss: 0.3471920609\n",
      "0.615\n",
      "Epoch 1425, Loss: 0.3165365683\n",
      "0.625\n",
      "Epoch 1426, Loss: 0.4353853147\n",
      "0.62375\n",
      "Epoch 1427, Loss: 0.4119020035\n",
      "0.61875\n",
      "Epoch 1428, Loss: 0.9374687288\n",
      "0.53875\n",
      "Epoch 1429, Loss: 0.4697905478\n",
      "0.61\n",
      "Epoch 1430, Loss: 0.4811001284\n",
      "0.61875\n",
      "Epoch 1431, Loss: 0.3702689871\n",
      "0.63125\n",
      "Epoch 1432, Loss: 0.3446067237\n",
      "0.64125\n",
      "Epoch 1433, Loss: 0.4259714792\n",
      "0.61\n",
      "Epoch 1434, Loss: 0.4700043091\n",
      "0.61\n",
      "Epoch 1435, Loss: 0.6177244851\n",
      "0.5725\n",
      "Epoch 1436, Loss: 0.5076511480\n",
      "0.605\n",
      "Epoch 1437, Loss: 0.6524297326\n",
      "0.575\n",
      "Epoch 1438, Loss: 0.4517805196\n",
      "0.62\n",
      "Epoch 1439, Loss: 0.3943982149\n",
      "0.65\n",
      "Epoch 1440, Loss: 0.4055616526\n",
      "0.6325\n",
      "Epoch 1441, Loss: 0.3901205111\n",
      "0.63875\n",
      "Epoch 1442, Loss: 0.3729748656\n",
      "0.62875\n",
      "Epoch 1443, Loss: 0.7678517689\n",
      "0.54625\n",
      "Epoch 1444, Loss: 0.6935537150\n",
      "0.58375\n",
      "Epoch 1445, Loss: 0.5023710624\n",
      "0.61875\n",
      "Epoch 1446, Loss: 0.4239206016\n",
      "0.635\n",
      "Epoch 1447, Loss: 0.4280625059\n",
      "0.6225\n",
      "Epoch 1448, Loss: 0.4091364223\n",
      "0.64625\n",
      "Epoch 1449, Loss: 0.3528320082\n",
      "0.6375\n",
      "Epoch 1450, Loss: 0.3117605057\n",
      "0.66875\n",
      "Epoch 1451, Loss: 0.3010998249\n",
      "0.665\n",
      "Epoch 1452, Loss: 0.2859008081\n",
      "0.67\n",
      "Epoch 1453, Loss: 0.3103218242\n",
      "0.6625\n",
      "Epoch 1454, Loss: 0.3292453238\n",
      "0.6575\n",
      "Epoch 1455, Loss: 0.3446309399\n",
      "0.65625\n",
      "Epoch 1456, Loss: 0.3429371907\n",
      "0.65125\n",
      "Epoch 1457, Loss: 0.4334339851\n",
      "0.63\n",
      "Epoch 1458, Loss: 0.5705382117\n",
      "0.60625\n",
      "Epoch 1459, Loss: 0.7214374986\n",
      "0.5825\n",
      "Epoch 1460, Loss: 0.4113777420\n",
      "0.63625\n",
      "Epoch 1461, Loss: 0.3751370495\n",
      "0.6325\n",
      "Epoch 1462, Loss: 0.3876138227\n",
      "0.63375\n",
      "Epoch 1463, Loss: 0.3627230261\n",
      "0.63625\n",
      "Epoch 1464, Loss: 0.3418478517\n",
      "0.6525\n",
      "Epoch 1465, Loss: 0.3731515986\n",
      "0.61875\n",
      "Epoch 1466, Loss: 0.4548878071\n",
      "0.625\n",
      "Epoch 1467, Loss: 0.6140005477\n",
      "0.615\n",
      "Epoch 1468, Loss: 0.3898891551\n",
      "0.655\n",
      "Epoch 1469, Loss: 0.4330655385\n",
      "0.62875\n",
      "Epoch 1470, Loss: 0.3514844809\n",
      "0.645\n",
      "Epoch 1471, Loss: 0.3473204338\n",
      "0.65375\n",
      "Epoch 1472, Loss: 0.3174368290\n",
      "0.65625\n",
      "Epoch 1473, Loss: 0.3087970971\n",
      "0.65125\n",
      "Epoch 1474, Loss: 0.3033621389\n",
      "0.65\n",
      "Epoch 1475, Loss: 0.2777555565\n",
      "0.64875\n",
      "Epoch 1476, Loss: 0.3252151863\n",
      "0.63125\n",
      "Epoch 1477, Loss: 0.4356262670\n",
      "0.625\n",
      "Epoch 1478, Loss: 0.4622629974\n",
      "0.6375\n",
      "Epoch 1479, Loss: 0.5662620776\n",
      "0.61\n",
      "Epoch 1480, Loss: 0.4271833871\n",
      "0.6325\n",
      "Epoch 1481, Loss: 0.4258584448\n",
      "0.6225\n",
      "Epoch 1482, Loss: 0.3309825419\n",
      "0.64\n",
      "Epoch 1483, Loss: 0.3248140013\n",
      "0.65625\n",
      "Epoch 1484, Loss: 0.3143076149\n",
      "0.64625\n",
      "Epoch 1485, Loss: 0.2915280314\n",
      "0.655\n",
      "Epoch 1486, Loss: 0.2920475624\n",
      "0.64625\n",
      "Epoch 1487, Loss: 0.2619989169\n",
      "0.66375\n",
      "Epoch 1488, Loss: 0.2762678145\n",
      "0.6575\n",
      "Epoch 1489, Loss: 0.3619529941\n",
      "0.6525\n",
      "Epoch 1490, Loss: 0.4684925453\n",
      "0.62875\n",
      "Epoch 1491, Loss: 0.4498217142\n",
      "0.6225\n",
      "Epoch 1492, Loss: 0.4849749408\n",
      "0.6275\n",
      "Epoch 1493, Loss: 0.3170476074\n",
      "0.65875\n",
      "Epoch 1494, Loss: 0.3237253427\n",
      "0.65625\n",
      "Epoch 1495, Loss: 0.3093010059\n",
      "0.6525\n",
      "Epoch 1496, Loss: 0.3403207857\n",
      "0.64375\n",
      "Epoch 1497, Loss: 0.3919497160\n",
      "0.63625\n",
      "Epoch 1498, Loss: 0.3587360617\n",
      "0.64625\n",
      "Epoch 1499, Loss: 0.4225261564\n",
      "0.625\n",
      "Epoch 1500, Loss: 0.4156635208\n",
      "0.63125\n",
      "Epoch 1501, Loss: 0.4234015296\n",
      "0.6125\n",
      "Epoch 1502, Loss: 0.6304981181\n",
      "0.61\n",
      "Epoch 1503, Loss: 0.5089064555\n",
      "0.60625\n",
      "Epoch 1504, Loss: 0.4470564600\n",
      "0.645\n",
      "Epoch 1505, Loss: 0.3746024453\n",
      "0.6375\n",
      "Epoch 1506, Loss: 0.3504869407\n",
      "0.655\n",
      "Epoch 1507, Loss: 0.3726320100\n",
      "0.63125\n",
      "Epoch 1508, Loss: 0.3942236626\n",
      "0.61875\n",
      "Epoch 1509, Loss: 0.3331684842\n",
      "0.64875\n",
      "Epoch 1510, Loss: 0.4023102557\n",
      "0.6375\n",
      "Epoch 1511, Loss: 0.3132430164\n",
      "0.65375\n",
      "Epoch 1512, Loss: 0.3728116020\n",
      "0.6325\n",
      "Epoch 1513, Loss: 0.4072435157\n",
      "0.63625\n",
      "Epoch 1514, Loss: 0.4022673393\n",
      "0.63875\n",
      "Epoch 1515, Loss: 0.4065883639\n",
      "0.635\n",
      "Epoch 1516, Loss: 0.4069161905\n",
      "0.6325\n",
      "Epoch 1517, Loss: 0.3287607622\n",
      "0.66625\n",
      "Epoch 1518, Loss: 0.2920738488\n",
      "0.65\n",
      "Epoch 1519, Loss: 0.2908833199\n",
      "0.655\n",
      "Epoch 1520, Loss: 0.3230031977\n",
      "0.64875\n",
      "Epoch 1521, Loss: 0.2934876040\n",
      "0.655\n",
      "Epoch 1522, Loss: 0.2788517370\n",
      "0.66625\n",
      "Epoch 1523, Loss: 0.3334918099\n",
      "0.64875\n",
      "Epoch 1524, Loss: 0.3393732716\n",
      "0.6525\n",
      "Epoch 1525, Loss: 0.2949632975\n",
      "0.66875\n",
      "Epoch 1526, Loss: 0.2721723756\n",
      "0.6625\n",
      "Epoch 1527, Loss: 0.3198627662\n",
      "0.65625\n",
      "Epoch 1528, Loss: 0.3098386053\n",
      "0.64625\n",
      "Epoch 1529, Loss: 0.3378966289\n",
      "0.64\n",
      "Epoch 1530, Loss: 0.3659642049\n",
      "0.63625\n",
      "Epoch 1531, Loss: 0.3767458752\n",
      "0.63875\n",
      "Epoch 1532, Loss: 0.3613283195\n",
      "0.64\n",
      "Epoch 1533, Loss: 0.3340733791\n",
      "0.6475\n",
      "Epoch 1534, Loss: 0.3053870912\n",
      "0.66625\n",
      "Epoch 1535, Loss: 0.3069612951\n",
      "0.66125\n",
      "Epoch 1536, Loss: 0.2948574472\n",
      "0.655\n",
      "Epoch 1537, Loss: 0.3033549212\n",
      "0.64\n",
      "Epoch 1538, Loss: 0.3856915682\n",
      "0.64125\n",
      "Epoch 1539, Loss: 0.3582017772\n",
      "0.635\n",
      "Epoch 1540, Loss: 0.3301674676\n",
      "0.6375\n",
      "Epoch 1541, Loss: 0.3533638973\n",
      "0.6425\n",
      "Epoch 1542, Loss: 0.4097981441\n",
      "0.6275\n",
      "Epoch 1543, Loss: 0.3331874311\n",
      "0.6475\n",
      "Epoch 1544, Loss: 0.3767292108\n",
      "0.64625\n",
      "Epoch 1545, Loss: 0.3223333338\n",
      "0.655\n",
      "Epoch 1546, Loss: 0.3035572861\n",
      "0.6475\n",
      "Epoch 1547, Loss: 0.2943938520\n",
      "0.6525\n",
      "Epoch 1548, Loss: 0.3224544849\n",
      "0.6525\n",
      "Epoch 1549, Loss: 0.3143815133\n",
      "0.64875\n",
      "Epoch 1550, Loss: 0.2708605421\n",
      "0.65375\n",
      "Epoch 1551, Loss: 0.2635725481\n",
      "0.66\n",
      "Epoch 1552, Loss: 0.2341716981\n",
      "0.66625\n",
      "Epoch 1553, Loss: 0.2886193238\n",
      "0.64875\n",
      "Epoch 1554, Loss: 0.3942527041\n",
      "0.62875\n",
      "Epoch 1555, Loss: 0.4590651071\n",
      "0.59625\n",
      "Epoch 1556, Loss: 0.4973625479\n",
      "0.6\n",
      "Epoch 1557, Loss: 0.3840061882\n",
      "0.64\n",
      "Epoch 1558, Loss: 0.4827386357\n",
      "0.6275\n",
      "Epoch 1559, Loss: 0.4086985383\n",
      "0.61875\n",
      "Epoch 1560, Loss: 0.4513423323\n",
      "0.625\n",
      "Epoch 1561, Loss: 0.5558419474\n",
      "0.5975\n",
      "Epoch 1562, Loss: 0.4520475170\n",
      "0.60875\n",
      "Epoch 1563, Loss: 0.2955260517\n",
      "0.635\n",
      "Epoch 1564, Loss: 0.2818246485\n",
      "0.635\n",
      "Epoch 1565, Loss: 0.2937220026\n",
      "0.63625\n",
      "Epoch 1566, Loss: 0.2948056370\n",
      "0.64875\n",
      "Epoch 1567, Loss: 0.2833761580\n",
      "0.655\n",
      "Epoch 1568, Loss: 0.3533317462\n",
      "0.64\n",
      "Epoch 1569, Loss: 0.3252131432\n",
      "0.6475\n",
      "Epoch 1570, Loss: 0.3080998712\n",
      "0.64125\n",
      "Epoch 1571, Loss: 0.3015566767\n",
      "0.65375\n",
      "Epoch 1572, Loss: 0.2867964397\n",
      "0.65\n",
      "Epoch 1573, Loss: 0.2848846507\n",
      "0.65375\n",
      "Epoch 1574, Loss: 0.2517867347\n",
      "0.65\n",
      "Epoch 1575, Loss: 0.3038252596\n",
      "0.62375\n",
      "Epoch 1576, Loss: 0.3162435634\n",
      "0.6375\n",
      "Epoch 1577, Loss: 0.3491408669\n",
      "0.635\n",
      "Epoch 1578, Loss: 0.3071425903\n",
      "0.65375\n",
      "Epoch 1579, Loss: 0.2642045901\n",
      "0.6525\n",
      "Epoch 1580, Loss: 0.2481019821\n",
      "0.65875\n",
      "Epoch 1581, Loss: 0.2492610525\n",
      "0.65\n",
      "Epoch 1582, Loss: 0.2497021708\n",
      "0.6375\n",
      "Epoch 1583, Loss: 0.2519006899\n",
      "0.64375\n",
      "Epoch 1584, Loss: 0.2715788431\n",
      "0.66\n",
      "Epoch 1585, Loss: 0.3279594261\n",
      "0.66125\n",
      "Epoch 1586, Loss: 0.3391531425\n",
      "0.64375\n",
      "Epoch 1587, Loss: 0.3119289305\n",
      "0.65375\n",
      "Epoch 1588, Loss: 0.2660765309\n",
      "0.655\n",
      "Epoch 1589, Loss: 0.3015647320\n",
      "0.63125\n",
      "Epoch 1590, Loss: 0.2562708573\n",
      "0.66875\n",
      "Epoch 1591, Loss: 0.3599378168\n",
      "0.64375\n",
      "Epoch 1592, Loss: 0.3206464017\n",
      "0.65875\n",
      "Epoch 1593, Loss: 0.2678354862\n",
      "0.655\n",
      "Epoch 1594, Loss: 0.2837143548\n",
      "0.64625\n",
      "Epoch 1595, Loss: 0.2673325446\n",
      "0.66125\n",
      "Epoch 1596, Loss: 0.3176946346\n",
      "0.655\n",
      "Epoch 1597, Loss: 0.3245220577\n",
      "0.65625\n",
      "Epoch 1598, Loss: 0.2606068283\n",
      "0.655\n",
      "Epoch 1599, Loss: 0.3453370843\n",
      "0.6375\n",
      "Epoch 1600, Loss: 0.2745032994\n",
      "0.67375\n",
      "Epoch 1601, Loss: 0.3259913824\n",
      "0.65\n",
      "Epoch 1602, Loss: 0.2847893109\n",
      "0.66625\n",
      "Epoch 1603, Loss: 0.3124596657\n",
      "0.6525\n",
      "Epoch 1604, Loss: 0.3568635445\n",
      "0.63125\n",
      "Epoch 1605, Loss: 0.4302792558\n",
      "0.61375\n",
      "Epoch 1606, Loss: 0.3884091244\n",
      "0.6275\n",
      "Epoch 1607, Loss: 0.2946965302\n",
      "0.65125\n",
      "Epoch 1608, Loss: 0.2636625887\n",
      "0.65\n",
      "Epoch 1609, Loss: 0.3809346782\n",
      "0.6325\n",
      "Epoch 1610, Loss: 0.3127345070\n",
      "0.63875\n",
      "Epoch 1611, Loss: 0.2940269654\n",
      "0.6525\n",
      "Epoch 1612, Loss: 0.2746882641\n",
      "0.6525\n",
      "Epoch 1613, Loss: 0.3703393290\n",
      "0.63875\n",
      "Epoch 1614, Loss: 0.3306673821\n",
      "0.65\n",
      "Epoch 1615, Loss: 0.3350104613\n",
      "0.64625\n",
      "Epoch 1616, Loss: 0.2837581682\n",
      "0.64375\n",
      "Epoch 1617, Loss: 0.3127437029\n",
      "0.6475\n",
      "Epoch 1618, Loss: 0.2479239152\n",
      "0.66\n",
      "Epoch 1619, Loss: 0.2876059167\n",
      "0.66375\n",
      "Epoch 1620, Loss: 0.2710244649\n",
      "0.655\n",
      "Epoch 1621, Loss: 0.2801655540\n",
      "0.65375\n",
      "Epoch 1622, Loss: 0.2747028579\n",
      "0.65625\n",
      "Epoch 1623, Loss: 0.3230517605\n",
      "0.6475\n",
      "Epoch 1624, Loss: 0.3534666543\n",
      "0.64125\n",
      "Epoch 1625, Loss: 0.3863087984\n",
      "0.63375\n",
      "Epoch 1626, Loss: 0.3308596319\n",
      "0.6375\n",
      "Epoch 1627, Loss: 0.3943238565\n",
      "0.605\n",
      "Epoch 1628, Loss: 0.2673768366\n",
      "0.6375\n",
      "Epoch 1629, Loss: 0.3439618691\n",
      "0.63625\n",
      "Epoch 1630, Loss: 0.3293679566\n",
      "0.635\n",
      "Epoch 1631, Loss: 0.2553815166\n",
      "0.64\n",
      "Epoch 1632, Loss: 0.2684671406\n",
      "0.65875\n",
      "Epoch 1633, Loss: 0.2489862546\n",
      "0.65625\n",
      "Epoch 1634, Loss: 0.2744932947\n",
      "0.6525\n",
      "Epoch 1635, Loss: 0.2809981896\n",
      "0.6525\n",
      "Epoch 1636, Loss: 0.3660659303\n",
      "0.63875\n",
      "Epoch 1637, Loss: 0.3657409324\n",
      "0.63375\n",
      "Epoch 1638, Loss: 0.3087102344\n",
      "0.64\n",
      "Epoch 1639, Loss: 0.2575629240\n",
      "0.6425\n",
      "Epoch 1640, Loss: 0.2830511921\n",
      "0.65375\n",
      "Epoch 1641, Loss: 0.2857766552\n",
      "0.65125\n",
      "Epoch 1642, Loss: 0.2437159024\n",
      "0.6325\n",
      "Epoch 1643, Loss: 0.3095951286\n",
      "0.6275\n",
      "Epoch 1644, Loss: 0.3845125689\n",
      "0.62625\n",
      "Epoch 1645, Loss: 0.3600826737\n",
      "0.63125\n",
      "Epoch 1646, Loss: 0.2792606287\n",
      "0.65\n",
      "Epoch 1647, Loss: 0.3046567719\n",
      "0.6375\n",
      "Epoch 1648, Loss: 0.3594380722\n",
      "0.64\n",
      "Epoch 1649, Loss: 0.2550846000\n",
      "0.66\n",
      "Epoch 1650, Loss: 0.2308652036\n",
      "0.6475\n",
      "Epoch 1651, Loss: 0.3295118416\n",
      "0.63625\n",
      "Epoch 1652, Loss: 0.4254957931\n",
      "0.62125\n",
      "Epoch 1653, Loss: 0.3241069742\n",
      "0.6325\n",
      "Epoch 1654, Loss: 0.4244219639\n",
      "0.61125\n",
      "Epoch 1655, Loss: 0.4257749574\n",
      "0.6275\n",
      "Epoch 1656, Loss: 0.3571129477\n",
      "0.63\n",
      "Epoch 1657, Loss: 0.4930009072\n",
      "0.61625\n",
      "Epoch 1658, Loss: 0.4494537286\n",
      "0.63625\n",
      "Epoch 1659, Loss: 0.4466942274\n",
      "0.62625\n",
      "Epoch 1660, Loss: 0.4739632558\n",
      "0.6025\n",
      "Epoch 1661, Loss: 0.4806577137\n",
      "0.61875\n",
      "Epoch 1662, Loss: 0.4783418349\n",
      "0.61\n",
      "Epoch 1663, Loss: 0.4403051719\n",
      "0.60625\n",
      "Epoch 1664, Loss: 0.3614715639\n",
      "0.62875\n",
      "Epoch 1665, Loss: 0.3057487077\n",
      "0.62375\n",
      "Epoch 1666, Loss: 0.3346565984\n",
      "0.63625\n",
      "Epoch 1667, Loss: 0.3523476037\n",
      "0.62875\n",
      "Epoch 1668, Loss: 0.4812900767\n",
      "0.5975\n",
      "Epoch 1669, Loss: 0.5699215663\n",
      "0.59125\n",
      "Epoch 1670, Loss: 0.8075625091\n",
      "0.555\n",
      "Epoch 1671, Loss: 0.6449956270\n",
      "0.58\n",
      "Epoch 1672, Loss: 0.4367242751\n",
      "0.61375\n",
      "Epoch 1673, Loss: 0.3721802675\n",
      "0.6175\n",
      "Epoch 1674, Loss: 0.3963307018\n",
      "0.61875\n",
      "Epoch 1675, Loss: 0.3593901962\n",
      "0.62\n",
      "Epoch 1676, Loss: 0.2810353571\n",
      "0.6425\n",
      "Epoch 1677, Loss: 0.3230938721\n",
      "0.6525\n",
      "Epoch 1678, Loss: 0.3230844938\n",
      "0.65875\n",
      "Epoch 1679, Loss: 0.3621482015\n",
      "0.645\n",
      "Epoch 1680, Loss: 0.2691078964\n",
      "0.65625\n",
      "Epoch 1681, Loss: 0.2547939855\n",
      "0.665\n",
      "Epoch 1682, Loss: 0.2470675118\n",
      "0.65625\n",
      "Epoch 1683, Loss: 0.2489814555\n",
      "0.6475\n",
      "Epoch 1684, Loss: 0.2431811295\n",
      "0.64125\n",
      "Epoch 1685, Loss: 0.2414382043\n",
      "0.64125\n",
      "Epoch 1686, Loss: 0.2850077658\n",
      "0.64375\n",
      "Epoch 1687, Loss: 0.3340508242\n",
      "0.63625\n",
      "Epoch 1688, Loss: 0.3098726462\n",
      "0.64125\n",
      "Epoch 1689, Loss: 0.2619031239\n",
      "0.64625\n",
      "Epoch 1690, Loss: 0.2756120315\n",
      "0.6425\n",
      "Epoch 1691, Loss: 0.2563260444\n",
      "0.645\n",
      "Epoch 1692, Loss: 0.2412367464\n",
      "0.65375\n",
      "Epoch 1693, Loss: 0.2214700626\n",
      "0.65125\n",
      "Epoch 1694, Loss: 0.2180336236\n",
      "0.65375\n",
      "Epoch 1695, Loss: 0.2614396482\n",
      "0.65125\n",
      "Epoch 1696, Loss: 0.2695123919\n",
      "0.65125\n",
      "Epoch 1697, Loss: 0.2497495856\n",
      "0.66125\n",
      "Epoch 1698, Loss: 0.2233411715\n",
      "0.665\n",
      "Epoch 1699, Loss: 0.2221103255\n",
      "0.6625\n",
      "Epoch 1700, Loss: 0.2420791334\n",
      "0.6525\n",
      "Epoch 1701, Loss: 0.2228122471\n",
      "0.65625\n",
      "Epoch 1702, Loss: 0.2216366681\n",
      "0.66875\n",
      "Epoch 1703, Loss: 0.2365991085\n",
      "0.65625\n",
      "Epoch 1704, Loss: 0.2326092538\n",
      "0.6575\n",
      "Epoch 1705, Loss: 0.2699046886\n",
      "0.6525\n",
      "Epoch 1706, Loss: 0.2636456647\n",
      "0.66\n",
      "Epoch 1707, Loss: 0.2531346718\n",
      "0.65\n",
      "Epoch 1708, Loss: 0.2466828890\n",
      "0.655\n",
      "Epoch 1709, Loss: 0.2358143080\n",
      "0.65375\n",
      "Epoch 1710, Loss: 0.2449787265\n",
      "0.6525\n",
      "Epoch 1711, Loss: 0.2448474296\n",
      "0.6525\n",
      "Epoch 1712, Loss: 0.2607322819\n",
      "0.655\n",
      "Epoch 1713, Loss: 0.2530889379\n",
      "0.6575\n",
      "Epoch 1714, Loss: 0.2401950780\n",
      "0.65125\n",
      "Epoch 1715, Loss: 0.2858035981\n",
      "0.6225\n",
      "Epoch 1716, Loss: 0.2400108468\n",
      "0.6425\n",
      "Epoch 1717, Loss: 0.2370182252\n",
      "0.6525\n",
      "Epoch 1718, Loss: 0.3422187465\n",
      "0.635\n",
      "Epoch 1719, Loss: 0.3035974678\n",
      "0.655\n",
      "Epoch 1720, Loss: 0.2889484806\n",
      "0.63\n",
      "Epoch 1721, Loss: 0.3061404049\n",
      "0.64125\n",
      "Epoch 1722, Loss: 0.3543998077\n",
      "0.63875\n",
      "Epoch 1723, Loss: 0.3165102165\n",
      "0.6275\n",
      "Epoch 1724, Loss: 0.3419647757\n",
      "0.625\n",
      "Epoch 1725, Loss: 0.3532301047\n",
      "0.62625\n",
      "Epoch 1726, Loss: 0.3557655828\n",
      "0.62125\n",
      "Epoch 1727, Loss: 0.3176275510\n",
      "0.6325\n",
      "Epoch 1728, Loss: 0.3475978681\n",
      "0.6425\n",
      "Epoch 1729, Loss: 0.3620217964\n",
      "0.635\n",
      "Epoch 1730, Loss: 0.3560526778\n",
      "0.6275\n",
      "Epoch 1731, Loss: 0.5507498214\n",
      "0.6075\n",
      "Epoch 1732, Loss: 0.4159217742\n",
      "0.62125\n",
      "Epoch 1733, Loss: 0.3730592816\n",
      "0.64\n",
      "Epoch 1734, Loss: 0.3936349170\n",
      "0.62375\n",
      "Epoch 1735, Loss: 0.3619196691\n",
      "0.63125\n",
      "Epoch 1736, Loss: 0.3644254428\n",
      "0.63\n",
      "Epoch 1737, Loss: 0.2951639607\n",
      "0.64\n",
      "Epoch 1738, Loss: 0.3970046824\n",
      "0.615\n",
      "Epoch 1739, Loss: 0.3544704173\n",
      "0.625\n",
      "Epoch 1740, Loss: 0.3459177351\n",
      "0.625\n",
      "Epoch 1741, Loss: 0.3796817271\n",
      "0.62375\n",
      "Epoch 1742, Loss: 0.3438493470\n",
      "0.62375\n",
      "Epoch 1743, Loss: 0.2777871862\n",
      "0.645\n",
      "Epoch 1744, Loss: 0.3413846044\n",
      "0.63625\n",
      "Epoch 1745, Loss: 0.3943634424\n",
      "0.62125\n",
      "Epoch 1746, Loss: 0.3194437469\n",
      "0.63625\n",
      "Epoch 1747, Loss: 0.3428881889\n",
      "0.64\n",
      "Epoch 1748, Loss: 0.2826220809\n",
      "0.63875\n",
      "Epoch 1749, Loss: 0.2392825881\n",
      "0.62125\n",
      "Epoch 1750, Loss: 0.3249541423\n",
      "0.62625\n",
      "Epoch 1751, Loss: 0.2527871226\n",
      "0.6475\n",
      "Epoch 1752, Loss: 0.2886659145\n",
      "0.63625\n",
      "Epoch 1753, Loss: 0.4272566958\n",
      "0.6225\n",
      "Epoch 1754, Loss: 0.2998198849\n",
      "0.64\n",
      "Epoch 1755, Loss: 0.3014559337\n",
      "0.64625\n",
      "Epoch 1756, Loss: 0.2691149577\n",
      "0.62375\n",
      "Epoch 1757, Loss: 0.2373046112\n",
      "0.63625\n",
      "Epoch 1758, Loss: 0.3187754967\n",
      "0.63625\n",
      "Epoch 1759, Loss: 0.3487254648\n",
      "0.6075\n",
      "Epoch 1760, Loss: 0.3139075091\n",
      "0.6075\n",
      "Epoch 1761, Loss: 0.2153129904\n",
      "0.65125\n",
      "Epoch 1762, Loss: 0.3349457731\n",
      "0.64125\n",
      "Epoch 1763, Loss: 0.3758881261\n",
      "0.6075\n",
      "Epoch 1764, Loss: 0.3983855424\n",
      "0.61875\n",
      "Epoch 1765, Loss: 0.3121323485\n",
      "0.62125\n",
      "Epoch 1766, Loss: 0.2898516819\n",
      "0.63875\n",
      "Epoch 1767, Loss: 0.2447493748\n",
      "0.64625\n",
      "Epoch 1768, Loss: 0.3718405697\n",
      "0.6325\n",
      "Epoch 1769, Loss: 0.3150149820\n",
      "0.63125\n",
      "Epoch 1770, Loss: 0.3123201123\n",
      "0.63\n",
      "Epoch 1771, Loss: 0.3151224868\n",
      "0.6375\n",
      "Epoch 1772, Loss: 0.3332483902\n",
      "0.63375\n",
      "Epoch 1773, Loss: 0.3404431011\n",
      "0.625\n",
      "Epoch 1774, Loss: 0.3757257031\n",
      "0.635\n",
      "Epoch 1775, Loss: 0.3468351388\n",
      "0.63125\n",
      "Epoch 1776, Loss: 0.3324340887\n",
      "0.65625\n",
      "Epoch 1777, Loss: 0.3735781258\n",
      "0.62125\n",
      "Epoch 1778, Loss: 0.3164006901\n",
      "0.64875\n",
      "Epoch 1779, Loss: 0.2851360036\n",
      "0.655\n",
      "Epoch 1780, Loss: 0.3528083520\n",
      "0.625\n",
      "Epoch 1781, Loss: 0.2283908232\n",
      "0.66125\n",
      "Epoch 1782, Loss: 0.3273949027\n",
      "0.64125\n",
      "Epoch 1783, Loss: 0.4831137025\n",
      "0.615\n",
      "Epoch 1784, Loss: 0.4684584356\n",
      "0.6125\n",
      "Epoch 1785, Loss: 0.3636607451\n",
      "0.635\n",
      "Epoch 1786, Loss: 0.3749906717\n",
      "0.6175\n",
      "Epoch 1787, Loss: 0.3136137072\n",
      "0.6625\n",
      "Epoch 1788, Loss: 0.2770923385\n",
      "0.645\n",
      "Epoch 1789, Loss: 0.3801595968\n",
      "0.65125\n",
      "Epoch 1790, Loss: 0.2659004788\n",
      "0.62875\n",
      "Epoch 1791, Loss: 0.3067489476\n",
      "0.635\n",
      "Epoch 1792, Loss: 0.2170711179\n",
      "0.65125\n",
      "Epoch 1793, Loss: 0.3073674105\n",
      "0.64375\n",
      "Epoch 1794, Loss: 0.2337336181\n",
      "0.6475\n",
      "Epoch 1795, Loss: 0.2906636331\n",
      "0.64375\n",
      "Epoch 1796, Loss: 0.2178272189\n",
      "0.6525\n",
      "Epoch 1797, Loss: 0.2396946937\n",
      "0.65375\n",
      "Epoch 1798, Loss: 0.2866002079\n",
      "0.65375\n",
      "Epoch 1799, Loss: 0.2487938187\n",
      "0.65375\n",
      "Epoch 1800, Loss: 0.2640010828\n",
      "0.65\n",
      "Epoch 1801, Loss: 0.2985194514\n",
      "0.635\n",
      "Epoch 1802, Loss: 0.2922112841\n",
      "0.6525\n",
      "Epoch 1803, Loss: 0.3494184204\n",
      "0.6425\n",
      "Epoch 1804, Loss: 0.3688914853\n",
      "0.64625\n",
      "Epoch 1805, Loss: 0.2484438002\n",
      "0.645\n",
      "Epoch 1806, Loss: 0.2583928931\n",
      "0.6475\n",
      "Epoch 1807, Loss: 0.2364558986\n",
      "0.6525\n",
      "Epoch 1808, Loss: 0.2342607220\n",
      "0.63375\n",
      "Epoch 1809, Loss: 0.2288510486\n",
      "0.6425\n",
      "Epoch 1810, Loss: 0.2050350940\n",
      "0.645\n",
      "Epoch 1811, Loss: 0.2023117816\n",
      "0.645\n",
      "Epoch 1812, Loss: 0.2023469890\n",
      "0.65125\n",
      "Epoch 1813, Loss: 0.2001507803\n",
      "0.6425\n",
      "Epoch 1814, Loss: 0.2044942922\n",
      "0.65375\n",
      "Epoch 1815, Loss: 0.1945413487\n",
      "0.65375\n",
      "Epoch 1816, Loss: 0.2022683573\n",
      "0.645\n",
      "Epoch 1817, Loss: 0.1883866709\n",
      "0.645\n",
      "Epoch 1818, Loss: 0.1899272389\n",
      "0.6475\n",
      "Epoch 1819, Loss: 0.2114287368\n",
      "0.66125\n",
      "Epoch 1820, Loss: 0.2191325158\n",
      "0.65\n",
      "Epoch 1821, Loss: 0.2272861735\n",
      "0.645\n",
      "Epoch 1822, Loss: 0.2163730396\n",
      "0.64875\n",
      "Epoch 1823, Loss: 0.2144124281\n",
      "0.64375\n",
      "Epoch 1824, Loss: 0.2301329442\n",
      "0.65\n",
      "Epoch 1825, Loss: 0.2359974011\n",
      "0.65625\n",
      "Epoch 1826, Loss: 0.2342176898\n",
      "0.65375\n",
      "Epoch 1827, Loss: 0.2333568434\n",
      "0.6525\n",
      "Epoch 1828, Loss: 0.2597652407\n",
      "0.65625\n",
      "Epoch 1829, Loss: 0.2959729747\n",
      "0.6525\n",
      "Epoch 1830, Loss: 0.2554435176\n",
      "0.63125\n",
      "Epoch 1831, Loss: 0.3704448503\n",
      "0.59625\n",
      "Epoch 1832, Loss: 0.2127790373\n",
      "0.65375\n",
      "Epoch 1833, Loss: 0.2410198739\n",
      "0.6375\n",
      "Epoch 1834, Loss: 0.2952662220\n",
      "0.65125\n",
      "Epoch 1835, Loss: 0.4313698253\n",
      "0.6175\n",
      "Epoch 1836, Loss: 0.3244037670\n",
      "0.61625\n",
      "Epoch 1837, Loss: 0.3608507468\n",
      "0.6225\n",
      "Epoch 1838, Loss: 0.2836028772\n",
      "0.63375\n",
      "Epoch 1839, Loss: 0.2548958410\n",
      "0.65375\n",
      "Epoch 1840, Loss: 0.2212438472\n",
      "0.65\n",
      "Epoch 1841, Loss: 0.3435382250\n",
      "0.64125\n",
      "Epoch 1842, Loss: 0.2470105153\n",
      "0.6425\n",
      "Epoch 1843, Loss: 0.2286471140\n",
      "0.65125\n",
      "Epoch 1844, Loss: 0.2351174640\n",
      "0.65875\n",
      "Epoch 1845, Loss: 0.2262167949\n",
      "0.66125\n",
      "Epoch 1846, Loss: 0.1746382567\n",
      "0.6575\n",
      "Epoch 1847, Loss: 0.2661307460\n",
      "0.6475\n",
      "Epoch 1848, Loss: 0.5008408579\n",
      "0.625\n",
      "Epoch 1849, Loss: 0.5710087837\n",
      "0.6275\n",
      "Epoch 1850, Loss: 0.2024819357\n",
      "0.6625\n",
      "Epoch 1851, Loss: 0.3508915568\n",
      "0.635\n",
      "Epoch 1852, Loss: 0.2262397485\n",
      "0.665\n",
      "Epoch 1853, Loss: 0.2657235524\n",
      "0.62375\n",
      "Epoch 1854, Loss: 0.4143282932\n",
      "0.63625\n",
      "Epoch 1855, Loss: 0.2119122541\n",
      "0.6525\n",
      "Epoch 1856, Loss: 0.2504023453\n",
      "0.65375\n",
      "Epoch 1857, Loss: 0.2536388054\n",
      "0.6475\n",
      "Epoch 1858, Loss: 0.2405274419\n",
      "0.64875\n",
      "Epoch 1859, Loss: 0.2167829820\n",
      "0.65875\n",
      "Epoch 1860, Loss: 0.2824994213\n",
      "0.65\n",
      "Epoch 1861, Loss: 0.3746520127\n",
      "0.64\n",
      "Epoch 1862, Loss: 0.4531238190\n",
      "0.61375\n",
      "Epoch 1863, Loss: 0.4158438542\n",
      "0.62125\n",
      "Epoch 1864, Loss: 0.2200380621\n",
      "0.64375\n",
      "Epoch 1865, Loss: 0.3466379553\n",
      "0.6325\n",
      "Epoch 1866, Loss: 0.2590701222\n",
      "0.63375\n",
      "Epoch 1867, Loss: 0.2335781732\n",
      "0.64875\n",
      "Epoch 1868, Loss: 0.2488340299\n",
      "0.65125\n",
      "Epoch 1869, Loss: 0.2416153490\n",
      "0.64375\n",
      "Epoch 1870, Loss: 0.3826585093\n",
      "0.6175\n",
      "Epoch 1871, Loss: 0.4196163838\n",
      "0.62625\n",
      "Epoch 1872, Loss: 0.4565661193\n",
      "0.61875\n",
      "Epoch 1873, Loss: 0.4715322194\n",
      "0.6125\n",
      "Epoch 1874, Loss: 0.2288899058\n",
      "0.65\n",
      "Epoch 1875, Loss: 0.2031711630\n",
      "0.6525\n",
      "Epoch 1876, Loss: 0.3185459305\n",
      "0.6375\n",
      "Epoch 1877, Loss: 0.2967783173\n",
      "0.63125\n",
      "Epoch 1878, Loss: 0.2388995665\n",
      "0.645\n",
      "Epoch 1879, Loss: 0.2751670357\n",
      "0.64375\n",
      "Epoch 1880, Loss: 0.2155964261\n",
      "0.645\n",
      "Epoch 1881, Loss: 0.3201522069\n",
      "0.64375\n",
      "Epoch 1882, Loss: 0.3290072767\n",
      "0.63\n",
      "Epoch 1883, Loss: 0.3029778374\n",
      "0.63875\n",
      "Epoch 1884, Loss: 0.3059090258\n",
      "0.63875\n",
      "Epoch 1885, Loss: 0.3003508244\n",
      "0.64\n",
      "Epoch 1886, Loss: 0.3121392764\n",
      "0.6375\n",
      "Epoch 1887, Loss: 0.3246438673\n",
      "0.63375\n",
      "Epoch 1888, Loss: 0.2992567705\n",
      "0.6375\n",
      "Epoch 1889, Loss: 0.3199759828\n",
      "0.62625\n",
      "Epoch 1890, Loss: 0.3297393649\n",
      "0.63\n",
      "Epoch 1891, Loss: 0.2515695036\n",
      "0.65125\n",
      "Epoch 1892, Loss: 0.2838334084\n",
      "0.6375\n",
      "Epoch 1893, Loss: 0.3345123078\n",
      "0.62875\n",
      "Epoch 1894, Loss: 0.3594351563\n",
      "0.62875\n",
      "Epoch 1895, Loss: 0.2963620024\n",
      "0.6425\n",
      "Epoch 1896, Loss: 0.2778280657\n",
      "0.6375\n",
      "Epoch 1897, Loss: 0.2564344382\n",
      "0.6425\n",
      "Epoch 1898, Loss: 0.2846934341\n",
      "0.63625\n",
      "Epoch 1899, Loss: 0.3291893916\n",
      "0.63125\n",
      "Epoch 1900, Loss: 0.3344799866\n",
      "0.6375\n",
      "Epoch 1901, Loss: 0.3344937268\n",
      "0.62875\n",
      "Epoch 1902, Loss: 0.3358725405\n",
      "0.63625\n",
      "Epoch 1903, Loss: 0.3399636002\n",
      "0.62\n",
      "Epoch 1904, Loss: 0.3585939971\n",
      "0.625\n",
      "Epoch 1905, Loss: 0.3404107930\n",
      "0.6125\n",
      "Epoch 1906, Loss: 0.3080560804\n",
      "0.61875\n",
      "Epoch 1907, Loss: 0.1979274886\n",
      "0.645\n",
      "Epoch 1908, Loss: 0.3176394974\n",
      "0.6375\n",
      "Epoch 1909, Loss: 0.3601710310\n",
      "0.63125\n",
      "Epoch 1910, Loss: 0.3466803955\n",
      "0.63125\n",
      "Epoch 1911, Loss: 0.2474418286\n",
      "0.6425\n",
      "Epoch 1912, Loss: 0.2687750770\n",
      "0.645\n",
      "Epoch 1913, Loss: 0.2883390980\n",
      "0.62875\n",
      "Epoch 1914, Loss: 0.2904146283\n",
      "0.63375\n",
      "Epoch 1915, Loss: 0.2801162437\n",
      "0.64375\n",
      "Epoch 1916, Loss: 0.3909337875\n",
      "0.61625\n",
      "Epoch 1917, Loss: 0.6628238620\n",
      "0.62125\n",
      "Epoch 1918, Loss: 0.3718490733\n",
      "0.6375\n",
      "Epoch 1919, Loss: 0.3757718191\n",
      "0.61375\n",
      "Epoch 1920, Loss: 0.3336357805\n",
      "0.64125\n",
      "Epoch 1921, Loss: 0.2446018402\n",
      "0.64625\n",
      "Epoch 1922, Loss: 0.1940277124\n",
      "0.6475\n",
      "Epoch 1923, Loss: 0.2596981641\n",
      "0.645\n",
      "Epoch 1924, Loss: 0.3009914309\n",
      "0.64875\n",
      "Epoch 1925, Loss: 0.2408298080\n",
      "0.63625\n",
      "Epoch 1926, Loss: 0.3147701167\n",
      "0.6475\n",
      "Epoch 1927, Loss: 0.3727890990\n",
      "0.62875\n",
      "Epoch 1928, Loss: 0.4569988138\n",
      "0.5875\n",
      "Epoch 1929, Loss: 0.4237958755\n",
      "0.585\n",
      "Epoch 1930, Loss: 0.4924554606\n",
      "0.59375\n",
      "Epoch 1931, Loss: 0.4596426937\n",
      "0.62\n",
      "Epoch 1932, Loss: 0.4749088335\n",
      "0.625\n",
      "Epoch 1933, Loss: 0.4098117355\n",
      "0.63125\n",
      "Epoch 1934, Loss: 0.2599673973\n",
      "0.63875\n",
      "Epoch 1935, Loss: 0.4362157111\n",
      "0.62375\n",
      "Epoch 1936, Loss: 0.3583029692\n",
      "0.605\n",
      "Epoch 1937, Loss: 0.3082083211\n",
      "0.63875\n",
      "Epoch 1938, Loss: 0.4229145349\n",
      "0.60125\n",
      "Epoch 1939, Loss: 0.3122691793\n",
      "0.6325\n",
      "Epoch 1940, Loss: 0.2829054962\n",
      "0.63625\n",
      "Epoch 1941, Loss: 0.4530276252\n",
      "0.59625\n",
      "Epoch 1942, Loss: 0.3782764251\n",
      "0.60875\n",
      "Epoch 1943, Loss: 0.4929096530\n",
      "0.59875\n",
      "Epoch 1944, Loss: 0.5351611190\n",
      "0.58625\n",
      "Epoch 1945, Loss: 0.4399728522\n",
      "0.6025\n",
      "Epoch 1946, Loss: 0.4253012501\n",
      "0.6325\n",
      "Epoch 1947, Loss: 0.4379049510\n",
      "0.60875\n",
      "Epoch 1948, Loss: 0.4421681501\n",
      "0.61375\n",
      "Epoch 1949, Loss: 0.5092869627\n",
      "0.5925\n",
      "Epoch 1950, Loss: 0.4406036681\n",
      "0.6175\n",
      "Epoch 1951, Loss: 0.5223626892\n",
      "0.59875\n",
      "Epoch 1952, Loss: 0.6610314156\n",
      "0.5975\n",
      "Epoch 1953, Loss: 0.4446335671\n",
      "0.62125\n",
      "Epoch 1954, Loss: 0.5017427715\n",
      "0.6125\n",
      "Epoch 1955, Loss: 0.4552619723\n",
      "0.62\n",
      "Epoch 1956, Loss: 0.2820814212\n",
      "0.64625\n",
      "Epoch 1957, Loss: 0.2961585494\n",
      "0.64625\n",
      "Epoch 1958, Loss: 0.2993009384\n",
      "0.6425\n",
      "Epoch 1959, Loss: 0.2520594228\n",
      "0.625\n",
      "Epoch 1960, Loss: 0.3111614022\n",
      "0.64125\n",
      "Epoch 1961, Loss: 0.2523557193\n",
      "0.6625\n",
      "Epoch 1962, Loss: 0.4609468882\n",
      "0.61875\n",
      "Epoch 1963, Loss: 0.2167189239\n",
      "0.65375\n",
      "Epoch 1964, Loss: 0.2871551241\n",
      "0.6575\n",
      "Epoch 1965, Loss: 0.1701084173\n",
      "0.64375\n",
      "Epoch 1966, Loss: 0.2414564138\n",
      "0.66\n",
      "Epoch 1967, Loss: 0.2162803595\n",
      "0.6525\n",
      "Epoch 1968, Loss: 0.3726454197\n",
      "0.63625\n",
      "Epoch 1969, Loss: 0.2420393324\n",
      "0.65\n",
      "Epoch 1970, Loss: 0.4385493857\n",
      "0.625\n",
      "Epoch 1971, Loss: 0.2112938491\n",
      "0.64375\n",
      "Epoch 1972, Loss: 0.2936364042\n",
      "0.6375\n",
      "Epoch 1973, Loss: 0.1714774240\n",
      "0.65375\n",
      "Epoch 1974, Loss: 0.2690246780\n",
      "0.6475\n",
      "Epoch 1975, Loss: 0.2056292645\n",
      "0.66\n",
      "Epoch 1976, Loss: 0.3617935894\n",
      "0.64\n",
      "Epoch 1977, Loss: 0.3202027121\n",
      "0.65\n",
      "Epoch 1978, Loss: 0.3562981993\n",
      "0.6175\n",
      "Epoch 1979, Loss: 0.2599975740\n",
      "0.6475\n",
      "Epoch 1980, Loss: 0.2643761476\n",
      "0.64625\n",
      "Epoch 1981, Loss: 0.2203150143\n",
      "0.6625\n",
      "Epoch 1982, Loss: 0.3180876546\n",
      "0.645\n",
      "Epoch 1983, Loss: 0.4816440129\n",
      "0.6175\n",
      "Epoch 1984, Loss: 0.2787650154\n",
      "0.6475\n",
      "Epoch 1985, Loss: 0.5172498835\n",
      "0.59125\n",
      "Epoch 1986, Loss: 0.3879600305\n",
      "0.61875\n",
      "Epoch 1987, Loss: 0.4924277788\n",
      "0.60625\n",
      "Epoch 1988, Loss: 0.5458489713\n",
      "0.605\n",
      "Epoch 1989, Loss: 0.4540052798\n",
      "0.625\n",
      "Epoch 1990, Loss: 0.3500782129\n",
      "0.635\n",
      "Epoch 1991, Loss: 0.3516460486\n",
      "0.64125\n",
      "Epoch 1992, Loss: 0.2199277933\n",
      "0.65125\n",
      "Epoch 1993, Loss: 0.2377411650\n",
      "0.64625\n",
      "Epoch 1994, Loss: 0.2953327181\n",
      "0.6425\n",
      "Epoch 1995, Loss: 0.2603655913\n",
      "0.64875\n",
      "Epoch 1996, Loss: 0.2809032957\n",
      "0.6475\n",
      "Epoch 1997, Loss: 0.2634393973\n",
      "0.6525\n",
      "Epoch 1998, Loss: 0.2444128364\n",
      "0.6425\n",
      "Epoch 1999, Loss: 0.2781133363\n",
      "0.64625\n",
      "Epoch 2000, Loss: 0.2709813988\n",
      "0.6475\n",
      "0.17010841729477666\n"
     ]
    }
   ],
   "source": [
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_weights_init = []\n",
    "best_biases_init = []\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "best_seed = 0\n",
    "\n",
    "for _ in range (1):\n",
    "    nn = NeuralNetwork_Adam(625, [256,128,64], 8, beta1=0.925, beta2=0.999)\n",
    "    nn.train(batches,14,learning_rate=0.00125)\n",
    "    if nn.get_best_loss() < best_loss:\n",
    "        best_loss = nn.get_best_loss()\n",
    "        best_weights = nn.get_best_weights()\n",
    "        best_biases = nn.get_best_biases()\n",
    "        best_seed = nn.get_best_seed()\n",
    "\n",
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17010841729477666\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "# print(best_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8, init_weights=best_weights, init_biases=best_biases, init_seed=best_seed)\n",
    "# nn.train(batches, 3, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.get_best_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Number of layers in the Neural Network\n",
    "N = 4  # Example value, replace with the actual number of layers\n",
    "\n",
    "# Initialize the dictionary\n",
    "weights_dict = {\n",
    "    'weights': {},\n",
    "    'bias': {}\n",
    "}\n",
    "\n",
    "weights = nn.get_best_weights()\n",
    "biases = nn.get_best_biases()\n",
    "\n",
    "# Populate the weights and bias dictionaries\n",
    "for i in range(N):\n",
    "    weights_dict['weights'][f'fc{i+1}'] = weights[i]\n",
    "    weights_dict['bias'][f'fc{i+1}'] = biases[i].flatten()\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.694354912103775\n",
      "0.6475\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'val' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))\n",
    "\n",
    "for X_val, Y_val in batches:\n",
    "    Y_pred= nn.predict(X_val)\n",
    "    print(cross_entropy_loss(Y_val,Y_pred)/len(dataset))\n",
    "    print(accuracy(Y_val, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
