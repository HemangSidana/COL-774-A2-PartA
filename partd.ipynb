{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Remember to import \"numpy_transforms\" functions if you wish to import these two classes in a different script.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "        label = row[\"class\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image), label\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image, label = self.dataset[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "        batch_labels = np.array(labels)\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(128, 625)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    print(labels.shape)  # Should be [batch_size]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# # ReLU activation and its derivative\n",
    "# def relu(x):\n",
    "#     return np.maximum(0, x)\n",
    "\n",
    "# def relu_derivative(x):\n",
    "#     return np.where(x > 0, 1, 0)\n",
    "\n",
    "# # Mean Squared Error loss\n",
    "# def mean_squared_error(y_true, y_pred):\n",
    "#     return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "# # Neural Network Class with ReLU in the Output Layer and Hidden Layers\n",
    "# class NeuralNetwork_Adam:\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "#         if init_seed is None:\n",
    "#             self.best_seed = int(time.time())\n",
    "#             np.random.seed(self.best_seed)\n",
    "#         else:\n",
    "#             np.random.seed(init_seed)\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "#         self.m_w = []\n",
    "#         self.v_w = []\n",
    "#         self.m_b = []\n",
    "#         self.v_b = []\n",
    "#         self.beta1 = beta1\n",
    "#         self.beta2 = beta2\n",
    "#         self.epsilon = epsilon\n",
    "#         self.t = 0  # Time step for Adam\n",
    "#         self.best_weights = []\n",
    "#         self.best_biases = []\n",
    "#         self.best_loss = float(\"inf\")\n",
    "\n",
    "#         layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "#         # Initialize weights, biases, and Adam parameters (m, v)\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             if (init_weights is not None) and (init_biases is not None):\n",
    "#                 self.weights.append(init_weights[i])\n",
    "#                 self.biases.append(init_biases[i])\n",
    "#             else:\n",
    "#                 self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "#                 self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "#             self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.best_weights = self.weights\n",
    "#             self.best_biases = self.biases\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         pre_activations = []\n",
    "\n",
    "#         # Pass through each layer except the output layer\n",
    "#         for i in range(len(self.weights) - 1):\n",
    "#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             pre_activations.append(z)\n",
    "#             a = relu(z)  # ReLU for hidden layers\n",
    "#             activations.append(a)\n",
    "\n",
    "#         # Pass through the output layer with ReLU\n",
    "#         z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         pre_activations.append(z)\n",
    "#         a = relu(z)  # ReLU for the output layer\n",
    "#         activations.append(a)\n",
    "\n",
    "#         return activations, pre_activations\n",
    "\n",
    "#     def backward(self, X, y, activations, pre_activations):\n",
    "#         grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "#         grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "#         # Start with output layer error\n",
    "#         delta = activations[-1] - y\n",
    "#         delta *= relu_derivative(pre_activations[-1])  # ReLU derivative for the output layer\n",
    "\n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "#             grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "#             if i > 0:\n",
    "#                 delta = np.dot(delta, self.weights[i].T) * relu_derivative(pre_activations[i - 1])\n",
    "\n",
    "#         return grad_w, grad_b\n",
    "\n",
    "#     def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "#         self.t += 1  # Increment time step for Adam\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             # Update biased first moment estimate\n",
    "#             self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "#             self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "#             # Update biased second moment estimate\n",
    "#             self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "#             self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "#             # Compute bias-corrected first moment estimate\n",
    "#             m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "#             m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "#             # Compute bias-corrected second moment estimate\n",
    "#             v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "#             v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "#             # Update weights and biases\n",
    "#             self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "#             self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "#     def train(self, batches, time_of_running, learning_rate):\n",
    "#         start_time = time.time()\n",
    "#         epoch = 0\n",
    "#         while True:\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 activations, pre_activations = self.forward(X_batch)\n",
    "#                 grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "#                 self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "#             # Calculate average loss over batches\n",
    "#             loss = 0\n",
    "#             z = 0\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 y_pred, _ = self.forward(X_batch)\n",
    "#                 loss += mean_squared_error(y_batch, y_pred[-1])\n",
    "#                 z += len(y_pred[-1])\n",
    "#             loss /= z\n",
    "            \n",
    "#             if loss < self.best_loss:\n",
    "#                 self.best_loss = loss\n",
    "#                 self.best_weights = self.weights\n",
    "#                 self.best_biases = self.biases\n",
    "#             print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "#             epoch += 1\n",
    "#             # if time elapsed is greater than 1 minute, break the loop\n",
    "#             if time.time() - start_time > 60 * time_of_running:\n",
    "#                 break\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "#     def get_best_weights(self):\n",
    "#         return self.best_weights\n",
    "\n",
    "#     def get_best_biases(self):\n",
    "#         return self.best_biases\n",
    "\n",
    "#     def get_best_loss(self):\n",
    "#         return self.best_loss\n",
    "\n",
    "#     def get_best_seed(self):\n",
    "#         return self.best_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'val' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset_val = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "# Create the DataLoader\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=len(dataset_val))\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches_val=[]\n",
    "for images,labels in dataloader_val:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches_val.append((images,one_hot_labels))\n",
    "\n",
    "def get_stat():\n",
    "    for X_val, Y_val in batches_val:\n",
    "        Y_pred= nn.predict(X_val)\n",
    "        # print(cross_entropy_loss(Y_val,Y_pred)/len(dataset))\n",
    "        print(accuracy(Y_val, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Leaky ReLU activation and its derivative\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Neural Network Class with Leaky ReLU and Adam Optimizer\n",
    "class NeuralNetwork_Adam:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, alpha=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        if init_seed is None:\n",
    "            self.best_seed = int(time.time())\n",
    "            np.random.seed(self.best_seed)\n",
    "        else:\n",
    "            np.random.seed(init_seed)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.m_w = []\n",
    "        self.v_w = []\n",
    "        self.m_b = []\n",
    "        self.v_b = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step for Adam\n",
    "        self.alpha = alpha  # Leaky ReLU parameter\n",
    "        self.best_weights = []\n",
    "        self.best_biases = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if init_weights is not None and init_biases is not None:\n",
    "                self.weights.append(init_weights[i])\n",
    "                self.biases.append(init_biases[i])\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "                self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "            self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "        self.best_weights = self.weights\n",
    "        self.best_biases = self.biases\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each hidden layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = leaky_relu(z, alpha=self.alpha)  # Leaky ReLU for hidden layers\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def forward_pred(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each hidden layer\n",
    "        for i in range(len(self.best_weights) - 1):\n",
    "            z = np.dot(activations[-1], self.best_weights[i]) + self.best_biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = leaky_relu(z, alpha=self.alpha)  # Leaky ReLU for hidden layers\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.best_weights[-1]) + self.best_biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * leaky_relu_derivative(pre_activations[i - 1], alpha=self.alpha)\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        self.t += 1  # Increment time step for Adam\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def train(self, batches, time_of_running, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while epoch<2000:\n",
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
    "            loss /= z\n",
    "            \n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = self.weights\n",
    "                self.best_biases = self.biases\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            get_stat()\n",
    "            epoch += 1\n",
    "            # if time elapsed is greater than 1 minute, break the loop\n",
    "            if time.time() - start_time > 60 * time_of_running:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_pred(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    \n",
    "    def get_best_biases(self):\n",
    "        return self.best_biases\n",
    "    \n",
    "    def get_best_loss(self):\n",
    "        return self.best_loss\n",
    "    \n",
    "    def get_best_seed(self):\n",
    "        return self.best_seed\n",
    "\n",
    "# Example usage:\n",
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8)\n",
    "# nn.train(batches, 1, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0934321338\n",
      "0.12625\n",
      "Epoch 2, Loss: 2.0568731743\n",
      "0.15125\n",
      "Epoch 3, Loss: 2.0466062485\n",
      "0.12625\n",
      "Epoch 4, Loss: 2.0251496950\n",
      "0.14125\n",
      "Epoch 5, Loss: 1.9793431503\n",
      "0.3325\n",
      "Epoch 6, Loss: 1.9342471499\n",
      "0.22875\n",
      "Epoch 7, Loss: 1.8783595957\n",
      "0.27375\n",
      "Epoch 8, Loss: 1.8529479193\n",
      "0.3\n",
      "Epoch 9, Loss: 1.8261771790\n",
      "0.29875\n",
      "Epoch 10, Loss: 1.7694323521\n",
      "0.26875\n",
      "Epoch 11, Loss: 1.7411779753\n",
      "0.33625\n",
      "Epoch 12, Loss: 1.6679243043\n",
      "0.38625\n",
      "Epoch 13, Loss: 1.6311448956\n",
      "0.37125\n",
      "Epoch 14, Loss: 1.6070330488\n",
      "0.4025\n",
      "Epoch 15, Loss: 1.5737450758\n",
      "0.4125\n",
      "Epoch 16, Loss: 1.5445472874\n",
      "0.42875\n",
      "Epoch 17, Loss: 1.5250925386\n",
      "0.43125\n",
      "Epoch 18, Loss: 1.5078206730\n",
      "0.43875\n",
      "Epoch 19, Loss: 1.4992073362\n",
      "0.4425\n",
      "Epoch 20, Loss: 1.5318544449\n",
      "0.39125\n",
      "Epoch 21, Loss: 1.5201708729\n",
      "0.4\n",
      "Epoch 22, Loss: 1.4777510447\n",
      "0.42375\n",
      "Epoch 23, Loss: 1.4242524599\n",
      "0.47375\n",
      "Epoch 24, Loss: 1.4322555368\n",
      "0.45\n",
      "Epoch 25, Loss: 1.4070017594\n",
      "0.4675\n",
      "Epoch 26, Loss: 1.3784392660\n",
      "0.48875\n",
      "Epoch 27, Loss: 1.3699587477\n",
      "0.49125\n",
      "Epoch 28, Loss: 1.3623724850\n",
      "0.49\n",
      "Epoch 29, Loss: 1.3587043551\n",
      "0.46875\n",
      "Epoch 30, Loss: 1.3538927635\n",
      "0.45375\n",
      "Epoch 31, Loss: 1.3410485569\n",
      "0.465\n",
      "Epoch 32, Loss: 1.3216486676\n",
      "0.49125\n",
      "Epoch 33, Loss: 1.3226534125\n",
      "0.4925\n",
      "Epoch 34, Loss: 1.3558107894\n",
      "0.47125\n",
      "Epoch 35, Loss: 1.3058593019\n",
      "0.495\n",
      "Epoch 36, Loss: 1.2774254694\n",
      "0.51625\n",
      "Epoch 37, Loss: 1.2612141517\n",
      "0.51125\n",
      "Epoch 38, Loss: 1.2758276299\n",
      "0.4925\n",
      "Epoch 39, Loss: 1.2401906007\n",
      "0.51875\n",
      "Epoch 40, Loss: 1.2718431929\n",
      "0.50625\n",
      "Epoch 41, Loss: 1.2262906511\n",
      "0.51625\n",
      "Epoch 42, Loss: 1.2022490464\n",
      "0.52375\n",
      "Epoch 43, Loss: 1.1923286958\n",
      "0.53\n",
      "Epoch 44, Loss: 1.1856303772\n",
      "0.53375\n",
      "Epoch 45, Loss: 1.1792040195\n",
      "0.535\n",
      "Epoch 46, Loss: 1.1865079022\n",
      "0.535\n",
      "Epoch 47, Loss: 1.1791680078\n",
      "0.535\n",
      "Epoch 48, Loss: 1.1872336873\n",
      "0.53\n",
      "Epoch 49, Loss: 1.1991525942\n",
      "0.52625\n",
      "Epoch 50, Loss: 1.2142519006\n",
      "0.51375\n",
      "Epoch 51, Loss: 1.2245656387\n",
      "0.51\n",
      "Epoch 52, Loss: 1.1591714825\n",
      "0.5275\n",
      "Epoch 53, Loss: 1.1396481033\n",
      "0.53625\n",
      "Epoch 54, Loss: 1.1440718244\n",
      "0.53625\n",
      "Epoch 55, Loss: 1.1233606125\n",
      "0.55\n",
      "Epoch 56, Loss: 1.1103412678\n",
      "0.55\n",
      "Epoch 57, Loss: 1.1061200628\n",
      "0.5575\n",
      "Epoch 58, Loss: 1.1055492472\n",
      "0.55375\n",
      "Epoch 59, Loss: 1.0981528151\n",
      "0.565\n",
      "Epoch 60, Loss: 1.0836983777\n",
      "0.5675\n",
      "Epoch 61, Loss: 1.0806619657\n",
      "0.565\n",
      "Epoch 62, Loss: 1.0887441624\n",
      "0.56625\n",
      "Epoch 63, Loss: 1.0748125588\n",
      "0.555\n",
      "Epoch 64, Loss: 1.1480297200\n",
      "0.535\n",
      "Epoch 65, Loss: 1.1531869003\n",
      "0.53\n",
      "Epoch 66, Loss: 1.0639053172\n",
      "0.56\n",
      "Epoch 67, Loss: 1.0404368513\n",
      "0.57\n",
      "Epoch 68, Loss: 1.0463060767\n",
      "0.58375\n",
      "Epoch 69, Loss: 1.0262616316\n",
      "0.57375\n",
      "Epoch 70, Loss: 1.0245438118\n",
      "0.575\n",
      "Epoch 71, Loss: 1.0097286369\n",
      "0.5725\n",
      "Epoch 72, Loss: 1.0203706046\n",
      "0.58125\n",
      "Epoch 73, Loss: 1.0063265326\n",
      "0.585\n",
      "Epoch 74, Loss: 1.0049722011\n",
      "0.58125\n",
      "Epoch 75, Loss: 0.9910064537\n",
      "0.59375\n",
      "Epoch 76, Loss: 1.0017746654\n",
      "0.58625\n",
      "Epoch 77, Loss: 0.9742329708\n",
      "0.58875\n",
      "Epoch 78, Loss: 0.9706160851\n",
      "0.59625\n",
      "Epoch 79, Loss: 0.9794442863\n",
      "0.5925\n",
      "Epoch 80, Loss: 0.9794273300\n",
      "0.58625\n",
      "Epoch 81, Loss: 0.9526999486\n",
      "0.59125\n",
      "Epoch 82, Loss: 0.9747981420\n",
      "0.5825\n",
      "Epoch 83, Loss: 0.9535243956\n",
      "0.59875\n",
      "Epoch 84, Loss: 1.0032915479\n",
      "0.57875\n",
      "Epoch 85, Loss: 0.9747576104\n",
      "0.58875\n",
      "Epoch 86, Loss: 0.9539485265\n",
      "0.5825\n",
      "Epoch 87, Loss: 0.9539136512\n",
      "0.58875\n",
      "Epoch 88, Loss: 0.9234142203\n",
      "0.61\n",
      "Epoch 89, Loss: 0.9498320864\n",
      "0.59125\n",
      "Epoch 90, Loss: 0.9145693591\n",
      "0.59125\n",
      "Epoch 91, Loss: 0.9605910807\n",
      "0.57875\n",
      "Epoch 92, Loss: 0.9533593067\n",
      "0.59\n",
      "Epoch 93, Loss: 0.9371635222\n",
      "0.58875\n",
      "Epoch 94, Loss: 1.0633471961\n",
      "0.5475\n",
      "Epoch 95, Loss: 1.1193585102\n",
      "0.53875\n",
      "Epoch 96, Loss: 1.0074485179\n",
      "0.58125\n",
      "Epoch 97, Loss: 1.1298817684\n",
      "0.53875\n",
      "Epoch 98, Loss: 1.1574058372\n",
      "0.525\n",
      "Epoch 99, Loss: 0.9367031295\n",
      "0.59375\n",
      "Epoch 100, Loss: 0.9135209674\n",
      "0.60375\n",
      "Epoch 101, Loss: 0.8971513440\n",
      "0.61125\n",
      "Epoch 102, Loss: 0.9442050319\n",
      "0.585\n",
      "Epoch 103, Loss: 1.0364210260\n",
      "0.56\n",
      "Epoch 104, Loss: 0.9091150161\n",
      "0.58625\n",
      "Epoch 105, Loss: 0.9245901106\n",
      "0.59875\n",
      "Epoch 106, Loss: 0.8824614625\n",
      "0.60375\n",
      "Epoch 107, Loss: 0.9098679796\n",
      "0.58625\n",
      "Epoch 108, Loss: 0.9330429697\n",
      "0.59125\n",
      "Epoch 109, Loss: 0.9315671702\n",
      "0.57875\n",
      "Epoch 110, Loss: 0.9355417559\n",
      "0.5875\n",
      "Epoch 111, Loss: 0.9474428217\n",
      "0.585\n",
      "Epoch 112, Loss: 0.9353196672\n",
      "0.5875\n",
      "Epoch 113, Loss: 0.9480629996\n",
      "0.5725\n",
      "Epoch 114, Loss: 0.8852353347\n",
      "0.57875\n",
      "Epoch 115, Loss: 0.8422083817\n",
      "0.6025\n",
      "Epoch 116, Loss: 0.8460104668\n",
      "0.59875\n",
      "Epoch 117, Loss: 0.8688621434\n",
      "0.595\n",
      "Epoch 118, Loss: 0.8401508812\n",
      "0.595\n",
      "Epoch 119, Loss: 0.8766978959\n",
      "0.6\n",
      "Epoch 120, Loss: 0.8701546904\n",
      "0.58625\n",
      "Epoch 121, Loss: 0.8737758351\n",
      "0.575\n",
      "Epoch 122, Loss: 0.8966735898\n",
      "0.58625\n",
      "Epoch 123, Loss: 0.8421932901\n",
      "0.60875\n",
      "Epoch 124, Loss: 0.8490290072\n",
      "0.60375\n",
      "Epoch 125, Loss: 0.8426565370\n",
      "0.58875\n",
      "Epoch 126, Loss: 0.8105067395\n",
      "0.61625\n",
      "Epoch 127, Loss: 0.8110104157\n",
      "0.61125\n",
      "Epoch 128, Loss: 0.8295401932\n",
      "0.6225\n",
      "Epoch 129, Loss: 0.8974573142\n",
      "0.5575\n",
      "Epoch 130, Loss: 0.8419358045\n",
      "0.60625\n",
      "Epoch 131, Loss: 0.8783576630\n",
      "0.59375\n",
      "Epoch 132, Loss: 0.9350000229\n",
      "0.56375\n",
      "Epoch 133, Loss: 0.9163297254\n",
      "0.55\n",
      "Epoch 134, Loss: 0.8955876105\n",
      "0.57125\n",
      "Epoch 135, Loss: 0.8113898546\n",
      "0.59125\n",
      "Epoch 136, Loss: 0.7843818389\n",
      "0.59375\n",
      "Epoch 137, Loss: 0.8556285805\n",
      "0.58\n",
      "Epoch 138, Loss: 0.8214908757\n",
      "0.6075\n",
      "Epoch 139, Loss: 0.8025798321\n",
      "0.5975\n",
      "Epoch 140, Loss: 0.7520104764\n",
      "0.62375\n",
      "Epoch 141, Loss: 0.7546532242\n",
      "0.625\n",
      "Epoch 142, Loss: 0.7589628062\n",
      "0.6225\n",
      "Epoch 143, Loss: 0.7834864427\n",
      "0.61625\n",
      "Epoch 144, Loss: 0.8055726408\n",
      "0.5925\n",
      "Epoch 145, Loss: 0.7947802231\n",
      "0.61875\n",
      "Epoch 146, Loss: 0.7831734212\n",
      "0.585\n",
      "Epoch 147, Loss: 0.7391414637\n",
      "0.6325\n",
      "Epoch 148, Loss: 0.7483686922\n",
      "0.6125\n",
      "Epoch 149, Loss: 0.7628999443\n",
      "0.62\n",
      "Epoch 150, Loss: 0.7615877977\n",
      "0.61125\n",
      "Epoch 151, Loss: 0.8075120645\n",
      "0.61125\n",
      "Epoch 152, Loss: 0.8163797062\n",
      "0.6175\n",
      "Epoch 153, Loss: 0.8659346668\n",
      "0.575\n",
      "Epoch 154, Loss: 0.9105631453\n",
      "0.58875\n",
      "Epoch 155, Loss: 0.9275622532\n",
      "0.58375\n",
      "Epoch 156, Loss: 0.7699272541\n",
      "0.605\n",
      "Epoch 157, Loss: 0.7518277387\n",
      "0.63125\n",
      "Epoch 158, Loss: 0.7801170504\n",
      "0.61125\n",
      "Epoch 159, Loss: 0.7601883987\n",
      "0.6025\n",
      "Epoch 160, Loss: 0.7192746179\n",
      "0.62\n",
      "Epoch 161, Loss: 0.7295136526\n",
      "0.62\n",
      "Epoch 162, Loss: 0.7117539602\n",
      "0.625\n",
      "Epoch 163, Loss: 0.7219800679\n",
      "0.615\n",
      "Epoch 164, Loss: 0.7637813134\n",
      "0.61875\n",
      "Epoch 165, Loss: 0.7186139432\n",
      "0.62125\n",
      "Epoch 166, Loss: 0.7075222950\n",
      "0.60625\n",
      "Epoch 167, Loss: 0.7124632110\n",
      "0.61625\n",
      "Epoch 168, Loss: 0.6996023172\n",
      "0.63375\n",
      "Epoch 169, Loss: 0.7457207802\n",
      "0.61625\n",
      "Epoch 170, Loss: 0.8916253813\n",
      "0.57875\n",
      "Epoch 171, Loss: 0.7738978571\n",
      "0.63125\n",
      "Epoch 172, Loss: 0.7689673943\n",
      "0.59125\n",
      "Epoch 173, Loss: 0.7626484203\n",
      "0.62125\n",
      "Epoch 174, Loss: 0.7192016158\n",
      "0.605\n",
      "Epoch 175, Loss: 0.7966922068\n",
      "0.585\n",
      "Epoch 176, Loss: 0.7447606835\n",
      "0.61625\n",
      "Epoch 177, Loss: 0.7513115992\n",
      "0.6225\n",
      "Epoch 178, Loss: 0.7038626032\n",
      "0.62\n",
      "Epoch 179, Loss: 0.6824171620\n",
      "0.61875\n",
      "Epoch 180, Loss: 0.7114907454\n",
      "0.62875\n",
      "Epoch 181, Loss: 0.6786054222\n",
      "0.62125\n",
      "Epoch 182, Loss: 0.7568332997\n",
      "0.615\n",
      "Epoch 183, Loss: 0.7001066669\n",
      "0.595\n",
      "Epoch 184, Loss: 0.6669397256\n",
      "0.62625\n",
      "Epoch 185, Loss: 0.6974695039\n",
      "0.6175\n",
      "Epoch 186, Loss: 0.6811057737\n",
      "0.61875\n",
      "Epoch 187, Loss: 0.8071260875\n",
      "0.5975\n",
      "Epoch 188, Loss: 1.0353221991\n",
      "0.52875\n",
      "Epoch 189, Loss: 0.9266006725\n",
      "0.555\n",
      "Epoch 190, Loss: 0.9823478730\n",
      "0.54875\n",
      "Epoch 191, Loss: 0.9297325246\n",
      "0.54125\n",
      "Epoch 192, Loss: 0.7633246435\n",
      "0.62625\n",
      "Epoch 193, Loss: 0.9115881138\n",
      "0.5625\n",
      "Epoch 194, Loss: 0.9633230397\n",
      "0.54625\n",
      "Epoch 195, Loss: 0.8163313802\n",
      "0.59125\n",
      "Epoch 196, Loss: 0.8352827949\n",
      "0.58875\n",
      "Epoch 197, Loss: 0.8466869230\n",
      "0.58875\n",
      "Epoch 198, Loss: 0.7105398323\n",
      "0.61875\n",
      "Epoch 199, Loss: 1.0286743945\n",
      "0.515\n",
      "Epoch 200, Loss: 0.8329765427\n",
      "0.57375\n",
      "Epoch 201, Loss: 0.8519746059\n",
      "0.55\n",
      "Epoch 202, Loss: 0.7478551459\n",
      "0.6075\n",
      "Epoch 203, Loss: 0.7420674897\n",
      "0.59375\n",
      "Epoch 204, Loss: 0.8934419829\n",
      "0.54375\n",
      "Epoch 205, Loss: 0.6857339125\n",
      "0.62\n",
      "Epoch 206, Loss: 0.6752963016\n",
      "0.61\n",
      "Epoch 207, Loss: 0.6994012500\n",
      "0.595\n",
      "Epoch 208, Loss: 0.7824333339\n",
      "0.57125\n",
      "Epoch 209, Loss: 0.7010507196\n",
      "0.59875\n",
      "Epoch 210, Loss: 0.7145547317\n",
      "0.6225\n",
      "Epoch 211, Loss: 0.7561091526\n",
      "0.5925\n",
      "Epoch 212, Loss: 0.7609540961\n",
      "0.585\n",
      "Epoch 213, Loss: 0.7027858052\n",
      "0.60375\n",
      "Epoch 214, Loss: 0.6760985599\n",
      "0.62625\n",
      "Epoch 215, Loss: 0.6624653693\n",
      "0.62625\n",
      "Epoch 216, Loss: 0.6541770595\n",
      "0.61875\n",
      "Epoch 217, Loss: 0.6864962960\n",
      "0.595\n",
      "Epoch 218, Loss: 0.6956278064\n",
      "0.60625\n",
      "Epoch 219, Loss: 0.7026114507\n",
      "0.6\n",
      "Epoch 220, Loss: 0.6227187633\n",
      "0.63\n",
      "Epoch 221, Loss: 0.7037403195\n",
      "0.60125\n",
      "Epoch 222, Loss: 0.6585634087\n",
      "0.6325\n",
      "Epoch 223, Loss: 0.6529342965\n",
      "0.60375\n",
      "Epoch 224, Loss: 0.6691632756\n",
      "0.6125\n",
      "Epoch 225, Loss: 0.6791999988\n",
      "0.62375\n",
      "Epoch 226, Loss: 0.6947181545\n",
      "0.61125\n",
      "Epoch 227, Loss: 0.6850922999\n",
      "0.6\n",
      "Epoch 228, Loss: 0.6360547365\n",
      "0.61625\n",
      "Epoch 229, Loss: 0.6502290273\n",
      "0.615\n",
      "Epoch 230, Loss: 0.6375837818\n",
      "0.61\n",
      "Epoch 231, Loss: 0.6840413822\n",
      "0.595\n",
      "Epoch 232, Loss: 0.7606447503\n",
      "0.5725\n",
      "Epoch 233, Loss: 0.7228456068\n",
      "0.59875\n",
      "Epoch 234, Loss: 0.7035288827\n",
      "0.62125\n",
      "Epoch 235, Loss: 0.7208386913\n",
      "0.60375\n",
      "Epoch 236, Loss: 0.6944672520\n",
      "0.61\n",
      "Epoch 237, Loss: 0.6163762803\n",
      "0.65375\n",
      "Epoch 238, Loss: 0.6397331122\n",
      "0.6\n",
      "Epoch 239, Loss: 0.7206745237\n",
      "0.58125\n",
      "Epoch 240, Loss: 0.6899540060\n",
      "0.58\n",
      "Epoch 241, Loss: 0.7433209427\n",
      "0.58375\n",
      "Epoch 242, Loss: 0.6467494585\n",
      "0.59625\n",
      "Epoch 243, Loss: 0.6409234034\n",
      "0.61\n",
      "Epoch 244, Loss: 0.7607231790\n",
      "0.57375\n",
      "Epoch 245, Loss: 0.6459003886\n",
      "0.615\n",
      "Epoch 246, Loss: 0.6181057447\n",
      "0.615\n",
      "Epoch 247, Loss: 0.6770346969\n",
      "0.59875\n",
      "Epoch 248, Loss: 0.7128733535\n",
      "0.57625\n",
      "Epoch 249, Loss: 0.7996840322\n",
      "0.5625\n",
      "Epoch 250, Loss: 0.6469735937\n",
      "0.6\n",
      "Epoch 251, Loss: 0.6231293188\n",
      "0.63875\n",
      "Epoch 252, Loss: 0.6839266369\n",
      "0.61375\n",
      "Epoch 253, Loss: 0.7762786043\n",
      "0.58\n",
      "Epoch 254, Loss: 0.7064026044\n",
      "0.595\n",
      "Epoch 255, Loss: 0.7031589822\n",
      "0.6\n",
      "Epoch 256, Loss: 0.7194480096\n",
      "0.59125\n",
      "Epoch 257, Loss: 0.8711446465\n",
      "0.5325\n",
      "Epoch 258, Loss: 0.6627002854\n",
      "0.62\n",
      "Epoch 259, Loss: 0.6966886202\n",
      "0.59125\n",
      "Epoch 260, Loss: 0.7277600761\n",
      "0.5925\n",
      "Epoch 261, Loss: 0.7258575152\n",
      "0.58375\n",
      "Epoch 262, Loss: 0.7112292067\n",
      "0.6075\n",
      "Epoch 263, Loss: 0.6489758183\n",
      "0.6125\n",
      "Epoch 264, Loss: 0.7470628252\n",
      "0.59\n",
      "Epoch 265, Loss: 0.8536596468\n",
      "0.54\n",
      "Epoch 266, Loss: 0.7048087801\n",
      "0.60375\n",
      "Epoch 267, Loss: 0.6975938467\n",
      "0.5975\n",
      "Epoch 268, Loss: 0.6881303073\n",
      "0.61125\n",
      "Epoch 269, Loss: 0.7319939357\n",
      "0.59375\n",
      "Epoch 270, Loss: 0.7775304360\n",
      "0.6\n",
      "Epoch 271, Loss: 0.6233248009\n",
      "0.61\n",
      "Epoch 272, Loss: 0.6530342609\n",
      "0.61\n",
      "Epoch 273, Loss: 0.6644195959\n",
      "0.61\n",
      "Epoch 274, Loss: 0.7006329704\n",
      "0.59875\n",
      "Epoch 275, Loss: 0.6488893333\n",
      "0.61625\n",
      "Epoch 276, Loss: 0.6745585780\n",
      "0.6025\n",
      "Epoch 277, Loss: 0.7388257994\n",
      "0.5875\n",
      "Epoch 278, Loss: 0.6532025549\n",
      "0.615\n",
      "Epoch 279, Loss: 0.6613573061\n",
      "0.59375\n",
      "Epoch 280, Loss: 0.6016793610\n",
      "0.62\n",
      "Epoch 281, Loss: 0.5835978286\n",
      "0.63125\n",
      "Epoch 282, Loss: 0.5927775892\n",
      "0.62375\n",
      "Epoch 283, Loss: 0.6040582675\n",
      "0.62875\n",
      "Epoch 284, Loss: 0.6220270068\n",
      "0.61625\n",
      "Epoch 285, Loss: 0.5978077214\n",
      "0.62875\n",
      "Epoch 286, Loss: 0.5746071013\n",
      "0.6425\n",
      "Epoch 287, Loss: 0.5517322237\n",
      "0.63375\n",
      "Epoch 288, Loss: 0.6190044491\n",
      "0.63625\n",
      "Epoch 289, Loss: 0.6285286626\n",
      "0.62375\n",
      "Epoch 290, Loss: 0.6507210721\n",
      "0.615\n",
      "Epoch 291, Loss: 0.6270728838\n",
      "0.62375\n",
      "Epoch 292, Loss: 0.6047155537\n",
      "0.61625\n",
      "Epoch 293, Loss: 0.6874497351\n",
      "0.58625\n",
      "Epoch 294, Loss: 0.7247547406\n",
      "0.59\n",
      "Epoch 295, Loss: 0.5785559076\n",
      "0.6175\n",
      "Epoch 296, Loss: 0.5491635636\n",
      "0.635\n",
      "Epoch 297, Loss: 0.5369146277\n",
      "0.6325\n",
      "Epoch 298, Loss: 0.5586527462\n",
      "0.6225\n",
      "Epoch 299, Loss: 0.5370492289\n",
      "0.6325\n",
      "Epoch 300, Loss: 0.5238211576\n",
      "0.6375\n",
      "Epoch 301, Loss: 0.5509237568\n",
      "0.62\n",
      "Epoch 302, Loss: 0.6056439879\n",
      "0.6075\n",
      "Epoch 303, Loss: 0.6600056711\n",
      "0.595\n",
      "Epoch 304, Loss: 0.5952624686\n",
      "0.61875\n",
      "Epoch 305, Loss: 0.5720981967\n",
      "0.6325\n",
      "Epoch 306, Loss: 0.5538076333\n",
      "0.6425\n",
      "Epoch 307, Loss: 0.5394460767\n",
      "0.65125\n",
      "Epoch 308, Loss: 0.5215230947\n",
      "0.63875\n",
      "Epoch 309, Loss: 0.5448432877\n",
      "0.62875\n",
      "Epoch 310, Loss: 0.5550593779\n",
      "0.63625\n",
      "Epoch 311, Loss: 0.5955278563\n",
      "0.61375\n",
      "Epoch 312, Loss: 0.5396073266\n",
      "0.6475\n",
      "Epoch 313, Loss: 0.5580782040\n",
      "0.62375\n",
      "Epoch 314, Loss: 0.5447401847\n",
      "0.6425\n",
      "Epoch 315, Loss: 0.5263540371\n",
      "0.62875\n",
      "Epoch 316, Loss: 0.5259162790\n",
      "0.63\n",
      "Epoch 317, Loss: 0.4846237619\n",
      "0.65\n",
      "Epoch 318, Loss: 0.4742493812\n",
      "0.64625\n",
      "Epoch 319, Loss: 0.5440717373\n",
      "0.64\n",
      "Epoch 320, Loss: 0.6058459676\n",
      "0.615\n",
      "Epoch 321, Loss: 0.5689213172\n",
      "0.6175\n",
      "Epoch 322, Loss: 0.5075810450\n",
      "0.635\n",
      "Epoch 323, Loss: 0.4843941892\n",
      "0.6475\n",
      "Epoch 324, Loss: 0.4769259136\n",
      "0.65\n",
      "Epoch 325, Loss: 0.5260413237\n",
      "0.6325\n",
      "Epoch 326, Loss: 0.5370980995\n",
      "0.64125\n",
      "Epoch 327, Loss: 0.5453123633\n",
      "0.6225\n",
      "Epoch 328, Loss: 0.5325725436\n",
      "0.6475\n",
      "Epoch 329, Loss: 0.5058527589\n",
      "0.6475\n",
      "Epoch 330, Loss: 0.5185937740\n",
      "0.635\n",
      "Epoch 331, Loss: 0.5780738883\n",
      "0.5975\n",
      "Epoch 332, Loss: 0.5424860473\n",
      "0.6175\n",
      "Epoch 333, Loss: 0.5269624133\n",
      "0.62375\n",
      "Epoch 334, Loss: 0.4896166969\n",
      "0.6375\n",
      "Epoch 335, Loss: 0.5017302364\n",
      "0.63125\n",
      "Epoch 336, Loss: 0.5576400475\n",
      "0.6025\n",
      "Epoch 337, Loss: 0.6534422671\n",
      "0.59875\n",
      "Epoch 338, Loss: 0.6364837818\n",
      "0.605\n",
      "Epoch 339, Loss: 0.4975395768\n",
      "0.635\n",
      "Epoch 340, Loss: 0.5566174775\n",
      "0.6325\n",
      "Epoch 341, Loss: 0.5113452607\n",
      "0.64875\n",
      "Epoch 342, Loss: 0.4917803289\n",
      "0.66875\n",
      "Epoch 343, Loss: 0.4986883998\n",
      "0.63875\n",
      "Epoch 344, Loss: 0.4574302980\n",
      "0.655\n",
      "Epoch 345, Loss: 0.4493825621\n",
      "0.635\n",
      "Epoch 346, Loss: 0.5100448119\n",
      "0.6075\n",
      "Epoch 347, Loss: 0.5867764462\n",
      "0.605\n",
      "Epoch 348, Loss: 0.5599235418\n",
      "0.62375\n",
      "Epoch 349, Loss: 0.5273092756\n",
      "0.6325\n",
      "Epoch 350, Loss: 0.5330978382\n",
      "0.6275\n",
      "Epoch 351, Loss: 0.5262751305\n",
      "0.62875\n",
      "Epoch 352, Loss: 0.5272548849\n",
      "0.6125\n",
      "Epoch 353, Loss: 0.4499808706\n",
      "0.64625\n",
      "Epoch 354, Loss: 0.4683533477\n",
      "0.645\n",
      "Epoch 355, Loss: 0.5689731222\n",
      "0.615\n",
      "Epoch 356, Loss: 0.4978714265\n",
      "0.62375\n",
      "Epoch 357, Loss: 0.4557516359\n",
      "0.6375\n",
      "Epoch 358, Loss: 0.4760039585\n",
      "0.63375\n",
      "Epoch 359, Loss: 0.4714914878\n",
      "0.61875\n",
      "Epoch 360, Loss: 0.4547706205\n",
      "0.63625\n",
      "Epoch 361, Loss: 0.4728447371\n",
      "0.625\n",
      "Epoch 362, Loss: 0.4335679348\n",
      "0.63875\n",
      "Epoch 363, Loss: 0.4524374933\n",
      "0.63625\n",
      "Epoch 364, Loss: 0.4703633483\n",
      "0.63\n",
      "Epoch 365, Loss: 0.6892085692\n",
      "0.56375\n",
      "Epoch 366, Loss: 0.5134871828\n",
      "0.6325\n",
      "Epoch 367, Loss: 0.5258159680\n",
      "0.61375\n",
      "Epoch 368, Loss: 0.4777113423\n",
      "0.63\n",
      "Epoch 369, Loss: 0.4826917832\n",
      "0.62\n",
      "Epoch 370, Loss: 0.4781589977\n",
      "0.62125\n",
      "Epoch 371, Loss: 0.5415407118\n",
      "0.6075\n",
      "Epoch 372, Loss: 0.4992189872\n",
      "0.61875\n",
      "Epoch 373, Loss: 0.5521282234\n",
      "0.59625\n",
      "Epoch 374, Loss: 0.5114259206\n",
      "0.6075\n",
      "Epoch 375, Loss: 0.4693558557\n",
      "0.6225\n",
      "Epoch 376, Loss: 0.4054591347\n",
      "0.66125\n",
      "Epoch 377, Loss: 0.3985615333\n",
      "0.6375\n",
      "Epoch 378, Loss: 0.4024162594\n",
      "0.6475\n",
      "Epoch 379, Loss: 0.4270543273\n",
      "0.62625\n",
      "Epoch 380, Loss: 0.3832305211\n",
      "0.66125\n",
      "Epoch 381, Loss: 0.3939714200\n",
      "0.6375\n",
      "Epoch 382, Loss: 0.3923108187\n",
      "0.64875\n",
      "Epoch 383, Loss: 0.4121800906\n",
      "0.63625\n",
      "Epoch 384, Loss: 0.3634589976\n",
      "0.64625\n",
      "Epoch 385, Loss: 0.4380271510\n",
      "0.63875\n",
      "Epoch 386, Loss: 0.3803630397\n",
      "0.6475\n",
      "Epoch 387, Loss: 0.4803202106\n",
      "0.62375\n",
      "Epoch 388, Loss: 0.6184969541\n",
      "0.5875\n",
      "Epoch 389, Loss: 0.4961050984\n",
      "0.61125\n",
      "Epoch 390, Loss: 0.4647195373\n",
      "0.6225\n",
      "Epoch 391, Loss: 0.4592218594\n",
      "0.62125\n",
      "Epoch 392, Loss: 0.4547165359\n",
      "0.63\n",
      "Epoch 393, Loss: 0.4051359258\n",
      "0.64875\n",
      "Epoch 394, Loss: 0.3914507713\n",
      "0.62875\n",
      "Epoch 395, Loss: 0.3783398496\n",
      "0.645\n",
      "Epoch 396, Loss: 0.4066941514\n",
      "0.63125\n",
      "Epoch 397, Loss: 0.4666246583\n",
      "0.6025\n",
      "Epoch 398, Loss: 0.5118252227\n",
      "0.6125\n",
      "Epoch 399, Loss: 0.4079037990\n",
      "0.635\n",
      "Epoch 400, Loss: 0.4138267831\n",
      "0.64125\n",
      "Epoch 401, Loss: 0.3949425820\n",
      "0.6425\n",
      "Epoch 402, Loss: 0.4238159974\n",
      "0.6475\n",
      "Epoch 403, Loss: 0.3942118290\n",
      "0.64625\n",
      "Epoch 404, Loss: 0.4465209040\n",
      "0.63125\n",
      "Epoch 405, Loss: 0.3781224796\n",
      "0.655\n",
      "Epoch 406, Loss: 0.4146018770\n",
      "0.635\n",
      "Epoch 407, Loss: 0.4074276344\n",
      "0.63375\n",
      "Epoch 408, Loss: 0.4201542431\n",
      "0.6425\n",
      "Epoch 409, Loss: 0.4205385640\n",
      "0.64125\n",
      "Epoch 410, Loss: 0.4620916856\n",
      "0.62125\n",
      "Epoch 411, Loss: 0.5876622909\n",
      "0.59375\n",
      "Epoch 412, Loss: 0.4256010931\n",
      "0.625\n",
      "Epoch 413, Loss: 0.3820836983\n",
      "0.64375\n",
      "Epoch 414, Loss: 0.6722011767\n",
      "0.57875\n",
      "Epoch 415, Loss: 0.8843101409\n",
      "0.515\n",
      "Epoch 416, Loss: 0.7800605897\n",
      "0.5375\n",
      "Epoch 417, Loss: 0.6680304936\n",
      "0.57625\n",
      "Epoch 418, Loss: 0.6436437592\n",
      "0.575\n",
      "Epoch 419, Loss: 0.5423871198\n",
      "0.60125\n",
      "Epoch 420, Loss: 0.5920248217\n",
      "0.6125\n",
      "Epoch 421, Loss: 0.5453986552\n",
      "0.61125\n",
      "Epoch 422, Loss: 0.4661375001\n",
      "0.61875\n",
      "Epoch 423, Loss: 0.5118737503\n",
      "0.60125\n",
      "Epoch 424, Loss: 0.5683517962\n",
      "0.5775\n",
      "Epoch 425, Loss: 0.5819160298\n",
      "0.59375\n",
      "Epoch 426, Loss: 0.4677592982\n",
      "0.63375\n",
      "Epoch 427, Loss: 0.5443518094\n",
      "0.605\n",
      "Epoch 428, Loss: 0.7313951715\n",
      "0.55125\n",
      "Epoch 429, Loss: 0.5250449685\n",
      "0.60375\n",
      "Epoch 430, Loss: 0.5034754320\n",
      "0.6\n",
      "Epoch 431, Loss: 0.4669846106\n",
      "0.61\n",
      "Epoch 432, Loss: 0.5659162079\n",
      "0.60125\n",
      "Epoch 433, Loss: 0.4744986491\n",
      "0.61375\n",
      "Epoch 434, Loss: 0.4449255535\n",
      "0.62125\n",
      "Epoch 435, Loss: 0.4282926971\n",
      "0.61625\n",
      "Epoch 436, Loss: 0.4077222501\n",
      "0.61125\n",
      "Epoch 437, Loss: 0.4805869057\n",
      "0.5975\n",
      "Epoch 438, Loss: 0.4196901833\n",
      "0.61375\n",
      "Epoch 439, Loss: 0.4176033159\n",
      "0.62375\n",
      "Epoch 440, Loss: 0.4571188156\n",
      "0.625\n",
      "Epoch 441, Loss: 0.3984759104\n",
      "0.6425\n",
      "Epoch 442, Loss: 0.3372854471\n",
      "0.65375\n",
      "Epoch 443, Loss: 0.4048826116\n",
      "0.62375\n",
      "Epoch 444, Loss: 0.3465494075\n",
      "0.6325\n",
      "Epoch 445, Loss: 0.3880318906\n",
      "0.635\n",
      "Epoch 446, Loss: 0.3888628452\n",
      "0.6275\n",
      "Epoch 447, Loss: 0.4017670294\n",
      "0.615\n",
      "Epoch 448, Loss: 0.4379032828\n",
      "0.62375\n",
      "Epoch 449, Loss: 0.4455428917\n",
      "0.60125\n",
      "Epoch 450, Loss: 0.5292010817\n",
      "0.5925\n",
      "Epoch 451, Loss: 0.5324106629\n",
      "0.5975\n",
      "Epoch 452, Loss: 0.6128342170\n",
      "0.58\n",
      "Epoch 453, Loss: 0.4745719039\n",
      "0.59875\n",
      "Epoch 454, Loss: 0.4141981937\n",
      "0.6375\n",
      "Epoch 455, Loss: 0.4689250946\n",
      "0.6025\n",
      "Epoch 456, Loss: 0.6946741961\n",
      "0.57875\n",
      "Epoch 457, Loss: 0.4396743700\n",
      "0.6525\n",
      "Epoch 458, Loss: 0.5413254676\n",
      "0.59125\n",
      "Epoch 459, Loss: 0.5072912769\n",
      "0.61375\n",
      "Epoch 460, Loss: 0.5838176048\n",
      "0.57875\n",
      "Epoch 461, Loss: 0.5259106554\n",
      "0.59875\n",
      "Epoch 462, Loss: 0.5153994973\n",
      "0.60375\n",
      "Epoch 463, Loss: 0.4293271390\n",
      "0.63875\n",
      "Epoch 464, Loss: 0.4661901755\n",
      "0.6175\n",
      "Epoch 465, Loss: 0.4586411160\n",
      "0.605\n",
      "Epoch 466, Loss: 0.7225475820\n",
      "0.57375\n",
      "Epoch 467, Loss: 0.7022383862\n",
      "0.58125\n",
      "Epoch 468, Loss: 0.4495605113\n",
      "0.62875\n",
      "Epoch 469, Loss: 0.5056023958\n",
      "0.6175\n",
      "Epoch 470, Loss: 0.6703483737\n",
      "0.56875\n",
      "Epoch 471, Loss: 0.5339155040\n",
      "0.5875\n",
      "Epoch 472, Loss: 0.5541559764\n",
      "0.57\n",
      "Epoch 473, Loss: 0.5509638743\n",
      "0.59875\n",
      "Epoch 474, Loss: 0.4557250473\n",
      "0.61375\n",
      "Epoch 475, Loss: 0.5003394874\n",
      "0.62125\n",
      "Epoch 476, Loss: 0.4497233809\n",
      "0.635\n",
      "Epoch 477, Loss: 0.7378346424\n",
      "0.5925\n",
      "Epoch 478, Loss: 0.4778368230\n",
      "0.61375\n",
      "Epoch 479, Loss: 0.4005455697\n",
      "0.6575\n",
      "Epoch 480, Loss: 0.4417083596\n",
      "0.63625\n",
      "Epoch 481, Loss: 0.4386017826\n",
      "0.6425\n",
      "Epoch 482, Loss: 0.4150403812\n",
      "0.63375\n",
      "Epoch 483, Loss: 0.5820393784\n",
      "0.59125\n",
      "Epoch 484, Loss: 0.6544736407\n",
      "0.57875\n",
      "Epoch 485, Loss: 0.6376137078\n",
      "0.57375\n",
      "Epoch 486, Loss: 0.5976521834\n",
      "0.5775\n",
      "Epoch 487, Loss: 0.5368418003\n",
      "0.57375\n",
      "Epoch 488, Loss: 0.5947816339\n",
      "0.58875\n",
      "Epoch 489, Loss: 0.5903458892\n",
      "0.59\n",
      "Epoch 490, Loss: 0.4764919421\n",
      "0.6075\n",
      "Epoch 491, Loss: 0.4385535422\n",
      "0.6075\n",
      "Epoch 492, Loss: 0.5091488541\n",
      "0.605\n",
      "Epoch 493, Loss: 0.4722805367\n",
      "0.60125\n",
      "Epoch 494, Loss: 0.4314848546\n",
      "0.605\n",
      "Epoch 495, Loss: 0.4330893829\n",
      "0.60375\n",
      "Epoch 496, Loss: 0.6177423204\n",
      "0.5725\n",
      "Epoch 497, Loss: 0.5592860310\n",
      "0.60625\n",
      "Epoch 498, Loss: 0.6665728205\n",
      "0.57375\n",
      "Epoch 499, Loss: 0.6495865713\n",
      "0.61125\n",
      "Epoch 500, Loss: 0.4291098194\n",
      "0.61\n",
      "Epoch 501, Loss: 0.3623703750\n",
      "0.6375\n",
      "Epoch 502, Loss: 0.4272533765\n",
      "0.61\n",
      "Epoch 503, Loss: 0.4767484588\n",
      "0.6\n",
      "Epoch 504, Loss: 0.5015278102\n",
      "0.60375\n",
      "Epoch 505, Loss: 0.3662982906\n",
      "0.63875\n",
      "Epoch 506, Loss: 0.5002896903\n",
      "0.60625\n",
      "Epoch 507, Loss: 0.4264176734\n",
      "0.60875\n",
      "Epoch 508, Loss: 0.4222990202\n",
      "0.6325\n",
      "Epoch 509, Loss: 0.4531475889\n",
      "0.60625\n",
      "Epoch 510, Loss: 0.4736500627\n",
      "0.6175\n",
      "Epoch 511, Loss: 0.4505849387\n",
      "0.6\n",
      "Epoch 512, Loss: 0.4297479583\n",
      "0.60375\n",
      "Epoch 513, Loss: 0.4504467569\n",
      "0.61375\n",
      "Epoch 514, Loss: 0.5386688616\n",
      "0.59375\n",
      "Epoch 515, Loss: 0.4079982294\n",
      "0.62125\n",
      "Epoch 516, Loss: 0.5508515310\n",
      "0.595\n",
      "Epoch 517, Loss: 0.4862789672\n",
      "0.61\n",
      "Epoch 518, Loss: 0.7049163339\n",
      "0.58\n",
      "Epoch 519, Loss: 0.4822665491\n",
      "0.6275\n",
      "Epoch 520, Loss: 0.4794523387\n",
      "0.6175\n",
      "Epoch 521, Loss: 0.5909798670\n",
      "0.59\n",
      "Epoch 522, Loss: 0.6814151544\n",
      "0.58125\n",
      "Epoch 523, Loss: 0.5616395679\n",
      "0.60875\n",
      "Epoch 524, Loss: 0.5253390580\n",
      "0.6025\n",
      "Epoch 525, Loss: 0.5183499485\n",
      "0.6025\n",
      "Epoch 526, Loss: 0.5522244107\n",
      "0.59375\n",
      "Epoch 527, Loss: 0.5557456751\n",
      "0.5875\n",
      "Epoch 528, Loss: 0.6409517040\n",
      "0.56875\n",
      "Epoch 529, Loss: 0.6868287838\n",
      "0.57375\n",
      "Epoch 530, Loss: 0.4271466640\n",
      "0.63875\n",
      "Epoch 531, Loss: 0.5832947240\n",
      "0.58375\n",
      "Epoch 532, Loss: 0.5244470935\n",
      "0.6\n",
      "Epoch 533, Loss: 0.5484698280\n",
      "0.5825\n",
      "Epoch 534, Loss: 0.4256495576\n",
      "0.63125\n",
      "Epoch 535, Loss: 0.5677514466\n",
      "0.57125\n",
      "Epoch 536, Loss: 0.4101225472\n",
      "0.6225\n",
      "Epoch 537, Loss: 0.5005226798\n",
      "0.595\n",
      "Epoch 538, Loss: 0.5163115051\n",
      "0.6225\n",
      "Epoch 539, Loss: 0.5221454339\n",
      "0.62375\n",
      "Epoch 540, Loss: 0.4756970355\n",
      "0.6125\n",
      "Epoch 541, Loss: 0.5433639827\n",
      "0.59\n",
      "Epoch 542, Loss: 0.6300803568\n",
      "0.55\n",
      "Epoch 543, Loss: 0.7657416725\n",
      "0.55875\n",
      "Epoch 544, Loss: 0.4262722764\n",
      "0.635\n",
      "Epoch 545, Loss: 0.4089683698\n",
      "0.6475\n",
      "Epoch 546, Loss: 0.5707376920\n",
      "0.60125\n",
      "Epoch 547, Loss: 0.3826186341\n",
      "0.655\n",
      "Epoch 548, Loss: 0.3917744663\n",
      "0.6325\n",
      "Epoch 549, Loss: 0.5353349429\n",
      "0.6025\n",
      "Epoch 550, Loss: 0.4284996969\n",
      "0.60875\n",
      "Epoch 551, Loss: 0.3457535873\n",
      "0.625\n",
      "Epoch 552, Loss: 0.3415215894\n",
      "0.62125\n",
      "Epoch 553, Loss: 0.4132466796\n",
      "0.61875\n",
      "Epoch 554, Loss: 0.4042177727\n",
      "0.6125\n",
      "Epoch 555, Loss: 0.4893270050\n",
      "0.595\n",
      "Epoch 556, Loss: 0.5736318331\n",
      "0.6075\n",
      "Epoch 557, Loss: 0.6310461334\n",
      "0.605\n",
      "Epoch 558, Loss: 0.6022785015\n",
      "0.6\n",
      "Epoch 559, Loss: 0.5573744415\n",
      "0.595\n",
      "Epoch 560, Loss: 0.5970028447\n",
      "0.585\n",
      "Epoch 561, Loss: 0.7825942506\n",
      "0.5725\n",
      "Epoch 562, Loss: 0.5321760851\n",
      "0.60625\n",
      "Epoch 563, Loss: 0.4759033430\n",
      "0.64625\n",
      "Epoch 564, Loss: 0.6108054838\n",
      "0.60125\n",
      "Epoch 565, Loss: 0.5594521805\n",
      "0.605\n",
      "Epoch 566, Loss: 0.6310361949\n",
      "0.6\n",
      "Epoch 567, Loss: 0.4786213792\n",
      "0.6025\n",
      "Epoch 568, Loss: 0.6034964147\n",
      "0.59875\n",
      "Epoch 569, Loss: 0.6607630195\n",
      "0.59625\n",
      "Epoch 570, Loss: 0.5606095247\n",
      "0.615\n",
      "Epoch 571, Loss: 0.3634887641\n",
      "0.6475\n",
      "Epoch 572, Loss: 0.3349711571\n",
      "0.65625\n",
      "Epoch 573, Loss: 0.3697206392\n",
      "0.64125\n",
      "Epoch 574, Loss: 0.4337139157\n",
      "0.61625\n",
      "Epoch 575, Loss: 0.4010793640\n",
      "0.6075\n",
      "Epoch 576, Loss: 0.4078946148\n",
      "0.635\n",
      "Epoch 577, Loss: 0.4321316356\n",
      "0.63625\n",
      "Epoch 578, Loss: 0.4498377811\n",
      "0.62\n",
      "Epoch 579, Loss: 0.3813085354\n",
      "0.64125\n",
      "Epoch 580, Loss: 0.4192928895\n",
      "0.62625\n",
      "Epoch 581, Loss: 0.3901792950\n",
      "0.6575\n",
      "Epoch 582, Loss: 0.3612273596\n",
      "0.6525\n",
      "Epoch 583, Loss: 0.5017583100\n",
      "0.62375\n",
      "Epoch 584, Loss: 0.5066284289\n",
      "0.61875\n",
      "Epoch 585, Loss: 0.4030714373\n",
      "0.63375\n",
      "Epoch 586, Loss: 0.4779932356\n",
      "0.62625\n",
      "Epoch 587, Loss: 0.3940817683\n",
      "0.63375\n",
      "Epoch 588, Loss: 0.3164234320\n",
      "0.65375\n",
      "Epoch 589, Loss: 0.2741304725\n",
      "0.63125\n",
      "Epoch 590, Loss: 0.3577539303\n",
      "0.65\n",
      "Epoch 591, Loss: 0.3148810173\n",
      "0.64625\n",
      "Epoch 592, Loss: 0.4460331855\n",
      "0.60875\n",
      "Epoch 593, Loss: 0.3701829024\n",
      "0.62625\n",
      "Epoch 594, Loss: 0.4248944540\n",
      "0.58125\n",
      "Epoch 595, Loss: 0.3641525931\n",
      "0.615\n",
      "Epoch 596, Loss: 0.3219443419\n",
      "0.64875\n",
      "Epoch 597, Loss: 0.2913968053\n",
      "0.6425\n",
      "Epoch 598, Loss: 0.3398655535\n",
      "0.64125\n",
      "Epoch 599, Loss: 0.4103721397\n",
      "0.6225\n",
      "Epoch 600, Loss: 0.4094272497\n",
      "0.62875\n",
      "Epoch 601, Loss: 0.3787187524\n",
      "0.64625\n",
      "Epoch 602, Loss: 0.3809898087\n",
      "0.61125\n",
      "Epoch 603, Loss: 0.3135173391\n",
      "0.63875\n",
      "Epoch 604, Loss: 0.3553610383\n",
      "0.60125\n",
      "Epoch 605, Loss: 0.3924273417\n",
      "0.60375\n",
      "Epoch 606, Loss: 0.3265970017\n",
      "0.62\n",
      "Epoch 607, Loss: 0.3159112408\n",
      "0.63875\n",
      "Epoch 608, Loss: 0.3147979569\n",
      "0.635\n",
      "Epoch 609, Loss: 0.4927481867\n",
      "0.6125\n",
      "Epoch 610, Loss: 0.4312453434\n",
      "0.62625\n",
      "Epoch 611, Loss: 0.3452398425\n",
      "0.61875\n",
      "Epoch 612, Loss: 0.3920852852\n",
      "0.60125\n",
      "Epoch 613, Loss: 0.4746853336\n",
      "0.6025\n",
      "Epoch 614, Loss: 0.4148166829\n",
      "0.63875\n",
      "Epoch 615, Loss: 0.3702951376\n",
      "0.63375\n",
      "Epoch 616, Loss: 0.4022870120\n",
      "0.59375\n",
      "Epoch 617, Loss: 0.3753521795\n",
      "0.61875\n",
      "Epoch 618, Loss: 0.5835493409\n",
      "0.5675\n",
      "Epoch 619, Loss: 0.5235708838\n",
      "0.58875\n",
      "Epoch 620, Loss: 0.5811812004\n",
      "0.58125\n",
      "Epoch 621, Loss: 0.5437465113\n",
      "0.605\n",
      "Epoch 622, Loss: 0.3859406593\n",
      "0.6375\n",
      "Epoch 623, Loss: 0.2938043513\n",
      "0.6425\n",
      "Epoch 624, Loss: 0.3844889367\n",
      "0.635\n",
      "Epoch 625, Loss: 0.4652242475\n",
      "0.58375\n",
      "Epoch 626, Loss: 0.4101841942\n",
      "0.6325\n",
      "Epoch 627, Loss: 0.4415178676\n",
      "0.61875\n",
      "Epoch 628, Loss: 0.3655608541\n",
      "0.62875\n",
      "Epoch 629, Loss: 0.3596435928\n",
      "0.63875\n",
      "Epoch 630, Loss: 0.3845997205\n",
      "0.6125\n",
      "Epoch 631, Loss: 0.5552833873\n",
      "0.58875\n",
      "Epoch 632, Loss: 0.4022448226\n",
      "0.605\n",
      "Epoch 633, Loss: 0.5345963244\n",
      "0.595\n",
      "Epoch 634, Loss: 0.4307133677\n",
      "0.60375\n",
      "Epoch 635, Loss: 0.4057305731\n",
      "0.61\n",
      "Epoch 636, Loss: 0.3823575139\n",
      "0.63\n",
      "Epoch 637, Loss: 0.4748972986\n",
      "0.61625\n",
      "Epoch 638, Loss: 0.3863650608\n",
      "0.62625\n",
      "Epoch 639, Loss: 0.4397847728\n",
      "0.60375\n",
      "Epoch 640, Loss: 0.4322846273\n",
      "0.61875\n",
      "Epoch 641, Loss: 0.4721837295\n",
      "0.5875\n",
      "Epoch 642, Loss: 0.3948608251\n",
      "0.625\n",
      "Epoch 643, Loss: 0.3850638488\n",
      "0.61375\n",
      "Epoch 644, Loss: 0.4645505770\n",
      "0.615\n",
      "Epoch 645, Loss: 0.3794776875\n",
      "0.62125\n",
      "Epoch 646, Loss: 0.3201063930\n",
      "0.6125\n",
      "Epoch 647, Loss: 0.3658624153\n",
      "0.61625\n",
      "Epoch 648, Loss: 0.4254382549\n",
      "0.6225\n",
      "Epoch 649, Loss: 0.3757898710\n",
      "0.62125\n",
      "Epoch 650, Loss: 0.3242727227\n",
      "0.64\n",
      "Epoch 651, Loss: 0.3062249461\n",
      "0.64\n",
      "Epoch 652, Loss: 0.3681320005\n",
      "0.6375\n",
      "Epoch 653, Loss: 0.3247443297\n",
      "0.6375\n",
      "Epoch 654, Loss: 0.2759363759\n",
      "0.6375\n",
      "Epoch 655, Loss: 0.3125423253\n",
      "0.6325\n",
      "Epoch 656, Loss: 0.3344139053\n",
      "0.64625\n",
      "Epoch 657, Loss: 0.3329287021\n",
      "0.64125\n",
      "Epoch 658, Loss: 0.3113173810\n",
      "0.63\n",
      "Epoch 659, Loss: 0.3260354877\n",
      "0.63125\n",
      "Epoch 660, Loss: 0.3021405641\n",
      "0.64\n",
      "Epoch 661, Loss: 0.3243975098\n",
      "0.61875\n",
      "Epoch 662, Loss: 0.2917719698\n",
      "0.635\n",
      "Epoch 663, Loss: 0.2747073297\n",
      "0.63375\n",
      "Epoch 664, Loss: 0.2887233975\n",
      "0.64625\n",
      "Epoch 665, Loss: 0.2366886541\n",
      "0.6375\n",
      "Epoch 666, Loss: 0.2501057536\n",
      "0.655\n",
      "Epoch 667, Loss: 0.2753351475\n",
      "0.64375\n",
      "Epoch 668, Loss: 0.3396391582\n",
      "0.64625\n",
      "Epoch 669, Loss: 0.3036592799\n",
      "0.6375\n",
      "Epoch 670, Loss: 0.3589780510\n",
      "0.6325\n",
      "Epoch 671, Loss: 0.2870302582\n",
      "0.6425\n",
      "Epoch 672, Loss: 0.2776965992\n",
      "0.63125\n",
      "Epoch 673, Loss: 0.4170803860\n",
      "0.59125\n",
      "Epoch 674, Loss: 0.3563825662\n",
      "0.6\n",
      "Epoch 675, Loss: 0.4443444775\n",
      "0.59875\n",
      "Epoch 676, Loss: 0.4510767785\n",
      "0.60375\n",
      "Epoch 677, Loss: 0.3936073665\n",
      "0.6275\n",
      "Epoch 678, Loss: 0.3753871860\n",
      "0.60875\n",
      "Epoch 679, Loss: 0.4345094822\n",
      "0.59\n",
      "Epoch 680, Loss: 0.3100252685\n",
      "0.6475\n",
      "Epoch 681, Loss: 0.3327530520\n",
      "0.63\n",
      "Epoch 682, Loss: 0.2895081201\n",
      "0.65125\n",
      "Epoch 683, Loss: 0.3287944073\n",
      "0.62\n",
      "Epoch 684, Loss: 0.3173693474\n",
      "0.6275\n",
      "Epoch 685, Loss: 0.3094546003\n",
      "0.6275\n",
      "Epoch 686, Loss: 0.4076012237\n",
      "0.61125\n",
      "Epoch 687, Loss: 0.3874928751\n",
      "0.6125\n",
      "Epoch 688, Loss: 0.3408033096\n",
      "0.63125\n",
      "Epoch 689, Loss: 0.2328849941\n",
      "0.65625\n",
      "Epoch 690, Loss: 0.3374355773\n",
      "0.6425\n",
      "Epoch 691, Loss: 0.3015970404\n",
      "0.6275\n",
      "Epoch 692, Loss: 0.2643962272\n",
      "0.63375\n",
      "Epoch 693, Loss: 0.2538193329\n",
      "0.64125\n",
      "Epoch 694, Loss: 0.2666847988\n",
      "0.625\n",
      "Epoch 695, Loss: 0.3617434959\n",
      "0.63875\n",
      "Epoch 696, Loss: 0.3175618138\n",
      "0.62875\n",
      "Epoch 697, Loss: 0.3019244097\n",
      "0.6325\n",
      "Epoch 698, Loss: 0.3616177526\n",
      "0.63\n",
      "Epoch 699, Loss: 0.3930739251\n",
      "0.61125\n",
      "Epoch 700, Loss: 0.5771699349\n",
      "0.56875\n",
      "Epoch 701, Loss: 0.3809357123\n",
      "0.61375\n",
      "Epoch 702, Loss: 0.3082069748\n",
      "0.62875\n",
      "Epoch 703, Loss: 0.3513049876\n",
      "0.58125\n",
      "Epoch 704, Loss: 0.3608451401\n",
      "0.6\n",
      "Epoch 705, Loss: 0.2630008894\n",
      "0.635\n",
      "Epoch 706, Loss: 0.3038550625\n",
      "0.63125\n",
      "Epoch 707, Loss: 0.3099602115\n",
      "0.63875\n",
      "Epoch 708, Loss: 0.3315768879\n",
      "0.6425\n",
      "Epoch 709, Loss: 0.3173202803\n",
      "0.64125\n",
      "Epoch 710, Loss: 0.2927673654\n",
      "0.64375\n",
      "Epoch 711, Loss: 0.3012311341\n",
      "0.63625\n",
      "Epoch 712, Loss: 0.2664445822\n",
      "0.63\n",
      "Epoch 713, Loss: 0.2415700371\n",
      "0.63\n",
      "Epoch 714, Loss: 0.2352103307\n",
      "0.66375\n",
      "Epoch 715, Loss: 0.2637146120\n",
      "0.64875\n",
      "Epoch 716, Loss: 0.2344517130\n",
      "0.6425\n",
      "Epoch 717, Loss: 0.2695529200\n",
      "0.6375\n",
      "Epoch 718, Loss: 0.2312918740\n",
      "0.64875\n",
      "Epoch 719, Loss: 0.2208851249\n",
      "0.6275\n",
      "Epoch 720, Loss: 0.2436306577\n",
      "0.64375\n",
      "Epoch 721, Loss: 0.2064083747\n",
      "0.63625\n",
      "Epoch 722, Loss: 0.2190929728\n",
      "0.64\n",
      "Epoch 723, Loss: 0.2084392531\n",
      "0.635\n",
      "Epoch 724, Loss: 0.2061561316\n",
      "0.64\n",
      "Epoch 725, Loss: 0.2044363365\n",
      "0.65\n",
      "Epoch 726, Loss: 0.1828915161\n",
      "0.65625\n",
      "Epoch 727, Loss: 0.2275186293\n",
      "0.64375\n",
      "Epoch 728, Loss: 0.2219972102\n",
      "0.65375\n",
      "Epoch 729, Loss: 0.2223346215\n",
      "0.65625\n",
      "Epoch 730, Loss: 0.3306394533\n",
      "0.62375\n",
      "Epoch 731, Loss: 0.3390894836\n",
      "0.6425\n",
      "Epoch 732, Loss: 0.4433212662\n",
      "0.6175\n",
      "Epoch 733, Loss: 0.4083179126\n",
      "0.62625\n",
      "Epoch 734, Loss: 0.4249203889\n",
      "0.60125\n",
      "Epoch 735, Loss: 0.3788621816\n",
      "0.6175\n",
      "Epoch 736, Loss: 0.3306202431\n",
      "0.6175\n",
      "Epoch 737, Loss: 0.3482651425\n",
      "0.61\n",
      "Epoch 738, Loss: 0.3308110032\n",
      "0.6125\n",
      "Epoch 739, Loss: 0.4451115566\n",
      "0.5875\n",
      "Epoch 740, Loss: 0.6756881858\n",
      "0.555\n",
      "Epoch 741, Loss: 0.2925960878\n",
      "0.62\n",
      "Epoch 742, Loss: 0.4480716681\n",
      "0.59125\n",
      "Epoch 743, Loss: 0.3487883643\n",
      "0.5975\n",
      "Epoch 744, Loss: 0.3524723272\n",
      "0.6025\n",
      "Epoch 745, Loss: 0.5265411724\n",
      "0.58875\n",
      "Epoch 746, Loss: 0.3011918106\n",
      "0.63\n",
      "Epoch 747, Loss: 0.3920782235\n",
      "0.61125\n",
      "Epoch 748, Loss: 0.3837922151\n",
      "0.60625\n",
      "Epoch 749, Loss: 0.4054284183\n",
      "0.59\n",
      "Epoch 750, Loss: 0.4222650626\n",
      "0.60625\n",
      "Epoch 751, Loss: 0.2304260084\n",
      "0.65125\n",
      "Epoch 752, Loss: 0.3108273784\n",
      "0.605\n",
      "Epoch 753, Loss: 0.3462311649\n",
      "0.5925\n",
      "Epoch 754, Loss: 0.3256782199\n",
      "0.61625\n",
      "Epoch 755, Loss: 0.3552164147\n",
      "0.62\n",
      "Epoch 756, Loss: 0.5280179483\n",
      "0.61375\n",
      "Epoch 757, Loss: 0.2819064070\n",
      "0.6325\n",
      "Epoch 758, Loss: 0.2583151602\n",
      "0.62625\n",
      "Epoch 759, Loss: 0.3875141783\n",
      "0.5925\n",
      "Epoch 760, Loss: 0.3261310217\n",
      "0.60625\n",
      "Epoch 761, Loss: 0.3256872737\n",
      "0.63\n",
      "Epoch 762, Loss: 0.2710207114\n",
      "0.6325\n",
      "Epoch 763, Loss: 0.3201935536\n",
      "0.63\n",
      "Epoch 764, Loss: 0.3215153363\n",
      "0.6375\n",
      "Epoch 765, Loss: 0.2943956822\n",
      "0.64\n",
      "Epoch 766, Loss: 0.3517549743\n",
      "0.6225\n",
      "Epoch 767, Loss: 0.4010717373\n",
      "0.6025\n",
      "Epoch 768, Loss: 0.5589120294\n",
      "0.58875\n",
      "Epoch 769, Loss: 0.4522689991\n",
      "0.6125\n",
      "Epoch 770, Loss: 0.3814996220\n",
      "0.62125\n",
      "Epoch 771, Loss: 0.3517760373\n",
      "0.62625\n",
      "Epoch 772, Loss: 0.5867386492\n",
      "0.6125\n",
      "Epoch 773, Loss: 0.4834105797\n",
      "0.6275\n",
      "Epoch 774, Loss: 0.2327313414\n",
      "0.63625\n",
      "Epoch 775, Loss: 0.3654477497\n",
      "0.595\n",
      "Epoch 776, Loss: 0.3323727051\n",
      "0.60625\n",
      "Epoch 777, Loss: 0.3121429880\n",
      "0.625\n",
      "Epoch 778, Loss: 0.2268065392\n",
      "0.635\n",
      "Epoch 779, Loss: 0.2911998025\n",
      "0.6325\n",
      "Epoch 780, Loss: 0.3491500663\n",
      "0.6375\n",
      "Epoch 781, Loss: 0.2816910475\n",
      "0.6175\n",
      "Epoch 782, Loss: 0.3807813892\n",
      "0.6375\n",
      "Epoch 783, Loss: 0.4044318412\n",
      "0.61125\n",
      "Epoch 784, Loss: 0.5494829556\n",
      "0.59625\n",
      "Epoch 785, Loss: 0.3577895238\n",
      "0.58125\n",
      "Epoch 786, Loss: 0.2132679988\n",
      "0.64875\n",
      "Epoch 787, Loss: 0.4538097033\n",
      "0.61125\n",
      "Epoch 788, Loss: 0.2069132593\n",
      "0.6375\n",
      "Epoch 789, Loss: 0.4495319726\n",
      "0.6\n",
      "Epoch 790, Loss: 0.2770521261\n",
      "0.62\n",
      "Epoch 791, Loss: 0.4318918541\n",
      "0.5975\n",
      "Epoch 792, Loss: 0.3639577455\n",
      "0.61\n",
      "Epoch 793, Loss: 0.4738530687\n",
      "0.6075\n",
      "Epoch 794, Loss: 0.3596797836\n",
      "0.635\n",
      "Epoch 795, Loss: 0.3831139542\n",
      "0.62125\n",
      "Epoch 796, Loss: 0.3666645926\n",
      "0.61625\n",
      "Epoch 797, Loss: 0.3024480085\n",
      "0.63875\n",
      "Epoch 798, Loss: 0.3868004781\n",
      "0.60375\n",
      "Epoch 799, Loss: 0.3711652485\n",
      "0.61875\n",
      "Epoch 800, Loss: 0.3891500302\n",
      "0.6125\n",
      "Epoch 801, Loss: 0.4480338691\n",
      "0.6275\n",
      "Epoch 802, Loss: 0.4122430914\n",
      "0.60625\n",
      "Epoch 803, Loss: 0.3806386459\n",
      "0.6225\n",
      "Epoch 804, Loss: 0.4174278906\n",
      "0.62125\n",
      "Epoch 805, Loss: 0.4283506772\n",
      "0.625\n",
      "Epoch 806, Loss: 0.8450202270\n",
      "0.57625\n",
      "Epoch 807, Loss: 0.5650281407\n",
      "0.60625\n",
      "Epoch 808, Loss: 0.3070759760\n",
      "0.62625\n",
      "Epoch 809, Loss: 0.4218787733\n",
      "0.61375\n",
      "Epoch 810, Loss: 0.5083219046\n",
      "0.61125\n",
      "Epoch 811, Loss: 0.5319382242\n",
      "0.6125\n",
      "Epoch 812, Loss: 0.4254764859\n",
      "0.6\n",
      "Epoch 813, Loss: 0.2545193583\n",
      "0.61375\n",
      "Epoch 814, Loss: 0.2961995910\n",
      "0.6225\n",
      "Epoch 815, Loss: 0.3025519982\n",
      "0.61625\n",
      "Epoch 816, Loss: 0.2892471925\n",
      "0.61625\n",
      "Epoch 817, Loss: 0.3009256520\n",
      "0.63125\n",
      "Epoch 818, Loss: 0.3041540528\n",
      "0.63375\n",
      "Epoch 819, Loss: 0.2710004137\n",
      "0.62625\n",
      "Epoch 820, Loss: 0.3003016254\n",
      "0.6125\n",
      "Epoch 821, Loss: 0.2530530299\n",
      "0.6125\n",
      "Epoch 822, Loss: 0.2149556579\n",
      "0.63875\n",
      "Epoch 823, Loss: 0.4191198470\n",
      "0.60625\n",
      "Epoch 824, Loss: 0.3303010432\n",
      "0.62375\n",
      "Epoch 825, Loss: 0.1836115751\n",
      "0.6325\n",
      "Epoch 826, Loss: 0.3133643463\n",
      "0.62625\n",
      "Epoch 827, Loss: 0.2653428648\n",
      "0.62375\n",
      "Epoch 828, Loss: 0.3761487917\n",
      "0.61125\n",
      "Epoch 829, Loss: 0.3501091007\n",
      "0.60875\n",
      "Epoch 830, Loss: 0.3958352955\n",
      "0.63875\n",
      "Epoch 831, Loss: 0.2473153405\n",
      "0.63375\n",
      "Epoch 832, Loss: 0.3383375005\n",
      "0.645\n",
      "Epoch 833, Loss: 0.3081742197\n",
      "0.62125\n",
      "Epoch 834, Loss: 0.3707577172\n",
      "0.61\n",
      "Epoch 835, Loss: 0.3393323898\n",
      "0.62875\n",
      "Epoch 836, Loss: 0.3980564904\n",
      "0.6075\n",
      "Epoch 837, Loss: 0.4144630225\n",
      "0.62875\n",
      "Epoch 838, Loss: 0.3240855930\n",
      "0.605\n",
      "Epoch 839, Loss: 0.4143946574\n",
      "0.61\n",
      "Epoch 840, Loss: 0.4407315584\n",
      "0.605\n",
      "Epoch 841, Loss: 0.3411060035\n",
      "0.59375\n",
      "Epoch 842, Loss: 0.2729720425\n",
      "0.605\n",
      "Epoch 843, Loss: 0.2290388378\n",
      "0.63875\n",
      "Epoch 844, Loss: 0.2287489763\n",
      "0.62875\n",
      "Epoch 845, Loss: 0.3834824891\n",
      "0.62\n",
      "Epoch 846, Loss: 0.3501563986\n",
      "0.625\n",
      "Epoch 847, Loss: 0.3934840735\n",
      "0.6275\n",
      "Epoch 848, Loss: 0.3586280189\n",
      "0.6075\n",
      "Epoch 849, Loss: 0.3859426046\n",
      "0.61\n",
      "Epoch 850, Loss: 0.4592008652\n",
      "0.58625\n",
      "Epoch 851, Loss: 0.4563230037\n",
      "0.57875\n",
      "Epoch 852, Loss: 0.4020879902\n",
      "0.5925\n",
      "Epoch 853, Loss: 0.3430594052\n",
      "0.59375\n",
      "Epoch 854, Loss: 0.2428002644\n",
      "0.61375\n",
      "Epoch 855, Loss: 0.2180648247\n",
      "0.64125\n",
      "Epoch 856, Loss: 0.2810116986\n",
      "0.635\n",
      "Epoch 857, Loss: 0.2922502250\n",
      "0.6175\n",
      "Epoch 858, Loss: 0.3653478344\n",
      "0.6175\n",
      "Epoch 859, Loss: 0.3564231366\n",
      "0.62375\n",
      "Epoch 860, Loss: 0.3824289641\n",
      "0.62375\n",
      "Epoch 861, Loss: 0.2785008315\n",
      "0.63625\n",
      "Epoch 862, Loss: 0.3557970122\n",
      "0.63\n",
      "Epoch 863, Loss: 0.3145076376\n",
      "0.62625\n",
      "Epoch 864, Loss: 0.2879237596\n",
      "0.6225\n",
      "Epoch 865, Loss: 0.3127293545\n",
      "0.60875\n",
      "Epoch 866, Loss: 0.2463945994\n",
      "0.6475\n",
      "Epoch 867, Loss: 0.3030785925\n",
      "0.64625\n",
      "Epoch 868, Loss: 0.3541655643\n",
      "0.62625\n",
      "Epoch 869, Loss: 0.3171223086\n",
      "0.6275\n",
      "Epoch 870, Loss: 0.3901806164\n",
      "0.5975\n",
      "Epoch 871, Loss: 0.3843381345\n",
      "0.61\n",
      "Epoch 872, Loss: 0.4909665008\n",
      "0.6\n",
      "Epoch 873, Loss: 0.2719101414\n",
      "0.63125\n",
      "Epoch 874, Loss: 0.3580847873\n",
      "0.6225\n",
      "Epoch 875, Loss: 0.4570069926\n",
      "0.6025\n",
      "Epoch 876, Loss: 0.4244257784\n",
      "0.6075\n",
      "Epoch 877, Loss: 0.5437868857\n",
      "0.58875\n",
      "Epoch 878, Loss: 0.6116022683\n",
      "0.58125\n",
      "Epoch 879, Loss: 0.5927062537\n",
      "0.58375\n",
      "Epoch 880, Loss: 0.5365829028\n",
      "0.59625\n",
      "Epoch 881, Loss: 0.3238557426\n",
      "0.6175\n",
      "Epoch 882, Loss: 0.2851414337\n",
      "0.62625\n",
      "Epoch 883, Loss: 0.3523598488\n",
      "0.61375\n",
      "Epoch 884, Loss: 0.6890588484\n",
      "0.57375\n",
      "Epoch 885, Loss: 0.6643036895\n",
      "0.57875\n",
      "Epoch 886, Loss: 0.4430523776\n",
      "0.6075\n",
      "Epoch 887, Loss: 0.3442909767\n",
      "0.60625\n",
      "Epoch 888, Loss: 0.5826619795\n",
      "0.56\n",
      "Epoch 889, Loss: 0.2350180490\n",
      "0.64125\n",
      "Epoch 890, Loss: 0.3833219356\n",
      "0.61375\n",
      "Epoch 891, Loss: 0.3326341709\n",
      "0.62625\n",
      "Epoch 892, Loss: 0.5049482577\n",
      "0.59875\n",
      "Epoch 893, Loss: 0.2824969543\n",
      "0.6075\n",
      "Epoch 894, Loss: 0.4752380531\n",
      "0.61\n",
      "Epoch 895, Loss: 0.2884306357\n",
      "0.61875\n",
      "Epoch 896, Loss: 0.3506390213\n",
      "0.6125\n",
      "Epoch 897, Loss: 0.3405114068\n",
      "0.6225\n",
      "Epoch 898, Loss: 0.3815343305\n",
      "0.61125\n",
      "Epoch 899, Loss: 0.3224406865\n",
      "0.64625\n",
      "Epoch 900, Loss: 0.3480516056\n",
      "0.5975\n",
      "Epoch 901, Loss: 0.3160371048\n",
      "0.61875\n",
      "Epoch 902, Loss: 0.3727793255\n",
      "0.6175\n",
      "Epoch 903, Loss: 0.3459468010\n",
      "0.62875\n",
      "Epoch 904, Loss: 0.4601827777\n",
      "0.60625\n",
      "Epoch 905, Loss: 0.4691466998\n",
      "0.60125\n",
      "Epoch 906, Loss: 0.4444203682\n",
      "0.615\n",
      "Epoch 907, Loss: 0.3217375694\n",
      "0.62125\n",
      "Epoch 908, Loss: 0.3111377574\n",
      "0.5975\n",
      "Epoch 909, Loss: 0.4232120180\n",
      "0.58625\n",
      "Epoch 910, Loss: 0.2963533049\n",
      "0.59375\n",
      "Epoch 911, Loss: 0.3256109326\n",
      "0.5925\n",
      "Epoch 912, Loss: 0.3603480449\n",
      "0.5925\n",
      "Epoch 913, Loss: 0.3482891473\n",
      "0.60625\n",
      "Epoch 914, Loss: 0.2724984076\n",
      "0.6075\n",
      "Epoch 915, Loss: 0.2402460514\n",
      "0.63625\n",
      "Epoch 916, Loss: 0.2407202831\n",
      "0.64\n",
      "Epoch 917, Loss: 0.2378920702\n",
      "0.62125\n",
      "Epoch 918, Loss: 0.2477474734\n",
      "0.6425\n",
      "Epoch 919, Loss: 0.2310805557\n",
      "0.61625\n",
      "Epoch 920, Loss: 0.2782981416\n",
      "0.61875\n",
      "Epoch 921, Loss: 0.2794378594\n",
      "0.615\n",
      "Epoch 922, Loss: 0.2829888819\n",
      "0.60125\n",
      "Epoch 923, Loss: 0.3240302520\n",
      "0.615\n",
      "Epoch 924, Loss: 0.2396355230\n",
      "0.6275\n",
      "Epoch 925, Loss: 0.3280465662\n",
      "0.59875\n",
      "Epoch 926, Loss: 0.4377079737\n",
      "0.59375\n",
      "Epoch 927, Loss: 0.4354941923\n",
      "0.60375\n",
      "Epoch 928, Loss: 0.4348985858\n",
      "0.61\n",
      "Epoch 929, Loss: 0.5094965020\n",
      "0.58875\n",
      "Epoch 930, Loss: 0.5221535387\n",
      "0.5975\n",
      "Epoch 931, Loss: 0.3379269195\n",
      "0.60375\n",
      "Epoch 932, Loss: 0.3614216752\n",
      "0.59875\n",
      "Epoch 933, Loss: 0.3380660410\n",
      "0.575\n",
      "Epoch 934, Loss: 0.4118482272\n",
      "0.5875\n",
      "Epoch 935, Loss: 0.4258454028\n",
      "0.58375\n",
      "Epoch 936, Loss: 0.7249497055\n",
      "0.58\n",
      "Epoch 937, Loss: 0.2558863166\n",
      "0.62125\n",
      "Epoch 938, Loss: 0.3342726301\n",
      "0.6025\n",
      "Epoch 939, Loss: 0.5240102394\n",
      "0.5625\n",
      "Epoch 940, Loss: 0.4839228372\n",
      "0.58\n",
      "Epoch 941, Loss: 0.6720409687\n",
      "0.5525\n",
      "Epoch 942, Loss: 0.4600300560\n",
      "0.57125\n",
      "Epoch 943, Loss: 0.2827515015\n",
      "0.6175\n",
      "Epoch 944, Loss: 0.8011119222\n",
      "0.55\n",
      "Epoch 945, Loss: 0.3659447802\n",
      "0.5925\n",
      "Epoch 946, Loss: 0.3483854899\n",
      "0.61\n",
      "Epoch 947, Loss: 0.5359905626\n",
      "0.56\n",
      "Epoch 948, Loss: 0.6850667474\n",
      "0.5775\n",
      "Epoch 949, Loss: 0.3966754637\n",
      "0.59125\n",
      "Epoch 950, Loss: 0.3870987383\n",
      "0.60375\n",
      "Epoch 951, Loss: 0.3630272100\n",
      "0.605\n",
      "Epoch 952, Loss: 0.4873993801\n",
      "0.5625\n",
      "Epoch 953, Loss: 0.4779682361\n",
      "0.59625\n",
      "Epoch 954, Loss: 0.4841713742\n",
      "0.595\n",
      "Epoch 955, Loss: 0.2952511088\n",
      "0.6225\n",
      "Epoch 956, Loss: 0.3525496434\n",
      "0.595\n",
      "Epoch 957, Loss: 0.2991796285\n",
      "0.605\n",
      "Epoch 958, Loss: 0.2537982856\n",
      "0.6475\n",
      "Epoch 959, Loss: 0.3524752854\n",
      "0.63875\n",
      "Epoch 960, Loss: 0.2740321224\n",
      "0.6375\n",
      "Epoch 961, Loss: 0.3043074041\n",
      "0.6275\n",
      "Epoch 962, Loss: 0.2930129113\n",
      "0.6375\n",
      "Epoch 963, Loss: 0.3465175965\n",
      "0.635\n",
      "Epoch 964, Loss: 0.3797602357\n",
      "0.64\n",
      "Epoch 965, Loss: 0.3794048187\n",
      "0.62125\n",
      "Epoch 966, Loss: 0.3970118387\n",
      "0.6025\n",
      "Epoch 967, Loss: 0.4887031591\n",
      "0.60625\n",
      "Epoch 968, Loss: 0.3406796680\n",
      "0.60875\n",
      "Epoch 969, Loss: 0.3690856811\n",
      "0.61875\n",
      "Epoch 970, Loss: 0.3234886873\n",
      "0.61125\n",
      "Epoch 971, Loss: 0.3046409077\n",
      "0.62125\n",
      "Epoch 972, Loss: 0.3632121910\n",
      "0.61125\n",
      "Epoch 973, Loss: 0.2636486817\n",
      "0.62625\n",
      "Epoch 974, Loss: 0.4926016421\n",
      "0.61875\n",
      "Epoch 975, Loss: 0.3201076942\n",
      "0.63\n",
      "Epoch 976, Loss: 0.2806828392\n",
      "0.64\n",
      "Epoch 977, Loss: 0.2820832520\n",
      "0.61875\n",
      "Epoch 978, Loss: 0.2926526107\n",
      "0.635\n",
      "Epoch 979, Loss: 0.3370159137\n",
      "0.6125\n",
      "Epoch 980, Loss: 0.3976391783\n",
      "0.60875\n",
      "Epoch 981, Loss: 0.4622820125\n",
      "0.6025\n",
      "Epoch 982, Loss: 0.3187867661\n",
      "0.6025\n",
      "Epoch 983, Loss: 0.3962746494\n",
      "0.595\n",
      "Epoch 984, Loss: 0.3905068738\n",
      "0.615\n",
      "Epoch 985, Loss: 0.3049407856\n",
      "0.605\n",
      "Epoch 986, Loss: 0.2594557204\n",
      "0.59625\n",
      "Epoch 987, Loss: 0.2412373356\n",
      "0.62125\n",
      "Epoch 988, Loss: 0.3010615328\n",
      "0.62\n",
      "Epoch 989, Loss: 0.4250957989\n",
      "0.595\n",
      "Epoch 990, Loss: 0.4849981191\n",
      "0.59375\n",
      "Epoch 991, Loss: 0.3197173071\n",
      "0.585\n",
      "Epoch 992, Loss: 0.3781247165\n",
      "0.59375\n",
      "Epoch 993, Loss: 0.4355731827\n",
      "0.59125\n",
      "Epoch 994, Loss: 0.4697218004\n",
      "0.6075\n",
      "Epoch 995, Loss: 0.3516822291\n",
      "0.61375\n",
      "Epoch 996, Loss: 0.3663385895\n",
      "0.59125\n",
      "Epoch 997, Loss: 0.3106255885\n",
      "0.63875\n",
      "Epoch 998, Loss: 0.3368418621\n",
      "0.6125\n",
      "Epoch 999, Loss: 0.2554720998\n",
      "0.615\n",
      "Epoch 1000, Loss: 0.4990589775\n",
      "0.57125\n",
      "Epoch 1001, Loss: 0.4334305256\n",
      "0.61\n",
      "Epoch 1002, Loss: 0.3526800263\n",
      "0.62125\n",
      "Epoch 1003, Loss: 0.4295687406\n",
      "0.6125\n",
      "Epoch 1004, Loss: 0.4584908384\n",
      "0.585\n",
      "Epoch 1005, Loss: 0.3334250549\n",
      "0.61875\n",
      "Epoch 1006, Loss: 0.2996155195\n",
      "0.6325\n",
      "Epoch 1007, Loss: 0.2530320427\n",
      "0.63625\n",
      "Epoch 1008, Loss: 0.3683937881\n",
      "0.61625\n",
      "Epoch 1009, Loss: 0.4213528193\n",
      "0.6075\n",
      "Epoch 1010, Loss: 0.3887237027\n",
      "0.61\n",
      "Epoch 1011, Loss: 0.3797810243\n",
      "0.6125\n",
      "Epoch 1012, Loss: 0.4082260001\n",
      "0.60375\n",
      "Epoch 1013, Loss: 0.3516978868\n",
      "0.60625\n",
      "Epoch 1014, Loss: 0.2992391359\n",
      "0.6325\n",
      "Epoch 1015, Loss: 0.3994264189\n",
      "0.615\n",
      "Epoch 1016, Loss: 0.3248372220\n",
      "0.63125\n",
      "Epoch 1017, Loss: 0.4631883308\n",
      "0.57\n",
      "Epoch 1018, Loss: 0.2725519966\n",
      "0.6275\n",
      "Epoch 1019, Loss: 0.3948494220\n",
      "0.5975\n",
      "Epoch 1020, Loss: 0.4251691059\n",
      "0.59375\n",
      "Epoch 1021, Loss: 0.3925077715\n",
      "0.57\n",
      "Epoch 1022, Loss: 0.3560523263\n",
      "0.5775\n",
      "Epoch 1023, Loss: 0.3375086408\n",
      "0.60125\n",
      "Epoch 1024, Loss: 0.2405555062\n",
      "0.6025\n",
      "Epoch 1025, Loss: 0.2476635517\n",
      "0.63625\n",
      "Epoch 1026, Loss: 0.3387711008\n",
      "0.605\n",
      "Epoch 1027, Loss: 0.3538158836\n",
      "0.6075\n",
      "Epoch 1028, Loss: 0.4237766377\n",
      "0.60375\n",
      "Epoch 1029, Loss: 0.3731734183\n",
      "0.59625\n",
      "Epoch 1030, Loss: 0.3231253301\n",
      "0.61\n",
      "Epoch 1031, Loss: 0.4782446448\n",
      "0.56875\n",
      "Epoch 1032, Loss: 0.5728488995\n",
      "0.5875\n",
      "Epoch 1033, Loss: 0.5482517938\n",
      "0.56625\n",
      "Epoch 1034, Loss: 0.3418928999\n",
      "0.6175\n",
      "Epoch 1035, Loss: 0.3459598826\n",
      "0.61125\n",
      "Epoch 1036, Loss: 0.2620221945\n",
      "0.63125\n",
      "Epoch 1037, Loss: 0.2711163463\n",
      "0.6075\n",
      "Epoch 1038, Loss: 0.2670968828\n",
      "0.625\n",
      "Epoch 1039, Loss: 0.2537506531\n",
      "0.60875\n",
      "Epoch 1040, Loss: 0.1938521997\n",
      "0.63875\n",
      "Epoch 1041, Loss: 0.2483622550\n",
      "0.62\n",
      "Epoch 1042, Loss: 0.3467763229\n",
      "0.59875\n",
      "Epoch 1043, Loss: 0.3306903669\n",
      "0.59125\n",
      "Epoch 1044, Loss: 0.1821034406\n",
      "0.635\n",
      "Epoch 1045, Loss: 0.1593974452\n",
      "0.65\n",
      "Epoch 1046, Loss: 0.1582432684\n",
      "0.64125\n",
      "Epoch 1047, Loss: 0.1620248243\n",
      "0.64625\n",
      "Epoch 1048, Loss: 0.2223083775\n",
      "0.6325\n",
      "Epoch 1049, Loss: 0.2173542448\n",
      "0.63875\n",
      "Epoch 1050, Loss: 0.2157218690\n",
      "0.63125\n",
      "Epoch 1051, Loss: 0.1634254446\n",
      "0.64125\n",
      "Epoch 1052, Loss: 0.1726239193\n",
      "0.6425\n",
      "Epoch 1053, Loss: 0.1518925852\n",
      "0.6425\n",
      "Epoch 1054, Loss: 0.1802476432\n",
      "0.61125\n",
      "Epoch 1055, Loss: 0.1788616052\n",
      "0.63875\n",
      "Epoch 1056, Loss: 0.1950196491\n",
      "0.64375\n",
      "Epoch 1057, Loss: 0.1973367129\n",
      "0.66\n",
      "Epoch 1058, Loss: 0.1685370927\n",
      "0.64625\n",
      "Epoch 1059, Loss: 0.1796908049\n",
      "0.6175\n",
      "Epoch 1060, Loss: 0.1799090437\n",
      "0.64375\n",
      "Epoch 1061, Loss: 0.2066066960\n",
      "0.6325\n",
      "Epoch 1062, Loss: 0.2449536355\n",
      "0.6125\n",
      "Epoch 1063, Loss: 0.2307189196\n",
      "0.62125\n",
      "Epoch 1064, Loss: 0.1830386178\n",
      "0.635\n",
      "Epoch 1065, Loss: 0.1619852638\n",
      "0.64125\n",
      "Epoch 1066, Loss: 0.1657051185\n",
      "0.63875\n",
      "Epoch 1067, Loss: 0.1466352127\n",
      "0.6475\n",
      "Epoch 1068, Loss: 0.2141251144\n",
      "0.6425\n",
      "Epoch 1069, Loss: 0.2211172354\n",
      "0.62375\n",
      "Epoch 1070, Loss: 0.2847985624\n",
      "0.60375\n",
      "Epoch 1071, Loss: 0.3962897894\n",
      "0.5825\n",
      "Epoch 1072, Loss: 0.4204242215\n",
      "0.59375\n",
      "Epoch 1073, Loss: 0.4296996663\n",
      "0.60125\n",
      "Epoch 1074, Loss: 0.4571345778\n",
      "0.5725\n",
      "Epoch 1075, Loss: 0.2854458167\n",
      "0.61375\n",
      "Epoch 1076, Loss: 0.2814307317\n",
      "0.62375\n",
      "Epoch 1077, Loss: 0.2531925850\n",
      "0.635\n",
      "Epoch 1078, Loss: 0.2104496875\n",
      "0.655\n",
      "Epoch 1079, Loss: 0.2189962688\n",
      "0.65\n",
      "Epoch 1080, Loss: 0.1628056974\n",
      "0.6625\n",
      "Epoch 1081, Loss: 0.1765473437\n",
      "0.64125\n",
      "Epoch 1082, Loss: 0.2099742102\n",
      "0.61125\n",
      "Epoch 1083, Loss: 0.1837077234\n",
      "0.63875\n",
      "Epoch 1084, Loss: 0.1776658213\n",
      "0.645\n",
      "Epoch 1085, Loss: 0.1910530731\n",
      "0.63375\n",
      "Epoch 1086, Loss: 0.1689711104\n",
      "0.645\n",
      "Epoch 1087, Loss: 0.1651434705\n",
      "0.64375\n",
      "Epoch 1088, Loss: 0.1719730078\n",
      "0.65625\n",
      "Epoch 1089, Loss: 0.2178788081\n",
      "0.64875\n",
      "Epoch 1090, Loss: 0.2029909533\n",
      "0.64375\n",
      "Epoch 1091, Loss: 0.1734690466\n",
      "0.63875\n",
      "Epoch 1092, Loss: 0.2758059617\n",
      "0.6375\n",
      "Epoch 1093, Loss: 0.3015859319\n",
      "0.63\n",
      "Epoch 1094, Loss: 0.2919818684\n",
      "0.61875\n",
      "Epoch 1095, Loss: 0.1954945358\n",
      "0.635\n",
      "Epoch 1096, Loss: 0.1927835531\n",
      "0.625\n",
      "Epoch 1097, Loss: 0.3443518509\n",
      "0.60875\n",
      "Epoch 1098, Loss: 0.2715436529\n",
      "0.61625\n",
      "Epoch 1099, Loss: 0.2029769138\n",
      "0.635\n",
      "Epoch 1100, Loss: 0.1968746165\n",
      "0.6275\n",
      "Epoch 1101, Loss: 0.1722387765\n",
      "0.63875\n",
      "Epoch 1102, Loss: 0.2450573143\n",
      "0.6075\n",
      "Epoch 1103, Loss: 0.2294210230\n",
      "0.6325\n",
      "Epoch 1104, Loss: 0.3069857569\n",
      "0.625\n",
      "Epoch 1105, Loss: 0.3328549607\n",
      "0.6375\n",
      "Epoch 1106, Loss: 0.2324437654\n",
      "0.65125\n",
      "Epoch 1107, Loss: 0.2145310083\n",
      "0.64375\n",
      "Epoch 1108, Loss: 0.1980681407\n",
      "0.6475\n",
      "Epoch 1109, Loss: 0.2256159656\n",
      "0.63125\n",
      "Epoch 1110, Loss: 0.2572989817\n",
      "0.6225\n",
      "Epoch 1111, Loss: 0.2646346384\n",
      "0.635\n",
      "Epoch 1112, Loss: 0.3371798413\n",
      "0.5975\n",
      "Epoch 1113, Loss: 0.1973843776\n",
      "0.64\n",
      "Epoch 1114, Loss: 0.2391962914\n",
      "0.6525\n",
      "Epoch 1115, Loss: 0.2667663765\n",
      "0.64125\n",
      "Epoch 1116, Loss: 0.3400572048\n",
      "0.60125\n",
      "Epoch 1117, Loss: 0.2743227771\n",
      "0.63625\n",
      "Epoch 1118, Loss: 0.4062417674\n",
      "0.625\n",
      "Epoch 1119, Loss: 0.2629139615\n",
      "0.61375\n",
      "Epoch 1120, Loss: 0.2852356962\n",
      "0.6225\n",
      "Epoch 1121, Loss: 0.2811179594\n",
      "0.63125\n",
      "Epoch 1122, Loss: 0.3084810883\n",
      "0.62875\n",
      "Epoch 1123, Loss: 0.1595495940\n",
      "0.65\n",
      "Epoch 1124, Loss: 0.1718128323\n",
      "0.63625\n",
      "Epoch 1125, Loss: 0.1945848248\n",
      "0.64125\n",
      "Epoch 1126, Loss: 0.1738988443\n",
      "0.63125\n",
      "Epoch 1127, Loss: 0.0915373405\n",
      "0.6475\n",
      "Epoch 1128, Loss: 0.1432629468\n",
      "0.65125\n",
      "Epoch 1129, Loss: 0.2442495691\n",
      "0.6375\n",
      "Epoch 1130, Loss: 0.1359283998\n",
      "0.6525\n",
      "Epoch 1131, Loss: 0.1433766542\n",
      "0.64875\n",
      "Epoch 1132, Loss: 0.2180338283\n",
      "0.63875\n",
      "Epoch 1133, Loss: 0.1336352891\n",
      "0.65125\n",
      "Epoch 1134, Loss: 0.1069233510\n",
      "0.64125\n",
      "Epoch 1135, Loss: 0.1142421551\n",
      "0.64875\n",
      "Epoch 1136, Loss: 0.1307020700\n",
      "0.6425\n",
      "Epoch 1137, Loss: 0.1151757378\n",
      "0.64625\n",
      "Epoch 1138, Loss: 0.1708039474\n",
      "0.6525\n",
      "Epoch 1139, Loss: 0.1966534818\n",
      "0.6425\n",
      "Epoch 1140, Loss: 0.2300586833\n",
      "0.63625\n",
      "Epoch 1141, Loss: 0.1814262055\n",
      "0.6375\n",
      "Epoch 1142, Loss: 0.2306957462\n",
      "0.64\n",
      "Epoch 1143, Loss: 0.2349338894\n",
      "0.63875\n",
      "Epoch 1144, Loss: 0.2014249217\n",
      "0.63\n",
      "Epoch 1145, Loss: 0.1934891667\n",
      "0.625\n",
      "Epoch 1146, Loss: 0.1998505237\n",
      "0.61375\n",
      "Epoch 1147, Loss: 0.1501077454\n",
      "0.63625\n",
      "Epoch 1148, Loss: 0.2220460285\n",
      "0.6325\n",
      "Epoch 1149, Loss: 0.1729538500\n",
      "0.62625\n",
      "Epoch 1150, Loss: 0.4280545074\n",
      "0.6075\n",
      "Epoch 1151, Loss: 0.3279793763\n",
      "0.6175\n",
      "Epoch 1152, Loss: 0.3109405136\n",
      "0.63375\n",
      "Epoch 1153, Loss: 0.2335172091\n",
      "0.64125\n",
      "Epoch 1154, Loss: 0.1464208905\n",
      "0.645\n",
      "Epoch 1155, Loss: 0.1272754333\n",
      "0.64125\n",
      "Epoch 1156, Loss: 0.1573876490\n",
      "0.635\n",
      "Epoch 1157, Loss: 0.1916844866\n",
      "0.63\n",
      "Epoch 1158, Loss: 0.1857734282\n",
      "0.63125\n",
      "Epoch 1159, Loss: 0.1974598462\n",
      "0.64\n",
      "Epoch 1160, Loss: 0.2656503584\n",
      "0.6325\n",
      "Epoch 1161, Loss: 0.2692820098\n",
      "0.63\n",
      "Epoch 1162, Loss: 0.2271476317\n",
      "0.635\n",
      "Epoch 1163, Loss: 0.1949840297\n",
      "0.63125\n",
      "Epoch 1164, Loss: 0.2037946778\n",
      "0.635\n",
      "Epoch 1165, Loss: 0.2301527212\n",
      "0.63375\n",
      "Epoch 1166, Loss: 0.3080219644\n",
      "0.635\n",
      "Epoch 1167, Loss: 0.2337753519\n",
      "0.645\n",
      "Epoch 1168, Loss: 0.2191413087\n",
      "0.6325\n",
      "Epoch 1169, Loss: 0.4505181682\n",
      "0.6025\n",
      "Epoch 1170, Loss: 0.4292048108\n",
      "0.58\n",
      "Epoch 1171, Loss: 0.2529863524\n",
      "0.6475\n",
      "Epoch 1172, Loss: 0.2413816439\n",
      "0.63125\n",
      "Epoch 1173, Loss: 0.1541821629\n",
      "0.6375\n",
      "Epoch 1174, Loss: 0.2966391456\n",
      "0.62375\n",
      "Epoch 1175, Loss: 0.3371513937\n",
      "0.6125\n",
      "Epoch 1176, Loss: 0.2470494163\n",
      "0.65375\n",
      "Epoch 1177, Loss: 0.2521148568\n",
      "0.63\n",
      "Epoch 1178, Loss: 0.2818677958\n",
      "0.64125\n",
      "Epoch 1179, Loss: 0.2371078547\n",
      "0.63625\n",
      "Epoch 1180, Loss: 0.3609266025\n",
      "0.61375\n",
      "Epoch 1181, Loss: 0.3830171074\n",
      "0.62125\n",
      "Epoch 1182, Loss: 0.2108778728\n",
      "0.6425\n",
      "Epoch 1183, Loss: 0.1911265772\n",
      "0.655\n",
      "Epoch 1184, Loss: 0.1400360701\n",
      "0.62625\n",
      "Epoch 1185, Loss: 0.2653622177\n",
      "0.59375\n",
      "Epoch 1186, Loss: 0.2434716602\n",
      "0.64625\n",
      "Epoch 1187, Loss: 0.2426636724\n",
      "0.6275\n",
      "Epoch 1188, Loss: 0.1559147978\n",
      "0.6325\n",
      "Epoch 1189, Loss: 0.2100236419\n",
      "0.60875\n",
      "Epoch 1190, Loss: 0.1387918244\n",
      "0.6425\n",
      "Epoch 1191, Loss: 0.2151149597\n",
      "0.625\n",
      "Epoch 1192, Loss: 0.2723221098\n",
      "0.60375\n",
      "Epoch 1193, Loss: 0.5157094599\n",
      "0.60875\n",
      "Epoch 1194, Loss: 0.1868295630\n",
      "0.63375\n",
      "Epoch 1195, Loss: 0.2272939962\n",
      "0.63\n",
      "Epoch 1196, Loss: 0.2723221221\n",
      "0.61375\n",
      "Epoch 1197, Loss: 0.1460417669\n",
      "0.655\n",
      "Epoch 1198, Loss: 0.2120719755\n",
      "0.615\n",
      "Epoch 1199, Loss: 0.1470483817\n",
      "0.6325\n",
      "Epoch 1200, Loss: 0.3875277239\n",
      "0.6025\n",
      "Epoch 1201, Loss: 0.3483824079\n",
      "0.61\n",
      "Epoch 1202, Loss: 0.2404424944\n",
      "0.6175\n",
      "Epoch 1203, Loss: 0.1536152156\n",
      "0.6225\n",
      "Epoch 1204, Loss: 0.1410908210\n",
      "0.62625\n",
      "Epoch 1205, Loss: 0.1981960317\n",
      "0.6375\n",
      "Epoch 1206, Loss: 0.2689037366\n",
      "0.5925\n",
      "Epoch 1207, Loss: 0.2137935912\n",
      "0.6125\n",
      "Epoch 1208, Loss: 0.2355383154\n",
      "0.58625\n",
      "Epoch 1209, Loss: 0.2828720836\n",
      "0.5925\n",
      "Epoch 1210, Loss: 0.2483212347\n",
      "0.60375\n",
      "Epoch 1211, Loss: 0.1926528259\n",
      "0.605\n",
      "Epoch 1212, Loss: 0.1397736701\n",
      "0.6125\n",
      "Epoch 1213, Loss: 0.3198577410\n",
      "0.5925\n",
      "Epoch 1214, Loss: 0.2619240971\n",
      "0.61375\n",
      "Epoch 1215, Loss: 0.2081568874\n",
      "0.61375\n",
      "Epoch 1216, Loss: 0.2180225060\n",
      "0.6125\n",
      "Epoch 1217, Loss: 0.2809667171\n",
      "0.605\n",
      "Epoch 1218, Loss: 0.2921942825\n",
      "0.57875\n",
      "Epoch 1219, Loss: 0.3119715856\n",
      "0.59125\n",
      "Epoch 1220, Loss: 0.3094362766\n",
      "0.5925\n",
      "Epoch 1221, Loss: 0.3358286918\n",
      "0.60875\n",
      "Epoch 1222, Loss: 0.2572589165\n",
      "0.6025\n",
      "Epoch 1223, Loss: 0.3362383102\n",
      "0.58875\n",
      "Epoch 1224, Loss: 0.3802119650\n",
      "0.6\n",
      "Epoch 1225, Loss: 0.3114040434\n",
      "0.6025\n",
      "Epoch 1226, Loss: 0.2912353633\n",
      "0.62375\n",
      "Epoch 1227, Loss: 0.3058889618\n",
      "0.5925\n",
      "Epoch 1228, Loss: 0.1840356834\n",
      "0.6425\n",
      "Epoch 1229, Loss: 0.3339344096\n",
      "0.5975\n",
      "Epoch 1230, Loss: 0.2225581151\n",
      "0.63625\n",
      "Epoch 1231, Loss: 0.3842217914\n",
      "0.60375\n",
      "Epoch 1232, Loss: 0.1421446223\n",
      "0.6475\n",
      "Epoch 1233, Loss: 0.1515263496\n",
      "0.63\n",
      "Epoch 1234, Loss: 0.1437122426\n",
      "0.63375\n",
      "Epoch 1235, Loss: 0.1808808881\n",
      "0.64\n",
      "Epoch 1236, Loss: 0.2865056772\n",
      "0.61625\n",
      "Epoch 1237, Loss: 0.1323018465\n",
      "0.63375\n",
      "Epoch 1238, Loss: 0.2264222630\n",
      "0.6075\n",
      "Epoch 1239, Loss: 0.2021585004\n",
      "0.63125\n",
      "Epoch 1240, Loss: 0.2343236237\n",
      "0.61375\n",
      "Epoch 1241, Loss: 0.2313333051\n",
      "0.61875\n",
      "Epoch 1242, Loss: 0.3759557377\n",
      "0.6025\n",
      "Epoch 1243, Loss: 0.3019345199\n",
      "0.6325\n",
      "Epoch 1244, Loss: 0.2597828676\n",
      "0.62\n",
      "Epoch 1245, Loss: 0.3051268832\n",
      "0.60125\n",
      "Epoch 1246, Loss: 0.2024390978\n",
      "0.6175\n",
      "Epoch 1247, Loss: 0.1783211674\n",
      "0.63125\n",
      "Epoch 1248, Loss: 0.1960594849\n",
      "0.64125\n",
      "Epoch 1249, Loss: 0.2409586768\n",
      "0.6125\n",
      "Epoch 1250, Loss: 0.1148332180\n",
      "0.65\n",
      "Epoch 1251, Loss: 0.2065302513\n",
      "0.61875\n",
      "Epoch 1252, Loss: 0.2072728321\n",
      "0.6225\n",
      "Epoch 1253, Loss: 0.2310498125\n",
      "0.62125\n",
      "Epoch 1254, Loss: 0.1116406385\n",
      "0.645\n",
      "Epoch 1255, Loss: 0.1492970139\n",
      "0.64375\n",
      "Epoch 1256, Loss: 0.2852939058\n",
      "0.6\n",
      "Epoch 1257, Loss: 0.3657191144\n",
      "0.62125\n",
      "Epoch 1258, Loss: 0.2322366820\n",
      "0.615\n",
      "Epoch 1259, Loss: 0.2042957021\n",
      "0.62125\n",
      "Epoch 1260, Loss: 0.3398425565\n",
      "0.6075\n",
      "Epoch 1261, Loss: 0.1997609327\n",
      "0.6425\n",
      "Epoch 1262, Loss: 0.1342680683\n",
      "0.625\n",
      "Epoch 1263, Loss: 0.1750829616\n",
      "0.60875\n",
      "Epoch 1264, Loss: 0.2327979333\n",
      "0.59875\n",
      "Epoch 1265, Loss: 0.2444961798\n",
      "0.5975\n",
      "Epoch 1266, Loss: 0.4306827259\n",
      "0.5525\n",
      "Epoch 1267, Loss: 0.1792718087\n",
      "0.62625\n",
      "Epoch 1268, Loss: 0.3487696128\n",
      "0.59125\n",
      "Epoch 1269, Loss: 0.1776890090\n",
      "0.6175\n",
      "Epoch 1270, Loss: 0.1334218830\n",
      "0.635\n",
      "Epoch 1271, Loss: 0.1342197816\n",
      "0.62625\n",
      "Epoch 1272, Loss: 0.1907188770\n",
      "0.60875\n",
      "Epoch 1273, Loss: 0.1216927645\n",
      "0.63875\n",
      "Epoch 1274, Loss: 0.1839952999\n",
      "0.625\n",
      "Epoch 1275, Loss: 0.2879232188\n",
      "0.5775\n",
      "Epoch 1276, Loss: 0.2220842257\n",
      "0.62875\n",
      "Epoch 1277, Loss: 0.2353249320\n",
      "0.615\n",
      "Epoch 1278, Loss: 0.2892768979\n",
      "0.625\n",
      "Epoch 1279, Loss: 0.1689278943\n",
      "0.6625\n",
      "Epoch 1280, Loss: 0.2532380470\n",
      "0.62875\n",
      "Epoch 1281, Loss: 0.3477725205\n",
      "0.63625\n",
      "Epoch 1282, Loss: 0.1626656471\n",
      "0.62\n",
      "Epoch 1283, Loss: 0.1690094786\n",
      "0.63875\n",
      "Epoch 1284, Loss: 0.1149076379\n",
      "0.64125\n",
      "Epoch 1285, Loss: 0.1806985743\n",
      "0.62125\n",
      "Epoch 1286, Loss: 0.4886572385\n",
      "0.565\n",
      "Epoch 1287, Loss: 0.3132090548\n",
      "0.62375\n",
      "Epoch 1288, Loss: 0.3126660597\n",
      "0.6275\n",
      "Epoch 1289, Loss: 0.2560571930\n",
      "0.61875\n",
      "Epoch 1290, Loss: 0.3035176776\n",
      "0.60625\n",
      "Epoch 1291, Loss: 0.2110927908\n",
      "0.61125\n",
      "Epoch 1292, Loss: 0.3053397656\n",
      "0.5925\n",
      "Epoch 1293, Loss: 0.3311632092\n",
      "0.59375\n",
      "Epoch 1294, Loss: 0.2713370270\n",
      "0.61\n",
      "Epoch 1295, Loss: 0.2773836039\n",
      "0.60875\n",
      "Epoch 1296, Loss: 0.2430926148\n",
      "0.60625\n",
      "Epoch 1297, Loss: 0.2190292767\n",
      "0.61875\n",
      "Epoch 1298, Loss: 0.2226085344\n",
      "0.615\n",
      "Epoch 1299, Loss: 0.2409065332\n",
      "0.61875\n",
      "Epoch 1300, Loss: 0.3115743278\n",
      "0.63625\n",
      "Epoch 1301, Loss: 0.4083003395\n",
      "0.5875\n",
      "Epoch 1302, Loss: 0.1926468253\n",
      "0.62875\n",
      "Epoch 1303, Loss: 0.1874098673\n",
      "0.6175\n",
      "Epoch 1304, Loss: 0.2178539391\n",
      "0.61625\n",
      "Epoch 1305, Loss: 0.2994303101\n",
      "0.60375\n",
      "Epoch 1306, Loss: 0.1861139515\n",
      "0.61875\n",
      "Epoch 1307, Loss: 0.2198224188\n",
      "0.64\n",
      "Epoch 1308, Loss: 0.2620019918\n",
      "0.615\n",
      "Epoch 1309, Loss: 0.2182161574\n",
      "0.63625\n",
      "Epoch 1310, Loss: 0.1942893332\n",
      "0.6375\n",
      "Epoch 1311, Loss: 0.1395905303\n",
      "0.64\n",
      "Epoch 1312, Loss: 0.1146835176\n",
      "0.64625\n",
      "Epoch 1313, Loss: 0.1943183948\n",
      "0.64\n",
      "Epoch 1314, Loss: 0.2761049001\n",
      "0.615\n",
      "Epoch 1315, Loss: 0.3038060541\n",
      "0.62\n",
      "Epoch 1316, Loss: 0.2353148900\n",
      "0.61625\n",
      "Epoch 1317, Loss: 0.3157563669\n",
      "0.61375\n",
      "Epoch 1318, Loss: 0.2352811661\n",
      "0.64375\n",
      "Epoch 1319, Loss: 0.1686531418\n",
      "0.65\n",
      "Epoch 1320, Loss: 0.1399141054\n",
      "0.6375\n",
      "Epoch 1321, Loss: 0.1224888384\n",
      "0.66375\n",
      "Epoch 1322, Loss: 0.1289474280\n",
      "0.6375\n",
      "Epoch 1323, Loss: 0.1495865398\n",
      "0.64625\n",
      "Epoch 1324, Loss: 0.2069413736\n",
      "0.65375\n",
      "Epoch 1325, Loss: 0.1497069652\n",
      "0.65\n",
      "Epoch 1326, Loss: 0.1722244647\n",
      "0.64\n",
      "Epoch 1327, Loss: 0.2784780639\n",
      "0.6325\n",
      "Epoch 1328, Loss: 0.2304845146\n",
      "0.645\n",
      "Epoch 1329, Loss: 0.1528423086\n",
      "0.63125\n",
      "Epoch 1330, Loss: 0.1867149536\n",
      "0.62125\n",
      "Epoch 1331, Loss: 0.2101782677\n",
      "0.615\n",
      "Epoch 1332, Loss: 0.1849099440\n",
      "0.64375\n",
      "Epoch 1333, Loss: 0.3284688852\n",
      "0.6275\n",
      "Epoch 1334, Loss: 0.1302158955\n",
      "0.6325\n",
      "Epoch 1335, Loss: 0.2419488273\n",
      "0.63375\n",
      "Epoch 1336, Loss: 0.2176063913\n",
      "0.64\n",
      "Epoch 1337, Loss: 0.2594828288\n",
      "0.64125\n",
      "Epoch 1338, Loss: 0.2680245063\n",
      "0.6375\n",
      "Epoch 1339, Loss: 0.3463377430\n",
      "0.62625\n",
      "Epoch 1340, Loss: 0.2293413406\n",
      "0.6125\n",
      "Epoch 1341, Loss: 0.2807597108\n",
      "0.6325\n",
      "Epoch 1342, Loss: 0.2583027266\n",
      "0.58\n",
      "Epoch 1343, Loss: 0.3043402629\n",
      "0.61125\n",
      "Epoch 1344, Loss: 0.1420348660\n",
      "0.66375\n",
      "Epoch 1345, Loss: 0.2822155442\n",
      "0.62125\n",
      "Epoch 1346, Loss: 0.2716006697\n",
      "0.625\n",
      "Epoch 1347, Loss: 0.2743142824\n",
      "0.62875\n",
      "Epoch 1348, Loss: 0.3928305786\n",
      "0.6025\n",
      "Epoch 1349, Loss: 0.2948314427\n",
      "0.59875\n",
      "Epoch 1350, Loss: 0.4345505687\n",
      "0.62\n",
      "Epoch 1351, Loss: 0.5444648047\n",
      "0.5725\n",
      "Epoch 1352, Loss: 0.3816238794\n",
      "0.61125\n",
      "Epoch 1353, Loss: 0.3864438276\n",
      "0.6175\n",
      "Epoch 1354, Loss: 0.2670588850\n",
      "0.61125\n",
      "Epoch 1355, Loss: 0.2313022069\n",
      "0.62\n",
      "Epoch 1356, Loss: 0.3681394125\n",
      "0.60125\n",
      "Epoch 1357, Loss: 0.1444351631\n",
      "0.65875\n",
      "Epoch 1358, Loss: 0.2680549894\n",
      "0.625\n",
      "Epoch 1359, Loss: 0.1741796007\n",
      "0.62875\n",
      "Epoch 1360, Loss: 0.2930304033\n",
      "0.615\n",
      "Epoch 1361, Loss: 0.2136024988\n",
      "0.65\n",
      "Epoch 1362, Loss: 0.1957388968\n",
      "0.64375\n",
      "Epoch 1363, Loss: 0.1491291902\n",
      "0.63125\n",
      "Epoch 1364, Loss: 0.1790888453\n",
      "0.64875\n",
      "Epoch 1365, Loss: 0.2973820337\n",
      "0.63875\n",
      "Epoch 1366, Loss: 0.2468468510\n",
      "0.65375\n",
      "Epoch 1367, Loss: 0.2407396427\n",
      "0.635\n",
      "Epoch 1368, Loss: 0.4352422453\n",
      "0.59625\n",
      "Epoch 1369, Loss: 0.5296726996\n",
      "0.61125\n",
      "Epoch 1370, Loss: 0.3105363938\n",
      "0.6025\n",
      "Epoch 1371, Loss: 0.4826097154\n",
      "0.57625\n",
      "Epoch 1372, Loss: 0.2758697689\n",
      "0.6475\n",
      "Epoch 1373, Loss: 0.3461170794\n",
      "0.62\n",
      "Epoch 1374, Loss: 0.3671275396\n",
      "0.6075\n",
      "Epoch 1375, Loss: 0.6965642619\n",
      "0.60375\n",
      "Epoch 1376, Loss: 0.4108079834\n",
      "0.61625\n",
      "Epoch 1377, Loss: 0.4426374918\n",
      "0.60375\n",
      "Epoch 1378, Loss: 0.5572099283\n",
      "0.56625\n",
      "Epoch 1379, Loss: 0.2533860346\n",
      "0.63375\n",
      "Epoch 1380, Loss: 0.4243926425\n",
      "0.57125\n",
      "Epoch 1381, Loss: 0.3508276130\n",
      "0.60625\n",
      "Epoch 1382, Loss: 0.2206154582\n",
      "0.60375\n",
      "Epoch 1383, Loss: 0.2317454489\n",
      "0.6375\n",
      "Epoch 1384, Loss: 0.1122723259\n",
      "0.625\n",
      "Epoch 1385, Loss: 0.2431945735\n",
      "0.605\n",
      "Epoch 1386, Loss: 0.3914867084\n",
      "0.60375\n",
      "Epoch 1387, Loss: 0.2380829434\n",
      "0.63875\n",
      "Epoch 1388, Loss: 0.1700178097\n",
      "0.6425\n",
      "Epoch 1389, Loss: 0.2252873868\n",
      "0.6325\n",
      "Epoch 1390, Loss: 0.2990089978\n",
      "0.61\n",
      "Epoch 1391, Loss: 0.2434057852\n",
      "0.64\n",
      "Epoch 1392, Loss: 0.1508484275\n",
      "0.65125\n",
      "Epoch 1393, Loss: 0.1581191198\n",
      "0.63\n",
      "Epoch 1394, Loss: 0.1623938648\n",
      "0.6425\n",
      "Epoch 1395, Loss: 0.1441816465\n",
      "0.64\n",
      "Epoch 1396, Loss: 0.1433716731\n",
      "0.635\n",
      "Epoch 1397, Loss: 0.1394252117\n",
      "0.64625\n",
      "Epoch 1398, Loss: 0.1150251124\n",
      "0.66\n",
      "Epoch 1399, Loss: 0.1339809643\n",
      "0.64625\n",
      "Epoch 1400, Loss: 0.0957126237\n",
      "0.66375\n",
      "Epoch 1401, Loss: 0.1438159999\n",
      "0.6475\n",
      "Epoch 1402, Loss: 0.2351168320\n",
      "0.63125\n",
      "Epoch 1403, Loss: 0.1367158257\n",
      "0.65\n",
      "Epoch 1404, Loss: 0.1368731705\n",
      "0.645\n",
      "Epoch 1405, Loss: 0.2490559457\n",
      "0.6425\n",
      "Epoch 1406, Loss: 0.2633330752\n",
      "0.6225\n",
      "Epoch 1407, Loss: 0.1527801205\n",
      "0.63625\n",
      "Epoch 1408, Loss: 0.3336578485\n",
      "0.63375\n",
      "Epoch 1409, Loss: 0.2800428011\n",
      "0.60375\n",
      "Epoch 1410, Loss: 0.1876664822\n",
      "0.62125\n",
      "Epoch 1411, Loss: 0.1053725018\n",
      "0.6425\n",
      "Epoch 1412, Loss: 0.3108836895\n",
      "0.60875\n",
      "Epoch 1413, Loss: 0.2014541195\n",
      "0.62375\n",
      "Epoch 1414, Loss: 0.1652336569\n",
      "0.63875\n",
      "Epoch 1415, Loss: 0.2081806647\n",
      "0.63125\n",
      "Epoch 1416, Loss: 0.1960218704\n",
      "0.62125\n",
      "Epoch 1417, Loss: 0.1997147410\n",
      "0.6275\n",
      "Epoch 1418, Loss: 0.2256419077\n",
      "0.63\n",
      "Epoch 1419, Loss: 0.1526441246\n",
      "0.6325\n",
      "Epoch 1420, Loss: 0.1342623368\n",
      "0.63875\n",
      "Epoch 1421, Loss: 0.2444591082\n",
      "0.62375\n",
      "Epoch 1422, Loss: 0.3212664566\n",
      "0.6175\n",
      "Epoch 1423, Loss: 0.2550647949\n",
      "0.59875\n",
      "Epoch 1424, Loss: 0.1711336302\n",
      "0.62\n",
      "Epoch 1425, Loss: 0.2205817515\n",
      "0.62375\n",
      "Epoch 1426, Loss: 0.2555386867\n",
      "0.61\n",
      "Epoch 1427, Loss: 0.1659829718\n",
      "0.61375\n",
      "Epoch 1428, Loss: 0.2679557634\n",
      "0.61375\n",
      "Epoch 1429, Loss: 0.4024679321\n",
      "0.6225\n",
      "Epoch 1430, Loss: 0.1658531517\n",
      "0.64\n",
      "Epoch 1431, Loss: 0.1976768055\n",
      "0.62375\n",
      "Epoch 1432, Loss: 0.1648221973\n",
      "0.63625\n",
      "Epoch 1433, Loss: 0.1120146290\n",
      "0.635\n",
      "Epoch 1434, Loss: 0.2943309520\n",
      "0.61875\n",
      "Epoch 1435, Loss: 0.1379378200\n",
      "0.63625\n",
      "Epoch 1436, Loss: 0.2365924484\n",
      "0.625\n",
      "Epoch 1437, Loss: 0.1350573067\n",
      "0.62875\n",
      "Epoch 1438, Loss: 0.1533905409\n",
      "0.63125\n",
      "Epoch 1439, Loss: 0.2458661648\n",
      "0.635\n",
      "Epoch 1440, Loss: 0.1011017705\n",
      "0.64375\n",
      "Epoch 1441, Loss: 0.1059535016\n",
      "0.64125\n",
      "Epoch 1442, Loss: 0.1198762453\n",
      "0.6475\n",
      "Epoch 1443, Loss: 0.1499733066\n",
      "0.64375\n",
      "Epoch 1444, Loss: 0.1992108308\n",
      "0.63625\n",
      "Epoch 1445, Loss: 0.1048842027\n",
      "0.65625\n",
      "Epoch 1446, Loss: 0.1701706881\n",
      "0.6325\n",
      "Epoch 1447, Loss: 0.1226409063\n",
      "0.65\n",
      "Epoch 1448, Loss: 0.0975196415\n",
      "0.6375\n",
      "Epoch 1449, Loss: 0.1499673786\n",
      "0.65875\n",
      "Epoch 1450, Loss: 0.1352277711\n",
      "0.645\n",
      "Epoch 1451, Loss: 0.2211398558\n",
      "0.6175\n",
      "Epoch 1452, Loss: 0.3258681229\n",
      "0.61375\n",
      "Epoch 1453, Loss: 0.2350192943\n",
      "0.63875\n",
      "Epoch 1454, Loss: 0.3123077207\n",
      "0.63\n",
      "Epoch 1455, Loss: 0.1623899664\n",
      "0.63625\n",
      "Epoch 1456, Loss: 0.1790479345\n",
      "0.6375\n",
      "Epoch 1457, Loss: 0.1359130691\n",
      "0.63625\n",
      "Epoch 1458, Loss: 0.2080035733\n",
      "0.645\n",
      "Epoch 1459, Loss: 0.3171164914\n",
      "0.605\n",
      "Epoch 1460, Loss: 0.1057071482\n",
      "0.63625\n",
      "Epoch 1461, Loss: 0.2766837740\n",
      "0.6\n",
      "Epoch 1462, Loss: 0.2876886810\n",
      "0.5975\n",
      "Epoch 1463, Loss: 0.2823670628\n",
      "0.6175\n",
      "Epoch 1464, Loss: 0.2551021244\n",
      "0.62125\n",
      "Epoch 1465, Loss: 0.1763860832\n",
      "0.61625\n",
      "Epoch 1466, Loss: 0.3756382977\n",
      "0.60625\n",
      "Epoch 1467, Loss: 0.3300215027\n",
      "0.62125\n",
      "Epoch 1468, Loss: 0.3315186006\n",
      "0.62\n",
      "Epoch 1469, Loss: 0.5281490060\n",
      "0.605\n",
      "Epoch 1470, Loss: 0.4330838760\n",
      "0.61875\n",
      "Epoch 1471, Loss: 0.5770090047\n",
      "0.59875\n",
      "Epoch 1472, Loss: 0.2095854109\n",
      "0.63\n",
      "Epoch 1473, Loss: 0.2855263993\n",
      "0.62\n",
      "Epoch 1474, Loss: 0.4387643828\n",
      "0.6125\n",
      "Epoch 1475, Loss: 0.3654216727\n",
      "0.61375\n",
      "Epoch 1476, Loss: 0.1729270756\n",
      "0.64\n",
      "Epoch 1477, Loss: 0.1673492219\n",
      "0.63625\n",
      "Epoch 1478, Loss: 0.2430590280\n",
      "0.58625\n",
      "Epoch 1479, Loss: 0.3129321895\n",
      "0.6125\n",
      "Epoch 1480, Loss: 0.3692178228\n",
      "0.6075\n",
      "Epoch 1481, Loss: 0.4128130994\n",
      "0.59875\n",
      "Epoch 1482, Loss: 0.4701773374\n",
      "0.57375\n",
      "Epoch 1483, Loss: 0.3454256015\n",
      "0.615\n",
      "Epoch 1484, Loss: 0.4783862434\n",
      "0.58625\n",
      "Epoch 1485, Loss: 0.5267684118\n",
      "0.5775\n",
      "Epoch 1486, Loss: 0.3878885525\n",
      "0.59125\n",
      "Epoch 1487, Loss: 0.2527676990\n",
      "0.62\n",
      "Epoch 1488, Loss: 0.2414590432\n",
      "0.61375\n",
      "Epoch 1489, Loss: 0.1778882230\n",
      "0.6175\n",
      "Epoch 1490, Loss: 0.2232569520\n",
      "0.63\n",
      "Epoch 1491, Loss: 0.3036175628\n",
      "0.6225\n",
      "Epoch 1492, Loss: 0.4688130436\n",
      "0.6\n",
      "Epoch 1493, Loss: 0.3351331129\n",
      "0.60625\n",
      "Epoch 1494, Loss: 0.4092769868\n",
      "0.61625\n",
      "Epoch 1495, Loss: 0.4747133437\n",
      "0.5875\n",
      "Epoch 1496, Loss: 0.5805498737\n",
      "0.56375\n",
      "Epoch 1497, Loss: 0.3280211645\n",
      "0.6\n",
      "Epoch 1498, Loss: 0.4737015683\n",
      "0.61375\n",
      "Epoch 1499, Loss: 0.2710741640\n",
      "0.61875\n",
      "Epoch 1500, Loss: 0.3903979939\n",
      "0.605\n",
      "Epoch 1501, Loss: 0.3405460935\n",
      "0.62375\n",
      "Epoch 1502, Loss: 0.4752172426\n",
      "0.61375\n",
      "Epoch 1503, Loss: 0.4476423340\n",
      "0.60375\n",
      "Epoch 1504, Loss: 0.3686877453\n",
      "0.61625\n",
      "Epoch 1505, Loss: 0.2568457981\n",
      "0.6225\n",
      "Epoch 1506, Loss: 0.3089755225\n",
      "0.595\n",
      "Epoch 1507, Loss: 0.1452815451\n",
      "0.64625\n",
      "Epoch 1508, Loss: 0.2190980137\n",
      "0.6075\n",
      "Epoch 1509, Loss: 0.2760156942\n",
      "0.60375\n",
      "Epoch 1510, Loss: 0.1729824252\n",
      "0.615\n",
      "Epoch 1511, Loss: 0.1436256590\n",
      "0.63375\n",
      "Epoch 1512, Loss: 0.1060343272\n",
      "0.64875\n",
      "Epoch 1513, Loss: 0.1139894782\n",
      "0.62875\n",
      "Epoch 1514, Loss: 0.0722401302\n",
      "0.6425\n",
      "Epoch 1515, Loss: 0.0631718760\n",
      "0.63375\n",
      "Epoch 1516, Loss: 0.0448204604\n",
      "0.64875\n",
      "Epoch 1517, Loss: 0.0617074515\n",
      "0.625\n",
      "Epoch 1518, Loss: 0.0471186538\n",
      "0.64375\n",
      "Epoch 1519, Loss: 0.0590296352\n",
      "0.62625\n",
      "Epoch 1520, Loss: 0.0599434526\n",
      "0.62875\n",
      "Epoch 1521, Loss: 0.0792200403\n",
      "0.62125\n",
      "Epoch 1522, Loss: 0.0675938367\n",
      "0.64375\n",
      "Epoch 1523, Loss: 0.1105079966\n",
      "0.63125\n",
      "Epoch 1524, Loss: 0.0982196533\n",
      "0.61625\n",
      "Epoch 1525, Loss: 0.0930350115\n",
      "0.6275\n",
      "Epoch 1526, Loss: 0.0788015655\n",
      "0.63125\n",
      "Epoch 1527, Loss: 0.0537284868\n",
      "0.63625\n",
      "Epoch 1528, Loss: 0.0447828162\n",
      "0.63125\n",
      "Epoch 1529, Loss: 0.0416591978\n",
      "0.6325\n",
      "Epoch 1530, Loss: 0.0358751395\n",
      "0.63625\n",
      "Epoch 1531, Loss: 0.0469429237\n",
      "0.62625\n",
      "Epoch 1532, Loss: 0.0265318917\n",
      "0.64125\n",
      "Epoch 1533, Loss: 0.0334710451\n",
      "0.64125\n",
      "Epoch 1534, Loss: 0.0223981099\n",
      "0.65375\n",
      "Epoch 1535, Loss: 0.0359749132\n",
      "0.6375\n",
      "Epoch 1536, Loss: 0.0251038165\n",
      "0.645\n",
      "Epoch 1537, Loss: 0.0315006893\n",
      "0.63625\n",
      "Epoch 1538, Loss: 0.0263366408\n",
      "0.64875\n",
      "Epoch 1539, Loss: 0.0333964702\n",
      "0.64375\n",
      "Epoch 1540, Loss: 0.0286392848\n",
      "0.64\n",
      "Epoch 1541, Loss: 0.0309152514\n",
      "0.645\n",
      "Epoch 1542, Loss: 0.0304389314\n",
      "0.6425\n",
      "Epoch 1543, Loss: 0.0374100485\n",
      "0.635\n",
      "Epoch 1544, Loss: 0.0287058258\n",
      "0.63625\n",
      "Epoch 1545, Loss: 0.0263144970\n",
      "0.64375\n",
      "Epoch 1546, Loss: 0.0380813057\n",
      "0.63125\n",
      "Epoch 1547, Loss: 0.0346642576\n",
      "0.645\n",
      "Epoch 1548, Loss: 0.0282263573\n",
      "0.64125\n",
      "Epoch 1549, Loss: 0.0334267543\n",
      "0.64\n",
      "Epoch 1550, Loss: 0.0730778176\n",
      "0.62125\n",
      "Epoch 1551, Loss: 0.0541777171\n",
      "0.6325\n",
      "Epoch 1552, Loss: 0.0338159825\n",
      "0.65625\n",
      "Epoch 1553, Loss: 0.0346271627\n",
      "0.65375\n",
      "Epoch 1554, Loss: 0.0543286881\n",
      "0.63625\n",
      "Epoch 1555, Loss: 0.0775048311\n",
      "0.6475\n",
      "Epoch 1556, Loss: 0.0490334083\n",
      "0.655\n",
      "Epoch 1557, Loss: 0.0640539647\n",
      "0.6425\n",
      "Epoch 1558, Loss: 0.0336125555\n",
      "0.64625\n",
      "Epoch 1559, Loss: 0.0481181996\n",
      "0.63125\n",
      "Epoch 1560, Loss: 0.0485854059\n",
      "0.63625\n",
      "Epoch 1561, Loss: 0.1396719435\n",
      "0.64125\n",
      "Epoch 1562, Loss: 0.0690152668\n",
      "0.63\n",
      "Epoch 1563, Loss: 0.0644928218\n",
      "0.64375\n",
      "Epoch 1564, Loss: 0.1050498386\n",
      "0.635\n",
      "Epoch 1565, Loss: 0.0902889426\n",
      "0.6425\n",
      "Epoch 1566, Loss: 0.0775838860\n",
      "0.62875\n",
      "Epoch 1567, Loss: 0.1170952655\n",
      "0.61125\n",
      "Epoch 1568, Loss: 0.1650583749\n",
      "0.605\n",
      "Epoch 1569, Loss: 0.0735116980\n",
      "0.63875\n",
      "Epoch 1570, Loss: 0.1067783482\n",
      "0.635\n",
      "Epoch 1571, Loss: 0.0900149874\n",
      "0.605\n",
      "Epoch 1572, Loss: 0.1026020898\n",
      "0.62125\n",
      "Epoch 1573, Loss: 0.2073463345\n",
      "0.61625\n",
      "Epoch 1574, Loss: 0.1540341265\n",
      "0.6\n",
      "Epoch 1575, Loss: 0.2097556382\n",
      "0.61\n",
      "Epoch 1576, Loss: 0.3843400050\n",
      "0.565\n",
      "Epoch 1577, Loss: 0.2533939757\n",
      "0.63\n",
      "Epoch 1578, Loss: 0.2680152781\n",
      "0.62625\n",
      "Epoch 1579, Loss: 0.1675265034\n",
      "0.6225\n",
      "Epoch 1580, Loss: 0.1819225290\n",
      "0.58625\n",
      "Epoch 1581, Loss: 0.1460514830\n",
      "0.6425\n",
      "Epoch 1582, Loss: 0.3531164983\n",
      "0.58875\n",
      "Epoch 1583, Loss: 0.2124872030\n",
      "0.62125\n",
      "Epoch 1584, Loss: 0.2441350580\n",
      "0.5875\n",
      "Epoch 1585, Loss: 0.4865134906\n",
      "0.5725\n",
      "Epoch 1586, Loss: 0.2506076609\n",
      "0.615\n",
      "Epoch 1587, Loss: 0.0774655954\n",
      "0.6425\n",
      "Epoch 1588, Loss: 0.0907741392\n",
      "0.6225\n",
      "Epoch 1589, Loss: 0.1998353019\n",
      "0.6275\n",
      "Epoch 1590, Loss: 0.1134588743\n",
      "0.64\n",
      "Epoch 1591, Loss: 0.1826572658\n",
      "0.625\n",
      "Epoch 1592, Loss: 0.1963703451\n",
      "0.63125\n",
      "Epoch 1593, Loss: 0.0900107233\n",
      "0.65\n",
      "Epoch 1594, Loss: 0.1161341008\n",
      "0.64625\n",
      "Epoch 1595, Loss: 0.1706675234\n",
      "0.61875\n",
      "Epoch 1596, Loss: 0.2144183760\n",
      "0.63375\n",
      "Epoch 1597, Loss: 0.0729229027\n",
      "0.63875\n",
      "Epoch 1598, Loss: 0.0952193934\n",
      "0.6225\n",
      "Epoch 1599, Loss: 0.0479728747\n",
      "0.64\n",
      "Epoch 1600, Loss: 0.1296637282\n",
      "0.62625\n",
      "Epoch 1601, Loss: 0.1630760232\n",
      "0.62\n",
      "Epoch 1602, Loss: 0.0999675804\n",
      "0.64375\n",
      "Epoch 1603, Loss: 0.1306217786\n",
      "0.6375\n",
      "Epoch 1604, Loss: 0.1701863151\n",
      "0.6225\n",
      "Epoch 1605, Loss: 0.1415252697\n",
      "0.6175\n",
      "Epoch 1606, Loss: 0.4250941432\n",
      "0.60125\n",
      "Epoch 1607, Loss: 0.1594062396\n",
      "0.6375\n",
      "Epoch 1608, Loss: 0.2010173393\n",
      "0.62\n",
      "Epoch 1609, Loss: 0.3041224944\n",
      "0.62875\n",
      "Epoch 1610, Loss: 0.2050731236\n",
      "0.61875\n",
      "Epoch 1611, Loss: 0.3061026049\n",
      "0.60625\n",
      "Epoch 1612, Loss: 0.5150415424\n",
      "0.56875\n",
      "Epoch 1613, Loss: 0.2874400475\n",
      "0.60375\n",
      "Epoch 1614, Loss: 0.3531868868\n",
      "0.625\n",
      "Epoch 1615, Loss: 0.3366466962\n",
      "0.5975\n",
      "Epoch 1616, Loss: 0.4883692601\n",
      "0.6175\n",
      "Epoch 1617, Loss: 0.3638117363\n",
      "0.6025\n",
      "Epoch 1618, Loss: 0.6432800526\n",
      "0.5925\n",
      "Epoch 1619, Loss: 0.3420533098\n",
      "0.6275\n",
      "Epoch 1620, Loss: 0.2880035998\n",
      "0.60125\n",
      "Epoch 1621, Loss: 0.2629343784\n",
      "0.61625\n",
      "Epoch 1622, Loss: 0.1469142135\n",
      "0.63625\n",
      "Epoch 1623, Loss: 0.2301660506\n",
      "0.62875\n",
      "Epoch 1624, Loss: 0.2295281043\n",
      "0.63125\n",
      "Epoch 1625, Loss: 0.2727731363\n",
      "0.62875\n",
      "Epoch 1626, Loss: 0.3074566706\n",
      "0.61125\n",
      "Epoch 1627, Loss: 0.2046957068\n",
      "0.63875\n",
      "Epoch 1628, Loss: 0.3873630810\n",
      "0.635\n",
      "Epoch 1629, Loss: 0.1358121849\n",
      "0.62625\n",
      "Epoch 1630, Loss: 0.1298716482\n",
      "0.59875\n",
      "Epoch 1631, Loss: 0.2038381340\n",
      "0.625\n",
      "Epoch 1632, Loss: 0.2058825981\n",
      "0.61125\n",
      "Epoch 1633, Loss: 0.1264736798\n",
      "0.63875\n",
      "Epoch 1634, Loss: 0.1412985705\n",
      "0.625\n",
      "Epoch 1635, Loss: 0.1919321567\n",
      "0.61125\n",
      "Epoch 1636, Loss: 0.1785561994\n",
      "0.6025\n",
      "Epoch 1637, Loss: 0.2186833093\n",
      "0.60625\n",
      "Epoch 1638, Loss: 0.2546575025\n",
      "0.60125\n",
      "Epoch 1639, Loss: 0.2732338243\n",
      "0.62\n",
      "Epoch 1640, Loss: 0.1924069218\n",
      "0.61875\n",
      "Epoch 1641, Loss: 0.1354572612\n",
      "0.635\n",
      "Epoch 1642, Loss: 0.1356950041\n",
      "0.64375\n",
      "Epoch 1643, Loss: 0.1866585592\n",
      "0.6225\n",
      "Epoch 1644, Loss: 0.1644611550\n",
      "0.63875\n",
      "Epoch 1645, Loss: 0.1182351170\n",
      "0.63125\n",
      "Epoch 1646, Loss: 0.1348079169\n",
      "0.64875\n",
      "Epoch 1647, Loss: 0.1243411723\n",
      "0.65\n",
      "Epoch 1648, Loss: 0.1243071325\n",
      "0.63625\n",
      "Epoch 1649, Loss: 0.0529484632\n",
      "0.6525\n",
      "Epoch 1650, Loss: 0.0516211788\n",
      "0.655\n",
      "Epoch 1651, Loss: 0.0480019226\n",
      "0.6625\n",
      "Epoch 1652, Loss: 0.0447794890\n",
      "0.6575\n",
      "Epoch 1653, Loss: 0.0429212584\n",
      "0.65625\n",
      "Epoch 1654, Loss: 0.0426102192\n",
      "0.63625\n",
      "Epoch 1655, Loss: 0.0761023022\n",
      "0.645\n",
      "Epoch 1656, Loss: 0.0715403731\n",
      "0.63875\n",
      "Epoch 1657, Loss: 0.0706419012\n",
      "0.6175\n",
      "Epoch 1658, Loss: 0.0542973474\n",
      "0.63\n",
      "Epoch 1659, Loss: 0.0528233282\n",
      "0.65125\n",
      "Epoch 1660, Loss: 0.0441597054\n",
      "0.64375\n",
      "Epoch 1661, Loss: 0.0680351218\n",
      "0.6275\n",
      "Epoch 1662, Loss: 0.0968991269\n",
      "0.63125\n",
      "Epoch 1663, Loss: 0.0363444961\n",
      "0.65\n",
      "Epoch 1664, Loss: 0.0519881131\n",
      "0.6425\n",
      "Epoch 1665, Loss: 0.0494730098\n",
      "0.63875\n",
      "Epoch 1666, Loss: 0.0627797157\n",
      "0.6225\n",
      "Epoch 1667, Loss: 0.0573251506\n",
      "0.62375\n",
      "Epoch 1668, Loss: 0.0908092460\n",
      "0.6275\n",
      "Epoch 1669, Loss: 0.0470682705\n",
      "0.64375\n",
      "Epoch 1670, Loss: 0.0338508994\n",
      "0.66375\n",
      "Epoch 1671, Loss: 0.0778337335\n",
      "0.6225\n",
      "Epoch 1672, Loss: 0.0875367043\n",
      "0.6275\n",
      "Epoch 1673, Loss: 0.0543170512\n",
      "0.63375\n",
      "Epoch 1674, Loss: 0.0834255727\n",
      "0.6175\n",
      "Epoch 1675, Loss: 0.0558517608\n",
      "0.64\n",
      "Epoch 1676, Loss: 0.0841403200\n",
      "0.62125\n",
      "Epoch 1677, Loss: 0.1295447350\n",
      "0.61\n",
      "Epoch 1678, Loss: 0.1034374189\n",
      "0.61625\n",
      "Epoch 1679, Loss: 0.0549117835\n",
      "0.6375\n",
      "Epoch 1680, Loss: 0.0478795897\n",
      "0.65625\n",
      "Epoch 1681, Loss: 0.0310369919\n",
      "0.645\n",
      "Epoch 1682, Loss: 0.0289317882\n",
      "0.665\n",
      "Epoch 1683, Loss: 0.0376916840\n",
      "0.6425\n",
      "Epoch 1684, Loss: 0.0221637839\n",
      "0.64375\n",
      "Epoch 1685, Loss: 0.0385362484\n",
      "0.6375\n",
      "Epoch 1686, Loss: 0.0573787746\n",
      "0.64875\n",
      "Epoch 1687, Loss: 0.0360157926\n",
      "0.6475\n",
      "Epoch 1688, Loss: 0.0379205214\n",
      "0.6425\n",
      "Epoch 1689, Loss: 0.0625508488\n",
      "0.635\n",
      "Epoch 1690, Loss: 0.0353260857\n",
      "0.63625\n",
      "Epoch 1691, Loss: 0.0712545828\n",
      "0.625\n",
      "Epoch 1692, Loss: 0.0624387651\n",
      "0.63625\n",
      "Epoch 1693, Loss: 0.0819773842\n",
      "0.6125\n",
      "Epoch 1694, Loss: 0.1007207457\n",
      "0.62375\n",
      "Epoch 1695, Loss: 0.1338213122\n",
      "0.62125\n",
      "Epoch 1696, Loss: 0.0829831427\n",
      "0.62\n",
      "Epoch 1697, Loss: 0.1127195826\n",
      "0.6275\n",
      "Epoch 1698, Loss: 0.0800740562\n",
      "0.63625\n",
      "Epoch 1699, Loss: 0.1642492877\n",
      "0.6\n",
      "Epoch 1700, Loss: 0.1015043758\n",
      "0.62\n",
      "Epoch 1701, Loss: 0.0986937019\n",
      "0.61875\n",
      "Epoch 1702, Loss: 0.1098301503\n",
      "0.64\n",
      "Epoch 1703, Loss: 0.0609002837\n",
      "0.655\n",
      "Epoch 1704, Loss: 0.0319596474\n",
      "0.64875\n",
      "Epoch 1705, Loss: 0.0253972953\n",
      "0.65625\n",
      "Epoch 1706, Loss: 0.0633500777\n",
      "0.64\n",
      "Epoch 1707, Loss: 0.1617238649\n",
      "0.6475\n",
      "Epoch 1708, Loss: 0.0824876828\n",
      "0.635\n",
      "Epoch 1709, Loss: 0.0607722526\n",
      "0.63625\n",
      "Epoch 1710, Loss: 0.1530053028\n",
      "0.62125\n",
      "Epoch 1711, Loss: 0.1784292687\n",
      "0.62875\n",
      "Epoch 1712, Loss: 0.0598956297\n",
      "0.65\n",
      "Epoch 1713, Loss: 0.0846368184\n",
      "0.6425\n",
      "Epoch 1714, Loss: 0.0700962988\n",
      "0.6625\n",
      "Epoch 1715, Loss: 0.0248244072\n",
      "0.65375\n",
      "Epoch 1716, Loss: 0.0667573208\n",
      "0.6375\n",
      "Epoch 1717, Loss: 0.0573316981\n",
      "0.62625\n",
      "Epoch 1718, Loss: 0.0969434430\n",
      "0.64125\n",
      "Epoch 1719, Loss: 0.0350572952\n",
      "0.6425\n",
      "Epoch 1720, Loss: 0.3198828263\n",
      "0.5725\n",
      "Epoch 1721, Loss: 0.0932592454\n",
      "0.6225\n",
      "Epoch 1722, Loss: 0.0865303020\n",
      "0.62875\n",
      "Epoch 1723, Loss: 0.1503144689\n",
      "0.62\n",
      "Epoch 1724, Loss: 0.1468428187\n",
      "0.63\n",
      "Epoch 1725, Loss: 0.1675165207\n",
      "0.60875\n",
      "Epoch 1726, Loss: 0.0325020904\n",
      "0.65625\n",
      "Epoch 1727, Loss: 0.0459074270\n",
      "0.6525\n",
      "Epoch 1728, Loss: 0.1077446597\n",
      "0.63625\n",
      "Epoch 1729, Loss: 0.3082635399\n",
      "0.62875\n",
      "Epoch 1730, Loss: 0.5578217296\n",
      "0.58125\n",
      "Epoch 1731, Loss: 0.8317543158\n",
      "0.56125\n",
      "Epoch 1732, Loss: 1.8345335732\n",
      "0.48125\n",
      "Epoch 1733, Loss: 0.9223169546\n",
      "0.55375\n",
      "Epoch 1734, Loss: 0.5043373665\n",
      "0.605\n",
      "Epoch 1735, Loss: 0.2896648574\n",
      "0.63125\n",
      "Epoch 1736, Loss: 0.1675688352\n",
      "0.645\n",
      "Epoch 1737, Loss: 0.1678897820\n",
      "0.64375\n",
      "Epoch 1738, Loss: 0.1040348668\n",
      "0.65875\n",
      "Epoch 1739, Loss: 0.1129088568\n",
      "0.63625\n",
      "Epoch 1740, Loss: 0.0879879646\n",
      "0.64625\n",
      "Epoch 1741, Loss: 0.1439050597\n",
      "0.63625\n",
      "Epoch 1742, Loss: 0.1836922052\n",
      "0.635\n",
      "Epoch 1743, Loss: 0.0992521039\n",
      "0.64625\n",
      "Epoch 1744, Loss: 0.1002695719\n",
      "0.6275\n",
      "Epoch 1745, Loss: 0.0811869298\n",
      "0.6575\n",
      "Epoch 1746, Loss: 0.0930577110\n",
      "0.635\n",
      "Epoch 1747, Loss: 0.1501627931\n",
      "0.6425\n",
      "Epoch 1748, Loss: 0.2284361511\n",
      "0.61875\n",
      "Epoch 1749, Loss: 0.1573247003\n",
      "0.62125\n",
      "Epoch 1750, Loss: 0.1017793080\n",
      "0.635\n",
      "Epoch 1751, Loss: 0.0939818010\n",
      "0.63875\n",
      "Epoch 1752, Loss: 0.1098288528\n",
      "0.6525\n",
      "Epoch 1753, Loss: 0.1071373935\n",
      "0.62625\n",
      "Epoch 1754, Loss: 0.0809832124\n",
      "0.64375\n",
      "Epoch 1755, Loss: 0.0773464605\n",
      "0.6425\n",
      "Epoch 1756, Loss: 0.0485940172\n",
      "0.65625\n",
      "Epoch 1757, Loss: 0.0696132351\n",
      "0.6375\n",
      "Epoch 1758, Loss: 0.0675628979\n",
      "0.6325\n",
      "Epoch 1759, Loss: 0.0787274550\n",
      "0.64625\n",
      "Epoch 1760, Loss: 0.0734733341\n",
      "0.645\n",
      "Epoch 1761, Loss: 0.0640644406\n",
      "0.64375\n",
      "Epoch 1762, Loss: 0.0699885346\n",
      "0.6575\n",
      "Epoch 1763, Loss: 0.0386901583\n",
      "0.655\n",
      "Epoch 1764, Loss: 0.0348226745\n",
      "0.64875\n",
      "Epoch 1765, Loss: 0.0566673726\n",
      "0.6425\n",
      "Epoch 1766, Loss: 0.0528347496\n",
      "0.65\n",
      "Epoch 1767, Loss: 0.0404088489\n",
      "0.655\n",
      "Epoch 1768, Loss: 0.0491431850\n",
      "0.6525\n",
      "Epoch 1769, Loss: 0.0673603391\n",
      "0.65125\n",
      "Epoch 1770, Loss: 0.0454242266\n",
      "0.65875\n",
      "Epoch 1771, Loss: 0.0481750767\n",
      "0.655\n",
      "Epoch 1772, Loss: 0.0958778286\n",
      "0.645\n",
      "Epoch 1773, Loss: 0.0535727376\n",
      "0.62125\n",
      "Epoch 1774, Loss: 0.0664470507\n",
      "0.63875\n",
      "Epoch 1775, Loss: 0.0587620396\n",
      "0.62875\n",
      "Epoch 1776, Loss: 0.0773005374\n",
      "0.63625\n",
      "Epoch 1777, Loss: 0.0501867544\n",
      "0.63625\n",
      "Epoch 1778, Loss: 0.0712570537\n",
      "0.64875\n",
      "Epoch 1779, Loss: 0.0640230260\n",
      "0.6425\n",
      "Epoch 1780, Loss: 0.1412452130\n",
      "0.6025\n",
      "Epoch 1781, Loss: 0.0634220205\n",
      "0.635\n",
      "Epoch 1782, Loss: 0.1516183647\n",
      "0.61625\n",
      "Epoch 1783, Loss: 0.0583629240\n",
      "0.64875\n",
      "Epoch 1784, Loss: 0.0435248119\n",
      "0.655\n",
      "Epoch 1785, Loss: 0.0399553854\n",
      "0.6425\n",
      "Epoch 1786, Loss: 0.0999383196\n",
      "0.6425\n",
      "Epoch 1787, Loss: 0.1740877952\n",
      "0.62375\n",
      "Epoch 1788, Loss: 0.0609725554\n",
      "0.63375\n",
      "Epoch 1789, Loss: 0.0455905422\n",
      "0.65375\n",
      "Epoch 1790, Loss: 0.0558069503\n",
      "0.64125\n",
      "Epoch 1791, Loss: 0.0607231009\n",
      "0.655\n",
      "Epoch 1792, Loss: 0.1089007587\n",
      "0.60625\n",
      "Epoch 1793, Loss: 0.0784281663\n",
      "0.65\n",
      "Epoch 1794, Loss: 0.0606275381\n",
      "0.63625\n",
      "Epoch 1795, Loss: 0.0887778963\n",
      "0.6325\n",
      "Epoch 1796, Loss: 0.0610236322\n",
      "0.64875\n",
      "Epoch 1797, Loss: 0.1209663188\n",
      "0.615\n",
      "Epoch 1798, Loss: 0.0924638647\n",
      "0.64\n",
      "Epoch 1799, Loss: 0.1940640559\n",
      "0.6\n",
      "Epoch 1800, Loss: 0.1514736675\n",
      "0.6425\n",
      "Epoch 1801, Loss: 0.0997673827\n",
      "0.64\n",
      "Epoch 1802, Loss: 0.0611236626\n",
      "0.67\n",
      "Epoch 1803, Loss: 0.0392557680\n",
      "0.65875\n",
      "Epoch 1804, Loss: 0.0862346058\n",
      "0.64625\n",
      "Epoch 1805, Loss: 0.0518936998\n",
      "0.64375\n",
      "Epoch 1806, Loss: 0.1723583054\n",
      "0.6325\n",
      "Epoch 1807, Loss: 0.0879896985\n",
      "0.63\n",
      "Epoch 1808, Loss: 0.0940241148\n",
      "0.66375\n",
      "Epoch 1809, Loss: 0.0375277959\n",
      "0.645\n",
      "Epoch 1810, Loss: 0.0541948370\n",
      "0.64625\n",
      "Epoch 1811, Loss: 0.0368727787\n",
      "0.65625\n",
      "Epoch 1812, Loss: 0.1058782562\n",
      "0.63125\n",
      "Epoch 1813, Loss: 0.0591184180\n",
      "0.66375\n",
      "Epoch 1814, Loss: 0.0646344515\n",
      "0.65125\n",
      "Epoch 1815, Loss: 0.0937168452\n",
      "0.6625\n",
      "Epoch 1816, Loss: 0.1266075632\n",
      "0.64875\n",
      "Epoch 1817, Loss: 0.1047380972\n",
      "0.61875\n",
      "Epoch 1818, Loss: 0.1642818034\n",
      "0.6425\n",
      "Epoch 1819, Loss: 0.1413236350\n",
      "0.63125\n",
      "Epoch 1820, Loss: 0.0664431360\n",
      "0.6675\n",
      "Epoch 1821, Loss: 0.0540519828\n",
      "0.64125\n",
      "Epoch 1822, Loss: 0.1739024624\n",
      "0.6225\n",
      "Epoch 1823, Loss: 0.0907258673\n",
      "0.63125\n",
      "Epoch 1824, Loss: 0.0400197213\n",
      "0.66125\n",
      "Epoch 1825, Loss: 0.0859051307\n",
      "0.65125\n",
      "Epoch 1826, Loss: 0.1019335711\n",
      "0.63875\n",
      "Epoch 1827, Loss: 0.0588774135\n",
      "0.65875\n",
      "Epoch 1828, Loss: 0.0793051628\n",
      "0.64\n",
      "Epoch 1829, Loss: 0.0690254484\n",
      "0.6725\n",
      "Epoch 1830, Loss: 0.0567534534\n",
      "0.66125\n",
      "Epoch 1831, Loss: 0.0576875092\n",
      "0.66875\n",
      "Epoch 1832, Loss: 0.0682931750\n",
      "0.65125\n",
      "Epoch 1833, Loss: 0.0707975633\n",
      "0.66\n",
      "Epoch 1834, Loss: 0.1411276918\n",
      "0.63125\n",
      "Epoch 1835, Loss: 0.0749277572\n",
      "0.63625\n",
      "Epoch 1836, Loss: 0.2144763313\n",
      "0.62875\n",
      "Epoch 1837, Loss: 0.0916159688\n",
      "0.65125\n",
      "Epoch 1838, Loss: 0.1120888373\n",
      "0.64\n",
      "Epoch 1839, Loss: 0.1012546196\n",
      "0.62\n",
      "Epoch 1840, Loss: 0.1213794030\n",
      "0.6475\n",
      "Epoch 1841, Loss: 0.1162174987\n",
      "0.63625\n",
      "Epoch 1842, Loss: 0.0560668149\n",
      "0.65875\n",
      "Epoch 1843, Loss: 0.0944861088\n",
      "0.65125\n",
      "Epoch 1844, Loss: 0.0977492419\n",
      "0.6425\n",
      "Epoch 1845, Loss: 0.2021935036\n",
      "0.62875\n",
      "Epoch 1846, Loss: 0.1096269429\n",
      "0.65625\n",
      "Epoch 1847, Loss: 0.1239339125\n",
      "0.63375\n",
      "Epoch 1848, Loss: 0.1383781271\n",
      "0.64125\n",
      "Epoch 1849, Loss: 0.2122533768\n",
      "0.62375\n",
      "Epoch 1850, Loss: 0.1361719000\n",
      "0.625\n",
      "Epoch 1851, Loss: 0.1526900770\n",
      "0.6225\n",
      "Epoch 1852, Loss: 0.1575967408\n",
      "0.62375\n",
      "Epoch 1853, Loss: 0.3961916696\n",
      "0.60625\n",
      "Epoch 1854, Loss: 0.3649150379\n",
      "0.60375\n",
      "Epoch 1855, Loss: 0.1875135886\n",
      "0.615\n",
      "Epoch 1856, Loss: 0.0848342102\n",
      "0.655\n",
      "Epoch 1857, Loss: 0.0939987367\n",
      "0.64375\n",
      "Epoch 1858, Loss: 0.0653864260\n",
      "0.6575\n",
      "Epoch 1859, Loss: 0.1256818803\n",
      "0.6375\n",
      "Epoch 1860, Loss: 0.0601040480\n",
      "0.6575\n",
      "Epoch 1861, Loss: 0.1005331175\n",
      "0.63875\n",
      "Epoch 1862, Loss: 0.2080357480\n",
      "0.62\n",
      "Epoch 1863, Loss: 0.1750815439\n",
      "0.64\n",
      "Epoch 1864, Loss: 0.1272854329\n",
      "0.63125\n",
      "Epoch 1865, Loss: 0.1453899052\n",
      "0.63375\n",
      "Epoch 1866, Loss: 0.4724036453\n",
      "0.59\n",
      "Epoch 1867, Loss: 0.2442593158\n",
      "0.63\n",
      "Epoch 1868, Loss: 0.1805891308\n",
      "0.64125\n",
      "Epoch 1869, Loss: 0.1173704428\n",
      "0.655\n",
      "Epoch 1870, Loss: 0.2097637922\n",
      "0.64375\n",
      "Epoch 1871, Loss: 0.2706994640\n",
      "0.63\n",
      "Epoch 1872, Loss: 0.2330694859\n",
      "0.6375\n",
      "Epoch 1873, Loss: 0.2892308596\n",
      "0.61625\n",
      "Epoch 1874, Loss: 0.1098860845\n",
      "0.64625\n",
      "Epoch 1875, Loss: 0.2000650738\n",
      "0.64375\n",
      "Epoch 1876, Loss: 0.3053415689\n",
      "0.58875\n",
      "Epoch 1877, Loss: 0.1408182560\n",
      "0.635\n",
      "Epoch 1878, Loss: 0.1691324519\n",
      "0.65\n",
      "Epoch 1879, Loss: 0.1752952764\n",
      "0.62125\n",
      "Epoch 1880, Loss: 0.1844333400\n",
      "0.60625\n",
      "Epoch 1881, Loss: 0.1778319537\n",
      "0.63\n",
      "Epoch 1882, Loss: 0.1970373407\n",
      "0.64375\n",
      "Epoch 1883, Loss: 0.0936476997\n",
      "0.65375\n",
      "Epoch 1884, Loss: 0.1498322874\n",
      "0.62875\n",
      "Epoch 1885, Loss: 0.2540092231\n",
      "0.605\n",
      "Epoch 1886, Loss: 0.2461990286\n",
      "0.61\n",
      "Epoch 1887, Loss: 0.1810057558\n",
      "0.63375\n",
      "Epoch 1888, Loss: 0.3086804680\n",
      "0.6075\n",
      "Epoch 1889, Loss: 0.2737530205\n",
      "0.60375\n",
      "Epoch 1890, Loss: 0.2565440521\n",
      "0.625\n",
      "Epoch 1891, Loss: 0.2166024266\n",
      "0.6125\n",
      "Epoch 1892, Loss: 0.2597095548\n",
      "0.60125\n",
      "Epoch 1893, Loss: 0.1921902897\n",
      "0.61875\n",
      "Epoch 1894, Loss: 0.1403793138\n",
      "0.63125\n",
      "Epoch 1895, Loss: 0.3063937483\n",
      "0.62\n",
      "Epoch 1896, Loss: 0.3787751976\n",
      "0.60125\n",
      "Epoch 1897, Loss: 0.2292952460\n",
      "0.6325\n",
      "Epoch 1898, Loss: 0.2507530579\n",
      "0.6075\n",
      "Epoch 1899, Loss: 0.2006428322\n",
      "0.625\n",
      "Epoch 1900, Loss: 0.1732789001\n",
      "0.6475\n",
      "Epoch 1901, Loss: 0.1339234404\n",
      "0.6375\n",
      "Epoch 1902, Loss: 0.1988462541\n",
      "0.615\n",
      "Epoch 1903, Loss: 0.2081659046\n",
      "0.63125\n",
      "Epoch 1904, Loss: 0.2594732813\n",
      "0.59875\n",
      "Epoch 1905, Loss: 0.1522903190\n",
      "0.6425\n",
      "Epoch 1906, Loss: 0.1026809863\n",
      "0.67125\n",
      "Epoch 1907, Loss: 0.1991944916\n",
      "0.615\n",
      "Epoch 1908, Loss: 0.0914240975\n",
      "0.655\n",
      "Epoch 1909, Loss: 0.1368055686\n",
      "0.6325\n",
      "Epoch 1910, Loss: 0.2343055043\n",
      "0.5925\n",
      "Epoch 1911, Loss: 0.1586133993\n",
      "0.63125\n",
      "Epoch 1912, Loss: 0.1622175047\n",
      "0.62875\n",
      "Epoch 1913, Loss: 0.1724662114\n",
      "0.6325\n",
      "Epoch 1914, Loss: 0.2533852478\n",
      "0.61125\n",
      "Epoch 1915, Loss: 0.2470083971\n",
      "0.62375\n",
      "Epoch 1916, Loss: 0.2288395390\n",
      "0.62125\n",
      "Epoch 1917, Loss: 0.2300569931\n",
      "0.6325\n",
      "Epoch 1918, Loss: 0.1348958371\n",
      "0.645\n",
      "Epoch 1919, Loss: 0.1344720715\n",
      "0.65875\n",
      "Epoch 1920, Loss: 0.1071344422\n",
      "0.66\n",
      "Epoch 1921, Loss: 0.1322336691\n",
      "0.6375\n",
      "Epoch 1922, Loss: 0.1534902836\n",
      "0.63875\n",
      "Epoch 1923, Loss: 0.1346126192\n",
      "0.63875\n",
      "Epoch 1924, Loss: 0.0731733249\n",
      "0.64625\n",
      "Epoch 1925, Loss: 0.1307361534\n",
      "0.65375\n",
      "Epoch 1926, Loss: 0.1761353670\n",
      "0.65625\n",
      "Epoch 1927, Loss: 0.0993925563\n",
      "0.65625\n",
      "Epoch 1928, Loss: 0.1041152948\n",
      "0.64375\n",
      "Epoch 1929, Loss: 0.1023325491\n",
      "0.64\n",
      "Epoch 1930, Loss: 0.0943802777\n",
      "0.655\n",
      "Epoch 1931, Loss: 0.0926148330\n",
      "0.62875\n",
      "Epoch 1932, Loss: 0.0942654251\n",
      "0.6175\n",
      "Epoch 1933, Loss: 0.1063899344\n",
      "0.63375\n",
      "Epoch 1934, Loss: 0.0835207389\n",
      "0.6475\n",
      "Epoch 1935, Loss: 0.0534653540\n",
      "0.6575\n",
      "Epoch 1936, Loss: 0.0777327276\n",
      "0.64625\n",
      "Epoch 1937, Loss: 0.0919795224\n",
      "0.64\n",
      "Epoch 1938, Loss: 0.0907869812\n",
      "0.64\n",
      "Epoch 1939, Loss: 0.1266778098\n",
      "0.64\n",
      "Epoch 1940, Loss: 0.1716553607\n",
      "0.6075\n",
      "Epoch 1941, Loss: 0.1218835380\n",
      "0.62375\n",
      "Epoch 1942, Loss: 0.1566329976\n",
      "0.62875\n",
      "Epoch 1943, Loss: 0.1698018983\n",
      "0.635\n",
      "Epoch 1944, Loss: 0.1455408246\n",
      "0.63875\n",
      "Epoch 1945, Loss: 0.3599623758\n",
      "0.60375\n",
      "Epoch 1946, Loss: 0.2962369638\n",
      "0.61875\n",
      "Epoch 1947, Loss: 0.2432118983\n",
      "0.62125\n",
      "Epoch 1948, Loss: 0.3182770704\n",
      "0.6175\n",
      "Epoch 1949, Loss: 0.3893513262\n",
      "0.61375\n",
      "Epoch 1950, Loss: 0.1988103441\n",
      "0.62875\n",
      "Epoch 1951, Loss: 0.3146453578\n",
      "0.59375\n",
      "Epoch 1952, Loss: 0.2547828922\n",
      "0.61625\n",
      "Epoch 1953, Loss: 0.1851241148\n",
      "0.64125\n",
      "Epoch 1954, Loss: 0.3095241152\n",
      "0.59875\n",
      "Epoch 1955, Loss: 0.2679983214\n",
      "0.605\n",
      "Epoch 1956, Loss: 0.2582597787\n",
      "0.60375\n",
      "Epoch 1957, Loss: 0.3829643930\n",
      "0.59625\n",
      "Epoch 1958, Loss: 0.3433681551\n",
      "0.61875\n",
      "Epoch 1959, Loss: 0.2640307419\n",
      "0.62625\n",
      "Epoch 1960, Loss: 0.2476406334\n",
      "0.60375\n",
      "Epoch 1961, Loss: 0.1895963842\n",
      "0.6325\n",
      "Epoch 1962, Loss: 0.4754235150\n",
      "0.5775\n",
      "Epoch 1963, Loss: 0.2889504966\n",
      "0.61375\n",
      "Epoch 1964, Loss: 0.2644663792\n",
      "0.60875\n",
      "Epoch 1965, Loss: 0.2588928959\n",
      "0.62\n",
      "Epoch 1966, Loss: 0.2939662919\n",
      "0.6\n",
      "Epoch 1967, Loss: 0.3581720590\n",
      "0.59375\n",
      "Epoch 1968, Loss: 0.1928463299\n",
      "0.6275\n",
      "Epoch 1969, Loss: 0.2351555166\n",
      "0.6275\n",
      "Epoch 1970, Loss: 0.2214302735\n",
      "0.6225\n",
      "Epoch 1971, Loss: 0.3479878203\n",
      "0.59375\n",
      "Epoch 1972, Loss: 0.3077897083\n",
      "0.59375\n",
      "Epoch 1973, Loss: 0.3578309072\n",
      "0.59625\n",
      "Epoch 1974, Loss: 0.1966634670\n",
      "0.6275\n",
      "Epoch 1975, Loss: 0.2446625662\n",
      "0.63375\n",
      "Epoch 1976, Loss: 0.2717169700\n",
      "0.62375\n",
      "Epoch 1977, Loss: 0.0965756652\n",
      "0.6375\n",
      "Epoch 1978, Loss: 0.1681984963\n",
      "0.62875\n",
      "Epoch 1979, Loss: 0.2440716461\n",
      "0.62875\n",
      "Epoch 1980, Loss: 0.3055714163\n",
      "0.6075\n",
      "Epoch 1981, Loss: 0.2666662763\n",
      "0.61\n",
      "Epoch 1982, Loss: 0.2924301133\n",
      "0.60625\n",
      "Epoch 1983, Loss: 0.2187709864\n",
      "0.5975\n",
      "Epoch 1984, Loss: 0.2899414785\n",
      "0.61125\n",
      "Epoch 1985, Loss: 0.3310900616\n",
      "0.585\n",
      "Epoch 1986, Loss: 0.2630694553\n",
      "0.63375\n",
      "Epoch 1987, Loss: 0.2860781716\n",
      "0.60625\n",
      "Epoch 1988, Loss: 0.3384607952\n",
      "0.58\n",
      "Epoch 1989, Loss: 0.3673823672\n",
      "0.61375\n",
      "Epoch 1990, Loss: 0.1837250257\n",
      "0.63\n",
      "Epoch 1991, Loss: 0.1184111502\n",
      "0.66375\n",
      "Epoch 1992, Loss: 0.2350510659\n",
      "0.63375\n",
      "Epoch 1993, Loss: 0.1610302863\n",
      "0.64625\n",
      "Epoch 1994, Loss: 0.1188941716\n",
      "0.66\n",
      "Epoch 1995, Loss: 0.2161954653\n",
      "0.6225\n",
      "Epoch 1996, Loss: 0.1044071319\n",
      "0.635\n",
      "Epoch 1997, Loss: 0.1117300667\n",
      "0.65375\n",
      "Epoch 1998, Loss: 0.0839428974\n",
      "0.665\n",
      "Epoch 1999, Loss: 0.1030781110\n",
      "0.65\n",
      "Epoch 2000, Loss: 0.0507524756\n",
      "0.64\n",
      "0.022163783850257857\n"
     ]
    }
   ],
   "source": [
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_weights_init = []\n",
    "best_biases_init = []\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "best_seed = 0\n",
    "\n",
    "for _ in range (1):\n",
    "    nn = NeuralNetwork_Adam(625, [256,128,64], 8)\n",
    "    nn.train(batches,10,0.001)\n",
    "    if nn.get_best_loss() < best_loss:\n",
    "        best_loss = nn.get_best_loss()\n",
    "        best_weights = nn.get_best_weights()\n",
    "        best_biases = nn.get_best_biases()\n",
    "        best_seed = nn.get_best_seed()\n",
    "\n",
    "print(best_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022163783850257857\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "# print(best_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8, init_weights=best_weights, init_biases=best_biases, init_seed=best_seed)\n",
    "# nn.train(batches, 3, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.get_best_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Number of layers in the Neural Network\n",
    "N = 4  # Example value, replace with the actual number of layers\n",
    "\n",
    "# Initialize the dictionary\n",
    "weights_dict = {\n",
    "    'weights': {},\n",
    "    'bias': {}\n",
    "}\n",
    "\n",
    "weights = nn.get_best_weights()\n",
    "biases = nn.get_best_biases()\n",
    "\n",
    "# Populate the weights and bias dictionaries\n",
    "for i in range(N):\n",
    "    weights_dict['weights'][f'fc{i+1}'] = weights[i]\n",
    "    weights_dict['bias'][f'fc{i+1}'] = biases[i].flatten()\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1026611207126207\n",
      "0.64\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'val' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))\n",
    "\n",
    "for X_val, Y_val in batches:\n",
    "    Y_pred= nn.predict(X_val)\n",
    "    print(cross_entropy_loss(Y_val,Y_pred)/len(dataset))\n",
    "    print(accuracy(Y_val, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
