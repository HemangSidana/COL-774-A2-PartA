{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Remember to import \"numpy_transforms\" functions if you wish to import these two classes in a different script.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "        label = row[\"class\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image), label\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image, label = self.dataset[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "        batch_labels = np.array(labels)\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(128, 625)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    print(labels.shape)  # Should be [batch_size]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# # ReLU activation and its derivative\n",
    "# def relu(x):\n",
    "#     return np.maximum(0, x)\n",
    "\n",
    "# def relu_derivative(x):\n",
    "#     return np.where(x > 0, 1, 0)\n",
    "\n",
    "# # Mean Squared Error loss\n",
    "# def mean_squared_error(y_true, y_pred):\n",
    "#     return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "# # Neural Network Class with ReLU in the Output Layer and Hidden Layers\n",
    "# class NeuralNetwork_Adam:\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "#         if init_seed is None:\n",
    "#             self.best_seed = int(time.time())\n",
    "#             np.random.seed(self.best_seed)\n",
    "#         else:\n",
    "#             np.random.seed(init_seed)\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "#         self.m_w = []\n",
    "#         self.v_w = []\n",
    "#         self.m_b = []\n",
    "#         self.v_b = []\n",
    "#         self.beta1 = beta1\n",
    "#         self.beta2 = beta2\n",
    "#         self.epsilon = epsilon\n",
    "#         self.t = 0  # Time step for Adam\n",
    "#         self.best_weights = []\n",
    "#         self.best_biases = []\n",
    "#         self.best_loss = float(\"inf\")\n",
    "\n",
    "#         layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "#         # Initialize weights, biases, and Adam parameters (m, v)\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             if (init_weights is not None) and (init_biases is not None):\n",
    "#                 self.weights.append(init_weights[i])\n",
    "#                 self.biases.append(init_biases[i])\n",
    "#             else:\n",
    "#                 self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "#                 self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "#             self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.best_weights = self.weights\n",
    "#             self.best_biases = self.biases\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         pre_activations = []\n",
    "\n",
    "#         # Pass through each layer except the output layer\n",
    "#         for i in range(len(self.weights) - 1):\n",
    "#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             pre_activations.append(z)\n",
    "#             a = relu(z)  # ReLU for hidden layers\n",
    "#             activations.append(a)\n",
    "\n",
    "#         # Pass through the output layer with ReLU\n",
    "#         z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         pre_activations.append(z)\n",
    "#         a = relu(z)  # ReLU for the output layer\n",
    "#         activations.append(a)\n",
    "\n",
    "#         return activations, pre_activations\n",
    "\n",
    "#     def backward(self, X, y, activations, pre_activations):\n",
    "#         grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "#         grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "#         # Start with output layer error\n",
    "#         delta = activations[-1] - y\n",
    "#         delta *= relu_derivative(pre_activations[-1])  # ReLU derivative for the output layer\n",
    "\n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "#             grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "#             if i > 0:\n",
    "#                 delta = np.dot(delta, self.weights[i].T) * relu_derivative(pre_activations[i - 1])\n",
    "\n",
    "#         return grad_w, grad_b\n",
    "\n",
    "#     def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "#         self.t += 1  # Increment time step for Adam\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             # Update biased first moment estimate\n",
    "#             self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "#             self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "#             # Update biased second moment estimate\n",
    "#             self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "#             self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "#             # Compute bias-corrected first moment estimate\n",
    "#             m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "#             m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "#             # Compute bias-corrected second moment estimate\n",
    "#             v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "#             v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "#             # Update weights and biases\n",
    "#             self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "#             self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "#     def train(self, batches, time_of_running, learning_rate):\n",
    "#         start_time = time.time()\n",
    "#         epoch = 0\n",
    "#         while True:\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 activations, pre_activations = self.forward(X_batch)\n",
    "#                 grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "#                 self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "#             # Calculate average loss over batches\n",
    "#             loss = 0\n",
    "#             z = 0\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 y_pred, _ = self.forward(X_batch)\n",
    "#                 loss += mean_squared_error(y_batch, y_pred[-1])\n",
    "#                 z += len(y_pred[-1])\n",
    "#             loss /= z\n",
    "            \n",
    "#             if loss < self.best_loss:\n",
    "#                 self.best_loss = loss\n",
    "#                 self.best_weights = self.weights\n",
    "#                 self.best_biases = self.biases\n",
    "#             print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "#             epoch += 1\n",
    "#             # if time elapsed is greater than 1 minute, break the loop\n",
    "#             if time.time() - start_time > 60 * time_of_running:\n",
    "#                 break\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "#     def get_best_weights(self):\n",
    "#         return self.best_weights\n",
    "\n",
    "#     def get_best_biases(self):\n",
    "#         return self.best_biases\n",
    "\n",
    "#     def get_best_loss(self):\n",
    "#         return self.best_loss\n",
    "\n",
    "#     def get_best_seed(self):\n",
    "#         return self.best_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'val' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset_val = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "# Create the DataLoader\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=len(dataset_val))\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches_val=[]\n",
    "for images,labels in dataloader_val:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches_val.append((images,one_hot_labels))\n",
    "\n",
    "accu=[]\n",
    "def get_stat():\n",
    "    for X_val, Y_val in batches_val:\n",
    "        Y_pred= nn.predict(X_val)\n",
    "        # print(cross_entropy_loss(Y_val,Y_pred)/len(dataset))\n",
    "        z=accuracy(Y_val, Y_pred)\n",
    "        accu.append(z)\n",
    "        print(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Leaky ReLU activation and its derivative\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Neural Network Class with Leaky ReLU and Adam Optimizer\n",
    "class NeuralNetwork_Adam:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, alpha=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        # if init_seed is None:\n",
    "        #     self.best_seed = int(time.time())\n",
    "        #     np.random.seed(self.best_seed)\n",
    "        # else:\n",
    "        #     np.random.seed(init_seed)\n",
    "        np.random.seed(0)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.m_w = []\n",
    "        self.v_w = []\n",
    "        self.m_b = []\n",
    "        self.v_b = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step for Adam\n",
    "        self.alpha = alpha  # Leaky ReLU parameter\n",
    "        self.best_weights = []\n",
    "        self.best_biases = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if init_weights is not None and init_biases is not None:\n",
    "                self.weights.append(init_weights[i])\n",
    "                self.biases.append(init_biases[i])\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "                self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "            self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "        # self.best_weights = self.weights\n",
    "        # self.best_biases = self.biases\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each hidden layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = leaky_relu(z, alpha=self.alpha)  # Leaky ReLU for hidden layers\n",
    "            # a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def forward_pred(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each hidden layer\n",
    "        for i in range(len(self.best_weights) - 1):\n",
    "            z = np.dot(activations[-1], self.best_weights[i]) + self.best_biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = leaky_relu(z, alpha=self.alpha)  # Leaky ReLU for hidden layers\n",
    "            # a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.best_weights[-1]) + self.best_biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * leaky_relu_derivative(pre_activations[i - 1], alpha=self.alpha)\n",
    "                # delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        self.t += 1  # Increment time step for Adam\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def train(self, batches, time_of_running, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while epoch < 3000:\n",
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
    "            loss /= z\n",
    "            \n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = [np.copy(w) for w in self.weights]  # Use np.copy to create independent copies\n",
    "                self.best_biases = [np.copy(b) for b in self.biases]  # Use np.copy for biases\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            get_stat()\n",
    "            epoch += 1\n",
    "            # if time elapsed is greater than 1 minute, break the loop\n",
    "            if time.time() - start_time > 60 * time_of_running:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_pred(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    \n",
    "    def get_best_biases(self):\n",
    "        return self.best_biases\n",
    "    \n",
    "    def get_best_loss(self):\n",
    "        return self.best_loss\n",
    "    \n",
    "    # def get_best_seed(self):\n",
    "    #     return self.best_seed\n",
    "\n",
    "# Example usage:\n",
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8)\n",
    "# nn.train(batches, 1, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0978879989\n",
      "0.14125\n",
      "Epoch 2, Loss: 2.0890121683\n",
      "0.14875\n",
      "Epoch 3, Loss: 2.0599367418\n",
      "0.1625\n",
      "Epoch 4, Loss: 2.0518723766\n",
      "0.17125\n",
      "Epoch 5, Loss: 2.0267792617\n",
      "0.22875\n",
      "Epoch 6, Loss: 2.0022242717\n",
      "0.3\n",
      "Epoch 7, Loss: 2.0026478192\n",
      "0.3\n",
      "Epoch 8, Loss: 1.9703399189\n",
      "0.2875\n",
      "Epoch 9, Loss: 1.9413749748\n",
      "0.2375\n",
      "Epoch 10, Loss: 1.8956135204\n",
      "0.2525\n",
      "Epoch 11, Loss: 1.8169289713\n",
      "0.31875\n",
      "Epoch 12, Loss: 1.7760009955\n",
      "0.25125\n",
      "Epoch 13, Loss: 1.7339833771\n",
      "0.2525\n",
      "Epoch 14, Loss: 1.7175438272\n",
      "0.28625\n",
      "Epoch 15, Loss: 1.7103136822\n",
      "0.28375\n",
      "Epoch 16, Loss: 1.6980575906\n",
      "0.25625\n",
      "Epoch 17, Loss: 1.5958425652\n",
      "0.3675\n",
      "Epoch 18, Loss: 1.5685995027\n",
      "0.39\n",
      "Epoch 19, Loss: 1.5548741217\n",
      "0.38625\n",
      "Epoch 20, Loss: 1.5545749622\n",
      "0.3875\n",
      "Epoch 21, Loss: 1.5429673454\n",
      "0.40375\n",
      "Epoch 22, Loss: 1.5253120746\n",
      "0.415\n",
      "Epoch 23, Loss: 1.4976801123\n",
      "0.43\n",
      "Epoch 24, Loss: 1.4671446761\n",
      "0.4625\n",
      "Epoch 25, Loss: 1.4591594246\n",
      "0.475\n",
      "Epoch 26, Loss: 1.4427275925\n",
      "0.48125\n",
      "Epoch 27, Loss: 1.4293024314\n",
      "0.46625\n",
      "Epoch 28, Loss: 1.4049688074\n",
      "0.4775\n",
      "Epoch 29, Loss: 1.3953102155\n",
      "0.47625\n",
      "Epoch 30, Loss: 1.3736403266\n",
      "0.475\n",
      "Epoch 31, Loss: 1.3739840225\n",
      "0.475\n",
      "Epoch 32, Loss: 1.3490624267\n",
      "0.47625\n",
      "Epoch 33, Loss: 1.3299172239\n",
      "0.5075\n",
      "Epoch 34, Loss: 1.3245483700\n",
      "0.48875\n",
      "Epoch 35, Loss: 1.3103817635\n",
      "0.495\n",
      "Epoch 36, Loss: 1.2993842788\n",
      "0.495\n",
      "Epoch 37, Loss: 1.2714011312\n",
      "0.5025\n",
      "Epoch 38, Loss: 1.2745370757\n",
      "0.5025\n",
      "Epoch 39, Loss: 1.2553082405\n",
      "0.505\n",
      "Epoch 40, Loss: 1.2441998362\n",
      "0.495\n",
      "Epoch 41, Loss: 1.2364668822\n",
      "0.50875\n",
      "Epoch 42, Loss: 1.2208334626\n",
      "0.51\n",
      "Epoch 43, Loss: 1.2111890309\n",
      "0.51125\n",
      "Epoch 44, Loss: 1.1991944854\n",
      "0.51\n",
      "Epoch 45, Loss: 1.1950063688\n",
      "0.52875\n",
      "Epoch 46, Loss: 1.1833074736\n",
      "0.51875\n",
      "Epoch 47, Loss: 1.1754717606\n",
      "0.52375\n",
      "Epoch 48, Loss: 1.1712308480\n",
      "0.52625\n",
      "Epoch 49, Loss: 1.1691695293\n",
      "0.51875\n",
      "Epoch 50, Loss: 1.1656389608\n",
      "0.52625\n",
      "Epoch 51, Loss: 1.1387214423\n",
      "0.53625\n",
      "Epoch 52, Loss: 1.1310216294\n",
      "0.5375\n",
      "Epoch 53, Loss: 1.1174407471\n",
      "0.53625\n",
      "Epoch 54, Loss: 1.1132301365\n",
      "0.53375\n",
      "Epoch 55, Loss: 1.1051724078\n",
      "0.55\n",
      "Epoch 56, Loss: 1.1097777744\n",
      "0.55\n",
      "Epoch 57, Loss: 1.1299981766\n",
      "0.55\n",
      "Epoch 58, Loss: 1.1220834571\n",
      "0.55\n",
      "Epoch 59, Loss: 1.0989558443\n",
      "0.53375\n",
      "Epoch 60, Loss: 1.1024257807\n",
      "0.53375\n",
      "Epoch 61, Loss: 1.0662350562\n",
      "0.54\n",
      "Epoch 62, Loss: 1.0642519200\n",
      "0.53875\n",
      "Epoch 63, Loss: 1.0565470175\n",
      "0.55375\n",
      "Epoch 64, Loss: 1.0629265841\n",
      "0.55375\n",
      "Epoch 65, Loss: 1.0859951965\n",
      "0.55375\n",
      "Epoch 66, Loss: 1.0992023159\n",
      "0.55375\n",
      "Epoch 67, Loss: 1.0457163504\n",
      "0.54\n",
      "Epoch 68, Loss: 1.0300761877\n",
      "0.54875\n",
      "Epoch 69, Loss: 1.0267315160\n",
      "0.54625\n",
      "Epoch 70, Loss: 1.0390314281\n",
      "0.54625\n",
      "Epoch 71, Loss: 1.0481740900\n",
      "0.54625\n",
      "Epoch 72, Loss: 1.0183749099\n",
      "0.55625\n",
      "Epoch 73, Loss: 1.0065342182\n",
      "0.54875\n",
      "Epoch 74, Loss: 0.9969100505\n",
      "0.5475\n",
      "Epoch 75, Loss: 1.0171986421\n",
      "0.5475\n",
      "Epoch 76, Loss: 1.0880897557\n",
      "0.5475\n",
      "Epoch 77, Loss: 1.0192933942\n",
      "0.5475\n",
      "Epoch 78, Loss: 0.9779670891\n",
      "0.55875\n",
      "Epoch 79, Loss: 0.9866379596\n",
      "0.55875\n",
      "Epoch 80, Loss: 0.9853293369\n",
      "0.55875\n",
      "Epoch 81, Loss: 0.9629047000\n",
      "0.5575\n",
      "Epoch 82, Loss: 1.0320945560\n",
      "0.5575\n",
      "Epoch 83, Loss: 1.0128382722\n",
      "0.5575\n",
      "Epoch 84, Loss: 1.0571249615\n",
      "0.5575\n",
      "Epoch 85, Loss: 0.9565989673\n",
      "0.57125\n",
      "Epoch 86, Loss: 0.9616703444\n",
      "0.57125\n",
      "Epoch 87, Loss: 0.9498854340\n",
      "0.55375\n",
      "Epoch 88, Loss: 0.9645145568\n",
      "0.55375\n",
      "Epoch 89, Loss: 0.9380420594\n",
      "0.57125\n",
      "Epoch 90, Loss: 0.9509546874\n",
      "0.57125\n",
      "Epoch 91, Loss: 0.9231733374\n",
      "0.58\n",
      "Epoch 92, Loss: 0.9672084464\n",
      "0.58\n",
      "Epoch 93, Loss: 0.9103592246\n",
      "0.585\n",
      "Epoch 94, Loss: 0.9473363556\n",
      "0.585\n",
      "Epoch 95, Loss: 0.9370282308\n",
      "0.585\n",
      "Epoch 96, Loss: 0.9723859574\n",
      "0.585\n",
      "Epoch 97, Loss: 0.9268932834\n",
      "0.585\n",
      "Epoch 98, Loss: 0.9732914447\n",
      "0.585\n",
      "Epoch 99, Loss: 0.9412222983\n",
      "0.585\n",
      "Epoch 100, Loss: 0.9873154420\n",
      "0.585\n",
      "Epoch 101, Loss: 0.9251604991\n",
      "0.585\n",
      "Epoch 102, Loss: 0.8866979662\n",
      "0.5775\n",
      "Epoch 103, Loss: 0.8828488728\n",
      "0.57\n",
      "Epoch 104, Loss: 1.0568886672\n",
      "0.57\n",
      "Epoch 105, Loss: 0.8962089376\n",
      "0.57\n",
      "Epoch 106, Loss: 0.9052535042\n",
      "0.57\n",
      "Epoch 107, Loss: 0.9225399404\n",
      "0.57\n",
      "Epoch 108, Loss: 0.8756588624\n",
      "0.57\n",
      "Epoch 109, Loss: 0.8867989018\n",
      "0.57\n",
      "Epoch 110, Loss: 0.9163671130\n",
      "0.57\n",
      "Epoch 111, Loss: 0.8499822451\n",
      "0.605\n",
      "Epoch 112, Loss: 0.8590647061\n",
      "0.605\n",
      "Epoch 113, Loss: 0.8951060331\n",
      "0.605\n",
      "Epoch 114, Loss: 0.9322937237\n",
      "0.605\n",
      "Epoch 115, Loss: 0.9074503110\n",
      "0.605\n",
      "Epoch 116, Loss: 0.9037968826\n",
      "0.605\n",
      "Epoch 117, Loss: 0.9020024772\n",
      "0.605\n",
      "Epoch 118, Loss: 1.1343466653\n",
      "0.605\n",
      "Epoch 119, Loss: 0.9144631806\n",
      "0.605\n",
      "Epoch 120, Loss: 0.8689852492\n",
      "0.605\n",
      "Epoch 121, Loss: 0.9877324912\n",
      "0.605\n",
      "Epoch 122, Loss: 0.9128545715\n",
      "0.605\n",
      "Epoch 123, Loss: 0.8514740666\n",
      "0.605\n",
      "Epoch 124, Loss: 0.8739471974\n",
      "0.605\n",
      "Epoch 125, Loss: 0.9341430748\n",
      "0.605\n",
      "Epoch 126, Loss: 0.8509754454\n",
      "0.605\n",
      "Epoch 127, Loss: 0.8277745910\n",
      "0.5825\n",
      "Epoch 128, Loss: 0.8476374216\n",
      "0.5825\n",
      "Epoch 129, Loss: 0.8968438285\n",
      "0.5825\n",
      "Epoch 130, Loss: 0.8361851182\n",
      "0.5825\n",
      "Epoch 131, Loss: 0.8163515677\n",
      "0.5975\n",
      "Epoch 132, Loss: 0.7909307822\n",
      "0.60875\n",
      "Epoch 133, Loss: 0.8354782953\n",
      "0.60875\n",
      "Epoch 134, Loss: 0.7900521737\n",
      "0.5975\n",
      "Epoch 135, Loss: 0.7921973277\n",
      "0.5975\n",
      "Epoch 136, Loss: 0.7888833004\n",
      "0.59\n",
      "Epoch 137, Loss: 0.8206169711\n",
      "0.59\n",
      "Epoch 138, Loss: 0.7879803542\n",
      "0.605\n",
      "Epoch 139, Loss: 0.7588921192\n",
      "0.5975\n",
      "Epoch 140, Loss: 0.7653877106\n",
      "0.5975\n",
      "Epoch 141, Loss: 0.7888404159\n",
      "0.5975\n",
      "Epoch 142, Loss: 0.8235392441\n",
      "0.5975\n",
      "Epoch 143, Loss: 0.7433523324\n",
      "0.61\n",
      "Epoch 144, Loss: 0.7360029360\n",
      "0.6\n",
      "Epoch 145, Loss: 0.7330715112\n",
      "0.6025\n",
      "Epoch 146, Loss: 0.7616951644\n",
      "0.6025\n",
      "Epoch 147, Loss: 0.7646460760\n",
      "0.6025\n",
      "Epoch 148, Loss: 0.7620296114\n",
      "0.6025\n",
      "Epoch 149, Loss: 0.7663838733\n",
      "0.6025\n",
      "Epoch 150, Loss: 0.7523957194\n",
      "0.6025\n",
      "Epoch 151, Loss: 0.7542034306\n",
      "0.6025\n",
      "Epoch 152, Loss: 0.7634304223\n",
      "0.6025\n",
      "Epoch 153, Loss: 0.7024353846\n",
      "0.60625\n",
      "Epoch 154, Loss: 0.7104065992\n",
      "0.60625\n",
      "Epoch 155, Loss: 0.6991625106\n",
      "0.61875\n",
      "Epoch 156, Loss: 0.6983134086\n",
      "0.62875\n",
      "Epoch 157, Loss: 0.7007062050\n",
      "0.62875\n",
      "Epoch 158, Loss: 0.7009665832\n",
      "0.62875\n",
      "Epoch 159, Loss: 0.7170751973\n",
      "0.62875\n",
      "Epoch 160, Loss: 0.7521175360\n",
      "0.62875\n",
      "Epoch 161, Loss: 0.7510529896\n",
      "0.62875\n",
      "Epoch 162, Loss: 0.7521730750\n",
      "0.62875\n",
      "Epoch 163, Loss: 0.7371699308\n",
      "0.62875\n",
      "Epoch 164, Loss: 0.6777807967\n",
      "0.62625\n",
      "Epoch 165, Loss: 0.7062799304\n",
      "0.62625\n",
      "Epoch 166, Loss: 0.6960601926\n",
      "0.62625\n",
      "Epoch 167, Loss: 0.7025360685\n",
      "0.62625\n",
      "Epoch 168, Loss: 0.7306287660\n",
      "0.62625\n",
      "Epoch 169, Loss: 0.7174490946\n",
      "0.62625\n",
      "Epoch 170, Loss: 0.6886803616\n",
      "0.62625\n",
      "Epoch 171, Loss: 0.6597536750\n",
      "0.60375\n",
      "Epoch 172, Loss: 0.6889946458\n",
      "0.60375\n",
      "Epoch 173, Loss: 0.6573899657\n",
      "0.62\n",
      "Epoch 174, Loss: 0.7582779663\n",
      "0.62\n",
      "Epoch 175, Loss: 0.6661278031\n",
      "0.62\n",
      "Epoch 176, Loss: 0.6478087719\n",
      "0.6175\n",
      "Epoch 177, Loss: 0.6300244024\n",
      "0.63\n",
      "Epoch 178, Loss: 0.6331467923\n",
      "0.63\n",
      "Epoch 179, Loss: 0.6362856964\n",
      "0.63\n",
      "Epoch 180, Loss: 0.6306947643\n",
      "0.63\n",
      "Epoch 181, Loss: 0.6379083620\n",
      "0.63\n",
      "Epoch 182, Loss: 0.6335971793\n",
      "0.63\n",
      "Epoch 183, Loss: 0.6915640609\n",
      "0.63\n",
      "Epoch 184, Loss: 0.6373962371\n",
      "0.63\n",
      "Epoch 185, Loss: 0.6517977750\n",
      "0.63\n",
      "Epoch 186, Loss: 0.6900329703\n",
      "0.63\n",
      "Epoch 187, Loss: 0.6204085321\n",
      "0.6175\n",
      "Epoch 188, Loss: 0.6192106854\n",
      "0.59625\n",
      "Epoch 189, Loss: 0.6154382330\n",
      "0.60625\n",
      "Epoch 190, Loss: 0.5985723191\n",
      "0.61625\n",
      "Epoch 191, Loss: 0.6262921682\n",
      "0.61625\n",
      "Epoch 192, Loss: 0.6017088897\n",
      "0.61625\n",
      "Epoch 193, Loss: 0.6124348259\n",
      "0.61625\n",
      "Epoch 194, Loss: 0.5803810649\n",
      "0.6225\n",
      "Epoch 195, Loss: 0.6713539504\n",
      "0.6225\n",
      "Epoch 196, Loss: 0.6429822277\n",
      "0.6225\n",
      "Epoch 197, Loss: 0.6379456677\n",
      "0.6225\n",
      "Epoch 198, Loss: 0.6604150437\n",
      "0.6225\n",
      "Epoch 199, Loss: 0.6441048974\n",
      "0.6225\n",
      "Epoch 200, Loss: 0.5914590621\n",
      "0.6225\n",
      "Epoch 201, Loss: 0.6345032193\n",
      "0.6225\n",
      "Epoch 202, Loss: 0.6836793158\n",
      "0.6225\n",
      "Epoch 203, Loss: 0.6292236486\n",
      "0.6225\n",
      "Epoch 204, Loss: 0.7472309079\n",
      "0.6225\n",
      "Epoch 205, Loss: 0.7398691128\n",
      "0.6225\n",
      "Epoch 206, Loss: 0.8316109080\n",
      "0.6225\n",
      "Epoch 207, Loss: 0.8396820744\n",
      "0.6225\n",
      "Epoch 208, Loss: 0.6052926733\n",
      "0.6225\n",
      "Epoch 209, Loss: 0.6158279825\n",
      "0.6225\n",
      "Epoch 210, Loss: 0.6531331022\n",
      "0.6225\n",
      "Epoch 211, Loss: 0.5673713240\n",
      "0.6225\n",
      "Epoch 212, Loss: 0.6055151519\n",
      "0.6225\n",
      "Epoch 213, Loss: 0.5515224434\n",
      "0.64\n",
      "Epoch 214, Loss: 0.5870479258\n",
      "0.64\n",
      "Epoch 215, Loss: 0.5962166960\n",
      "0.64\n",
      "Epoch 216, Loss: 0.5908887513\n",
      "0.64\n",
      "Epoch 217, Loss: 0.6286170240\n",
      "0.64\n",
      "Epoch 218, Loss: 0.6663593042\n",
      "0.64\n",
      "Epoch 219, Loss: 0.6036424618\n",
      "0.64\n",
      "Epoch 220, Loss: 0.6011253463\n",
      "0.64\n",
      "Epoch 221, Loss: 0.5840623800\n",
      "0.64\n",
      "Epoch 222, Loss: 0.5734662975\n",
      "0.64\n",
      "Epoch 223, Loss: 0.5444006434\n",
      "0.6275\n",
      "Epoch 224, Loss: 0.6058216230\n",
      "0.6275\n",
      "Epoch 225, Loss: 0.5746735333\n",
      "0.6275\n",
      "Epoch 226, Loss: 0.5976676079\n",
      "0.6275\n",
      "Epoch 227, Loss: 0.5907654772\n",
      "0.6275\n",
      "Epoch 228, Loss: 0.5696421338\n",
      "0.6275\n",
      "Epoch 229, Loss: 0.5386905732\n",
      "0.63375\n",
      "Epoch 230, Loss: 0.5680701727\n",
      "0.63375\n",
      "Epoch 231, Loss: 0.5600693067\n",
      "0.63375\n",
      "Epoch 232, Loss: 0.5452475672\n",
      "0.63375\n",
      "Epoch 233, Loss: 0.5401847121\n",
      "0.63375\n",
      "Epoch 234, Loss: 0.5507593074\n",
      "0.63375\n",
      "Epoch 235, Loss: 0.5378962292\n",
      "0.645\n",
      "Epoch 236, Loss: 0.5323755423\n",
      "0.645\n",
      "Epoch 237, Loss: 0.6042670386\n",
      "0.645\n",
      "Epoch 238, Loss: 0.4998855708\n",
      "0.6425\n",
      "Epoch 239, Loss: 0.5687507661\n",
      "0.6425\n",
      "Epoch 240, Loss: 0.6164403610\n",
      "0.6425\n",
      "Epoch 241, Loss: 0.5536198672\n",
      "0.6425\n",
      "Epoch 242, Loss: 0.5287228345\n",
      "0.6425\n",
      "Epoch 243, Loss: 0.4921858191\n",
      "0.62625\n",
      "Epoch 244, Loss: 0.5745192447\n",
      "0.62625\n",
      "Epoch 245, Loss: 0.5507078601\n",
      "0.62625\n",
      "Epoch 246, Loss: 0.5184999838\n",
      "0.62625\n",
      "Epoch 247, Loss: 0.5607180877\n",
      "0.62625\n",
      "Epoch 248, Loss: 0.5569067694\n",
      "0.62625\n",
      "Epoch 249, Loss: 0.5406216522\n",
      "0.62625\n",
      "Epoch 250, Loss: 0.5378882186\n",
      "0.62625\n",
      "Epoch 251, Loss: 0.6031193390\n",
      "0.62625\n",
      "Epoch 252, Loss: 0.5321444874\n",
      "0.62625\n",
      "Epoch 253, Loss: 0.5057512259\n",
      "0.62625\n",
      "Epoch 254, Loss: 0.5605008116\n",
      "0.62625\n",
      "Epoch 255, Loss: 0.6266869809\n",
      "0.62625\n",
      "Epoch 256, Loss: 0.5563447931\n",
      "0.62625\n",
      "Epoch 257, Loss: 0.5799479263\n",
      "0.62625\n",
      "Epoch 258, Loss: 0.6668162513\n",
      "0.62625\n",
      "Epoch 259, Loss: 0.6563261860\n",
      "0.62625\n",
      "Epoch 260, Loss: 0.5367927097\n",
      "0.62625\n",
      "Epoch 261, Loss: 0.6160484252\n",
      "0.62625\n",
      "Epoch 262, Loss: 0.5412129856\n",
      "0.62625\n",
      "Epoch 263, Loss: 0.5373880313\n",
      "0.62625\n",
      "Epoch 264, Loss: 0.4806083791\n",
      "0.6325\n",
      "Epoch 265, Loss: 0.4966532255\n",
      "0.6325\n",
      "Epoch 266, Loss: 0.5149291042\n",
      "0.6325\n",
      "Epoch 267, Loss: 0.5763188671\n",
      "0.6325\n",
      "Epoch 268, Loss: 0.5346243035\n",
      "0.6325\n",
      "Epoch 269, Loss: 0.6289771352\n",
      "0.6325\n",
      "Epoch 270, Loss: 0.7036393456\n",
      "0.6325\n",
      "Epoch 271, Loss: 0.5650443660\n",
      "0.6325\n",
      "Epoch 272, Loss: 0.5067749674\n",
      "0.6325\n",
      "Epoch 273, Loss: 0.4756037292\n",
      "0.60625\n",
      "Epoch 274, Loss: 0.5830726757\n",
      "0.60625\n",
      "Epoch 275, Loss: 0.5093814471\n",
      "0.60625\n",
      "Epoch 276, Loss: 0.4616426939\n",
      "0.64375\n",
      "Epoch 277, Loss: 0.5101239403\n",
      "0.64375\n",
      "Epoch 278, Loss: 0.4487031677\n",
      "0.63125\n",
      "Epoch 279, Loss: 0.5457630385\n",
      "0.63125\n",
      "Epoch 280, Loss: 0.6120268902\n",
      "0.63125\n",
      "Epoch 281, Loss: 0.4926538365\n",
      "0.63125\n",
      "Epoch 282, Loss: 0.5323745244\n",
      "0.63125\n",
      "Epoch 283, Loss: 0.4499446821\n",
      "0.63125\n",
      "Epoch 284, Loss: 0.4344586241\n",
      "0.64125\n",
      "Epoch 285, Loss: 0.5610925750\n",
      "0.64125\n",
      "Epoch 286, Loss: 0.4823383863\n",
      "0.64125\n",
      "Epoch 287, Loss: 0.4305405574\n",
      "0.63875\n",
      "Epoch 288, Loss: 0.5150430551\n",
      "0.63875\n",
      "Epoch 289, Loss: 0.4895131854\n",
      "0.63875\n",
      "Epoch 290, Loss: 0.4379767093\n",
      "0.63875\n",
      "Epoch 291, Loss: 0.4832321334\n",
      "0.63875\n",
      "Epoch 292, Loss: 0.4313837144\n",
      "0.63875\n",
      "Epoch 293, Loss: 0.5382160974\n",
      "0.63875\n",
      "Epoch 294, Loss: 0.5951752346\n",
      "0.63875\n",
      "Epoch 295, Loss: 0.4954502863\n",
      "0.63875\n",
      "Epoch 296, Loss: 0.3931864903\n",
      "0.63875\n",
      "Epoch 297, Loss: 0.4502042848\n",
      "0.63875\n",
      "Epoch 298, Loss: 0.5400419509\n",
      "0.63875\n",
      "Epoch 299, Loss: 0.4938119695\n",
      "0.63875\n",
      "Epoch 300, Loss: 0.5408942359\n",
      "0.63875\n",
      "Epoch 301, Loss: 0.5245880378\n",
      "0.63875\n",
      "Epoch 302, Loss: 0.6211587493\n",
      "0.63875\n",
      "Epoch 303, Loss: 0.5996949712\n",
      "0.63875\n",
      "Epoch 304, Loss: 0.4471237504\n",
      "0.63875\n",
      "Epoch 305, Loss: 0.5742602777\n",
      "0.63875\n",
      "Epoch 306, Loss: 0.5786738458\n",
      "0.63875\n",
      "Epoch 307, Loss: 0.7509941675\n",
      "0.63875\n",
      "Epoch 308, Loss: 0.6835482197\n",
      "0.63875\n",
      "Epoch 309, Loss: 0.6347420480\n",
      "0.63875\n",
      "Epoch 310, Loss: 0.7687107536\n",
      "0.63875\n",
      "Epoch 311, Loss: 0.6059516607\n",
      "0.63875\n",
      "Epoch 312, Loss: 0.6223935533\n",
      "0.63875\n",
      "Epoch 313, Loss: 0.5748009213\n",
      "0.63875\n",
      "Epoch 314, Loss: 0.5634148480\n",
      "0.63875\n",
      "Epoch 315, Loss: 0.5933403437\n",
      "0.63875\n",
      "Epoch 316, Loss: 0.6015599167\n",
      "0.63875\n",
      "Epoch 317, Loss: 0.6462961821\n",
      "0.63875\n",
      "Epoch 318, Loss: 0.5167687871\n",
      "0.63875\n",
      "Epoch 319, Loss: 0.6306473084\n",
      "0.63875\n",
      "Epoch 320, Loss: 0.6304838430\n",
      "0.63875\n",
      "Epoch 321, Loss: 0.6553793225\n",
      "0.63875\n",
      "Epoch 322, Loss: 0.5404433289\n",
      "0.63875\n",
      "Epoch 323, Loss: 0.6488067603\n",
      "0.63875\n",
      "Epoch 324, Loss: 0.6464623703\n",
      "0.63875\n",
      "Epoch 325, Loss: 0.5566610924\n",
      "0.63875\n",
      "Epoch 326, Loss: 0.6667909407\n",
      "0.63875\n",
      "Epoch 327, Loss: 0.5878476404\n",
      "0.63875\n",
      "Epoch 328, Loss: 0.6441800322\n",
      "0.63875\n",
      "Epoch 329, Loss: 0.5497572295\n",
      "0.63875\n",
      "Epoch 330, Loss: 0.6055671623\n",
      "0.63875\n",
      "Epoch 331, Loss: 0.7083953253\n",
      "0.63875\n",
      "Epoch 332, Loss: 0.6031769569\n",
      "0.63875\n",
      "Epoch 333, Loss: 0.5834585383\n",
      "0.63875\n",
      "Epoch 334, Loss: 0.5243559235\n",
      "0.63875\n",
      "Epoch 335, Loss: 0.4707819825\n",
      "0.63875\n",
      "Epoch 336, Loss: 0.4671892463\n",
      "0.63875\n",
      "Epoch 337, Loss: 0.5457391578\n",
      "0.63875\n",
      "Epoch 338, Loss: 0.4742681244\n",
      "0.63875\n",
      "Epoch 339, Loss: 0.5612331917\n",
      "0.63875\n",
      "Epoch 340, Loss: 0.4440107878\n",
      "0.63875\n",
      "Epoch 341, Loss: 0.4646600879\n",
      "0.63875\n",
      "Epoch 342, Loss: 0.4479525289\n",
      "0.63875\n",
      "Epoch 343, Loss: 0.4680241597\n",
      "0.63875\n",
      "Epoch 344, Loss: 0.4015927642\n",
      "0.63875\n",
      "Epoch 345, Loss: 0.4201420211\n",
      "0.63875\n",
      "Epoch 346, Loss: 0.4074845728\n",
      "0.63875\n",
      "Epoch 347, Loss: 0.4301160297\n",
      "0.63875\n",
      "Epoch 348, Loss: 0.6402007213\n",
      "0.63875\n",
      "Epoch 349, Loss: 0.4637223345\n",
      "0.63875\n",
      "Epoch 350, Loss: 0.4000933347\n",
      "0.63875\n",
      "Epoch 351, Loss: 0.4412262848\n",
      "0.63875\n",
      "Epoch 352, Loss: 0.4650271395\n",
      "0.63875\n",
      "Epoch 353, Loss: 0.4073872803\n",
      "0.63875\n",
      "Epoch 354, Loss: 0.4003167427\n",
      "0.63875\n",
      "Epoch 355, Loss: 0.3878811430\n",
      "0.62375\n",
      "Epoch 356, Loss: 0.3612097134\n",
      "0.6475\n",
      "Epoch 357, Loss: 0.4117635246\n",
      "0.6475\n",
      "Epoch 358, Loss: 0.3610666815\n",
      "0.6475\n",
      "Epoch 359, Loss: 0.4243996440\n",
      "0.6475\n",
      "Epoch 360, Loss: 0.3661087947\n",
      "0.6475\n",
      "Epoch 361, Loss: 0.3518433295\n",
      "0.63125\n",
      "Epoch 362, Loss: 0.3597551447\n",
      "0.63125\n",
      "Epoch 363, Loss: 0.3104383837\n",
      "0.65\n",
      "Epoch 364, Loss: 0.3273118736\n",
      "0.65\n",
      "Epoch 365, Loss: 0.3755844059\n",
      "0.65\n",
      "Epoch 366, Loss: 0.4076751614\n",
      "0.65\n",
      "Epoch 367, Loss: 0.3859156791\n",
      "0.65\n",
      "Epoch 368, Loss: 0.4063985225\n",
      "0.65\n",
      "Epoch 369, Loss: 0.4874736760\n",
      "0.65\n",
      "Epoch 370, Loss: 0.5091853441\n",
      "0.65\n",
      "Epoch 371, Loss: 0.4745314155\n",
      "0.65\n",
      "Epoch 372, Loss: 0.4268605560\n",
      "0.65\n",
      "Epoch 373, Loss: 0.3966910923\n",
      "0.65\n",
      "Epoch 374, Loss: 0.3737451078\n",
      "0.65\n",
      "Epoch 375, Loss: 0.3927832005\n",
      "0.65\n",
      "Epoch 376, Loss: 0.4118143070\n",
      "0.65\n",
      "Epoch 377, Loss: 0.5003342478\n",
      "0.65\n",
      "Epoch 378, Loss: 0.4364437661\n",
      "0.65\n",
      "Epoch 379, Loss: 0.4435245548\n",
      "0.65\n",
      "Epoch 380, Loss: 0.4456233873\n",
      "0.65\n",
      "Epoch 381, Loss: 0.4473247358\n",
      "0.65\n",
      "Epoch 382, Loss: 0.4293790774\n",
      "0.65\n",
      "Epoch 383, Loss: 0.5363162703\n",
      "0.65\n",
      "Epoch 384, Loss: 0.5087920898\n",
      "0.65\n",
      "Epoch 385, Loss: 0.4490816205\n",
      "0.65\n",
      "Epoch 386, Loss: 0.3945125051\n",
      "0.65\n",
      "Epoch 387, Loss: 0.3494616528\n",
      "0.65\n",
      "Epoch 388, Loss: 0.3909364917\n",
      "0.65\n",
      "Epoch 389, Loss: 0.4243786320\n",
      "0.65\n",
      "Epoch 390, Loss: 0.3623339950\n",
      "0.65\n",
      "Epoch 391, Loss: 0.4164147031\n",
      "0.65\n",
      "Epoch 392, Loss: 0.5278648983\n",
      "0.65\n",
      "Epoch 393, Loss: 0.5409062750\n",
      "0.65\n",
      "Epoch 394, Loss: 0.3843219111\n",
      "0.65\n",
      "Epoch 395, Loss: 0.4949337406\n",
      "0.65\n",
      "Epoch 396, Loss: 0.4470668258\n",
      "0.65\n",
      "Epoch 397, Loss: 0.6020618387\n",
      "0.65\n",
      "Epoch 398, Loss: 0.5423338378\n",
      "0.65\n",
      "Epoch 399, Loss: 0.4375122270\n",
      "0.65\n",
      "Epoch 400, Loss: 0.4737323776\n",
      "0.65\n",
      "Epoch 401, Loss: 0.3804661136\n",
      "0.65\n",
      "Epoch 402, Loss: 0.3512546345\n",
      "0.65\n",
      "Epoch 403, Loss: 0.3818081805\n",
      "0.65\n",
      "Epoch 404, Loss: 0.3773895403\n",
      "0.65\n",
      "Epoch 405, Loss: 0.3229300896\n",
      "0.65\n",
      "Epoch 406, Loss: 0.3787295096\n",
      "0.65\n",
      "Epoch 407, Loss: 0.3739636245\n",
      "0.65\n",
      "Epoch 408, Loss: 0.3612940685\n",
      "0.65\n",
      "Epoch 409, Loss: 0.3895573402\n",
      "0.65\n",
      "Epoch 410, Loss: 0.3504484713\n",
      "0.65\n",
      "Epoch 411, Loss: 0.4450802463\n",
      "0.65\n",
      "Epoch 412, Loss: 0.4818265634\n",
      "0.65\n",
      "Epoch 413, Loss: 0.3305748805\n",
      "0.65\n",
      "Epoch 414, Loss: 0.5025910946\n",
      "0.65\n",
      "Epoch 415, Loss: 0.3750983708\n",
      "0.65\n",
      "Epoch 416, Loss: 0.3014347060\n",
      "0.6275\n",
      "Epoch 417, Loss: 0.4513476453\n",
      "0.6275\n",
      "Epoch 418, Loss: 0.3437481099\n",
      "0.6275\n",
      "Epoch 419, Loss: 0.3798029697\n",
      "0.6275\n",
      "Epoch 420, Loss: 0.4549492226\n",
      "0.6275\n",
      "Epoch 421, Loss: 0.7449221857\n",
      "0.6275\n",
      "Epoch 422, Loss: 0.6274825295\n",
      "0.6275\n",
      "Epoch 423, Loss: 0.3959540444\n",
      "0.6275\n",
      "Epoch 424, Loss: 0.4623614029\n",
      "0.6275\n",
      "Epoch 425, Loss: 0.4804427638\n",
      "0.6275\n",
      "Epoch 426, Loss: 0.4534680677\n",
      "0.6275\n",
      "Epoch 427, Loss: 0.4546397526\n",
      "0.6275\n",
      "Epoch 428, Loss: 0.4211456647\n",
      "0.6275\n",
      "Epoch 429, Loss: 0.3800635577\n",
      "0.6275\n",
      "Epoch 430, Loss: 0.3545540989\n",
      "0.6275\n",
      "Epoch 431, Loss: 0.4424355430\n",
      "0.6275\n",
      "Epoch 432, Loss: 0.3560996185\n",
      "0.6275\n",
      "Epoch 433, Loss: 0.3318737952\n",
      "0.6275\n",
      "Epoch 434, Loss: 0.2864468714\n",
      "0.6375\n",
      "Epoch 435, Loss: 0.3139304409\n",
      "0.6375\n",
      "Epoch 436, Loss: 0.3590317794\n",
      "0.6375\n",
      "Epoch 437, Loss: 0.3085526341\n",
      "0.6375\n",
      "Epoch 438, Loss: 0.3086656823\n",
      "0.6375\n",
      "Epoch 439, Loss: 0.3412520103\n",
      "0.6375\n",
      "Epoch 440, Loss: 0.3366968130\n",
      "0.6375\n",
      "Epoch 441, Loss: 0.3683022202\n",
      "0.6375\n",
      "Epoch 442, Loss: 0.4116339085\n",
      "0.6375\n",
      "Epoch 443, Loss: 0.3813554444\n",
      "0.6375\n",
      "Epoch 444, Loss: 0.3165563011\n",
      "0.6375\n",
      "Epoch 445, Loss: 0.3621024551\n",
      "0.6375\n",
      "Epoch 446, Loss: 0.4834038380\n",
      "0.6375\n",
      "Epoch 447, Loss: 0.3044390052\n",
      "0.6375\n",
      "Epoch 448, Loss: 0.2961700507\n",
      "0.6375\n",
      "Epoch 449, Loss: 0.3389943181\n",
      "0.6375\n",
      "Epoch 450, Loss: 0.4039389694\n",
      "0.6375\n",
      "Epoch 451, Loss: 0.2794438083\n",
      "0.6425\n",
      "Epoch 452, Loss: 0.3168540899\n",
      "0.6425\n",
      "Epoch 453, Loss: 0.3086075524\n",
      "0.6425\n",
      "Epoch 454, Loss: 0.2942071397\n",
      "0.6425\n",
      "Epoch 455, Loss: 0.3596217672\n",
      "0.6425\n",
      "Epoch 456, Loss: 0.3053449508\n",
      "0.6425\n",
      "Epoch 457, Loss: 0.2858223730\n",
      "0.6425\n",
      "Epoch 458, Loss: 0.3450126553\n",
      "0.6425\n",
      "Epoch 459, Loss: 0.3940249653\n",
      "0.6425\n",
      "Epoch 460, Loss: 0.3597875046\n",
      "0.6425\n",
      "Epoch 461, Loss: 0.2978575611\n",
      "0.6425\n",
      "Epoch 462, Loss: 0.2616883167\n",
      "0.6375\n",
      "Epoch 463, Loss: 0.3538682201\n",
      "0.6375\n",
      "Epoch 464, Loss: 0.4102039356\n",
      "0.6375\n",
      "Epoch 465, Loss: 0.2842189409\n",
      "0.6375\n",
      "Epoch 466, Loss: 0.3621233443\n",
      "0.6375\n",
      "Epoch 467, Loss: 0.4459138689\n",
      "0.6375\n",
      "Epoch 468, Loss: 0.4582730183\n",
      "0.6375\n",
      "Epoch 469, Loss: 0.3556642650\n",
      "0.6375\n",
      "Epoch 470, Loss: 0.3808218434\n",
      "0.6375\n",
      "Epoch 471, Loss: 0.3753968521\n",
      "0.6375\n",
      "Epoch 472, Loss: 0.3083108591\n",
      "0.6375\n",
      "Epoch 473, Loss: 0.4325573983\n",
      "0.6375\n",
      "Epoch 474, Loss: 0.3872782227\n",
      "0.6375\n",
      "Epoch 475, Loss: 0.4308797176\n",
      "0.6375\n",
      "Epoch 476, Loss: 0.3703319158\n",
      "0.6375\n",
      "Epoch 477, Loss: 0.4029275773\n",
      "0.6375\n",
      "Epoch 478, Loss: 0.4294508436\n",
      "0.6375\n",
      "Epoch 479, Loss: 0.5499998724\n",
      "0.6375\n",
      "Epoch 480, Loss: 0.2598364510\n",
      "0.6525\n",
      "Epoch 481, Loss: 0.2612117821\n",
      "0.6525\n",
      "Epoch 482, Loss: 0.3040297514\n",
      "0.6525\n",
      "Epoch 483, Loss: 0.3251920821\n",
      "0.6525\n",
      "Epoch 484, Loss: 0.3067729818\n",
      "0.6525\n",
      "Epoch 485, Loss: 0.3809556981\n",
      "0.6525\n",
      "Epoch 486, Loss: 0.2951125690\n",
      "0.6525\n",
      "Epoch 487, Loss: 0.4408590300\n",
      "0.6525\n",
      "Epoch 488, Loss: 0.5479160860\n",
      "0.6525\n",
      "Epoch 489, Loss: 0.4166248142\n",
      "0.6525\n",
      "Epoch 490, Loss: 0.5091529334\n",
      "0.6525\n",
      "Epoch 491, Loss: 0.7325113070\n",
      "0.6525\n",
      "Epoch 492, Loss: 0.3928198385\n",
      "0.6525\n",
      "Epoch 493, Loss: 0.5949994162\n",
      "0.6525\n",
      "Epoch 494, Loss: 0.6967865895\n",
      "0.6525\n",
      "Epoch 495, Loss: 0.7111588105\n",
      "0.6525\n",
      "Epoch 496, Loss: 0.6701673553\n",
      "0.6525\n",
      "Epoch 497, Loss: 0.6693935318\n",
      "0.6525\n",
      "Epoch 498, Loss: 0.4095481738\n",
      "0.6525\n",
      "Epoch 499, Loss: 0.5617629771\n",
      "0.6525\n",
      "Epoch 500, Loss: 0.5728995362\n",
      "0.6525\n",
      "Epoch 501, Loss: 0.4331520323\n",
      "0.6525\n",
      "Epoch 502, Loss: 0.6010700327\n",
      "0.6525\n",
      "Epoch 503, Loss: 0.4589097376\n",
      "0.6525\n",
      "Epoch 504, Loss: 0.3802746391\n",
      "0.6525\n",
      "Epoch 505, Loss: 0.3123736854\n",
      "0.6525\n",
      "Epoch 506, Loss: 0.2955722574\n",
      "0.6525\n",
      "Epoch 507, Loss: 0.3330851485\n",
      "0.6525\n",
      "Epoch 508, Loss: 0.3147118204\n",
      "0.6525\n",
      "Epoch 509, Loss: 0.3056087010\n",
      "0.6525\n",
      "Epoch 510, Loss: 0.4087534927\n",
      "0.6525\n",
      "Epoch 511, Loss: 0.3740908092\n",
      "0.6525\n",
      "Epoch 512, Loss: 0.3834172165\n",
      "0.6525\n",
      "Epoch 513, Loss: 0.3677596492\n",
      "0.6525\n",
      "Epoch 514, Loss: 0.5522802264\n",
      "0.6525\n",
      "Epoch 515, Loss: 0.5891248448\n",
      "0.6525\n",
      "Epoch 516, Loss: 0.4036965591\n",
      "0.6525\n",
      "Epoch 517, Loss: 0.4415897572\n",
      "0.6525\n",
      "Epoch 518, Loss: 0.6093983638\n",
      "0.6525\n",
      "Epoch 519, Loss: 0.5207847669\n",
      "0.6525\n",
      "Epoch 520, Loss: 0.6749030924\n",
      "0.6525\n",
      "Epoch 521, Loss: 0.4448767967\n",
      "0.6525\n",
      "Epoch 522, Loss: 0.4823743937\n",
      "0.6525\n",
      "Epoch 523, Loss: 0.3880631094\n",
      "0.6525\n",
      "Epoch 524, Loss: 0.4217357705\n",
      "0.6525\n",
      "Epoch 525, Loss: 0.4630177080\n",
      "0.6525\n",
      "Epoch 526, Loss: 0.6825219073\n",
      "0.6525\n",
      "Epoch 527, Loss: 0.7575873439\n",
      "0.6525\n",
      "Epoch 528, Loss: 0.9032996759\n",
      "0.6525\n",
      "Epoch 529, Loss: 0.6445913850\n",
      "0.6525\n",
      "Epoch 530, Loss: 0.5604033036\n",
      "0.6525\n",
      "Epoch 531, Loss: 0.6084901640\n",
      "0.6525\n",
      "Epoch 532, Loss: 0.6540258475\n",
      "0.6525\n",
      "Epoch 533, Loss: 0.7058364733\n",
      "0.6525\n",
      "Epoch 534, Loss: 0.7780606756\n",
      "0.6525\n",
      "Epoch 535, Loss: 0.9662603782\n",
      "0.6525\n",
      "Epoch 536, Loss: 0.7194405367\n",
      "0.6525\n",
      "Epoch 537, Loss: 0.6310912734\n",
      "0.6525\n",
      "Epoch 538, Loss: 0.4716524384\n",
      "0.6525\n",
      "Epoch 539, Loss: 0.4099931073\n",
      "0.6525\n",
      "Epoch 540, Loss: 0.3775844416\n",
      "0.6525\n",
      "Epoch 541, Loss: 0.3249208050\n",
      "0.6525\n",
      "Epoch 542, Loss: 0.2852202674\n",
      "0.6525\n",
      "Epoch 543, Loss: 0.4118021450\n",
      "0.6525\n",
      "Epoch 544, Loss: 0.3448322491\n",
      "0.6525\n",
      "Epoch 545, Loss: 0.4891614140\n",
      "0.6525\n",
      "Epoch 546, Loss: 0.3486348278\n",
      "0.6525\n",
      "Epoch 547, Loss: 0.2641445310\n",
      "0.6525\n",
      "Epoch 548, Loss: 0.3453172614\n",
      "0.6525\n",
      "Epoch 549, Loss: 0.3014871346\n",
      "0.6525\n",
      "Epoch 550, Loss: 0.2692665081\n",
      "0.6525\n",
      "Epoch 551, Loss: 0.2453871210\n",
      "0.6225\n",
      "Epoch 552, Loss: 0.2868729525\n",
      "0.6225\n",
      "Epoch 553, Loss: 0.2469594436\n",
      "0.6225\n",
      "Epoch 554, Loss: 0.2042201308\n",
      "0.66\n",
      "Epoch 555, Loss: 0.2460291931\n",
      "0.66\n",
      "Epoch 556, Loss: 0.2624385578\n",
      "0.66\n",
      "Epoch 557, Loss: 0.2976159991\n",
      "0.66\n",
      "Epoch 558, Loss: 0.2806453871\n",
      "0.66\n",
      "Epoch 559, Loss: 0.2825574507\n",
      "0.66\n",
      "Epoch 560, Loss: 0.2277288438\n",
      "0.66\n",
      "Epoch 561, Loss: 0.2174117349\n",
      "0.66\n",
      "Epoch 562, Loss: 0.2279957075\n",
      "0.66\n",
      "Epoch 563, Loss: 0.3599883764\n",
      "0.66\n",
      "Epoch 564, Loss: 0.4258625025\n",
      "0.66\n",
      "Epoch 565, Loss: 0.2983263047\n",
      "0.66\n",
      "Epoch 566, Loss: 0.4971366292\n",
      "0.66\n",
      "Epoch 567, Loss: 0.3147027448\n",
      "0.66\n",
      "Epoch 568, Loss: 0.3702444795\n",
      "0.66\n",
      "Epoch 569, Loss: 0.3086831844\n",
      "0.66\n",
      "Epoch 570, Loss: 0.3638153878\n",
      "0.66\n",
      "Epoch 571, Loss: 0.3505626792\n",
      "0.66\n",
      "Epoch 572, Loss: 0.4576585867\n",
      "0.66\n",
      "Epoch 573, Loss: 0.4088980980\n",
      "0.66\n",
      "Epoch 574, Loss: 0.4832748538\n",
      "0.66\n",
      "Epoch 575, Loss: 0.2881979300\n",
      "0.66\n",
      "Epoch 576, Loss: 0.3676997662\n",
      "0.66\n",
      "Epoch 577, Loss: 0.3512227555\n",
      "0.66\n",
      "Epoch 578, Loss: 0.3027217517\n",
      "0.66\n",
      "Epoch 579, Loss: 0.3022947549\n",
      "0.66\n",
      "Epoch 580, Loss: 0.2827373514\n",
      "0.66\n",
      "Epoch 581, Loss: 0.4494423684\n",
      "0.66\n",
      "Epoch 582, Loss: 0.3411642918\n",
      "0.66\n",
      "Epoch 583, Loss: 0.3730953189\n",
      "0.66\n",
      "Epoch 584, Loss: 0.3117794340\n",
      "0.66\n",
      "Epoch 585, Loss: 0.4140784824\n",
      "0.66\n",
      "Epoch 586, Loss: 0.3331720222\n",
      "0.66\n",
      "Epoch 587, Loss: 0.2764456314\n",
      "0.66\n",
      "Epoch 588, Loss: 0.4624581698\n",
      "0.66\n",
      "Epoch 589, Loss: 0.3948710570\n",
      "0.66\n",
      "Epoch 590, Loss: 0.2936499225\n",
      "0.66\n",
      "Epoch 591, Loss: 0.4172523195\n",
      "0.66\n",
      "Epoch 592, Loss: 0.3406542260\n",
      "0.66\n",
      "Epoch 593, Loss: 0.2558861183\n",
      "0.66\n",
      "Epoch 594, Loss: 0.2667392879\n",
      "0.66\n",
      "Epoch 595, Loss: 0.2378092741\n",
      "0.66\n",
      "Epoch 596, Loss: 0.2988202398\n",
      "0.66\n",
      "Epoch 597, Loss: 0.2537298399\n",
      "0.66\n",
      "Epoch 598, Loss: 0.3874077593\n",
      "0.66\n",
      "Epoch 599, Loss: 0.2035725626\n",
      "0.635\n",
      "Epoch 600, Loss: 0.2383785114\n",
      "0.635\n",
      "Epoch 601, Loss: 0.2689329667\n",
      "0.635\n",
      "Epoch 602, Loss: 0.3031234244\n",
      "0.635\n",
      "Epoch 603, Loss: 0.2572553536\n",
      "0.635\n",
      "Epoch 604, Loss: 0.2212019419\n",
      "0.635\n",
      "Epoch 605, Loss: 0.2864881881\n",
      "0.635\n",
      "Epoch 606, Loss: 0.3298168965\n",
      "0.635\n",
      "Epoch 607, Loss: 0.2592867244\n",
      "0.635\n",
      "Epoch 608, Loss: 0.2704798507\n",
      "0.635\n",
      "Epoch 609, Loss: 0.3050693560\n",
      "0.635\n",
      "Epoch 610, Loss: 0.5115845152\n",
      "0.635\n",
      "Epoch 611, Loss: 0.2422714819\n",
      "0.635\n",
      "Epoch 612, Loss: 0.3500827234\n",
      "0.635\n",
      "Epoch 613, Loss: 0.2618806090\n",
      "0.635\n",
      "Epoch 614, Loss: 0.3598072936\n",
      "0.635\n",
      "Epoch 615, Loss: 0.3379978685\n",
      "0.635\n",
      "Epoch 616, Loss: 0.5346339375\n",
      "0.635\n",
      "Epoch 617, Loss: 0.3681665535\n",
      "0.635\n",
      "Epoch 618, Loss: 0.3789788435\n",
      "0.635\n",
      "Epoch 619, Loss: 0.4029330681\n",
      "0.635\n",
      "Epoch 620, Loss: 0.6053215091\n",
      "0.635\n",
      "Epoch 621, Loss: 0.3402187916\n",
      "0.635\n",
      "Epoch 622, Loss: 0.4166660407\n",
      "0.635\n",
      "Epoch 623, Loss: 0.2899476470\n",
      "0.635\n",
      "Epoch 624, Loss: 0.4033382346\n",
      "0.635\n",
      "Epoch 625, Loss: 0.3139758590\n",
      "0.635\n",
      "Epoch 626, Loss: 0.4500799203\n",
      "0.635\n",
      "Epoch 627, Loss: 0.2801867478\n",
      "0.635\n",
      "Epoch 628, Loss: 0.3807185214\n",
      "0.635\n",
      "Epoch 629, Loss: 0.3669873046\n",
      "0.635\n",
      "Epoch 630, Loss: 0.4074099299\n",
      "0.635\n",
      "Epoch 631, Loss: 0.4412838504\n",
      "0.635\n",
      "Epoch 632, Loss: 0.3921123380\n",
      "0.635\n",
      "Epoch 633, Loss: 0.4315415417\n",
      "0.635\n",
      "Epoch 634, Loss: 0.3683474978\n",
      "0.635\n",
      "Epoch 635, Loss: 0.3773892046\n",
      "0.635\n",
      "Epoch 636, Loss: 0.4102881180\n",
      "0.635\n",
      "Epoch 637, Loss: 0.3414961637\n",
      "0.635\n",
      "Epoch 638, Loss: 0.4746787808\n",
      "0.635\n",
      "Epoch 639, Loss: 0.3877156333\n",
      "0.635\n",
      "Epoch 640, Loss: 0.6732421512\n",
      "0.635\n",
      "Epoch 641, Loss: 0.3483893547\n",
      "0.635\n",
      "Epoch 642, Loss: 0.3313146240\n",
      "0.635\n",
      "Epoch 643, Loss: 0.3655608662\n",
      "0.635\n",
      "Epoch 644, Loss: 0.4751763528\n",
      "0.635\n",
      "Epoch 645, Loss: 0.3769128485\n",
      "0.635\n",
      "Epoch 646, Loss: 0.5880881280\n",
      "0.635\n",
      "Epoch 647, Loss: 0.3598364593\n",
      "0.635\n",
      "Epoch 648, Loss: 0.4291246424\n",
      "0.635\n",
      "Epoch 649, Loss: 0.3618965737\n",
      "0.635\n",
      "Epoch 650, Loss: 0.4814085890\n",
      "0.635\n",
      "Epoch 651, Loss: 0.4517953064\n",
      "0.635\n",
      "Epoch 652, Loss: 0.4220690159\n",
      "0.635\n",
      "Epoch 653, Loss: 0.4092546229\n",
      "0.635\n",
      "Epoch 654, Loss: 0.3831239831\n",
      "0.635\n",
      "Epoch 655, Loss: 0.2949867112\n",
      "0.635\n",
      "Epoch 656, Loss: 0.2929166394\n",
      "0.635\n",
      "Epoch 657, Loss: 0.3026775342\n",
      "0.635\n",
      "Epoch 658, Loss: 0.2731857666\n",
      "0.635\n",
      "Epoch 659, Loss: 0.2338046207\n",
      "0.635\n",
      "Epoch 660, Loss: 0.2474493062\n",
      "0.635\n",
      "Epoch 661, Loss: 0.2364966186\n",
      "0.635\n",
      "Epoch 662, Loss: 0.4228853787\n",
      "0.635\n",
      "Epoch 663, Loss: 0.2881355066\n",
      "0.635\n",
      "Epoch 664, Loss: 0.3727775132\n",
      "0.635\n",
      "Epoch 665, Loss: 0.3458248853\n",
      "0.635\n",
      "Epoch 666, Loss: 0.4902085743\n",
      "0.635\n",
      "Epoch 667, Loss: 0.3184289793\n",
      "0.635\n",
      "Epoch 668, Loss: 0.4156709278\n",
      "0.635\n",
      "Epoch 669, Loss: 0.3099735791\n",
      "0.635\n",
      "Epoch 670, Loss: 0.3966281098\n",
      "0.635\n",
      "Epoch 671, Loss: 0.3429586196\n",
      "0.635\n",
      "Epoch 672, Loss: 0.4427250957\n",
      "0.635\n",
      "Epoch 673, Loss: 0.5257078662\n",
      "0.635\n",
      "Epoch 674, Loss: 0.4970113581\n",
      "0.635\n",
      "Epoch 675, Loss: 0.2833692818\n",
      "0.635\n",
      "Epoch 676, Loss: 0.3284222937\n",
      "0.635\n",
      "Epoch 677, Loss: 0.3767402722\n",
      "0.635\n",
      "Epoch 678, Loss: 0.2515619003\n",
      "0.635\n",
      "Epoch 679, Loss: 0.2879884267\n",
      "0.635\n",
      "Epoch 680, Loss: 0.3858177308\n",
      "0.635\n",
      "Epoch 681, Loss: 0.4630128834\n",
      "0.635\n",
      "Epoch 682, Loss: 0.2531155930\n",
      "0.635\n",
      "Epoch 683, Loss: 0.4052967506\n",
      "0.635\n",
      "Epoch 684, Loss: 0.3841703400\n",
      "0.635\n",
      "Epoch 685, Loss: 0.4167028810\n",
      "0.635\n",
      "Epoch 686, Loss: 0.4829449363\n",
      "0.635\n",
      "Epoch 687, Loss: 0.3321042549\n",
      "0.635\n",
      "Epoch 688, Loss: 0.3477823076\n",
      "0.635\n",
      "Epoch 689, Loss: 0.4062368093\n",
      "0.635\n",
      "Epoch 690, Loss: 0.4910477637\n",
      "0.635\n",
      "Epoch 691, Loss: 0.4927732407\n",
      "0.635\n",
      "Epoch 692, Loss: 0.4307518102\n",
      "0.635\n",
      "Epoch 693, Loss: 0.2793341279\n",
      "0.635\n",
      "Epoch 694, Loss: 0.4297329887\n",
      "0.635\n",
      "Epoch 695, Loss: 0.2340810230\n",
      "0.635\n",
      "Epoch 696, Loss: 0.3461877229\n",
      "0.635\n",
      "Epoch 697, Loss: 0.2846709011\n",
      "0.635\n",
      "Epoch 698, Loss: 0.3490369760\n",
      "0.635\n",
      "Epoch 699, Loss: 0.6170927089\n",
      "0.635\n",
      "Epoch 700, Loss: 0.5023224843\n",
      "0.635\n",
      "Epoch 701, Loss: 0.3631360458\n",
      "0.635\n",
      "Epoch 702, Loss: 0.3708047631\n",
      "0.635\n",
      "Epoch 703, Loss: 0.2854517822\n",
      "0.635\n",
      "Epoch 704, Loss: 0.2969876039\n",
      "0.635\n",
      "Epoch 705, Loss: 0.2897099751\n",
      "0.635\n",
      "Epoch 706, Loss: 0.2211241523\n",
      "0.635\n",
      "Epoch 707, Loss: 0.6137257703\n",
      "0.635\n",
      "Epoch 708, Loss: 0.2710558655\n",
      "0.635\n",
      "Epoch 709, Loss: 0.5914628019\n",
      "0.635\n",
      "Epoch 710, Loss: 0.4224420878\n",
      "0.635\n",
      "Epoch 711, Loss: 0.4067959941\n",
      "0.635\n",
      "Epoch 712, Loss: 0.3903998503\n",
      "0.635\n",
      "Epoch 713, Loss: 0.2744865143\n",
      "0.635\n",
      "Epoch 714, Loss: 0.4840752331\n",
      "0.635\n",
      "Epoch 715, Loss: 0.4024278732\n",
      "0.635\n",
      "Epoch 716, Loss: 0.3281533734\n",
      "0.635\n",
      "Epoch 717, Loss: 0.3959541154\n",
      "0.635\n",
      "Epoch 718, Loss: 1.0151538181\n",
      "0.635\n",
      "Epoch 719, Loss: 0.5958742219\n",
      "0.635\n",
      "Epoch 720, Loss: 0.4826948807\n",
      "0.635\n",
      "Epoch 721, Loss: 0.3662334797\n",
      "0.635\n",
      "Epoch 722, Loss: 0.3304460411\n",
      "0.635\n",
      "Epoch 723, Loss: 0.3500020198\n",
      "0.635\n",
      "Epoch 724, Loss: 0.3379145508\n",
      "0.635\n",
      "Epoch 725, Loss: 0.3960517834\n",
      "0.635\n",
      "Epoch 726, Loss: 0.3946893749\n",
      "0.635\n",
      "Epoch 727, Loss: 0.3515667342\n",
      "0.635\n",
      "Epoch 728, Loss: 0.5697058022\n",
      "0.635\n",
      "Epoch 729, Loss: 0.3916006081\n",
      "0.635\n",
      "Epoch 730, Loss: 0.5190345017\n",
      "0.635\n",
      "Epoch 731, Loss: 0.3503048680\n",
      "0.635\n",
      "Epoch 732, Loss: 0.4614856430\n",
      "0.635\n",
      "Epoch 733, Loss: 0.4794340835\n",
      "0.635\n",
      "Epoch 734, Loss: 0.6283033475\n",
      "0.635\n",
      "Epoch 735, Loss: 0.3458970561\n",
      "0.635\n",
      "Epoch 736, Loss: 0.5452594997\n",
      "0.635\n",
      "Epoch 737, Loss: 0.5618335208\n",
      "0.635\n",
      "Epoch 738, Loss: 0.4652056019\n",
      "0.635\n",
      "Epoch 739, Loss: 0.4191713051\n",
      "0.635\n",
      "Epoch 740, Loss: 0.4753482947\n",
      "0.635\n",
      "Epoch 741, Loss: 0.3945066200\n",
      "0.635\n",
      "Epoch 742, Loss: 0.4495470758\n",
      "0.635\n",
      "Epoch 743, Loss: 0.3658297859\n",
      "0.635\n",
      "Epoch 744, Loss: 0.4719260685\n",
      "0.635\n",
      "Epoch 745, Loss: 0.4615853596\n",
      "0.635\n",
      "Epoch 746, Loss: 0.7660396206\n",
      "0.635\n",
      "Epoch 747, Loss: 0.7887998864\n",
      "0.635\n",
      "Epoch 748, Loss: 0.5100061633\n",
      "0.635\n",
      "Epoch 749, Loss: 0.5786700871\n",
      "0.635\n",
      "Epoch 750, Loss: 0.6388303840\n",
      "0.635\n",
      "Epoch 751, Loss: 0.6797276831\n",
      "0.635\n",
      "Epoch 752, Loss: 0.4688086227\n",
      "0.635\n",
      "Epoch 753, Loss: 0.5363405016\n",
      "0.635\n",
      "Epoch 754, Loss: 0.4336952734\n",
      "0.635\n",
      "Epoch 755, Loss: 0.4458639193\n",
      "0.635\n",
      "Epoch 756, Loss: 0.5964643282\n",
      "0.635\n",
      "Epoch 757, Loss: 0.4604392718\n",
      "0.635\n",
      "Epoch 758, Loss: 0.5969823225\n",
      "0.635\n",
      "Epoch 759, Loss: 0.4373550189\n",
      "0.635\n",
      "Epoch 760, Loss: 0.4796864218\n",
      "0.635\n",
      "Epoch 761, Loss: 0.4036921238\n",
      "0.635\n",
      "Epoch 762, Loss: 0.4089551086\n",
      "0.635\n",
      "Epoch 763, Loss: 0.4505200488\n",
      "0.635\n",
      "Epoch 764, Loss: 0.4700108153\n",
      "0.635\n",
      "Epoch 765, Loss: 0.4225293217\n",
      "0.635\n",
      "Epoch 766, Loss: 0.4232985570\n",
      "0.635\n",
      "Epoch 767, Loss: 0.2975853892\n",
      "0.635\n",
      "Epoch 768, Loss: 0.4175394838\n",
      "0.635\n",
      "Epoch 769, Loss: 0.3383478999\n",
      "0.635\n",
      "Epoch 770, Loss: 0.3366443571\n",
      "0.635\n",
      "Epoch 771, Loss: 0.2838933819\n",
      "0.635\n",
      "Epoch 772, Loss: 0.3685478816\n",
      "0.635\n",
      "Epoch 773, Loss: 0.3802149574\n",
      "0.635\n",
      "Epoch 774, Loss: 0.3390207061\n",
      "0.635\n",
      "Epoch 775, Loss: 0.3593652487\n",
      "0.635\n",
      "Epoch 776, Loss: 0.3252251625\n",
      "0.635\n",
      "Epoch 777, Loss: 0.4010415032\n",
      "0.635\n",
      "Epoch 778, Loss: 0.5133365868\n",
      "0.635\n",
      "Epoch 779, Loss: 0.4103805262\n",
      "0.635\n",
      "Epoch 780, Loss: 0.5168363965\n",
      "0.635\n",
      "Epoch 781, Loss: 0.3817463411\n",
      "0.635\n",
      "Epoch 782, Loss: 0.3679978498\n",
      "0.635\n",
      "Epoch 783, Loss: 0.3501526029\n",
      "0.635\n",
      "Epoch 784, Loss: 0.3475178051\n",
      "0.635\n",
      "Epoch 785, Loss: 0.3658445328\n",
      "0.635\n",
      "Epoch 786, Loss: 0.3410493013\n",
      "0.635\n",
      "Epoch 787, Loss: 0.4011353521\n",
      "0.635\n",
      "Epoch 788, Loss: 0.3200942916\n",
      "0.635\n",
      "Epoch 789, Loss: 0.3138559837\n",
      "0.635\n",
      "Epoch 790, Loss: 0.3774125606\n",
      "0.635\n",
      "Epoch 791, Loss: 0.3626693502\n",
      "0.635\n",
      "Epoch 792, Loss: 0.4018509938\n",
      "0.635\n",
      "Epoch 793, Loss: 0.3805610055\n",
      "0.635\n",
      "Epoch 794, Loss: 0.3190378699\n",
      "0.635\n",
      "Epoch 795, Loss: 0.4603232603\n",
      "0.635\n",
      "Epoch 796, Loss: 0.3591472967\n",
      "0.635\n",
      "Epoch 797, Loss: 0.3587979414\n",
      "0.635\n",
      "Epoch 798, Loss: 0.5212695159\n",
      "0.635\n",
      "Epoch 799, Loss: 0.4093059012\n",
      "0.635\n",
      "Epoch 800, Loss: 0.3974696813\n",
      "0.635\n",
      "Epoch 801, Loss: 0.4947857796\n",
      "0.635\n",
      "Epoch 802, Loss: 0.7248739501\n",
      "0.635\n",
      "Epoch 803, Loss: 0.5582472522\n",
      "0.635\n",
      "Epoch 804, Loss: 0.4458078550\n",
      "0.635\n",
      "Epoch 805, Loss: 0.3590501100\n",
      "0.635\n",
      "Epoch 806, Loss: 0.3932745095\n",
      "0.635\n",
      "Epoch 807, Loss: 0.5415077126\n",
      "0.635\n",
      "Epoch 808, Loss: 0.3130779240\n",
      "0.635\n",
      "Epoch 809, Loss: 0.3468322184\n",
      "0.635\n",
      "Epoch 810, Loss: 0.4169879461\n",
      "0.635\n",
      "Epoch 811, Loss: 0.3479250929\n",
      "0.635\n",
      "Epoch 812, Loss: 0.4832245243\n",
      "0.635\n",
      "Epoch 813, Loss: 0.6033525477\n",
      "0.635\n",
      "Epoch 814, Loss: 0.6755793341\n",
      "0.635\n",
      "Epoch 815, Loss: 0.4543907063\n",
      "0.635\n",
      "Epoch 816, Loss: 0.4241385609\n",
      "0.635\n",
      "Epoch 817, Loss: 0.5516607831\n",
      "0.635\n",
      "Epoch 818, Loss: 0.4384847875\n",
      "0.635\n",
      "Epoch 819, Loss: 0.4077854325\n",
      "0.635\n",
      "Epoch 820, Loss: 0.6236185918\n",
      "0.635\n",
      "Epoch 821, Loss: 0.4818763449\n",
      "0.635\n",
      "Epoch 822, Loss: 0.4781228968\n",
      "0.635\n",
      "Epoch 823, Loss: 0.4418707478\n",
      "0.635\n",
      "Epoch 824, Loss: 0.4697325384\n",
      "0.635\n",
      "Epoch 825, Loss: 0.3598453650\n",
      "0.635\n",
      "Epoch 826, Loss: 0.5238954097\n",
      "0.635\n",
      "Epoch 827, Loss: 0.4782146316\n",
      "0.635\n",
      "Epoch 828, Loss: 0.4269072802\n",
      "0.635\n",
      "Epoch 829, Loss: 0.4629913868\n",
      "0.635\n",
      "Epoch 830, Loss: 0.3374372783\n",
      "0.635\n",
      "Epoch 831, Loss: 0.4443177577\n",
      "0.635\n",
      "Epoch 832, Loss: 0.4132548859\n",
      "0.635\n",
      "Epoch 833, Loss: 0.6065572209\n",
      "0.635\n",
      "Epoch 834, Loss: 0.3159504927\n",
      "0.635\n",
      "Epoch 835, Loss: 0.4918714266\n",
      "0.635\n",
      "Epoch 836, Loss: 0.3574662310\n",
      "0.635\n",
      "Epoch 837, Loss: 0.3682201596\n",
      "0.635\n",
      "Epoch 838, Loss: 0.3770937163\n",
      "0.635\n",
      "Epoch 839, Loss: 0.2659159516\n",
      "0.635\n",
      "Epoch 840, Loss: 0.3129842229\n",
      "0.635\n",
      "Epoch 841, Loss: 0.4072182806\n",
      "0.635\n",
      "Epoch 842, Loss: 0.3631503821\n",
      "0.635\n",
      "Epoch 843, Loss: 0.3814601408\n",
      "0.635\n",
      "Epoch 844, Loss: 0.4020443959\n",
      "0.635\n",
      "Epoch 845, Loss: 0.2926565484\n",
      "0.635\n",
      "Epoch 846, Loss: 0.4075302632\n",
      "0.635\n",
      "Epoch 847, Loss: 0.3189963631\n",
      "0.635\n",
      "Epoch 848, Loss: 0.3315499344\n",
      "0.635\n",
      "Epoch 849, Loss: 0.3641570053\n",
      "0.635\n",
      "Epoch 850, Loss: 0.3432964854\n",
      "0.635\n",
      "Epoch 851, Loss: 0.4382236952\n",
      "0.635\n",
      "Epoch 852, Loss: 0.3630177972\n",
      "0.635\n",
      "Epoch 853, Loss: 0.3378429674\n",
      "0.635\n",
      "Epoch 854, Loss: 0.2474091447\n",
      "0.635\n",
      "Epoch 855, Loss: 0.4051163807\n",
      "0.635\n",
      "Epoch 856, Loss: 0.4263246751\n",
      "0.635\n",
      "Epoch 857, Loss: 0.4143948606\n",
      "0.635\n",
      "Epoch 858, Loss: 0.3246660688\n",
      "0.635\n",
      "Epoch 859, Loss: 0.4963125525\n",
      "0.635\n",
      "Epoch 860, Loss: 0.2989906694\n",
      "0.635\n",
      "Epoch 861, Loss: 0.3886141580\n",
      "0.635\n",
      "Epoch 862, Loss: 0.4254736316\n",
      "0.635\n",
      "Epoch 863, Loss: 0.5283699899\n",
      "0.635\n",
      "Epoch 864, Loss: 0.4068675114\n",
      "0.635\n",
      "Epoch 865, Loss: 0.2908749324\n",
      "0.635\n",
      "Epoch 866, Loss: 0.3281706451\n",
      "0.635\n",
      "Epoch 867, Loss: 0.3656222774\n",
      "0.635\n",
      "Epoch 868, Loss: 0.2451024667\n",
      "0.635\n",
      "Epoch 869, Loss: 0.2276753363\n",
      "0.635\n",
      "Epoch 870, Loss: 0.2020228944\n",
      "0.6375\n",
      "Epoch 871, Loss: 0.3243809444\n",
      "0.6375\n",
      "Epoch 872, Loss: 0.2492754168\n",
      "0.6375\n",
      "Epoch 873, Loss: 0.2786015364\n",
      "0.6375\n",
      "Epoch 874, Loss: 0.1924291789\n",
      "0.6525\n",
      "Epoch 875, Loss: 0.1543236031\n",
      "0.65875\n",
      "Epoch 876, Loss: 0.2481375241\n",
      "0.65875\n",
      "Epoch 877, Loss: 0.2299498417\n",
      "0.65875\n",
      "Epoch 878, Loss: 0.2197495410\n",
      "0.65875\n",
      "Epoch 879, Loss: 0.1940659507\n",
      "0.65875\n",
      "Epoch 880, Loss: 0.1877064372\n",
      "0.65875\n",
      "Epoch 881, Loss: 0.2371053475\n",
      "0.65875\n",
      "Epoch 882, Loss: 0.1789511728\n",
      "0.65875\n",
      "Epoch 883, Loss: 0.2861900444\n",
      "0.65875\n",
      "Epoch 884, Loss: 0.1728990790\n",
      "0.65875\n",
      "Epoch 885, Loss: 0.2202675356\n",
      "0.65875\n",
      "Epoch 886, Loss: 0.1325730053\n",
      "0.64375\n",
      "Epoch 887, Loss: 0.2787904553\n",
      "0.64375\n",
      "Epoch 888, Loss: 0.1742229552\n",
      "0.64375\n",
      "Epoch 889, Loss: 0.2359085508\n",
      "0.64375\n",
      "Epoch 890, Loss: 0.1517678797\n",
      "0.64375\n",
      "Epoch 891, Loss: 0.1490857681\n",
      "0.64375\n",
      "Epoch 892, Loss: 0.1390135447\n",
      "0.64375\n",
      "Epoch 893, Loss: 0.1877177300\n",
      "0.64375\n",
      "Epoch 894, Loss: 0.1942331201\n",
      "0.64375\n",
      "Epoch 895, Loss: 0.2142471865\n",
      "0.64375\n",
      "Epoch 896, Loss: 0.2754426815\n",
      "0.64375\n",
      "Epoch 897, Loss: 0.4216206092\n",
      "0.64375\n",
      "Epoch 898, Loss: 0.4241654545\n",
      "0.64375\n",
      "Epoch 899, Loss: 0.3201526861\n",
      "0.64375\n",
      "Epoch 900, Loss: 0.5209073424\n",
      "0.64375\n",
      "Epoch 901, Loss: 0.3987384049\n",
      "0.64375\n",
      "Epoch 902, Loss: 0.4081842718\n",
      "0.64375\n",
      "Epoch 903, Loss: 0.3897586859\n",
      "0.64375\n",
      "Epoch 904, Loss: 0.2476783120\n",
      "0.64375\n",
      "Epoch 905, Loss: 0.3463367540\n",
      "0.64375\n",
      "Epoch 906, Loss: 0.3669053766\n",
      "0.64375\n",
      "Epoch 907, Loss: 0.4606449708\n",
      "0.64375\n",
      "Epoch 908, Loss: 0.5303041907\n",
      "0.64375\n",
      "Epoch 909, Loss: 0.3858385571\n",
      "0.64375\n",
      "Epoch 910, Loss: 0.5168141334\n",
      "0.64375\n",
      "Epoch 911, Loss: 0.2875620358\n",
      "0.64375\n",
      "Epoch 912, Loss: 0.3736794377\n",
      "0.64375\n",
      "Epoch 913, Loss: 0.5130419052\n",
      "0.64375\n",
      "Epoch 914, Loss: 0.2226272243\n",
      "0.64375\n",
      "Epoch 915, Loss: 0.4818864591\n",
      "0.64375\n",
      "Epoch 916, Loss: 0.2863556575\n",
      "0.64375\n",
      "Epoch 917, Loss: 0.2789924808\n",
      "0.64375\n",
      "Epoch 918, Loss: 0.3511012046\n",
      "0.64375\n",
      "Epoch 919, Loss: 0.2869860035\n",
      "0.64375\n",
      "Epoch 920, Loss: 0.2143995426\n",
      "0.64375\n",
      "Epoch 921, Loss: 0.2926572376\n",
      "0.64375\n",
      "Epoch 922, Loss: 0.2872358829\n",
      "0.64375\n",
      "Epoch 923, Loss: 0.3235607294\n",
      "0.64375\n",
      "Epoch 924, Loss: 0.2193893706\n",
      "0.64375\n",
      "Epoch 925, Loss: 0.2694517057\n",
      "0.64375\n",
      "Epoch 926, Loss: 0.2364959929\n",
      "0.64375\n",
      "Epoch 927, Loss: 0.2494474690\n",
      "0.64375\n",
      "Epoch 928, Loss: 0.1765567713\n",
      "0.64375\n",
      "Epoch 929, Loss: 0.1689166360\n",
      "0.64375\n",
      "Epoch 930, Loss: 0.1327974049\n",
      "0.64375\n",
      "Epoch 931, Loss: 0.1369364110\n",
      "0.64375\n",
      "Epoch 932, Loss: 0.1427864426\n",
      "0.64375\n",
      "Epoch 933, Loss: 0.1780192115\n",
      "0.64375\n",
      "Epoch 934, Loss: 0.1523245219\n",
      "0.64375\n",
      "Epoch 935, Loss: 0.1742903247\n",
      "0.64375\n",
      "Epoch 936, Loss: 0.1358098907\n",
      "0.64375\n",
      "Epoch 937, Loss: 0.1805367222\n",
      "0.64375\n",
      "Epoch 938, Loss: 0.2075685426\n",
      "0.64375\n",
      "Epoch 939, Loss: 0.1456977948\n",
      "0.64375\n",
      "Epoch 940, Loss: 0.2277804518\n",
      "0.64375\n",
      "Epoch 941, Loss: 0.2636153874\n",
      "0.64375\n",
      "Epoch 942, Loss: 0.3791701488\n",
      "0.64375\n",
      "Epoch 943, Loss: 0.4296354935\n",
      "0.64375\n",
      "Epoch 944, Loss: 0.5020468893\n",
      "0.64375\n",
      "Epoch 945, Loss: 0.3109425787\n",
      "0.64375\n",
      "Epoch 946, Loss: 0.3357456134\n",
      "0.64375\n",
      "Epoch 947, Loss: 0.4281445843\n",
      "0.64375\n",
      "Epoch 948, Loss: 0.5960431586\n",
      "0.64375\n",
      "Epoch 949, Loss: 0.4227876156\n",
      "0.64375\n",
      "Epoch 950, Loss: 0.4143910208\n",
      "0.64375\n",
      "Epoch 951, Loss: 0.2854968816\n",
      "0.64375\n",
      "Epoch 952, Loss: 0.2198772654\n",
      "0.64375\n",
      "Epoch 953, Loss: 0.2194397283\n",
      "0.64375\n",
      "Epoch 954, Loss: 0.2700381638\n",
      "0.64375\n",
      "Epoch 955, Loss: 0.3374095837\n",
      "0.64375\n",
      "Epoch 956, Loss: 0.3400266811\n",
      "0.64375\n",
      "Epoch 957, Loss: 0.2152675988\n",
      "0.64375\n",
      "Epoch 958, Loss: 0.2458637132\n",
      "0.64375\n",
      "Epoch 959, Loss: 0.3726209539\n",
      "0.64375\n",
      "Epoch 960, Loss: 0.3154347346\n",
      "0.64375\n",
      "Epoch 961, Loss: 0.4784430716\n",
      "0.64375\n",
      "Epoch 962, Loss: 0.4091299137\n",
      "0.64375\n",
      "Epoch 963, Loss: 0.3443478459\n",
      "0.64375\n",
      "Epoch 964, Loss: 0.4319422411\n",
      "0.64375\n",
      "Epoch 965, Loss: 0.3618350687\n",
      "0.64375\n",
      "Epoch 966, Loss: 0.4701398200\n",
      "0.64375\n",
      "Epoch 967, Loss: 0.3270263648\n",
      "0.64375\n",
      "Epoch 968, Loss: 0.2519952873\n",
      "0.64375\n",
      "Epoch 969, Loss: 0.2733722792\n",
      "0.64375\n",
      "Epoch 970, Loss: 0.3663298305\n",
      "0.64375\n",
      "Epoch 971, Loss: 0.2932673477\n",
      "0.64375\n",
      "Epoch 972, Loss: 0.3524172538\n",
      "0.64375\n",
      "Epoch 973, Loss: 0.1887828658\n",
      "0.64375\n",
      "Epoch 974, Loss: 0.3755483402\n",
      "0.64375\n",
      "Epoch 975, Loss: 0.2423578802\n",
      "0.64375\n",
      "Epoch 976, Loss: 0.2886167727\n",
      "0.64375\n",
      "Epoch 977, Loss: 0.1479342230\n",
      "0.64375\n",
      "Epoch 978, Loss: 0.3982138308\n",
      "0.64375\n",
      "Epoch 979, Loss: 0.2496935640\n",
      "0.64375\n",
      "Epoch 980, Loss: 0.2082356930\n",
      "0.64375\n",
      "Epoch 981, Loss: 0.2385726260\n",
      "0.64375\n",
      "Epoch 982, Loss: 0.2764415886\n",
      "0.64375\n",
      "Epoch 983, Loss: 0.2900995661\n",
      "0.64375\n",
      "Epoch 984, Loss: 0.5515015301\n",
      "0.64375\n",
      "Epoch 985, Loss: 0.3674498171\n",
      "0.64375\n",
      "Epoch 986, Loss: 0.4056415929\n",
      "0.64375\n",
      "Epoch 987, Loss: 0.2647156919\n",
      "0.64375\n",
      "Epoch 988, Loss: 0.3370096160\n",
      "0.64375\n",
      "Epoch 989, Loss: 0.2542406681\n",
      "0.64375\n",
      "Epoch 990, Loss: 0.3153080995\n",
      "0.64375\n",
      "Epoch 991, Loss: 0.3638746298\n",
      "0.64375\n",
      "Epoch 992, Loss: 0.5661924151\n",
      "0.64375\n",
      "Epoch 993, Loss: 0.2735736350\n",
      "0.64375\n",
      "Epoch 994, Loss: 0.3526803325\n",
      "0.64375\n",
      "Epoch 995, Loss: 0.4157449623\n",
      "0.64375\n",
      "Epoch 996, Loss: 0.4836130201\n",
      "0.64375\n",
      "Epoch 997, Loss: 0.3284677259\n",
      "0.64375\n",
      "Epoch 998, Loss: 0.4133524612\n",
      "0.64375\n",
      "Epoch 999, Loss: 0.3912520697\n",
      "0.64375\n",
      "Epoch 1000, Loss: 0.2741070752\n",
      "0.64375\n",
      "Epoch 1001, Loss: 0.4538496236\n",
      "0.64375\n",
      "Epoch 1002, Loss: 0.4726480597\n",
      "0.64375\n",
      "Epoch 1003, Loss: 0.3675003479\n",
      "0.64375\n",
      "Epoch 1004, Loss: 0.4397468717\n",
      "0.64375\n",
      "Epoch 1005, Loss: 0.2396692276\n",
      "0.64375\n",
      "Epoch 1006, Loss: 0.3478511102\n",
      "0.64375\n",
      "Epoch 1007, Loss: 0.2732564159\n",
      "0.64375\n",
      "Epoch 1008, Loss: 0.2233656869\n",
      "0.64375\n",
      "Epoch 1009, Loss: 0.1941414915\n",
      "0.64375\n",
      "Epoch 1010, Loss: 0.2081783351\n",
      "0.64375\n",
      "Epoch 1011, Loss: 0.2496957555\n",
      "0.64375\n",
      "Epoch 1012, Loss: 0.1888678727\n",
      "0.64375\n",
      "Epoch 1013, Loss: 0.3932846279\n",
      "0.64375\n",
      "Epoch 1014, Loss: 0.7227540256\n",
      "0.64375\n",
      "Epoch 1015, Loss: 0.5417060272\n",
      "0.64375\n",
      "Epoch 1016, Loss: 0.3502137825\n",
      "0.64375\n",
      "Epoch 1017, Loss: 0.3113159213\n",
      "0.64375\n",
      "Epoch 1018, Loss: 0.2675670420\n",
      "0.64375\n",
      "Epoch 1019, Loss: 0.2883300589\n",
      "0.64375\n",
      "Epoch 1020, Loss: 0.4689300961\n",
      "0.64375\n",
      "Epoch 1021, Loss: 0.3800782927\n",
      "0.64375\n",
      "Epoch 1022, Loss: 0.4122713535\n",
      "0.64375\n",
      "Epoch 1023, Loss: 0.6265326386\n",
      "0.64375\n",
      "Epoch 1024, Loss: 0.3536113690\n",
      "0.64375\n",
      "Epoch 1025, Loss: 0.3351085008\n",
      "0.64375\n",
      "Epoch 1026, Loss: 0.2943816899\n",
      "0.64375\n",
      "Epoch 1027, Loss: 0.2119624316\n",
      "0.64375\n",
      "Epoch 1028, Loss: 0.2809067217\n",
      "0.64375\n",
      "Epoch 1029, Loss: 0.2566856540\n",
      "0.64375\n",
      "Epoch 1030, Loss: 0.3277414742\n",
      "0.64375\n",
      "Epoch 1031, Loss: 0.3077016675\n",
      "0.64375\n",
      "Epoch 1032, Loss: 0.4924972877\n",
      "0.64375\n",
      "Epoch 1033, Loss: 0.4804758819\n",
      "0.64375\n",
      "Epoch 1034, Loss: 0.4883939312\n",
      "0.64375\n",
      "Epoch 1035, Loss: 0.4155975516\n",
      "0.64375\n",
      "Epoch 1036, Loss: 0.4900988073\n",
      "0.64375\n",
      "Epoch 1037, Loss: 0.4343978734\n",
      "0.64375\n",
      "Epoch 1038, Loss: 0.4809855688\n",
      "0.64375\n",
      "Epoch 1039, Loss: 0.5212329041\n",
      "0.64375\n",
      "Epoch 1040, Loss: 0.4297399405\n",
      "0.64375\n",
      "Epoch 1041, Loss: 0.3630918331\n",
      "0.64375\n",
      "Epoch 1042, Loss: 0.2769055908\n",
      "0.64375\n",
      "Epoch 1043, Loss: 0.3173546441\n",
      "0.64375\n",
      "Epoch 1044, Loss: 0.2938226457\n",
      "0.64375\n",
      "Epoch 1045, Loss: 0.3708746110\n",
      "0.64375\n",
      "Epoch 1046, Loss: 0.2698004885\n",
      "0.64375\n",
      "Epoch 1047, Loss: 0.3946053563\n",
      "0.64375\n",
      "Epoch 1048, Loss: 0.3269232931\n",
      "0.64375\n",
      "Epoch 1049, Loss: 0.5720417330\n",
      "0.64375\n",
      "Epoch 1050, Loss: 0.2960903342\n",
      "0.64375\n",
      "Epoch 1051, Loss: 0.4864064316\n",
      "0.64375\n",
      "Epoch 1052, Loss: 0.2868473231\n",
      "0.64375\n",
      "Epoch 1053, Loss: 0.4131538431\n",
      "0.64375\n",
      "Epoch 1054, Loss: 0.2889948848\n",
      "0.64375\n",
      "Epoch 1055, Loss: 0.3024172089\n",
      "0.64375\n",
      "Epoch 1056, Loss: 0.3018691622\n",
      "0.64375\n",
      "Epoch 1057, Loss: 0.2051262024\n",
      "0.64375\n",
      "Epoch 1058, Loss: 0.2100313819\n",
      "0.64375\n",
      "Epoch 1059, Loss: 0.2761076533\n",
      "0.64375\n",
      "Epoch 1060, Loss: 0.1678176075\n",
      "0.64375\n",
      "Epoch 1061, Loss: 0.1749447776\n",
      "0.64375\n",
      "Epoch 1062, Loss: 0.1497873711\n",
      "0.64375\n",
      "Epoch 1063, Loss: 0.1921742683\n",
      "0.64375\n",
      "Epoch 1064, Loss: 0.1304376229\n",
      "0.64375\n",
      "Epoch 1065, Loss: 0.1410129908\n",
      "0.64375\n",
      "Epoch 1066, Loss: 0.2072024467\n",
      "0.64375\n",
      "Epoch 1067, Loss: 0.2668823289\n",
      "0.64375\n",
      "Epoch 1068, Loss: 0.2524047053\n",
      "0.64375\n",
      "Epoch 1069, Loss: 0.2789995755\n",
      "0.64375\n",
      "Epoch 1070, Loss: 0.3734900616\n",
      "0.64375\n",
      "Epoch 1071, Loss: 0.3493682990\n",
      "0.64375\n",
      "Epoch 1072, Loss: 0.3412651648\n",
      "0.64375\n",
      "Epoch 1073, Loss: 0.4540771891\n",
      "0.64375\n",
      "Epoch 1074, Loss: 0.3773385616\n",
      "0.64375\n",
      "Epoch 1075, Loss: 0.4907150954\n",
      "0.64375\n",
      "Epoch 1076, Loss: 0.3871566475\n",
      "0.64375\n",
      "Epoch 1077, Loss: 0.2949517568\n",
      "0.64375\n",
      "Epoch 1078, Loss: 0.1928825297\n",
      "0.64375\n",
      "Epoch 1079, Loss: 0.2758650651\n",
      "0.64375\n",
      "Epoch 1080, Loss: 0.3864194437\n",
      "0.64375\n",
      "Epoch 1081, Loss: 0.3031598077\n",
      "0.64375\n",
      "Epoch 1082, Loss: 0.2680212126\n",
      "0.64375\n",
      "Epoch 1083, Loss: 0.3476013934\n",
      "0.64375\n",
      "Epoch 1084, Loss: 0.5587530493\n",
      "0.64375\n",
      "Epoch 1085, Loss: 0.3444119456\n",
      "0.64375\n",
      "Epoch 1086, Loss: 0.3135086951\n",
      "0.64375\n",
      "Epoch 1087, Loss: 0.3212312715\n",
      "0.64375\n",
      "Epoch 1088, Loss: 0.3704486990\n",
      "0.64375\n",
      "Epoch 1089, Loss: 0.6454521038\n",
      "0.64375\n",
      "Epoch 1090, Loss: 0.4299044342\n",
      "0.64375\n",
      "Epoch 1091, Loss: 0.5161094576\n",
      "0.64375\n",
      "Epoch 1092, Loss: 0.6541896560\n",
      "0.64375\n",
      "Epoch 1093, Loss: 0.3338925588\n",
      "0.64375\n",
      "Epoch 1094, Loss: 0.3270514804\n",
      "0.64375\n",
      "Epoch 1095, Loss: 0.2431840598\n",
      "0.64375\n",
      "Epoch 1096, Loss: 0.2375934800\n",
      "0.64375\n",
      "Epoch 1097, Loss: 0.2449511008\n",
      "0.64375\n",
      "Epoch 1098, Loss: 0.3935284262\n",
      "0.64375\n",
      "Epoch 1099, Loss: 0.2749189094\n",
      "0.64375\n",
      "Epoch 1100, Loss: 0.2281910872\n",
      "0.64375\n",
      "Epoch 1101, Loss: 0.2271492093\n",
      "0.64375\n",
      "Epoch 1102, Loss: 0.1656805950\n",
      "0.64375\n",
      "Epoch 1103, Loss: 0.1809172537\n",
      "0.64375\n",
      "Epoch 1104, Loss: 0.2272904955\n",
      "0.64375\n",
      "Epoch 1105, Loss: 0.2024265579\n",
      "0.64375\n",
      "Epoch 1106, Loss: 0.1069799274\n",
      "0.66125\n",
      "Epoch 1107, Loss: 0.2091579316\n",
      "0.66125\n",
      "Epoch 1108, Loss: 0.1640717381\n",
      "0.66125\n",
      "Epoch 1109, Loss: 0.1540223460\n",
      "0.66125\n",
      "Epoch 1110, Loss: 0.1586240333\n",
      "0.66125\n",
      "Epoch 1111, Loss: 0.1384651204\n",
      "0.66125\n",
      "Epoch 1112, Loss: 0.1427723620\n",
      "0.66125\n",
      "Epoch 1113, Loss: 0.1774820580\n",
      "0.66125\n",
      "Epoch 1114, Loss: 0.1722566912\n",
      "0.66125\n",
      "Epoch 1115, Loss: 0.1904045697\n",
      "0.66125\n",
      "Epoch 1116, Loss: 0.1819225258\n",
      "0.66125\n",
      "Epoch 1117, Loss: 0.2042607426\n",
      "0.66125\n",
      "Epoch 1118, Loss: 0.2638394444\n",
      "0.66125\n",
      "Epoch 1119, Loss: 0.3141878517\n",
      "0.66125\n",
      "Epoch 1120, Loss: 0.2454594859\n",
      "0.66125\n",
      "Epoch 1121, Loss: 0.2842108622\n",
      "0.66125\n",
      "Epoch 1122, Loss: 0.4019582917\n",
      "0.66125\n",
      "Epoch 1123, Loss: 0.3832317092\n",
      "0.66125\n",
      "Epoch 1124, Loss: 0.2161987819\n",
      "0.66125\n",
      "Epoch 1125, Loss: 0.1952788281\n",
      "0.66125\n",
      "Epoch 1126, Loss: 0.2163539544\n",
      "0.66125\n",
      "Epoch 1127, Loss: 0.3071578632\n",
      "0.66125\n",
      "Epoch 1128, Loss: 0.3186266901\n",
      "0.66125\n",
      "Epoch 1129, Loss: 0.4248840814\n",
      "0.66125\n",
      "Epoch 1130, Loss: 0.3757615731\n",
      "0.66125\n",
      "Epoch 1131, Loss: 0.4743713994\n",
      "0.66125\n",
      "Epoch 1132, Loss: 0.2200254593\n",
      "0.66125\n",
      "Epoch 1133, Loss: 0.2012254993\n",
      "0.66125\n",
      "Epoch 1134, Loss: 0.1358637135\n",
      "0.66125\n",
      "Epoch 1135, Loss: 0.2567414885\n",
      "0.66125\n",
      "Epoch 1136, Loss: 0.1957738076\n",
      "0.66125\n",
      "Epoch 1137, Loss: 0.2330713224\n",
      "0.66125\n",
      "Epoch 1138, Loss: 0.1728989900\n",
      "0.66125\n",
      "Epoch 1139, Loss: 0.1362011035\n",
      "0.66125\n",
      "Epoch 1140, Loss: 0.1980294110\n",
      "0.66125\n",
      "Epoch 1141, Loss: 0.2073858285\n",
      "0.66125\n",
      "Epoch 1142, Loss: 0.3127610400\n",
      "0.66125\n",
      "Epoch 1143, Loss: 0.3585354610\n",
      "0.66125\n",
      "Epoch 1144, Loss: 0.1898192744\n",
      "0.66125\n",
      "Epoch 1145, Loss: 0.2170375581\n",
      "0.66125\n",
      "Epoch 1146, Loss: 0.1678949482\n",
      "0.66125\n",
      "Epoch 1147, Loss: 0.1612772810\n",
      "0.66125\n",
      "Epoch 1148, Loss: 0.2706613294\n",
      "0.66125\n",
      "Epoch 1149, Loss: 0.3522447623\n",
      "0.66125\n",
      "Epoch 1150, Loss: 0.2028164396\n",
      "0.66125\n",
      "Epoch 1151, Loss: 0.1665332984\n",
      "0.66125\n",
      "Epoch 1152, Loss: 0.1741849962\n",
      "0.66125\n",
      "Epoch 1153, Loss: 0.1393391476\n",
      "0.66125\n",
      "Epoch 1154, Loss: 0.1802925902\n",
      "0.66125\n",
      "Epoch 1155, Loss: 0.2311634127\n",
      "0.66125\n",
      "Epoch 1156, Loss: 0.2610689375\n",
      "0.66125\n",
      "Epoch 1157, Loss: 0.2409388770\n",
      "0.66125\n",
      "Epoch 1158, Loss: 0.2628169737\n",
      "0.66125\n",
      "Epoch 1159, Loss: 0.3474082225\n",
      "0.66125\n",
      "Epoch 1160, Loss: 0.2367491502\n",
      "0.66125\n",
      "Epoch 1161, Loss: 0.2135154668\n",
      "0.66125\n",
      "Epoch 1162, Loss: 0.2362353477\n",
      "0.66125\n",
      "Epoch 1163, Loss: 0.4386903218\n",
      "0.66125\n",
      "Epoch 1164, Loss: 0.4724798940\n",
      "0.66125\n",
      "Epoch 1165, Loss: 0.2663923493\n",
      "0.66125\n",
      "Epoch 1166, Loss: 0.2419455950\n",
      "0.66125\n",
      "Epoch 1167, Loss: 0.1771915526\n",
      "0.66125\n",
      "Epoch 1168, Loss: 0.2961125578\n",
      "0.66125\n",
      "Epoch 1169, Loss: 0.2889553166\n",
      "0.66125\n",
      "Epoch 1170, Loss: 0.2528369571\n",
      "0.66125\n",
      "Epoch 1171, Loss: 0.1955669957\n",
      "0.66125\n",
      "Epoch 1172, Loss: 0.2309191986\n",
      "0.66125\n",
      "Epoch 1173, Loss: 0.2844932999\n",
      "0.66125\n",
      "Epoch 1174, Loss: 0.2331339499\n",
      "0.66125\n",
      "Epoch 1175, Loss: 0.3020143029\n",
      "0.66125\n",
      "Epoch 1176, Loss: 0.4342054239\n",
      "0.66125\n",
      "Epoch 1177, Loss: 0.2050572809\n",
      "0.66125\n",
      "Epoch 1178, Loss: 0.2416662293\n",
      "0.66125\n",
      "Epoch 1179, Loss: 0.2870252402\n",
      "0.66125\n",
      "Epoch 1180, Loss: 0.2766023744\n",
      "0.66125\n",
      "Epoch 1181, Loss: 0.3091637930\n",
      "0.66125\n",
      "Epoch 1182, Loss: 0.3603913294\n",
      "0.66125\n",
      "Epoch 1183, Loss: 0.3391202121\n",
      "0.66125\n",
      "Epoch 1184, Loss: 0.2847128775\n",
      "0.66125\n",
      "Epoch 1185, Loss: 0.2740335377\n",
      "0.66125\n",
      "Epoch 1186, Loss: 0.4934798416\n",
      "0.66125\n",
      "Epoch 1187, Loss: 0.3622487558\n",
      "0.66125\n",
      "Epoch 1188, Loss: 0.3663087792\n",
      "0.66125\n",
      "Epoch 1189, Loss: 0.1921642859\n",
      "0.66125\n",
      "Epoch 1190, Loss: 0.3697186895\n",
      "0.66125\n",
      "Epoch 1191, Loss: 0.1909702564\n",
      "0.66125\n",
      "Epoch 1192, Loss: 0.1716302459\n",
      "0.66125\n",
      "Epoch 1193, Loss: 0.1586668338\n",
      "0.66125\n",
      "Epoch 1194, Loss: 0.1814027563\n",
      "0.66125\n",
      "Epoch 1195, Loss: 0.1967562654\n",
      "0.66125\n",
      "Epoch 1196, Loss: 0.2027888767\n",
      "0.66125\n",
      "Epoch 1197, Loss: 0.1910784909\n",
      "0.66125\n",
      "Epoch 1198, Loss: 0.1567786717\n",
      "0.66125\n",
      "Epoch 1199, Loss: 0.1835958013\n",
      "0.66125\n",
      "Epoch 1200, Loss: 0.2202634809\n",
      "0.66125\n",
      "Epoch 1201, Loss: 0.1138946799\n",
      "0.66125\n",
      "Epoch 1202, Loss: 0.1924058266\n",
      "0.66125\n",
      "Epoch 1203, Loss: 0.1641479081\n",
      "0.66125\n",
      "Epoch 1204, Loss: 0.1865950192\n",
      "0.66125\n",
      "Epoch 1205, Loss: 0.1228673913\n",
      "0.66125\n",
      "Epoch 1206, Loss: 0.1993311160\n",
      "0.66125\n",
      "Epoch 1207, Loss: 0.0942805779\n",
      "0.65875\n",
      "Epoch 1208, Loss: 0.2925403884\n",
      "0.65875\n",
      "Epoch 1209, Loss: 0.1068733071\n",
      "0.65875\n",
      "Epoch 1210, Loss: 0.2481753301\n",
      "0.65875\n",
      "Epoch 1211, Loss: 0.1331206097\n",
      "0.65875\n",
      "Epoch 1212, Loss: 0.2567748266\n",
      "0.65875\n",
      "Epoch 1213, Loss: 0.2399722284\n",
      "0.65875\n",
      "Epoch 1214, Loss: 0.0947681357\n",
      "0.65875\n",
      "Epoch 1215, Loss: 0.1848102680\n",
      "0.65875\n",
      "Epoch 1216, Loss: 0.2049560888\n",
      "0.65875\n",
      "Epoch 1217, Loss: 0.1405592959\n",
      "0.65875\n",
      "Epoch 1218, Loss: 0.2304930136\n",
      "0.65875\n",
      "Epoch 1219, Loss: 0.2206885964\n",
      "0.65875\n",
      "Epoch 1220, Loss: 0.2321568488\n",
      "0.65875\n",
      "Epoch 1221, Loss: 0.3288745110\n",
      "0.65875\n",
      "Epoch 1222, Loss: 0.3897794750\n",
      "0.65875\n",
      "Epoch 1223, Loss: 0.1578224208\n",
      "0.65875\n",
      "Epoch 1224, Loss: 0.1772228421\n",
      "0.65875\n",
      "Epoch 1225, Loss: 0.2156823984\n",
      "0.65875\n",
      "Epoch 1226, Loss: 0.2386840201\n",
      "0.65875\n",
      "Epoch 1227, Loss: 0.4694243162\n",
      "0.65875\n",
      "Epoch 1228, Loss: 0.9055742990\n",
      "0.65875\n",
      "Epoch 1229, Loss: 0.4244500481\n",
      "0.65875\n",
      "Epoch 1230, Loss: 0.3777229992\n",
      "0.65875\n",
      "Epoch 1231, Loss: 0.3256813664\n",
      "0.65875\n",
      "Epoch 1232, Loss: 0.3585391430\n",
      "0.65875\n",
      "Epoch 1233, Loss: 0.3599205403\n",
      "0.65875\n",
      "Epoch 1234, Loss: 0.2410536885\n",
      "0.65875\n",
      "Epoch 1235, Loss: 0.2184267865\n",
      "0.65875\n",
      "Epoch 1236, Loss: 0.2483250982\n",
      "0.65875\n",
      "Epoch 1237, Loss: 0.2420137079\n",
      "0.65875\n",
      "Epoch 1238, Loss: 0.1539240378\n",
      "0.65875\n",
      "Epoch 1239, Loss: 0.2690690723\n",
      "0.65875\n",
      "Epoch 1240, Loss: 0.2724562334\n",
      "0.65875\n",
      "Epoch 1241, Loss: 0.1279842809\n",
      "0.65875\n",
      "Epoch 1242, Loss: 0.1443629898\n",
      "0.65875\n",
      "Epoch 1243, Loss: 0.2499322947\n",
      "0.65875\n",
      "Epoch 1244, Loss: 0.2884720596\n",
      "0.65875\n",
      "Epoch 1245, Loss: 0.2894001522\n",
      "0.65875\n",
      "Epoch 1246, Loss: 0.1929773555\n",
      "0.65875\n",
      "Epoch 1247, Loss: 0.1537265644\n",
      "0.65875\n",
      "Epoch 1248, Loss: 0.1835172428\n",
      "0.65875\n",
      "Epoch 1249, Loss: 0.1108435312\n",
      "0.65875\n",
      "Epoch 1250, Loss: 0.1747282038\n",
      "0.65875\n",
      "Epoch 1251, Loss: 0.1645330901\n",
      "0.65875\n",
      "Epoch 1252, Loss: 0.1693299565\n",
      "0.65875\n",
      "Epoch 1253, Loss: 0.2290908257\n",
      "0.65875\n",
      "Epoch 1254, Loss: 0.1464240108\n",
      "0.65875\n",
      "Epoch 1255, Loss: 0.1999480953\n",
      "0.65875\n",
      "Epoch 1256, Loss: 0.2434369823\n",
      "0.65875\n",
      "Epoch 1257, Loss: 0.1963126947\n",
      "0.65875\n",
      "Epoch 1258, Loss: 0.2034657861\n",
      "0.65875\n",
      "Epoch 1259, Loss: 0.1900861703\n",
      "0.65875\n",
      "Epoch 1260, Loss: 0.4437262319\n",
      "0.65875\n",
      "Epoch 1261, Loss: 0.1707178088\n",
      "0.65875\n",
      "Epoch 1262, Loss: 0.1217334190\n",
      "0.65875\n",
      "Epoch 1263, Loss: 0.1298763569\n",
      "0.65875\n",
      "Epoch 1264, Loss: 0.2891839099\n",
      "0.65875\n",
      "Epoch 1265, Loss: 0.1734785401\n",
      "0.65875\n",
      "Epoch 1266, Loss: 0.1748758427\n",
      "0.65875\n",
      "Epoch 1267, Loss: 0.2645908747\n",
      "0.65875\n",
      "Epoch 1268, Loss: 0.1307773016\n",
      "0.65875\n",
      "Epoch 1269, Loss: 0.2518969948\n",
      "0.65875\n",
      "Epoch 1270, Loss: 0.2303924477\n",
      "0.65875\n",
      "Epoch 1271, Loss: 0.3137752809\n",
      "0.65875\n",
      "Epoch 1272, Loss: 0.2956399321\n",
      "0.65875\n",
      "Epoch 1273, Loss: 0.2172613698\n",
      "0.65875\n",
      "Epoch 1274, Loss: 0.3297273841\n",
      "0.65875\n",
      "Epoch 1275, Loss: 0.2045748941\n",
      "0.65875\n",
      "Epoch 1276, Loss: 0.2293968399\n",
      "0.65875\n",
      "Epoch 1277, Loss: 0.4528392466\n",
      "0.65875\n",
      "Epoch 1278, Loss: 0.3774104876\n",
      "0.65875\n",
      "Epoch 1279, Loss: 0.3016975594\n",
      "0.65875\n",
      "Epoch 1280, Loss: 0.2336800722\n",
      "0.65875\n",
      "Epoch 1281, Loss: 0.2143306170\n",
      "0.65875\n",
      "Epoch 1282, Loss: 0.1654064419\n",
      "0.65875\n",
      "Epoch 1283, Loss: 0.3768179592\n",
      "0.65875\n",
      "Epoch 1284, Loss: 0.2067530182\n",
      "0.65875\n",
      "Epoch 1285, Loss: 0.2977368011\n",
      "0.65875\n",
      "Epoch 1286, Loss: 0.3311389456\n",
      "0.65875\n",
      "Epoch 1287, Loss: 0.3800244904\n",
      "0.65875\n",
      "Epoch 1288, Loss: 0.3011627194\n",
      "0.65875\n",
      "Epoch 1289, Loss: 0.4007044857\n",
      "0.65875\n",
      "Epoch 1290, Loss: 0.3068576528\n",
      "0.65875\n",
      "Epoch 1291, Loss: 0.2488304197\n",
      "0.65875\n",
      "Epoch 1292, Loss: 0.4067716848\n",
      "0.65875\n",
      "Epoch 1293, Loss: 0.3552112253\n",
      "0.65875\n",
      "Epoch 1294, Loss: 0.2342347710\n",
      "0.65875\n",
      "Epoch 1295, Loss: 0.2081354923\n",
      "0.65875\n",
      "Epoch 1296, Loss: 0.2203329741\n",
      "0.65875\n",
      "Epoch 1297, Loss: 0.1434235143\n",
      "0.65875\n",
      "Epoch 1298, Loss: 0.3060975319\n",
      "0.65875\n",
      "Epoch 1299, Loss: 0.1635240292\n",
      "0.65875\n",
      "Epoch 1300, Loss: 0.1755656113\n",
      "0.65875\n",
      "Epoch 1301, Loss: 0.2245900577\n",
      "0.65875\n",
      "Epoch 1302, Loss: 0.1695025592\n",
      "0.65875\n",
      "Epoch 1303, Loss: 0.2113978733\n",
      "0.65875\n",
      "Epoch 1304, Loss: 0.1917209111\n",
      "0.65875\n",
      "Epoch 1305, Loss: 0.1639832266\n",
      "0.65875\n",
      "Epoch 1306, Loss: 0.2590102823\n",
      "0.65875\n",
      "Epoch 1307, Loss: 0.1571561755\n",
      "0.65875\n",
      "Epoch 1308, Loss: 0.1735544598\n",
      "0.65875\n",
      "Epoch 1309, Loss: 0.1822134277\n",
      "0.65875\n",
      "Epoch 1310, Loss: 0.2240532940\n",
      "0.65875\n",
      "Epoch 1311, Loss: 0.3101976393\n",
      "0.65875\n",
      "Epoch 1312, Loss: 0.3163329153\n",
      "0.65875\n",
      "Epoch 1313, Loss: 0.1796120385\n",
      "0.65875\n",
      "Epoch 1314, Loss: 0.3059492546\n",
      "0.65875\n",
      "Epoch 1315, Loss: 0.2373671460\n",
      "0.65875\n",
      "Epoch 1316, Loss: 0.2361244047\n",
      "0.65875\n",
      "Epoch 1317, Loss: 0.2410853873\n",
      "0.65875\n",
      "Epoch 1318, Loss: 0.1918556671\n",
      "0.65875\n",
      "Epoch 1319, Loss: 0.2287967571\n",
      "0.65875\n",
      "Epoch 1320, Loss: 0.1004067872\n",
      "0.65875\n",
      "Epoch 1321, Loss: 0.1003056620\n",
      "0.65875\n",
      "Epoch 1322, Loss: 0.2231284547\n",
      "0.65875\n",
      "Epoch 1323, Loss: 0.3383268920\n",
      "0.65875\n",
      "Epoch 1324, Loss: 0.4187464840\n",
      "0.65875\n",
      "Epoch 1325, Loss: 0.1533510214\n",
      "0.65875\n",
      "Epoch 1326, Loss: 0.2147998463\n",
      "0.65875\n",
      "Epoch 1327, Loss: 0.3480833800\n",
      "0.65875\n",
      "Epoch 1328, Loss: 0.1243905614\n",
      "0.65875\n",
      "Epoch 1329, Loss: 0.1459802771\n",
      "0.65875\n",
      "Epoch 1330, Loss: 0.1647708564\n",
      "0.65875\n",
      "Epoch 1331, Loss: 0.1978153665\n",
      "0.65875\n",
      "Epoch 1332, Loss: 0.1456178925\n",
      "0.65875\n",
      "Epoch 1333, Loss: 0.2332757178\n",
      "0.65875\n",
      "Epoch 1334, Loss: 0.1293316995\n",
      "0.65875\n",
      "Epoch 1335, Loss: 0.2825662715\n",
      "0.65875\n",
      "Epoch 1336, Loss: 0.1475200594\n",
      "0.65875\n",
      "Epoch 1337, Loss: 0.1896981988\n",
      "0.65875\n",
      "Epoch 1338, Loss: 0.3243194721\n",
      "0.65875\n",
      "Epoch 1339, Loss: 0.2051653473\n",
      "0.65875\n",
      "Epoch 1340, Loss: 0.1281247154\n",
      "0.65875\n",
      "Epoch 1341, Loss: 0.5142226316\n",
      "0.65875\n",
      "Epoch 1342, Loss: 0.1151652848\n",
      "0.65875\n",
      "Epoch 1343, Loss: 0.3433087051\n",
      "0.65875\n",
      "Epoch 1344, Loss: 0.1312618520\n",
      "0.65875\n",
      "Epoch 1345, Loss: 0.2521992405\n",
      "0.65875\n",
      "Epoch 1346, Loss: 0.2554540786\n",
      "0.65875\n",
      "Epoch 1347, Loss: 0.2225243472\n",
      "0.65875\n",
      "Epoch 1348, Loss: 0.1666362771\n",
      "0.65875\n",
      "Epoch 1349, Loss: 0.2201493287\n",
      "0.65875\n",
      "Epoch 1350, Loss: 0.1422429613\n",
      "0.65875\n",
      "Epoch 1351, Loss: 0.1932136777\n",
      "0.65875\n",
      "Epoch 1352, Loss: 0.1932010744\n",
      "0.65875\n",
      "Epoch 1353, Loss: 0.1860161297\n",
      "0.65875\n",
      "Epoch 1354, Loss: 0.1355471792\n",
      "0.65875\n",
      "Epoch 1355, Loss: 0.1136785502\n",
      "0.65875\n",
      "Epoch 1356, Loss: 0.1740708883\n",
      "0.65875\n",
      "Epoch 1357, Loss: 0.2515747589\n",
      "0.65875\n",
      "Epoch 1358, Loss: 0.1225978128\n",
      "0.65875\n",
      "Epoch 1359, Loss: 0.2043093909\n",
      "0.65875\n",
      "Epoch 1360, Loss: 0.1221894635\n",
      "0.65875\n",
      "Epoch 1361, Loss: 0.2759239882\n",
      "0.65875\n",
      "Epoch 1362, Loss: 0.2073529633\n",
      "0.65875\n",
      "Epoch 1363, Loss: 0.3291176040\n",
      "0.65875\n",
      "Epoch 1364, Loss: 0.2203474007\n",
      "0.65875\n",
      "Epoch 1365, Loss: 0.2163886288\n",
      "0.65875\n",
      "Epoch 1366, Loss: 0.1843208665\n",
      "0.65875\n",
      "Epoch 1367, Loss: 0.2225617233\n",
      "0.65875\n",
      "Epoch 1368, Loss: 0.1888683373\n",
      "0.65875\n",
      "Epoch 1369, Loss: 0.1685058100\n",
      "0.65875\n",
      "Epoch 1370, Loss: 0.1267380925\n",
      "0.65875\n",
      "Epoch 1371, Loss: 0.2349796604\n",
      "0.65875\n",
      "Epoch 1372, Loss: 0.2202212355\n",
      "0.65875\n",
      "Epoch 1373, Loss: 0.2567026777\n",
      "0.65875\n",
      "Epoch 1374, Loss: 0.2206492836\n",
      "0.65875\n",
      "Epoch 1375, Loss: 0.3252790702\n",
      "0.65875\n",
      "Epoch 1376, Loss: 0.3431416671\n",
      "0.65875\n",
      "Epoch 1377, Loss: 0.2700884619\n",
      "0.65875\n",
      "Epoch 1378, Loss: 0.4199562188\n",
      "0.65875\n",
      "Epoch 1379, Loss: 0.2440085467\n",
      "0.65875\n",
      "Epoch 1380, Loss: 0.4017054777\n",
      "0.65875\n",
      "Epoch 1381, Loss: 0.2108428365\n",
      "0.65875\n",
      "Epoch 1382, Loss: 0.3791166168\n",
      "0.65875\n",
      "Epoch 1383, Loss: 0.2110088970\n",
      "0.65875\n",
      "Epoch 1384, Loss: 0.2231538601\n",
      "0.65875\n",
      "Epoch 1385, Loss: 0.2424771507\n",
      "0.65875\n",
      "Epoch 1386, Loss: 0.6015306887\n",
      "0.65875\n",
      "Epoch 1387, Loss: 0.3996821284\n",
      "0.65875\n",
      "Epoch 1388, Loss: 0.1770767982\n",
      "0.65875\n",
      "Epoch 1389, Loss: 0.2772781942\n",
      "0.65875\n",
      "Epoch 1390, Loss: 0.3227188146\n",
      "0.65875\n",
      "Epoch 1391, Loss: 0.2312991249\n",
      "0.65875\n",
      "Epoch 1392, Loss: 0.5379610196\n",
      "0.65875\n",
      "Epoch 1393, Loss: 0.4210962964\n",
      "0.65875\n",
      "Epoch 1394, Loss: 0.3877155617\n",
      "0.65875\n",
      "Epoch 1395, Loss: 0.3329786991\n",
      "0.65875\n",
      "Epoch 1396, Loss: 0.2842730182\n",
      "0.65875\n",
      "Epoch 1397, Loss: 0.5953151411\n",
      "0.65875\n",
      "Epoch 1398, Loss: 0.1571583297\n",
      "0.65875\n",
      "Epoch 1399, Loss: 0.2089521999\n",
      "0.65875\n",
      "Epoch 1400, Loss: 0.3265900683\n",
      "0.65875\n",
      "Epoch 1401, Loss: 0.2487099357\n",
      "0.65875\n",
      "Epoch 1402, Loss: 0.2916799225\n",
      "0.65875\n",
      "Epoch 1403, Loss: 0.2329538418\n",
      "0.65875\n",
      "Epoch 1404, Loss: 0.3291172513\n",
      "0.65875\n",
      "Epoch 1405, Loss: 0.2244656972\n",
      "0.65875\n",
      "Epoch 1406, Loss: 0.2218235473\n",
      "0.65875\n",
      "Epoch 1407, Loss: 0.2486422344\n",
      "0.65875\n",
      "Epoch 1408, Loss: 0.2790485190\n",
      "0.65875\n",
      "Epoch 1409, Loss: 0.1707270553\n",
      "0.65875\n",
      "Epoch 1410, Loss: 0.1801578664\n",
      "0.65875\n",
      "Epoch 1411, Loss: 0.2250259743\n",
      "0.65875\n",
      "Epoch 1412, Loss: 0.3577327683\n",
      "0.65875\n",
      "Epoch 1413, Loss: 0.3055467969\n",
      "0.65875\n",
      "Epoch 1414, Loss: 0.2986561648\n",
      "0.65875\n",
      "Epoch 1415, Loss: 0.4004265539\n",
      "0.65875\n",
      "Epoch 1416, Loss: 0.4633747012\n",
      "0.65875\n",
      "Epoch 1417, Loss: 0.3351680731\n",
      "0.65875\n",
      "Epoch 1418, Loss: 0.3091656084\n",
      "0.65875\n",
      "Epoch 1419, Loss: 0.3144540154\n",
      "0.65875\n",
      "Epoch 1420, Loss: 0.2765588441\n",
      "0.65875\n",
      "Epoch 1421, Loss: 0.2659160989\n",
      "0.65875\n",
      "Epoch 1422, Loss: 0.3412424536\n",
      "0.65875\n",
      "Epoch 1423, Loss: 0.4364138027\n",
      "0.65875\n",
      "Epoch 1424, Loss: 0.3764042380\n",
      "0.65875\n",
      "Epoch 1425, Loss: 0.2181877940\n",
      "0.65875\n",
      "Epoch 1426, Loss: 0.5136059579\n",
      "0.65875\n",
      "Epoch 1427, Loss: 0.7912361848\n",
      "0.65875\n",
      "Epoch 1428, Loss: 0.4716546160\n",
      "0.65875\n",
      "Epoch 1429, Loss: 0.5542689983\n",
      "0.65875\n",
      "Epoch 1430, Loss: 0.2255796173\n",
      "0.65875\n",
      "Epoch 1431, Loss: 0.3336320472\n",
      "0.65875\n",
      "Epoch 1432, Loss: 0.3536991790\n",
      "0.65875\n",
      "Epoch 1433, Loss: 0.4193197206\n",
      "0.65875\n",
      "Epoch 1434, Loss: 0.5082506913\n",
      "0.65875\n",
      "Epoch 1435, Loss: 0.4489620336\n",
      "0.65875\n",
      "Epoch 1436, Loss: 0.6735745207\n",
      "0.65875\n",
      "Epoch 1437, Loss: 0.3906915473\n",
      "0.65875\n",
      "Epoch 1438, Loss: 0.5408378632\n",
      "0.65875\n",
      "Epoch 1439, Loss: 0.6063980398\n",
      "0.65875\n",
      "Epoch 1440, Loss: 0.2867257530\n",
      "0.65875\n",
      "Epoch 1441, Loss: 0.3647691285\n",
      "0.65875\n",
      "Epoch 1442, Loss: 0.3789013607\n",
      "0.65875\n",
      "Epoch 1443, Loss: 0.3430227844\n",
      "0.65875\n",
      "Epoch 1444, Loss: 0.1713144664\n",
      "0.65875\n",
      "Epoch 1445, Loss: 0.4561781242\n",
      "0.65875\n",
      "Epoch 1446, Loss: 0.4366592665\n",
      "0.65875\n",
      "Epoch 1447, Loss: 0.2082568365\n",
      "0.65875\n",
      "Epoch 1448, Loss: 0.1273281719\n",
      "0.65875\n",
      "Epoch 1449, Loss: 0.2118016814\n",
      "0.65875\n",
      "Epoch 1450, Loss: 0.2309462511\n",
      "0.65875\n",
      "Epoch 1451, Loss: 0.3308325042\n",
      "0.65875\n",
      "Epoch 1452, Loss: 0.2691892056\n",
      "0.65875\n",
      "Epoch 1453, Loss: 0.3482703268\n",
      "0.65875\n",
      "Epoch 1454, Loss: 0.4278269219\n",
      "0.65875\n",
      "Epoch 1455, Loss: 0.3518881260\n",
      "0.65875\n",
      "Epoch 1456, Loss: 0.3776941719\n",
      "0.65875\n",
      "Epoch 1457, Loss: 0.4465727852\n",
      "0.65875\n",
      "Epoch 1458, Loss: 0.5090383697\n",
      "0.65875\n",
      "Epoch 1459, Loss: 0.4810357204\n",
      "0.65875\n",
      "Epoch 1460, Loss: 0.5502198163\n",
      "0.65875\n",
      "Epoch 1461, Loss: 0.3174481635\n",
      "0.65875\n",
      "Epoch 1462, Loss: 0.3668273426\n",
      "0.65875\n",
      "Epoch 1463, Loss: 0.3128310518\n",
      "0.65875\n",
      "Epoch 1464, Loss: 0.3273511102\n",
      "0.65875\n",
      "Epoch 1465, Loss: 0.3280647025\n",
      "0.65875\n",
      "Epoch 1466, Loss: 0.4363189689\n",
      "0.65875\n",
      "Epoch 1467, Loss: 0.3161851968\n",
      "0.65875\n",
      "Epoch 1468, Loss: 0.5507461166\n",
      "0.65875\n",
      "Epoch 1469, Loss: 0.2985399850\n",
      "0.65875\n",
      "Epoch 1470, Loss: 0.3614762150\n",
      "0.65875\n",
      "Epoch 1471, Loss: 0.2294092294\n",
      "0.65875\n",
      "Epoch 1472, Loss: 0.5448558136\n",
      "0.65875\n",
      "Epoch 1473, Loss: 0.2727419290\n",
      "0.65875\n",
      "Epoch 1474, Loss: 0.3819136054\n",
      "0.65875\n",
      "Epoch 1475, Loss: 0.4383794706\n",
      "0.65875\n",
      "Epoch 1476, Loss: 0.5100683138\n",
      "0.65875\n",
      "Epoch 1477, Loss: 0.2716305620\n",
      "0.65875\n",
      "Epoch 1478, Loss: 0.4787088294\n",
      "0.65875\n",
      "Epoch 1479, Loss: 0.5536909220\n",
      "0.65875\n",
      "Epoch 1480, Loss: 0.4432795474\n",
      "0.65875\n",
      "Epoch 1481, Loss: 0.4951335219\n",
      "0.65875\n",
      "Epoch 1482, Loss: 0.3781957513\n",
      "0.65875\n",
      "Epoch 1483, Loss: 0.5243991058\n",
      "0.65875\n",
      "Epoch 1484, Loss: 0.4862474652\n",
      "0.65875\n",
      "Epoch 1485, Loss: 0.3466496894\n",
      "0.65875\n",
      "Epoch 1486, Loss: 0.4083288819\n",
      "0.65875\n",
      "Epoch 1487, Loss: 0.2725218790\n",
      "0.65875\n",
      "Epoch 1488, Loss: 0.5437535073\n",
      "0.65875\n",
      "Epoch 1489, Loss: 0.3091478361\n",
      "0.65875\n",
      "Epoch 1490, Loss: 0.4811385459\n",
      "0.65875\n",
      "Epoch 1491, Loss: 0.2294157106\n",
      "0.65875\n",
      "Epoch 1492, Loss: 0.5551191444\n",
      "0.65875\n",
      "Epoch 1493, Loss: 0.2682013943\n",
      "0.65875\n",
      "Epoch 1494, Loss: 0.4022456244\n",
      "0.65875\n",
      "Epoch 1495, Loss: 0.1919063942\n",
      "0.65875\n",
      "Epoch 1496, Loss: 0.2481847493\n",
      "0.65875\n",
      "Epoch 1497, Loss: 0.2197730309\n",
      "0.65875\n",
      "Epoch 1498, Loss: 0.2032699921\n",
      "0.65875\n",
      "Epoch 1499, Loss: 0.2455879351\n",
      "0.65875\n",
      "Epoch 1500, Loss: 0.3370184866\n",
      "0.65875\n",
      "Epoch 1501, Loss: 0.2389095013\n",
      "0.65875\n",
      "Epoch 1502, Loss: 0.1995285685\n",
      "0.65875\n",
      "Epoch 1503, Loss: 0.4080578002\n",
      "0.65875\n",
      "Epoch 1504, Loss: 0.2474120565\n",
      "0.65875\n",
      "Epoch 1505, Loss: 0.4197763950\n",
      "0.65875\n",
      "Epoch 1506, Loss: 0.2680605174\n",
      "0.65875\n",
      "Epoch 1507, Loss: 0.3596851933\n",
      "0.65875\n",
      "Epoch 1508, Loss: 0.3471969291\n",
      "0.65875\n",
      "Epoch 1509, Loss: 0.3153594011\n",
      "0.65875\n",
      "Epoch 1510, Loss: 0.3428742209\n",
      "0.65875\n",
      "Epoch 1511, Loss: 0.2050386838\n",
      "0.65875\n",
      "Epoch 1512, Loss: 0.3219722142\n",
      "0.65875\n",
      "Epoch 1513, Loss: 0.2747861896\n",
      "0.65875\n",
      "Epoch 1514, Loss: 0.2842212864\n",
      "0.65875\n",
      "Epoch 1515, Loss: 0.2202050584\n",
      "0.65875\n",
      "Epoch 1516, Loss: 0.3112086834\n",
      "0.65875\n",
      "Epoch 1517, Loss: 0.2290682360\n",
      "0.65875\n",
      "Epoch 1518, Loss: 0.2755830898\n",
      "0.65875\n",
      "Epoch 1519, Loss: 0.3198798299\n",
      "0.65875\n",
      "Epoch 1520, Loss: 0.2606502207\n",
      "0.65875\n",
      "Epoch 1521, Loss: 0.2668699474\n",
      "0.65875\n",
      "Epoch 1522, Loss: 0.5123103171\n",
      "0.65875\n",
      "Epoch 1523, Loss: 0.1829340061\n",
      "0.65875\n",
      "Epoch 1524, Loss: 0.4364564491\n",
      "0.65875\n",
      "Epoch 1525, Loss: 0.2341546294\n",
      "0.65875\n",
      "Epoch 1526, Loss: 0.3929694051\n",
      "0.65875\n",
      "Epoch 1527, Loss: 0.2953674097\n",
      "0.65875\n",
      "Epoch 1528, Loss: 0.3423853370\n",
      "0.65875\n",
      "Epoch 1529, Loss: 0.2246272690\n",
      "0.65875\n",
      "Epoch 1530, Loss: 0.3840336470\n",
      "0.65875\n",
      "Epoch 1531, Loss: 0.2112900267\n",
      "0.65875\n",
      "Epoch 1532, Loss: 0.4079900883\n",
      "0.65875\n",
      "Epoch 1533, Loss: 0.3382131286\n",
      "0.65875\n",
      "Epoch 1534, Loss: 0.3770993695\n",
      "0.65875\n",
      "Epoch 1535, Loss: 0.4061422393\n",
      "0.65875\n",
      "Epoch 1536, Loss: 0.3131451636\n",
      "0.65875\n",
      "Epoch 1537, Loss: 0.5218622761\n",
      "0.65875\n",
      "Epoch 1538, Loss: 0.1729927989\n",
      "0.65875\n",
      "Epoch 1539, Loss: 0.3043437479\n",
      "0.65875\n",
      "Epoch 1540, Loss: 0.2696850225\n",
      "0.65875\n",
      "Epoch 1541, Loss: 0.3144826164\n",
      "0.65875\n",
      "Epoch 1542, Loss: 0.4848351529\n",
      "0.65875\n",
      "Epoch 1543, Loss: 0.2891964365\n",
      "0.65875\n",
      "Epoch 1544, Loss: 0.3150679205\n",
      "0.65875\n",
      "Epoch 1545, Loss: 0.3769167041\n",
      "0.65875\n",
      "Epoch 1546, Loss: 0.5696291272\n",
      "0.65875\n",
      "Epoch 1547, Loss: 0.2728935776\n",
      "0.65875\n",
      "Epoch 1548, Loss: 0.3730989618\n",
      "0.65875\n",
      "Epoch 1549, Loss: 0.3288039551\n",
      "0.65875\n",
      "Epoch 1550, Loss: 0.3915448701\n",
      "0.65875\n",
      "Epoch 1551, Loss: 0.2598646946\n",
      "0.65875\n",
      "Epoch 1552, Loss: 0.2796047529\n",
      "0.65875\n",
      "Epoch 1553, Loss: 0.2808086056\n",
      "0.65875\n",
      "Epoch 1554, Loss: 0.3985808231\n",
      "0.65875\n",
      "Epoch 1555, Loss: 0.3180192242\n",
      "0.65875\n",
      "Epoch 1556, Loss: 0.6041419614\n",
      "0.65875\n",
      "Epoch 1557, Loss: 0.5221712290\n",
      "0.65875\n",
      "Epoch 1558, Loss: 0.3863378221\n",
      "0.65875\n",
      "Epoch 1559, Loss: 0.3067659058\n",
      "0.65875\n",
      "Epoch 1560, Loss: 0.1770448998\n",
      "0.65875\n",
      "Epoch 1561, Loss: 0.5282552917\n",
      "0.65875\n",
      "Epoch 1562, Loss: 0.5568453323\n",
      "0.65875\n",
      "Epoch 1563, Loss: 0.2976350844\n",
      "0.65875\n",
      "Epoch 1564, Loss: 0.2501793826\n",
      "0.65875\n",
      "Epoch 1565, Loss: 0.2986109483\n",
      "0.65875\n",
      "Epoch 1566, Loss: 0.6473785874\n",
      "0.65875\n",
      "Epoch 1567, Loss: 0.3179242526\n",
      "0.65875\n",
      "Epoch 1568, Loss: 0.3522992369\n",
      "0.65875\n",
      "Epoch 1569, Loss: 0.4640417000\n",
      "0.65875\n",
      "Epoch 1570, Loss: 0.5619519947\n",
      "0.65875\n",
      "Epoch 1571, Loss: 0.4115738127\n",
      "0.65875\n",
      "Epoch 1572, Loss: 0.5645511913\n",
      "0.65875\n",
      "Epoch 1573, Loss: 0.4156905741\n",
      "0.65875\n",
      "Epoch 1574, Loss: 0.2737790483\n",
      "0.65875\n",
      "Epoch 1575, Loss: 0.2855821158\n",
      "0.65875\n",
      "Epoch 1576, Loss: 0.4160848821\n",
      "0.65875\n",
      "Epoch 1577, Loss: 0.2592253992\n",
      "0.65875\n",
      "Epoch 1578, Loss: 0.3008217465\n",
      "0.65875\n",
      "Epoch 1579, Loss: 0.3697808280\n",
      "0.65875\n",
      "Epoch 1580, Loss: 0.2836378398\n",
      "0.65875\n",
      "Epoch 1581, Loss: 0.2279647825\n",
      "0.65875\n",
      "Epoch 1582, Loss: 0.5880817521\n",
      "0.65875\n",
      "Epoch 1583, Loss: 0.3099650229\n",
      "0.65875\n",
      "Epoch 1584, Loss: 0.5359658141\n",
      "0.65875\n",
      "Epoch 1585, Loss: 0.3110803809\n",
      "0.65875\n",
      "Epoch 1586, Loss: 0.4156301916\n",
      "0.65875\n",
      "Epoch 1587, Loss: 0.3840458373\n",
      "0.65875\n",
      "Epoch 1588, Loss: 0.4569966209\n",
      "0.65875\n",
      "Epoch 1589, Loss: 0.6010159841\n",
      "0.65875\n",
      "Epoch 1590, Loss: 0.5700691927\n",
      "0.65875\n",
      "Epoch 1591, Loss: 0.4814157016\n",
      "0.65875\n",
      "Epoch 1592, Loss: 0.2836838979\n",
      "0.65875\n",
      "Epoch 1593, Loss: 0.3354068954\n",
      "0.65875\n",
      "Epoch 1594, Loss: 0.2874538574\n",
      "0.65875\n",
      "Epoch 1595, Loss: 0.6429597512\n",
      "0.65875\n",
      "Epoch 1596, Loss: 0.6399848773\n",
      "0.65875\n",
      "Epoch 1597, Loss: 0.3031423708\n",
      "0.65875\n",
      "Epoch 1598, Loss: 0.2064243175\n",
      "0.65875\n",
      "Epoch 1599, Loss: 0.1755599365\n",
      "0.65875\n",
      "Epoch 1600, Loss: 0.2285482981\n",
      "0.65875\n",
      "Epoch 1601, Loss: 0.2390442999\n",
      "0.65875\n",
      "Epoch 1602, Loss: 0.2170332681\n",
      "0.65875\n",
      "Epoch 1603, Loss: 0.3519430770\n",
      "0.65875\n",
      "Epoch 1604, Loss: 0.3314361420\n",
      "0.65875\n",
      "Epoch 1605, Loss: 0.4454745022\n",
      "0.65875\n",
      "Epoch 1606, Loss: 0.2621726213\n",
      "0.65875\n",
      "Epoch 1607, Loss: 0.1872571096\n",
      "0.65875\n",
      "Epoch 1608, Loss: 0.1296061678\n",
      "0.65875\n",
      "Epoch 1609, Loss: 0.2094235127\n",
      "0.65875\n",
      "Epoch 1610, Loss: 0.2265417910\n",
      "0.65875\n",
      "Epoch 1611, Loss: 0.1962447135\n",
      "0.65875\n",
      "Epoch 1612, Loss: 0.2384920514\n",
      "0.65875\n",
      "Epoch 1613, Loss: 0.2263967066\n",
      "0.65875\n",
      "Epoch 1614, Loss: 0.2684051054\n",
      "0.65875\n",
      "Epoch 1615, Loss: 0.2389662571\n",
      "0.65875\n",
      "Epoch 1616, Loss: 0.3378988095\n",
      "0.65875\n",
      "Epoch 1617, Loss: 0.2486473342\n",
      "0.65875\n",
      "Epoch 1618, Loss: 0.2395400744\n",
      "0.65875\n",
      "Epoch 1619, Loss: 0.4987147279\n",
      "0.65875\n",
      "Epoch 1620, Loss: 0.6999799487\n",
      "0.65875\n",
      "Epoch 1621, Loss: 0.5874894129\n",
      "0.65875\n",
      "Epoch 1622, Loss: 0.6150049562\n",
      "0.65875\n",
      "Epoch 1623, Loss: 0.3526093961\n",
      "0.65875\n",
      "Epoch 1624, Loss: 0.3068801064\n",
      "0.65875\n",
      "Epoch 1625, Loss: 0.2708738041\n",
      "0.65875\n",
      "Epoch 1626, Loss: 0.2800772396\n",
      "0.65875\n",
      "Epoch 1627, Loss: 0.4375672853\n",
      "0.65875\n",
      "Epoch 1628, Loss: 0.4587924866\n",
      "0.65875\n",
      "Epoch 1629, Loss: 0.4622355352\n",
      "0.65875\n",
      "Epoch 1630, Loss: 0.4465718210\n",
      "0.65875\n",
      "Epoch 1631, Loss: 0.4189795026\n",
      "0.65875\n",
      "Epoch 1632, Loss: 0.3267560357\n",
      "0.65875\n",
      "Epoch 1633, Loss: 0.2480021596\n",
      "0.65875\n",
      "Epoch 1634, Loss: 0.3553827104\n",
      "0.65875\n",
      "Epoch 1635, Loss: 0.3807956959\n",
      "0.65875\n",
      "Epoch 1636, Loss: 0.3112419846\n",
      "0.65875\n",
      "Epoch 1637, Loss: 0.3238857403\n",
      "0.65875\n",
      "Epoch 1638, Loss: 0.2923357696\n",
      "0.65875\n",
      "Epoch 1639, Loss: 0.2827542061\n",
      "0.65875\n",
      "Epoch 1640, Loss: 0.2208965269\n",
      "0.65875\n",
      "Epoch 1641, Loss: 0.1958313363\n",
      "0.65875\n",
      "Epoch 1642, Loss: 0.1632312423\n",
      "0.65875\n",
      "Epoch 1643, Loss: 0.1677547214\n",
      "0.65875\n",
      "Epoch 1644, Loss: 0.1135293991\n",
      "0.65875\n",
      "Epoch 1645, Loss: 0.1266457372\n",
      "0.65875\n",
      "Epoch 1646, Loss: 0.1682877015\n",
      "0.65875\n",
      "Epoch 1647, Loss: 0.1746729347\n",
      "0.65875\n",
      "Epoch 1648, Loss: 0.2438263518\n",
      "0.65875\n",
      "Epoch 1649, Loss: 0.2359409123\n",
      "0.65875\n",
      "Epoch 1650, Loss: 0.2965919941\n",
      "0.65875\n",
      "Epoch 1651, Loss: 0.3866336946\n",
      "0.65875\n",
      "Epoch 1652, Loss: 0.3407288965\n",
      "0.65875\n",
      "Epoch 1653, Loss: 0.2249735821\n",
      "0.65875\n",
      "Epoch 1654, Loss: 0.2865280804\n",
      "0.65875\n",
      "Epoch 1655, Loss: 0.3185374651\n",
      "0.65875\n",
      "Epoch 1656, Loss: 0.3324118003\n",
      "0.65875\n",
      "Epoch 1657, Loss: 0.3334162569\n",
      "0.65875\n",
      "Epoch 1658, Loss: 0.3493871754\n",
      "0.65875\n",
      "Epoch 1659, Loss: 0.3061916337\n",
      "0.65875\n",
      "Epoch 1660, Loss: 0.1784529263\n",
      "0.65875\n",
      "Epoch 1661, Loss: 0.1110107938\n",
      "0.65875\n",
      "Epoch 1662, Loss: 0.1120859254\n",
      "0.65875\n",
      "Epoch 1663, Loss: 0.1330329957\n",
      "0.65875\n",
      "Epoch 1664, Loss: 0.1452390735\n",
      "0.65875\n",
      "Epoch 1665, Loss: 0.1064624146\n",
      "0.65875\n",
      "Epoch 1666, Loss: 0.0729494429\n",
      "0.65125\n",
      "Epoch 1667, Loss: 0.0601093507\n",
      "0.665\n",
      "Epoch 1668, Loss: 0.0552986358\n",
      "0.665\n",
      "Epoch 1669, Loss: 0.0561355163\n",
      "0.665\n",
      "Epoch 1670, Loss: 0.0586058048\n",
      "0.665\n",
      "Epoch 1671, Loss: 0.0473069256\n",
      "0.64875\n",
      "Epoch 1672, Loss: 0.0500772094\n",
      "0.64875\n",
      "Epoch 1673, Loss: 0.0638977888\n",
      "0.64875\n",
      "Epoch 1674, Loss: 0.0642414782\n",
      "0.64875\n",
      "Epoch 1675, Loss: 0.0532857022\n",
      "0.64875\n",
      "Epoch 1676, Loss: 0.0372902125\n",
      "0.6525\n",
      "Epoch 1677, Loss: 0.0330989894\n",
      "0.65625\n",
      "Epoch 1678, Loss: 0.0321644047\n",
      "0.655\n",
      "Epoch 1679, Loss: 0.0324955228\n",
      "0.655\n",
      "Epoch 1680, Loss: 0.0308796709\n",
      "0.65\n",
      "Epoch 1681, Loss: 0.0294522068\n",
      "0.6525\n",
      "Epoch 1682, Loss: 0.0280647781\n",
      "0.6475\n",
      "Epoch 1683, Loss: 0.0263600401\n",
      "0.64625\n",
      "Epoch 1684, Loss: 0.0257895675\n",
      "0.65625\n",
      "Epoch 1685, Loss: 0.0290651314\n",
      "0.65625\n",
      "Epoch 1686, Loss: 0.0267711440\n",
      "0.65625\n",
      "Epoch 1687, Loss: 0.0254080644\n",
      "0.64375\n",
      "Epoch 1688, Loss: 0.0247191158\n",
      "0.6475\n",
      "Epoch 1689, Loss: 0.0253835390\n",
      "0.6475\n",
      "Epoch 1690, Loss: 0.0255447421\n",
      "0.6475\n",
      "Epoch 1691, Loss: 0.0242473746\n",
      "0.6475\n",
      "Epoch 1692, Loss: 0.0235353660\n",
      "0.6475\n",
      "Epoch 1693, Loss: 0.0234043254\n",
      "0.64875\n",
      "Epoch 1694, Loss: 0.0230752329\n",
      "0.65625\n",
      "Epoch 1695, Loss: 0.0236050936\n",
      "0.65625\n",
      "Epoch 1696, Loss: 0.0215723952\n",
      "0.65125\n",
      "Epoch 1697, Loss: 0.0209152057\n",
      "0.645\n",
      "Epoch 1698, Loss: 0.0211626112\n",
      "0.645\n",
      "Epoch 1699, Loss: 0.0225862145\n",
      "0.645\n",
      "Epoch 1700, Loss: 0.0224451971\n",
      "0.645\n",
      "Epoch 1701, Loss: 0.0203636412\n",
      "0.645\n",
      "Epoch 1702, Loss: 0.0201898106\n",
      "0.64875\n",
      "Epoch 1703, Loss: 0.0207212653\n",
      "0.64875\n",
      "Epoch 1704, Loss: 0.0201642966\n",
      "0.65125\n",
      "Epoch 1705, Loss: 0.0222529615\n",
      "0.65125\n",
      "Epoch 1706, Loss: 0.0225758631\n",
      "0.65125\n",
      "Epoch 1707, Loss: 0.0196550786\n",
      "0.64875\n",
      "Epoch 1708, Loss: 0.0209920712\n",
      "0.64875\n",
      "Epoch 1709, Loss: 0.0193834444\n",
      "0.64125\n",
      "Epoch 1710, Loss: 0.0223012912\n",
      "0.64125\n",
      "Epoch 1711, Loss: 0.0225926442\n",
      "0.64125\n",
      "Epoch 1712, Loss: 0.0223206833\n",
      "0.64125\n",
      "Epoch 1713, Loss: 0.0222150469\n",
      "0.64125\n",
      "Epoch 1714, Loss: 0.0219184759\n",
      "0.64125\n",
      "Epoch 1715, Loss: 0.0207895751\n",
      "0.64125\n",
      "Epoch 1716, Loss: 0.0188302586\n",
      "0.65\n",
      "Epoch 1717, Loss: 0.0203263859\n",
      "0.65\n",
      "Epoch 1718, Loss: 0.0195781514\n",
      "0.65\n",
      "Epoch 1719, Loss: 0.0234244442\n",
      "0.65\n",
      "Epoch 1720, Loss: 0.0259061081\n",
      "0.65\n",
      "Epoch 1721, Loss: 0.0281730331\n",
      "0.65\n",
      "Epoch 1722, Loss: 0.0372662733\n",
      "0.65\n",
      "Epoch 1723, Loss: 0.0428716878\n",
      "0.65\n",
      "Epoch 1724, Loss: 0.0453208944\n",
      "0.65\n",
      "Epoch 1725, Loss: 0.0489296715\n",
      "0.65\n",
      "Epoch 1726, Loss: 0.0549858495\n",
      "0.65\n",
      "Epoch 1727, Loss: 0.0558497086\n",
      "0.65\n",
      "Epoch 1728, Loss: 0.0689883598\n",
      "0.65\n",
      "Epoch 1729, Loss: 0.0939305133\n",
      "0.65\n",
      "Epoch 1730, Loss: 0.1165294932\n",
      "0.65\n",
      "Epoch 1731, Loss: 0.1601245418\n",
      "0.65\n",
      "Epoch 1732, Loss: 0.2570888601\n",
      "0.65\n",
      "Epoch 1733, Loss: 0.2350846001\n",
      "0.65\n",
      "Epoch 1734, Loss: 0.1414765046\n",
      "0.65\n",
      "Epoch 1735, Loss: 0.1473667708\n",
      "0.65\n",
      "Epoch 1736, Loss: 0.1952916857\n",
      "0.65\n",
      "Epoch 1737, Loss: 0.2759179322\n",
      "0.65\n",
      "Epoch 1738, Loss: 0.2182179169\n",
      "0.65\n",
      "Epoch 1739, Loss: 0.1567807737\n",
      "0.65\n",
      "Epoch 1740, Loss: 0.2382620044\n",
      "0.65\n",
      "Epoch 1741, Loss: 0.1552436243\n",
      "0.65\n",
      "Epoch 1742, Loss: 0.3019739913\n",
      "0.65\n",
      "Epoch 1743, Loss: 0.1048284398\n",
      "0.65\n",
      "Epoch 1744, Loss: 0.1457385553\n",
      "0.65\n",
      "Epoch 1745, Loss: 0.1610768130\n",
      "0.65\n",
      "Epoch 1746, Loss: 0.1788934508\n",
      "0.65\n",
      "Epoch 1747, Loss: 0.2325675642\n",
      "0.65\n",
      "Epoch 1748, Loss: 0.2332541470\n",
      "0.65\n",
      "Epoch 1749, Loss: 0.1212534652\n",
      "0.65\n",
      "Epoch 1750, Loss: 0.1330715511\n",
      "0.65\n",
      "Epoch 1751, Loss: 0.1782553228\n",
      "0.65\n",
      "Epoch 1752, Loss: 0.2554428798\n",
      "0.65\n",
      "Epoch 1753, Loss: 0.1383441321\n",
      "0.65\n",
      "Epoch 1754, Loss: 0.2464261609\n",
      "0.65\n",
      "Epoch 1755, Loss: 0.4271003114\n",
      "0.65\n",
      "Epoch 1756, Loss: 0.3630232937\n",
      "0.65\n",
      "Epoch 1757, Loss: 0.2342095551\n",
      "0.65\n",
      "Epoch 1758, Loss: 0.2269750408\n",
      "0.65\n",
      "Epoch 1759, Loss: 0.3994908403\n",
      "0.65\n",
      "Epoch 1760, Loss: 0.5118479820\n",
      "0.65\n",
      "Epoch 1761, Loss: 0.3643439638\n",
      "0.65\n",
      "Epoch 1762, Loss: 0.3062706251\n",
      "0.65\n",
      "Epoch 1763, Loss: 0.2453802694\n",
      "0.65\n",
      "Epoch 1764, Loss: 0.3360102453\n",
      "0.65\n",
      "Epoch 1765, Loss: 0.3335053038\n",
      "0.65\n",
      "Epoch 1766, Loss: 0.2286440693\n",
      "0.65\n",
      "Epoch 1767, Loss: 0.4557434784\n",
      "0.65\n",
      "Epoch 1768, Loss: 0.2389228914\n",
      "0.65\n",
      "Epoch 1769, Loss: 0.3346799474\n",
      "0.65\n",
      "Epoch 1770, Loss: 0.2011922456\n",
      "0.65\n",
      "Epoch 1771, Loss: 0.1751704671\n",
      "0.65\n",
      "Epoch 1772, Loss: 0.2152501716\n",
      "0.65\n",
      "Epoch 1773, Loss: 0.1516530174\n",
      "0.65\n",
      "Epoch 1774, Loss: 0.1482060510\n",
      "0.65\n",
      "Epoch 1775, Loss: 0.1283005824\n",
      "0.65\n",
      "Epoch 1776, Loss: 0.0953669120\n",
      "0.65\n",
      "Epoch 1777, Loss: 0.1520420091\n",
      "0.65\n",
      "Epoch 1778, Loss: 0.0805073396\n",
      "0.65\n",
      "Epoch 1779, Loss: 0.1277979971\n",
      "0.65\n",
      "Epoch 1780, Loss: 0.1411863667\n",
      "0.65\n",
      "Epoch 1781, Loss: 0.1823079722\n",
      "0.65\n",
      "Epoch 1782, Loss: 0.1032037814\n",
      "0.65\n",
      "Epoch 1783, Loss: 0.2097007175\n",
      "0.65\n",
      "Epoch 1784, Loss: 0.2107480173\n",
      "0.65\n",
      "Epoch 1785, Loss: 0.2628208842\n",
      "0.65\n",
      "Epoch 1786, Loss: 0.1577161370\n",
      "0.65\n",
      "Epoch 1787, Loss: 0.3555844401\n",
      "0.65\n",
      "Epoch 1788, Loss: 0.4270607181\n",
      "0.65\n",
      "Epoch 1789, Loss: 0.2712412507\n",
      "0.65\n",
      "Epoch 1790, Loss: 0.2453568468\n",
      "0.65\n",
      "Epoch 1791, Loss: 0.3216888100\n",
      "0.65\n",
      "Epoch 1792, Loss: 0.2679448625\n",
      "0.65\n",
      "Epoch 1793, Loss: 0.4677508844\n",
      "0.65\n",
      "Epoch 1794, Loss: 0.6553798444\n",
      "0.65\n",
      "Epoch 1795, Loss: 0.4202327302\n",
      "0.65\n",
      "Epoch 1796, Loss: 0.4162356938\n",
      "0.65\n",
      "Epoch 1797, Loss: 0.3466008061\n",
      "0.65\n",
      "Epoch 1798, Loss: 0.2328582502\n",
      "0.65\n",
      "Epoch 1799, Loss: 0.2426394716\n",
      "0.65\n",
      "Epoch 1800, Loss: 0.1767534758\n",
      "0.65\n",
      "Epoch 1801, Loss: 0.1872241993\n",
      "0.65\n",
      "Epoch 1802, Loss: 0.3599658086\n",
      "0.65\n",
      "Epoch 1803, Loss: 0.1765485082\n",
      "0.65\n",
      "Epoch 1804, Loss: 0.0883580966\n",
      "0.65\n",
      "Epoch 1805, Loss: 0.2120651021\n",
      "0.65\n",
      "Epoch 1806, Loss: 0.1227444839\n",
      "0.65\n",
      "Epoch 1807, Loss: 0.1498475974\n",
      "0.65\n",
      "Epoch 1808, Loss: 0.2449332511\n",
      "0.65\n",
      "Epoch 1809, Loss: 0.1673869150\n",
      "0.65\n",
      "Epoch 1810, Loss: 0.3596381229\n",
      "0.65\n",
      "Epoch 1811, Loss: 0.1484800322\n",
      "0.65\n",
      "Epoch 1812, Loss: 0.2425083164\n",
      "0.65\n",
      "Epoch 1813, Loss: 0.1406611147\n",
      "0.65\n",
      "Epoch 1814, Loss: 0.3403816204\n",
      "0.65\n",
      "Epoch 1815, Loss: 0.2815876326\n",
      "0.65\n",
      "Epoch 1816, Loss: 0.2352831185\n",
      "0.65\n",
      "Epoch 1817, Loss: 0.3245284504\n",
      "0.65\n",
      "Epoch 1818, Loss: 0.2517318716\n",
      "0.65\n",
      "Epoch 1819, Loss: 0.1726743619\n",
      "0.65\n",
      "Epoch 1820, Loss: 0.2375757104\n",
      "0.65\n",
      "Epoch 1821, Loss: 0.1759211276\n",
      "0.65\n",
      "Epoch 1822, Loss: 0.4035273021\n",
      "0.65\n",
      "Epoch 1823, Loss: 0.3682195648\n",
      "0.65\n",
      "Epoch 1824, Loss: 0.3299699074\n",
      "0.65\n",
      "Epoch 1825, Loss: 0.2265811640\n",
      "0.65\n",
      "Epoch 1826, Loss: 0.1836856492\n",
      "0.65\n",
      "Epoch 1827, Loss: 0.2529216196\n",
      "0.65\n",
      "Epoch 1828, Loss: 0.2112771673\n",
      "0.65\n",
      "Epoch 1829, Loss: 0.2648246330\n",
      "0.65\n",
      "Epoch 1830, Loss: 0.2892602259\n",
      "0.65\n",
      "Epoch 1831, Loss: 0.1802863019\n",
      "0.65\n",
      "Epoch 1832, Loss: 0.3703492679\n",
      "0.65\n",
      "Epoch 1833, Loss: 0.5690434246\n",
      "0.65\n",
      "Epoch 1834, Loss: 0.2292157437\n",
      "0.65\n",
      "Epoch 1835, Loss: 0.4478193961\n",
      "0.65\n",
      "Epoch 1836, Loss: 0.3848897007\n",
      "0.65\n",
      "Epoch 1837, Loss: 0.4302993225\n",
      "0.65\n",
      "Epoch 1838, Loss: 0.2155646921\n",
      "0.65\n",
      "Epoch 1839, Loss: 0.2047713509\n",
      "0.65\n",
      "Epoch 1840, Loss: 0.1912454441\n",
      "0.65\n",
      "Epoch 1841, Loss: 0.1434190520\n",
      "0.65\n",
      "Epoch 1842, Loss: 0.1640405353\n",
      "0.65\n",
      "Epoch 1843, Loss: 0.2766551279\n",
      "0.65\n",
      "Epoch 1844, Loss: 0.2112017978\n",
      "0.65\n",
      "Epoch 1845, Loss: 0.3363366609\n",
      "0.65\n",
      "Epoch 1846, Loss: 0.2698144342\n",
      "0.65\n",
      "Epoch 1847, Loss: 0.2709314329\n",
      "0.65\n",
      "Epoch 1848, Loss: 0.1722346064\n",
      "0.65\n",
      "Epoch 1849, Loss: 0.2234824662\n",
      "0.65\n",
      "Epoch 1850, Loss: 0.2117334331\n",
      "0.65\n",
      "Epoch 1851, Loss: 0.2038366451\n",
      "0.65\n",
      "Epoch 1852, Loss: 0.1178913190\n",
      "0.65\n",
      "Epoch 1853, Loss: 0.1555390491\n",
      "0.65\n",
      "Epoch 1854, Loss: 0.3040634769\n",
      "0.65\n",
      "Epoch 1855, Loss: 0.2178499814\n",
      "0.65\n",
      "Epoch 1856, Loss: 0.2366881773\n",
      "0.65\n",
      "Epoch 1857, Loss: 0.1066539978\n",
      "0.65\n",
      "Epoch 1858, Loss: 0.2050437920\n",
      "0.65\n",
      "Epoch 1859, Loss: 0.2472371318\n",
      "0.65\n",
      "Epoch 1860, Loss: 0.2229940116\n",
      "0.65\n",
      "Epoch 1861, Loss: 0.1831662361\n",
      "0.65\n",
      "Epoch 1862, Loss: 0.1460980643\n",
      "0.65\n",
      "Epoch 1863, Loss: 0.1507720837\n",
      "0.65\n",
      "Epoch 1864, Loss: 0.1157750142\n",
      "0.65\n",
      "Epoch 1865, Loss: 0.2644663861\n",
      "0.65\n",
      "Epoch 1866, Loss: 0.2221867247\n",
      "0.65\n",
      "Epoch 1867, Loss: 0.2387687759\n",
      "0.65\n",
      "Epoch 1868, Loss: 0.1990490911\n",
      "0.65\n",
      "Epoch 1869, Loss: 0.2043734531\n",
      "0.65\n",
      "Epoch 1870, Loss: 0.2352353312\n",
      "0.65\n",
      "Epoch 1871, Loss: 0.2472794841\n",
      "0.65\n",
      "Epoch 1872, Loss: 0.1356541261\n",
      "0.65\n",
      "Epoch 1873, Loss: 0.1130011277\n",
      "0.65\n",
      "Epoch 1874, Loss: 0.0769115174\n",
      "0.65\n",
      "Epoch 1875, Loss: 0.1174825583\n",
      "0.65\n",
      "Epoch 1876, Loss: 0.1067464912\n",
      "0.65\n",
      "Epoch 1877, Loss: 0.2514066460\n",
      "0.65\n",
      "Epoch 1878, Loss: 0.2277655546\n",
      "0.65\n",
      "Epoch 1879, Loss: 0.1657713871\n",
      "0.65\n",
      "Epoch 1880, Loss: 0.1879578288\n",
      "0.65\n",
      "Epoch 1881, Loss: 0.1961255831\n",
      "0.65\n",
      "Epoch 1882, Loss: 0.2455321979\n",
      "0.65\n",
      "Epoch 1883, Loss: 0.1638136408\n",
      "0.65\n",
      "Epoch 1884, Loss: 0.3172702948\n",
      "0.65\n",
      "Epoch 1885, Loss: 0.3110914582\n",
      "0.65\n",
      "Epoch 1886, Loss: 0.2268750059\n",
      "0.65\n",
      "Epoch 1887, Loss: 0.2206007445\n",
      "0.65\n",
      "Epoch 1888, Loss: 0.2334770585\n",
      "0.65\n",
      "Epoch 1889, Loss: 0.3518765103\n",
      "0.65\n",
      "Epoch 1890, Loss: 0.4041983927\n",
      "0.65\n",
      "Epoch 1891, Loss: 0.1452571850\n",
      "0.65\n",
      "Epoch 1892, Loss: 0.1517892719\n",
      "0.65\n",
      "Epoch 1893, Loss: 0.3629303638\n",
      "0.65\n",
      "Epoch 1894, Loss: 0.2261586445\n",
      "0.65\n",
      "Epoch 1895, Loss: 0.2429388288\n",
      "0.65\n",
      "Epoch 1896, Loss: 0.3025511505\n",
      "0.65\n",
      "Epoch 1897, Loss: 0.2660401562\n",
      "0.65\n",
      "Epoch 1898, Loss: 0.4617209629\n",
      "0.65\n",
      "Epoch 1899, Loss: 0.4656884719\n",
      "0.65\n",
      "Epoch 1900, Loss: 0.2818354822\n",
      "0.65\n",
      "Epoch 1901, Loss: 0.2056251367\n",
      "0.65\n",
      "Epoch 1902, Loss: 0.2578844539\n",
      "0.65\n",
      "Epoch 1903, Loss: 0.1787088654\n",
      "0.65\n",
      "Epoch 1904, Loss: 0.2478504945\n",
      "0.65\n",
      "Epoch 1905, Loss: 0.3224353253\n",
      "0.65\n",
      "Epoch 1906, Loss: 0.3335367182\n",
      "0.65\n",
      "Epoch 1907, Loss: 0.3308931426\n",
      "0.65\n",
      "Epoch 1908, Loss: 0.1320726491\n",
      "0.65\n",
      "Epoch 1909, Loss: 0.0892134744\n",
      "0.65\n",
      "Epoch 1910, Loss: 0.0809231527\n",
      "0.65\n",
      "Epoch 1911, Loss: 0.1638644235\n",
      "0.65\n",
      "Epoch 1912, Loss: 0.0980954174\n",
      "0.65\n",
      "Epoch 1913, Loss: 0.0855912294\n",
      "0.65\n",
      "Epoch 1914, Loss: 0.0612574111\n",
      "0.65\n",
      "Epoch 1915, Loss: 0.0891491348\n",
      "0.65\n",
      "Epoch 1916, Loss: 0.0524741645\n",
      "0.65\n",
      "Epoch 1917, Loss: 0.0593858134\n",
      "0.65\n",
      "Epoch 1918, Loss: 0.0530553319\n",
      "0.65\n",
      "Epoch 1919, Loss: 0.0440274370\n",
      "0.65\n",
      "Epoch 1920, Loss: 0.0394224643\n",
      "0.65\n",
      "Epoch 1921, Loss: 0.0378994833\n",
      "0.65\n",
      "Epoch 1922, Loss: 0.0487222560\n",
      "0.65\n",
      "Epoch 1923, Loss: 0.0367681130\n",
      "0.65\n",
      "Epoch 1924, Loss: 0.0446702057\n",
      "0.65\n",
      "Epoch 1925, Loss: 0.0428060878\n",
      "0.65\n",
      "Epoch 1926, Loss: 0.0387835099\n",
      "0.65\n",
      "Epoch 1927, Loss: 0.0490348531\n",
      "0.65\n",
      "Epoch 1928, Loss: 0.0486228935\n",
      "0.65\n",
      "Epoch 1929, Loss: 0.0475661737\n",
      "0.65\n",
      "Epoch 1930, Loss: 0.0516148083\n",
      "0.65\n",
      "Epoch 1931, Loss: 0.0524379479\n",
      "0.65\n",
      "Epoch 1932, Loss: 0.0523966153\n",
      "0.65\n",
      "Epoch 1933, Loss: 0.0469076798\n",
      "0.65\n",
      "Epoch 1934, Loss: 0.0286554368\n",
      "0.65\n",
      "Epoch 1935, Loss: 0.0473857304\n",
      "0.65\n",
      "Epoch 1936, Loss: 0.0540374810\n",
      "0.65\n",
      "Epoch 1937, Loss: 0.0676731602\n",
      "0.65\n",
      "Epoch 1938, Loss: 0.0490933926\n",
      "0.65\n",
      "Epoch 1939, Loss: 0.0484083650\n",
      "0.65\n",
      "Epoch 1940, Loss: 0.0811080033\n",
      "0.65\n",
      "Epoch 1941, Loss: 0.0508593359\n",
      "0.65\n",
      "Epoch 1942, Loss: 0.0642464737\n",
      "0.65\n",
      "Epoch 1943, Loss: 0.0708960056\n",
      "0.65\n",
      "Epoch 1944, Loss: 0.0518684564\n",
      "0.65\n",
      "Epoch 1945, Loss: 0.0362821938\n",
      "0.65\n",
      "Epoch 1946, Loss: 0.0518004911\n",
      "0.65\n",
      "Epoch 1947, Loss: 0.0291860997\n",
      "0.65\n",
      "Epoch 1948, Loss: 0.0510829715\n",
      "0.65\n",
      "Epoch 1949, Loss: 0.0676576264\n",
      "0.65\n",
      "Epoch 1950, Loss: 0.0249597750\n",
      "0.65\n",
      "Epoch 1951, Loss: 0.0474117653\n",
      "0.65\n",
      "Epoch 1952, Loss: 0.0790857735\n",
      "0.65\n",
      "Epoch 1953, Loss: 0.0511081172\n",
      "0.65\n",
      "Epoch 1954, Loss: 0.0959110872\n",
      "0.65\n",
      "Epoch 1955, Loss: 0.0596281275\n",
      "0.65\n",
      "Epoch 1956, Loss: 0.1609875672\n",
      "0.65\n",
      "Epoch 1957, Loss: 0.0559910804\n",
      "0.65\n",
      "Epoch 1958, Loss: 0.0854199763\n",
      "0.65\n",
      "Epoch 1959, Loss: 0.1186534671\n",
      "0.65\n",
      "Epoch 1960, Loss: 0.1818719896\n",
      "0.65\n",
      "Epoch 1961, Loss: 0.1671548151\n",
      "0.65\n",
      "Epoch 1962, Loss: 0.1675528111\n",
      "0.65\n",
      "Epoch 1963, Loss: 0.3210001048\n",
      "0.65\n",
      "Epoch 1964, Loss: 0.3353607236\n",
      "0.65\n",
      "Epoch 1965, Loss: 0.1658596854\n",
      "0.65\n",
      "Epoch 1966, Loss: 0.0926433842\n",
      "0.65\n",
      "Epoch 1967, Loss: 0.1948834320\n",
      "0.65\n",
      "Epoch 1968, Loss: 0.1343898933\n",
      "0.65\n",
      "Epoch 1969, Loss: 0.2299410939\n",
      "0.65\n",
      "Epoch 1970, Loss: 0.2019993505\n",
      "0.65\n",
      "Epoch 1971, Loss: 0.1875524305\n",
      "0.65\n",
      "Epoch 1972, Loss: 0.1848089968\n",
      "0.65\n",
      "Epoch 1973, Loss: 0.2944120478\n",
      "0.65\n",
      "Epoch 1974, Loss: 0.2642177579\n",
      "0.65\n",
      "Epoch 1975, Loss: 0.1779762053\n",
      "0.65\n",
      "Epoch 1976, Loss: 0.2864380569\n",
      "0.65\n",
      "Epoch 1977, Loss: 0.2309119533\n",
      "0.65\n",
      "Epoch 1978, Loss: 0.1821224999\n",
      "0.65\n",
      "Epoch 1979, Loss: 0.1970096635\n",
      "0.65\n",
      "Epoch 1980, Loss: 0.1659994289\n",
      "0.65\n",
      "Epoch 1981, Loss: 0.2378636973\n",
      "0.65\n",
      "Epoch 1982, Loss: 0.2963553485\n",
      "0.65\n",
      "Epoch 1983, Loss: 0.3777315593\n",
      "0.65\n",
      "Epoch 1984, Loss: 0.3392641405\n",
      "0.65\n",
      "Epoch 1985, Loss: 0.2118960661\n",
      "0.65\n",
      "Epoch 1986, Loss: 0.2271343783\n",
      "0.65\n",
      "Epoch 1987, Loss: 0.3265638132\n",
      "0.65\n",
      "Epoch 1988, Loss: 0.2636853223\n",
      "0.65\n",
      "Epoch 1989, Loss: 0.1999356688\n",
      "0.65\n",
      "Epoch 1990, Loss: 0.2599180500\n",
      "0.65\n",
      "Epoch 1991, Loss: 0.2747699029\n",
      "0.65\n",
      "Epoch 1992, Loss: 0.1506605933\n",
      "0.65\n",
      "Epoch 1993, Loss: 0.2876948348\n",
      "0.65\n",
      "Epoch 1994, Loss: 0.4011061381\n",
      "0.65\n",
      "Epoch 1995, Loss: 0.2480509711\n",
      "0.65\n",
      "Epoch 1996, Loss: 0.2085869970\n",
      "0.65\n",
      "Epoch 1997, Loss: 0.0803470782\n",
      "0.65\n",
      "Epoch 1998, Loss: 0.1625063594\n",
      "0.65\n",
      "Epoch 1999, Loss: 0.1965318534\n",
      "0.65\n",
      "Epoch 2000, Loss: 0.2212568441\n",
      "0.65\n",
      "Epoch 2001, Loss: 0.3051269624\n",
      "0.65\n",
      "Epoch 2002, Loss: 0.1857504516\n",
      "0.65\n",
      "Epoch 2003, Loss: 0.1794831439\n",
      "0.65\n",
      "Epoch 2004, Loss: 0.2111386913\n",
      "0.65\n",
      "Epoch 2005, Loss: 0.2800167448\n",
      "0.65\n",
      "Epoch 2006, Loss: 0.1389067270\n",
      "0.65\n",
      "Epoch 2007, Loss: 0.2418067717\n",
      "0.65\n",
      "Epoch 2008, Loss: 0.2172295461\n",
      "0.65\n",
      "Epoch 2009, Loss: 0.1994621900\n",
      "0.65\n",
      "Epoch 2010, Loss: 0.2539753731\n",
      "0.65\n",
      "Epoch 2011, Loss: 0.2327276796\n",
      "0.65\n",
      "Epoch 2012, Loss: 0.2190556648\n",
      "0.65\n",
      "Epoch 2013, Loss: 0.2105098928\n",
      "0.65\n",
      "Epoch 2014, Loss: 0.2961813048\n",
      "0.65\n",
      "Epoch 2015, Loss: 0.1301601862\n",
      "0.65\n",
      "Epoch 2016, Loss: 0.0792505139\n",
      "0.65\n",
      "Epoch 2017, Loss: 0.0705800066\n",
      "0.65\n",
      "Epoch 2018, Loss: 0.1035662365\n",
      "0.65\n",
      "Epoch 2019, Loss: 0.1289963705\n",
      "0.65\n",
      "Epoch 2020, Loss: 0.1938117278\n",
      "0.65\n",
      "Epoch 2021, Loss: 0.2719624581\n",
      "0.65\n",
      "Epoch 2022, Loss: 0.2188695459\n",
      "0.65\n",
      "Epoch 2023, Loss: 0.1873456641\n",
      "0.65\n",
      "Epoch 2024, Loss: 0.2250978715\n",
      "0.65\n",
      "Epoch 2025, Loss: 0.1647430953\n",
      "0.65\n",
      "Epoch 2026, Loss: 0.1311945631\n",
      "0.65\n",
      "Epoch 2027, Loss: 0.1591911413\n",
      "0.65\n",
      "Epoch 2028, Loss: 0.1961735080\n",
      "0.65\n",
      "Epoch 2029, Loss: 0.1105852436\n",
      "0.65\n",
      "Epoch 2030, Loss: 0.1455242096\n",
      "0.65\n",
      "Epoch 2031, Loss: 0.2394616064\n",
      "0.65\n",
      "Epoch 2032, Loss: 0.1786993098\n",
      "0.65\n",
      "Epoch 2033, Loss: 0.2153310184\n",
      "0.65\n",
      "Epoch 2034, Loss: 0.0841137720\n",
      "0.65\n",
      "Epoch 2035, Loss: 0.0994312888\n",
      "0.65\n",
      "Epoch 2036, Loss: 0.1179165818\n",
      "0.65\n",
      "Epoch 2037, Loss: 0.1322170951\n",
      "0.65\n",
      "Epoch 2038, Loss: 0.1994634024\n",
      "0.65\n",
      "Epoch 2039, Loss: 0.2116383909\n",
      "0.65\n",
      "Epoch 2040, Loss: 0.1613492771\n",
      "0.65\n",
      "Epoch 2041, Loss: 0.1954952738\n",
      "0.65\n",
      "Epoch 2042, Loss: 0.2047990091\n",
      "0.65\n",
      "Epoch 2043, Loss: 0.2243795822\n",
      "0.65\n",
      "Epoch 2044, Loss: 0.2184975176\n",
      "0.65\n",
      "Epoch 2045, Loss: 0.2074034491\n",
      "0.65\n",
      "Epoch 2046, Loss: 0.1767284625\n",
      "0.65\n",
      "Epoch 2047, Loss: 0.1332928778\n",
      "0.65\n",
      "Epoch 2048, Loss: 0.1782951657\n",
      "0.65\n",
      "Epoch 2049, Loss: 0.1897623214\n",
      "0.65\n",
      "Epoch 2050, Loss: 0.0603720356\n",
      "0.65\n",
      "Epoch 2051, Loss: 0.1058832276\n",
      "0.65\n",
      "Epoch 2052, Loss: 0.0620619232\n",
      "0.65\n",
      "Epoch 2053, Loss: 0.0530985939\n",
      "0.65\n",
      "Epoch 2054, Loss: 0.0282398798\n",
      "0.65\n",
      "Epoch 2055, Loss: 0.0300520912\n",
      "0.65\n",
      "Epoch 2056, Loss: 0.0329799643\n",
      "0.65\n",
      "Epoch 2057, Loss: 0.0417992922\n",
      "0.65\n",
      "Epoch 2058, Loss: 0.0464827897\n",
      "0.65\n",
      "Epoch 2059, Loss: 0.0899398567\n",
      "0.65\n",
      "Epoch 2060, Loss: 0.0650708578\n",
      "0.65\n",
      "Epoch 2061, Loss: 0.0900164430\n",
      "0.65\n",
      "Epoch 2062, Loss: 0.0710361506\n",
      "0.65\n",
      "Epoch 2063, Loss: 0.0156247302\n",
      "0.6625\n",
      "Epoch 2064, Loss: 0.0230803088\n",
      "0.6625\n",
      "Epoch 2065, Loss: 0.0218614545\n",
      "0.6625\n",
      "Epoch 2066, Loss: 0.0407293547\n",
      "0.6625\n",
      "Epoch 2067, Loss: 0.0356793344\n",
      "0.6625\n",
      "Epoch 2068, Loss: 0.0208672573\n",
      "0.6625\n",
      "Epoch 2069, Loss: 0.0335807770\n",
      "0.6625\n",
      "Epoch 2070, Loss: 0.0812000130\n",
      "0.6625\n",
      "Epoch 2071, Loss: 0.0118555439\n",
      "0.66\n",
      "Epoch 2072, Loss: 0.0228572471\n",
      "0.66\n",
      "Epoch 2073, Loss: 0.0534528024\n",
      "0.66\n",
      "Epoch 2074, Loss: 0.0123957708\n",
      "0.66\n",
      "Epoch 2075, Loss: 0.0179935974\n",
      "0.66\n",
      "Epoch 2076, Loss: 0.0112386173\n",
      "0.66375\n",
      "Epoch 2077, Loss: 0.0100201497\n",
      "0.67\n",
      "Epoch 2078, Loss: 0.0208621224\n",
      "0.67\n",
      "Epoch 2079, Loss: 0.0431842428\n",
      "0.67\n",
      "Epoch 2080, Loss: 0.0356099794\n",
      "0.67\n",
      "Epoch 2081, Loss: 0.0654831373\n",
      "0.67\n",
      "Epoch 2082, Loss: 0.0512726439\n",
      "0.67\n",
      "Epoch 2083, Loss: 0.0420125263\n",
      "0.67\n",
      "Epoch 2084, Loss: 0.0741833240\n",
      "0.67\n",
      "Epoch 2085, Loss: 0.0707247291\n",
      "0.67\n",
      "Epoch 2086, Loss: 0.0306629375\n",
      "0.67\n",
      "Epoch 2087, Loss: 0.1020607034\n",
      "0.67\n",
      "Epoch 2088, Loss: 0.0370251376\n",
      "0.67\n",
      "Epoch 2089, Loss: 0.1309775592\n",
      "0.67\n",
      "Epoch 2090, Loss: 0.0315374282\n",
      "0.67\n",
      "Epoch 2091, Loss: 0.1347088494\n",
      "0.67\n",
      "Epoch 2092, Loss: 0.0257348446\n",
      "0.67\n",
      "Epoch 2093, Loss: 0.1409392901\n",
      "0.67\n",
      "Epoch 2094, Loss: 0.0660172577\n",
      "0.67\n",
      "Epoch 2095, Loss: 0.1307815560\n",
      "0.67\n",
      "Epoch 2096, Loss: 0.0595738864\n",
      "0.67\n",
      "Epoch 2097, Loss: 0.0376593528\n",
      "0.67\n",
      "Epoch 2098, Loss: 0.1166800043\n",
      "0.67\n",
      "Epoch 2099, Loss: 0.0887039533\n",
      "0.67\n",
      "Epoch 2100, Loss: 0.0455737992\n",
      "0.67\n",
      "Epoch 2101, Loss: 0.0799064663\n",
      "0.67\n",
      "Epoch 2102, Loss: 0.1800953596\n",
      "0.67\n",
      "Epoch 2103, Loss: 0.0812750720\n",
      "0.67\n",
      "Epoch 2104, Loss: 0.1893301941\n",
      "0.67\n",
      "Epoch 2105, Loss: 0.1395893133\n",
      "0.67\n",
      "Epoch 2106, Loss: 0.1836430868\n",
      "0.67\n",
      "Epoch 2107, Loss: 0.1650807352\n",
      "0.67\n",
      "Epoch 2108, Loss: 0.1616100102\n",
      "0.67\n",
      "Epoch 2109, Loss: 0.1675594220\n",
      "0.67\n",
      "Epoch 2110, Loss: 0.1334332288\n",
      "0.67\n",
      "Epoch 2111, Loss: 0.1527109846\n",
      "0.67\n",
      "Epoch 2112, Loss: 0.2672699294\n",
      "0.67\n",
      "Epoch 2113, Loss: 0.5192988364\n",
      "0.67\n",
      "Epoch 2114, Loss: 0.3301848852\n",
      "0.67\n",
      "Epoch 2115, Loss: 0.1461568919\n",
      "0.67\n",
      "Epoch 2116, Loss: 0.2057970351\n",
      "0.67\n",
      "Epoch 2117, Loss: 0.1073828018\n",
      "0.67\n",
      "Epoch 2118, Loss: 0.0757472235\n",
      "0.67\n",
      "Epoch 2119, Loss: 0.1517960766\n",
      "0.67\n",
      "Epoch 2120, Loss: 0.0666295733\n",
      "0.67\n",
      "Epoch 2121, Loss: 0.1275177845\n",
      "0.67\n",
      "Epoch 2122, Loss: 0.0431454457\n",
      "0.67\n",
      "Epoch 2123, Loss: 0.0375007179\n",
      "0.67\n",
      "Epoch 2124, Loss: 0.0709125308\n",
      "0.67\n",
      "Epoch 2125, Loss: 0.0454159798\n",
      "0.67\n",
      "Epoch 2126, Loss: 0.1360099146\n",
      "0.67\n",
      "Epoch 2127, Loss: 0.0742806395\n",
      "0.67\n",
      "Epoch 2128, Loss: 0.1311474691\n",
      "0.67\n",
      "Epoch 2129, Loss: 0.1304226731\n",
      "0.67\n",
      "Epoch 2130, Loss: 0.1352341504\n",
      "0.67\n",
      "Epoch 2131, Loss: 0.0942002198\n",
      "0.67\n",
      "Epoch 2132, Loss: 0.0822946362\n",
      "0.67\n",
      "Epoch 2133, Loss: 0.0512596346\n",
      "0.67\n",
      "Epoch 2134, Loss: 0.0363321708\n",
      "0.67\n",
      "Epoch 2135, Loss: 0.0504712551\n",
      "0.67\n",
      "Epoch 2136, Loss: 0.0518903897\n",
      "0.67\n",
      "Epoch 2137, Loss: 0.1033088222\n",
      "0.67\n",
      "Epoch 2138, Loss: 0.0285137026\n",
      "0.67\n",
      "Epoch 2139, Loss: 0.0972801003\n",
      "0.67\n",
      "Epoch 2140, Loss: 0.0719068628\n",
      "0.67\n",
      "Epoch 2141, Loss: 0.1129076421\n",
      "0.67\n",
      "Epoch 2142, Loss: 0.1171601227\n",
      "0.67\n",
      "Epoch 2143, Loss: 0.1131829860\n",
      "0.67\n",
      "Epoch 2144, Loss: 0.1133902974\n",
      "0.67\n",
      "Epoch 2145, Loss: 0.0664275942\n",
      "0.67\n",
      "Epoch 2146, Loss: 0.2882015451\n",
      "0.67\n",
      "Epoch 2147, Loss: 0.2277554282\n",
      "0.67\n",
      "Epoch 2148, Loss: 0.0823330995\n",
      "0.67\n",
      "Epoch 2149, Loss: 0.0729600093\n",
      "0.67\n",
      "Epoch 2150, Loss: 0.0995742464\n",
      "0.67\n",
      "Epoch 2151, Loss: 0.1009585858\n",
      "0.67\n",
      "Epoch 2152, Loss: 0.1738069646\n",
      "0.67\n",
      "Epoch 2153, Loss: 0.1632658271\n",
      "0.67\n",
      "Epoch 2154, Loss: 0.1409949190\n",
      "0.67\n",
      "Epoch 2155, Loss: 0.1537642671\n",
      "0.67\n",
      "Epoch 2156, Loss: 0.1439847364\n",
      "0.67\n",
      "Epoch 2157, Loss: 0.1399674771\n",
      "0.67\n",
      "Epoch 2158, Loss: 0.1846383298\n",
      "0.67\n",
      "Epoch 2159, Loss: 0.0952719570\n",
      "0.67\n",
      "Epoch 2160, Loss: 0.0811624921\n",
      "0.67\n",
      "Epoch 2161, Loss: 0.0327732402\n",
      "0.67\n",
      "Epoch 2162, Loss: 0.0359376104\n",
      "0.67\n",
      "Epoch 2163, Loss: 0.0835072283\n",
      "0.67\n",
      "Epoch 2164, Loss: 0.2425665159\n",
      "0.67\n",
      "Epoch 2165, Loss: 0.0933323749\n",
      "0.67\n",
      "Epoch 2166, Loss: 0.1715202292\n",
      "0.67\n",
      "Epoch 2167, Loss: 0.1733769204\n",
      "0.67\n",
      "Epoch 2168, Loss: 0.1604671190\n",
      "0.67\n",
      "Epoch 2169, Loss: 0.1776695344\n",
      "0.67\n",
      "Epoch 2170, Loss: 0.1565062597\n",
      "0.67\n",
      "Epoch 2171, Loss: 0.1168427640\n",
      "0.67\n",
      "Epoch 2172, Loss: 0.1855025715\n",
      "0.67\n",
      "Epoch 2173, Loss: 0.1406965047\n",
      "0.67\n",
      "Epoch 2174, Loss: 0.1068928822\n",
      "0.67\n",
      "Epoch 2175, Loss: 0.0609280307\n",
      "0.67\n",
      "Epoch 2176, Loss: 0.0453275259\n",
      "0.67\n",
      "Epoch 2177, Loss: 0.0864621495\n",
      "0.67\n",
      "Epoch 2178, Loss: 0.0870329737\n",
      "0.67\n",
      "Epoch 2179, Loss: 0.0610482524\n",
      "0.67\n",
      "Epoch 2180, Loss: 0.0703155205\n",
      "0.67\n",
      "Epoch 2181, Loss: 0.0471119981\n",
      "0.67\n",
      "Epoch 2182, Loss: 0.0482092532\n",
      "0.67\n",
      "Epoch 2183, Loss: 0.0199792297\n",
      "0.67\n",
      "Epoch 2184, Loss: 0.0158378003\n",
      "0.67\n",
      "Epoch 2185, Loss: 0.0257444135\n",
      "0.67\n",
      "Epoch 2186, Loss: 0.0353880232\n",
      "0.67\n",
      "Epoch 2187, Loss: 0.0240972268\n",
      "0.67\n",
      "Epoch 2188, Loss: 0.0314220049\n",
      "0.67\n",
      "Epoch 2189, Loss: 0.1157741219\n",
      "0.67\n",
      "Epoch 2190, Loss: 0.0186795694\n",
      "0.67\n",
      "Epoch 2191, Loss: 0.0274200403\n",
      "0.67\n",
      "Epoch 2192, Loss: 0.0829620736\n",
      "0.67\n",
      "Epoch 2193, Loss: 0.0178269786\n",
      "0.67\n",
      "Epoch 2194, Loss: 0.0170684749\n",
      "0.67\n",
      "Epoch 2195, Loss: 0.0906344052\n",
      "0.67\n",
      "Epoch 2196, Loss: 0.0239799658\n",
      "0.67\n",
      "Epoch 2197, Loss: 0.0535518766\n",
      "0.67\n",
      "Epoch 2198, Loss: 0.0319935215\n",
      "0.67\n",
      "Epoch 2199, Loss: 0.1215184757\n",
      "0.67\n",
      "Epoch 2200, Loss: 0.0806183548\n",
      "0.67\n",
      "Epoch 2201, Loss: 0.0135722945\n",
      "0.67\n",
      "Epoch 2202, Loss: 0.0854520779\n",
      "0.67\n",
      "Epoch 2203, Loss: 0.0188479026\n",
      "0.67\n",
      "Epoch 2204, Loss: 0.0213599052\n",
      "0.67\n",
      "Epoch 2205, Loss: 0.0852669283\n",
      "0.67\n",
      "Epoch 2206, Loss: 0.0247838507\n",
      "0.67\n",
      "Epoch 2207, Loss: 0.0576899587\n",
      "0.67\n",
      "Epoch 2208, Loss: 0.0915858980\n",
      "0.67\n",
      "Epoch 2209, Loss: 0.0629891046\n",
      "0.67\n",
      "Epoch 2210, Loss: 0.0314556698\n",
      "0.67\n",
      "Epoch 2211, Loss: 0.0398312802\n",
      "0.67\n",
      "Epoch 2212, Loss: 0.1383320264\n",
      "0.67\n",
      "Epoch 2213, Loss: 0.0633077336\n",
      "0.67\n",
      "Epoch 2214, Loss: 0.0931211856\n",
      "0.67\n",
      "Epoch 2215, Loss: 0.1214029573\n",
      "0.67\n",
      "Epoch 2216, Loss: 0.0444825803\n",
      "0.67\n",
      "Epoch 2217, Loss: 0.1019004584\n",
      "0.67\n",
      "Epoch 2218, Loss: 0.1672091090\n",
      "0.67\n",
      "Epoch 2219, Loss: 0.1070924665\n",
      "0.67\n",
      "Epoch 2220, Loss: 0.2110291706\n",
      "0.67\n",
      "Epoch 2221, Loss: 0.0869279515\n",
      "0.67\n",
      "Epoch 2222, Loss: 0.1007714137\n",
      "0.67\n",
      "Epoch 2223, Loss: 0.0659614976\n",
      "0.67\n",
      "Epoch 2224, Loss: 0.1481217532\n",
      "0.67\n",
      "Epoch 2225, Loss: 0.2448172838\n",
      "0.67\n",
      "Epoch 2226, Loss: 0.2596446663\n",
      "0.67\n",
      "Epoch 2227, Loss: 0.0663950887\n",
      "0.67\n",
      "Epoch 2228, Loss: 0.0377473783\n",
      "0.67\n",
      "Epoch 2229, Loss: 0.0895006986\n",
      "0.67\n",
      "Epoch 2230, Loss: 0.1404873036\n",
      "0.67\n",
      "Epoch 2231, Loss: 0.0892662040\n",
      "0.67\n",
      "Epoch 2232, Loss: 0.0463274323\n",
      "0.67\n",
      "Epoch 2233, Loss: 0.1212287654\n",
      "0.67\n",
      "Epoch 2234, Loss: 0.0857331880\n",
      "0.67\n",
      "Epoch 2235, Loss: 0.1078408992\n",
      "0.67\n",
      "Epoch 2236, Loss: 0.0635100326\n",
      "0.67\n",
      "Epoch 2237, Loss: 0.2590589615\n",
      "0.67\n",
      "Epoch 2238, Loss: 0.0733254539\n",
      "0.67\n",
      "Epoch 2239, Loss: 0.0608321655\n",
      "0.67\n",
      "Epoch 2240, Loss: 0.1156136554\n",
      "0.67\n",
      "Epoch 2241, Loss: 0.0825398722\n",
      "0.67\n",
      "Epoch 2242, Loss: 0.1753051868\n",
      "0.67\n",
      "Epoch 2243, Loss: 0.1362692059\n",
      "0.67\n",
      "Epoch 2244, Loss: 0.0572761026\n",
      "0.67\n",
      "Epoch 2245, Loss: 0.1307634819\n",
      "0.67\n",
      "Epoch 2246, Loss: 0.3348680998\n",
      "0.67\n",
      "Epoch 2247, Loss: 0.2197345322\n",
      "0.67\n",
      "Epoch 2248, Loss: 0.3063385685\n",
      "0.67\n",
      "Epoch 2249, Loss: 0.1171833946\n",
      "0.67\n",
      "Epoch 2250, Loss: 0.0840695017\n",
      "0.67\n",
      "Epoch 2251, Loss: 0.0582809850\n",
      "0.67\n",
      "Epoch 2252, Loss: 0.1795349457\n",
      "0.67\n",
      "Epoch 2253, Loss: 0.1782928348\n",
      "0.67\n",
      "Epoch 2254, Loss: 0.1392859957\n",
      "0.67\n",
      "Epoch 2255, Loss: 0.1036588213\n",
      "0.67\n",
      "Epoch 2256, Loss: 0.1744929459\n",
      "0.67\n",
      "Epoch 2257, Loss: 0.2596778707\n",
      "0.67\n",
      "Epoch 2258, Loss: 0.3596398623\n",
      "0.67\n",
      "Epoch 2259, Loss: 0.1350013900\n",
      "0.67\n",
      "Epoch 2260, Loss: 0.1484341006\n",
      "0.67\n",
      "Epoch 2261, Loss: 0.1120946146\n",
      "0.67\n",
      "Epoch 2262, Loss: 0.1233368912\n",
      "0.67\n",
      "Epoch 2263, Loss: 0.0740531316\n",
      "0.67\n",
      "Epoch 2264, Loss: 0.1776437698\n",
      "0.67\n",
      "Epoch 2265, Loss: 0.1825925492\n",
      "0.67\n",
      "Epoch 2266, Loss: 0.1375992164\n",
      "0.67\n",
      "Epoch 2267, Loss: 0.0890195815\n",
      "0.67\n",
      "Epoch 2268, Loss: 0.0864348014\n",
      "0.67\n",
      "Epoch 2269, Loss: 0.2433731614\n",
      "0.67\n",
      "Epoch 2270, Loss: 0.2017109796\n",
      "0.67\n",
      "Epoch 2271, Loss: 0.2541394229\n",
      "0.67\n",
      "Epoch 2272, Loss: 0.1185607375\n",
      "0.67\n",
      "Epoch 2273, Loss: 0.0742127371\n",
      "0.67\n",
      "Epoch 2274, Loss: 0.1150621419\n",
      "0.67\n",
      "Epoch 2275, Loss: 0.0786706304\n",
      "0.67\n",
      "Epoch 2276, Loss: 0.1097721349\n",
      "0.67\n",
      "Epoch 2277, Loss: 0.1712862829\n",
      "0.67\n",
      "Epoch 2278, Loss: 0.1764017722\n",
      "0.67\n",
      "Epoch 2279, Loss: 0.1586338474\n",
      "0.67\n",
      "Epoch 2280, Loss: 0.2091479981\n",
      "0.67\n",
      "Epoch 2281, Loss: 0.2208866151\n",
      "0.67\n",
      "Epoch 2282, Loss: 0.2344882217\n",
      "0.67\n",
      "Epoch 2283, Loss: 0.2977944000\n",
      "0.67\n",
      "Epoch 2284, Loss: 0.2348776446\n",
      "0.67\n",
      "Epoch 2285, Loss: 0.3338196741\n",
      "0.67\n",
      "Epoch 2286, Loss: 0.4088776005\n",
      "0.67\n",
      "Epoch 2287, Loss: 0.0853522780\n",
      "0.67\n",
      "Epoch 2288, Loss: 0.1361266707\n",
      "0.67\n",
      "Epoch 2289, Loss: 0.0560213730\n",
      "0.67\n",
      "Epoch 2290, Loss: 0.0444004270\n",
      "0.67\n",
      "Epoch 2291, Loss: 0.1533431259\n",
      "0.67\n",
      "Epoch 2292, Loss: 0.0906335581\n",
      "0.67\n",
      "Epoch 2293, Loss: 0.0807671727\n",
      "0.67\n",
      "Epoch 2294, Loss: 0.1673568691\n",
      "0.67\n",
      "Epoch 2295, Loss: 0.0867273249\n",
      "0.67\n",
      "Epoch 2296, Loss: 0.3105717192\n",
      "0.67\n",
      "Epoch 2297, Loss: 0.0700860220\n",
      "0.67\n",
      "Epoch 2298, Loss: 0.2097957190\n",
      "0.67\n",
      "Epoch 2299, Loss: 0.0957444587\n",
      "0.67\n",
      "Epoch 2300, Loss: 0.0829480308\n",
      "0.67\n",
      "Epoch 2301, Loss: 0.1560084900\n",
      "0.67\n",
      "Epoch 2302, Loss: 0.1000595463\n",
      "0.67\n",
      "Epoch 2303, Loss: 0.1243412224\n",
      "0.67\n",
      "Epoch 2304, Loss: 0.1132420432\n",
      "0.67\n",
      "Epoch 2305, Loss: 0.1996815347\n",
      "0.67\n",
      "Epoch 2306, Loss: 0.1411012236\n",
      "0.67\n",
      "Epoch 2307, Loss: 0.1359295025\n",
      "0.67\n",
      "Epoch 2308, Loss: 0.0780639386\n",
      "0.67\n",
      "Epoch 2309, Loss: 0.0428456490\n",
      "0.67\n",
      "Epoch 2310, Loss: 0.0316830404\n",
      "0.67\n",
      "Epoch 2311, Loss: 0.0475791350\n",
      "0.67\n",
      "Epoch 2312, Loss: 0.0703642555\n",
      "0.67\n",
      "Epoch 2313, Loss: 0.0280870199\n",
      "0.67\n",
      "Epoch 2314, Loss: 0.0304710842\n",
      "0.67\n",
      "Epoch 2315, Loss: 0.0533937912\n",
      "0.67\n",
      "Epoch 2316, Loss: 0.0769775904\n",
      "0.67\n",
      "Epoch 2317, Loss: 0.1071596640\n",
      "0.67\n",
      "Epoch 2318, Loss: 0.1172659782\n",
      "0.67\n",
      "Epoch 2319, Loss: 0.1231355083\n",
      "0.67\n",
      "Epoch 2320, Loss: 0.0220838084\n",
      "0.67\n",
      "Epoch 2321, Loss: 0.0871957237\n",
      "0.67\n",
      "Epoch 2322, Loss: 0.1036945699\n",
      "0.67\n",
      "Epoch 2323, Loss: 0.1082611108\n",
      "0.67\n",
      "Epoch 2324, Loss: 0.0960282435\n",
      "0.67\n",
      "Epoch 2325, Loss: 0.1196909201\n",
      "0.67\n",
      "Epoch 2326, Loss: 0.0153771828\n",
      "0.67\n",
      "Epoch 2327, Loss: 0.0624258507\n",
      "0.67\n",
      "Epoch 2328, Loss: 0.2446745671\n",
      "0.67\n",
      "Epoch 2329, Loss: 0.0340126929\n",
      "0.67\n",
      "Epoch 2330, Loss: 0.0948778073\n",
      "0.67\n",
      "Epoch 2331, Loss: 0.0367237649\n",
      "0.67\n",
      "Epoch 2332, Loss: 0.1353499554\n",
      "0.67\n",
      "Epoch 2333, Loss: 0.0366617323\n",
      "0.67\n",
      "Epoch 2334, Loss: 0.0943270293\n",
      "0.67\n",
      "Epoch 2335, Loss: 0.0568908387\n",
      "0.67\n",
      "Epoch 2336, Loss: 0.1327558628\n",
      "0.67\n",
      "Epoch 2337, Loss: 0.0660460717\n",
      "0.67\n",
      "Epoch 2338, Loss: 0.0815728697\n",
      "0.67\n",
      "Epoch 2339, Loss: 0.0771045660\n",
      "0.67\n",
      "Epoch 2340, Loss: 0.1121193825\n",
      "0.67\n",
      "Epoch 2341, Loss: 0.1610304966\n",
      "0.67\n",
      "Epoch 2342, Loss: 0.0257800695\n",
      "0.67\n",
      "Epoch 2343, Loss: 0.2034701345\n",
      "0.67\n",
      "Epoch 2344, Loss: 0.0578240089\n",
      "0.67\n",
      "Epoch 2345, Loss: 0.0574821189\n",
      "0.67\n",
      "Epoch 2346, Loss: 0.0308972465\n",
      "0.67\n",
      "Epoch 2347, Loss: 0.0967008222\n",
      "0.67\n",
      "Epoch 2348, Loss: 0.1625406521\n",
      "0.67\n",
      "Epoch 2349, Loss: 0.0959031611\n",
      "0.67\n",
      "Epoch 2350, Loss: 0.0522745561\n",
      "0.67\n",
      "Epoch 2351, Loss: 0.2217477728\n",
      "0.67\n",
      "Epoch 2352, Loss: 0.0519857104\n",
      "0.67\n",
      "Epoch 2353, Loss: 0.2247348831\n",
      "0.67\n",
      "Epoch 2354, Loss: 0.0715762705\n",
      "0.67\n",
      "Epoch 2355, Loss: 0.0657572669\n",
      "0.67\n",
      "Epoch 2356, Loss: 0.0895010382\n",
      "0.67\n",
      "Epoch 2357, Loss: 0.0386201129\n",
      "0.67\n",
      "Epoch 2358, Loss: 0.2870892715\n",
      "0.67\n",
      "Epoch 2359, Loss: 0.0571430514\n",
      "0.67\n",
      "Epoch 2360, Loss: 0.1822712475\n",
      "0.67\n",
      "Epoch 2361, Loss: 0.2011495806\n",
      "0.67\n",
      "Epoch 2362, Loss: 0.1241541207\n",
      "0.67\n",
      "Epoch 2363, Loss: 0.0636149038\n",
      "0.67\n",
      "Epoch 2364, Loss: 0.0371832343\n",
      "0.67\n",
      "Epoch 2365, Loss: 0.1582934878\n",
      "0.67\n",
      "Epoch 2366, Loss: 0.0229934855\n",
      "0.67\n",
      "Epoch 2367, Loss: 0.0318408510\n",
      "0.67\n",
      "Epoch 2368, Loss: 0.0795055148\n",
      "0.67\n",
      "Epoch 2369, Loss: 0.1672495906\n",
      "0.67\n",
      "Epoch 2370, Loss: 0.0743733550\n",
      "0.67\n",
      "Epoch 2371, Loss: 0.3798234685\n",
      "0.67\n",
      "Epoch 2372, Loss: 0.1594271940\n",
      "0.67\n",
      "Epoch 2373, Loss: 0.1905795954\n",
      "0.67\n",
      "Epoch 2374, Loss: 0.0394399078\n",
      "0.67\n",
      "Epoch 2375, Loss: 0.0528849014\n",
      "0.67\n",
      "Epoch 2376, Loss: 0.0364443700\n",
      "0.67\n",
      "Epoch 2377, Loss: 0.0767286746\n",
      "0.67\n",
      "Epoch 2378, Loss: 0.2252845739\n",
      "0.67\n",
      "Epoch 2379, Loss: 0.0639777572\n",
      "0.67\n",
      "Epoch 2380, Loss: 0.1354479120\n",
      "0.67\n",
      "Epoch 2381, Loss: 0.0605275515\n",
      "0.67\n",
      "Epoch 2382, Loss: 0.0778441208\n",
      "0.67\n",
      "Epoch 2383, Loss: 0.2460780655\n",
      "0.67\n",
      "Epoch 2384, Loss: 0.1443867879\n",
      "0.67\n",
      "Epoch 2385, Loss: 0.1372622523\n",
      "0.67\n",
      "Epoch 2386, Loss: 0.1279065841\n",
      "0.67\n",
      "Epoch 2387, Loss: 0.1049235020\n",
      "0.67\n",
      "Epoch 2388, Loss: 0.1109489108\n",
      "0.67\n",
      "Epoch 2389, Loss: 0.1214296253\n",
      "0.67\n",
      "Epoch 2390, Loss: 0.0620163145\n",
      "0.67\n",
      "Epoch 2391, Loss: 0.1544615165\n",
      "0.67\n",
      "Epoch 2392, Loss: 0.1084780445\n",
      "0.67\n",
      "Epoch 2393, Loss: 0.1596564601\n",
      "0.67\n",
      "Epoch 2394, Loss: 0.1121757951\n",
      "0.67\n",
      "Epoch 2395, Loss: 0.3502626933\n",
      "0.67\n",
      "Epoch 2396, Loss: 0.1332729365\n",
      "0.67\n",
      "Epoch 2397, Loss: 0.2895910435\n",
      "0.67\n",
      "Epoch 2398, Loss: 0.1105255128\n",
      "0.67\n",
      "Epoch 2399, Loss: 0.2107812080\n",
      "0.67\n",
      "Epoch 2400, Loss: 0.1106324862\n",
      "0.67\n",
      "Epoch 2401, Loss: 0.2276064328\n",
      "0.67\n",
      "Epoch 2402, Loss: 0.1273736761\n",
      "0.67\n",
      "Epoch 2403, Loss: 0.0679259176\n",
      "0.67\n",
      "Epoch 2404, Loss: 0.3191824251\n",
      "0.67\n",
      "Epoch 2405, Loss: 0.2751067897\n",
      "0.67\n",
      "Epoch 2406, Loss: 0.2802605075\n",
      "0.67\n",
      "Epoch 2407, Loss: 0.0672204874\n",
      "0.67\n",
      "Epoch 2408, Loss: 0.3805206711\n",
      "0.67\n",
      "Epoch 2409, Loss: 0.2097186294\n",
      "0.67\n",
      "Epoch 2410, Loss: 0.1921785690\n",
      "0.67\n",
      "Epoch 2411, Loss: 0.0613585171\n",
      "0.67\n",
      "Epoch 2412, Loss: 0.2050581433\n",
      "0.67\n",
      "Epoch 2413, Loss: 0.0250619588\n",
      "0.67\n",
      "Epoch 2414, Loss: 0.1555653218\n",
      "0.67\n",
      "Epoch 2415, Loss: 0.0395423027\n",
      "0.67\n",
      "Epoch 2416, Loss: 0.0864560786\n",
      "0.67\n",
      "Epoch 2417, Loss: 0.0521812983\n",
      "0.67\n",
      "Epoch 2418, Loss: 0.3420093995\n",
      "0.67\n",
      "Epoch 2419, Loss: 0.1094857447\n",
      "0.67\n",
      "Epoch 2420, Loss: 0.1957089933\n",
      "0.67\n",
      "Epoch 2421, Loss: 0.1787571688\n",
      "0.67\n",
      "Epoch 2422, Loss: 0.1725301840\n",
      "0.67\n",
      "Epoch 2423, Loss: 0.0605521213\n",
      "0.67\n",
      "Epoch 2424, Loss: 0.1158737114\n",
      "0.67\n",
      "Epoch 2425, Loss: 0.0388743448\n",
      "0.67\n",
      "Epoch 2426, Loss: 0.0822478176\n",
      "0.67\n",
      "Epoch 2427, Loss: 0.3936152881\n",
      "0.67\n",
      "Epoch 2428, Loss: 0.1736007364\n",
      "0.67\n",
      "Epoch 2429, Loss: 0.1778409250\n",
      "0.67\n",
      "Epoch 2430, Loss: 0.1366978666\n",
      "0.67\n",
      "Epoch 2431, Loss: 0.4556264296\n",
      "0.67\n",
      "Epoch 2432, Loss: 0.1302652812\n",
      "0.67\n",
      "Epoch 2433, Loss: 0.0877400889\n",
      "0.67\n",
      "Epoch 2434, Loss: 0.1251401569\n",
      "0.67\n",
      "Epoch 2435, Loss: 0.1322959330\n",
      "0.67\n",
      "Epoch 2436, Loss: 0.1723377796\n",
      "0.67\n",
      "Epoch 2437, Loss: 0.1550279088\n",
      "0.67\n",
      "Epoch 2438, Loss: 0.1188979737\n",
      "0.67\n",
      "Epoch 2439, Loss: 0.1101528708\n",
      "0.67\n",
      "Epoch 2440, Loss: 0.0620576378\n",
      "0.67\n",
      "Epoch 2441, Loss: 0.1180848192\n",
      "0.67\n",
      "Epoch 2442, Loss: 0.1626139017\n",
      "0.67\n",
      "Epoch 2443, Loss: 0.2009074489\n",
      "0.67\n",
      "Epoch 2444, Loss: 0.1286904397\n",
      "0.67\n",
      "Epoch 2445, Loss: 0.1673590680\n",
      "0.67\n",
      "Epoch 2446, Loss: 0.2477085259\n",
      "0.67\n",
      "Epoch 2447, Loss: 0.2633145540\n",
      "0.67\n",
      "Epoch 2448, Loss: 0.0459395229\n",
      "0.67\n",
      "Epoch 2449, Loss: 0.0482938958\n",
      "0.67\n",
      "Epoch 2450, Loss: 0.1924162558\n",
      "0.67\n",
      "Epoch 2451, Loss: 0.0740976293\n",
      "0.67\n",
      "Epoch 2452, Loss: 0.1092762893\n",
      "0.67\n",
      "Epoch 2453, Loss: 0.0906596800\n",
      "0.67\n",
      "Epoch 2454, Loss: 0.1487798198\n",
      "0.67\n",
      "Epoch 2455, Loss: 0.0648350884\n",
      "0.67\n",
      "Epoch 2456, Loss: 0.2366534464\n",
      "0.67\n",
      "Epoch 2457, Loss: 0.0555231814\n",
      "0.67\n",
      "Epoch 2458, Loss: 0.1694769782\n",
      "0.67\n",
      "Epoch 2459, Loss: 0.1680303513\n",
      "0.67\n",
      "Epoch 2460, Loss: 0.1478188154\n",
      "0.67\n",
      "Epoch 2461, Loss: 0.3003869087\n",
      "0.67\n",
      "Epoch 2462, Loss: 0.2880288646\n",
      "0.67\n",
      "Epoch 2463, Loss: 0.1803087648\n",
      "0.67\n",
      "Epoch 2464, Loss: 0.3333967802\n",
      "0.67\n",
      "Epoch 2465, Loss: 0.1361180991\n",
      "0.67\n",
      "Epoch 2466, Loss: 0.1537340367\n",
      "0.67\n",
      "Epoch 2467, Loss: 0.0947713797\n",
      "0.67\n",
      "Epoch 2468, Loss: 0.3138248625\n",
      "0.67\n",
      "Epoch 2469, Loss: 0.0930682709\n",
      "0.67\n",
      "Epoch 2470, Loss: 0.0484579889\n",
      "0.67\n",
      "Epoch 2471, Loss: 0.3231829118\n",
      "0.67\n",
      "Epoch 2472, Loss: 0.1835592876\n",
      "0.67\n",
      "Epoch 2473, Loss: 0.2152600482\n",
      "0.67\n",
      "Epoch 2474, Loss: 0.0930368296\n",
      "0.67\n",
      "Epoch 2475, Loss: 0.1013482410\n",
      "0.67\n",
      "Epoch 2476, Loss: 0.2504653796\n",
      "0.67\n",
      "Epoch 2477, Loss: 0.1471970253\n",
      "0.67\n",
      "Epoch 2478, Loss: 0.1464263632\n",
      "0.67\n",
      "Epoch 2479, Loss: 0.2380857203\n",
      "0.67\n",
      "Epoch 2480, Loss: 0.1356511655\n",
      "0.67\n",
      "Epoch 2481, Loss: 0.1908317297\n",
      "0.67\n",
      "Epoch 2482, Loss: 0.1482560437\n",
      "0.67\n",
      "Epoch 2483, Loss: 0.1408563370\n",
      "0.67\n",
      "Epoch 2484, Loss: 0.5090162600\n",
      "0.67\n",
      "Epoch 2485, Loss: 0.2482353130\n",
      "0.67\n",
      "Epoch 2486, Loss: 0.2454977410\n",
      "0.67\n",
      "Epoch 2487, Loss: 0.1862855296\n",
      "0.67\n",
      "Epoch 2488, Loss: 0.1407126379\n",
      "0.67\n",
      "Epoch 2489, Loss: 0.3548323485\n",
      "0.67\n",
      "Epoch 2490, Loss: 0.1599030741\n",
      "0.67\n",
      "Epoch 2491, Loss: 0.1388731054\n",
      "0.67\n",
      "Epoch 2492, Loss: 0.0980307873\n",
      "0.67\n",
      "Epoch 2493, Loss: 0.1255969157\n",
      "0.67\n",
      "Epoch 2494, Loss: 0.1038544551\n",
      "0.67\n",
      "Epoch 2495, Loss: 0.2877074573\n",
      "0.67\n",
      "Epoch 2496, Loss: 0.1797863166\n",
      "0.67\n",
      "Epoch 2497, Loss: 0.1045965963\n",
      "0.67\n",
      "Epoch 2498, Loss: 0.1776677521\n",
      "0.67\n",
      "Epoch 2499, Loss: 0.1274666645\n",
      "0.67\n",
      "Epoch 2500, Loss: 0.1615753562\n",
      "0.67\n",
      "Epoch 2501, Loss: 0.0712220807\n",
      "0.67\n",
      "Epoch 2502, Loss: 0.3818637362\n",
      "0.67\n",
      "Epoch 2503, Loss: 0.2031942723\n",
      "0.67\n",
      "Epoch 2504, Loss: 0.1093102438\n",
      "0.67\n",
      "Epoch 2505, Loss: 0.1657742784\n",
      "0.67\n",
      "Epoch 2506, Loss: 0.4682172935\n",
      "0.67\n",
      "Epoch 2507, Loss: 0.3230428225\n",
      "0.67\n",
      "Epoch 2508, Loss: 0.4579228518\n",
      "0.67\n",
      "Epoch 2509, Loss: 0.3974883422\n",
      "0.67\n",
      "Epoch 2510, Loss: 0.2693109953\n",
      "0.67\n",
      "Epoch 2511, Loss: 0.0952482099\n",
      "0.67\n",
      "Epoch 2512, Loss: 0.2622746261\n",
      "0.67\n",
      "Epoch 2513, Loss: 0.1785006134\n",
      "0.67\n",
      "Epoch 2514, Loss: 0.4852296897\n",
      "0.67\n",
      "Epoch 2515, Loss: 0.2347653743\n",
      "0.67\n",
      "Epoch 2516, Loss: 0.1048808579\n",
      "0.67\n",
      "Epoch 2517, Loss: 0.1559741418\n",
      "0.67\n",
      "Epoch 2518, Loss: 0.1871679299\n",
      "0.67\n",
      "Epoch 2519, Loss: 0.1372944739\n",
      "0.67\n",
      "Epoch 2520, Loss: 0.1147110255\n",
      "0.67\n",
      "Epoch 2521, Loss: 0.3862224938\n",
      "0.67\n",
      "Epoch 2522, Loss: 0.4431504659\n",
      "0.67\n",
      "Epoch 2523, Loss: 0.2469231515\n",
      "0.67\n",
      "Epoch 2524, Loss: 0.5002176204\n",
      "0.67\n",
      "Epoch 2525, Loss: 0.1441346641\n",
      "0.67\n",
      "Epoch 2526, Loss: 0.1973268863\n",
      "0.67\n",
      "Epoch 2527, Loss: 0.2848849526\n",
      "0.67\n",
      "Epoch 2528, Loss: 0.2090573689\n",
      "0.67\n",
      "Epoch 2529, Loss: 0.4210436325\n",
      "0.67\n",
      "Epoch 2530, Loss: 0.2549163610\n",
      "0.67\n",
      "Epoch 2531, Loss: 0.2888929290\n",
      "0.67\n",
      "Epoch 2532, Loss: 0.1919369578\n",
      "0.67\n",
      "Epoch 2533, Loss: 0.3736179714\n",
      "0.67\n",
      "Epoch 2534, Loss: 0.4247172445\n",
      "0.67\n",
      "Epoch 2535, Loss: 0.3667296683\n",
      "0.67\n",
      "Epoch 2536, Loss: 0.3041498141\n",
      "0.67\n",
      "Epoch 2537, Loss: 0.3652685954\n",
      "0.67\n",
      "Epoch 2538, Loss: 0.2346870017\n",
      "0.67\n",
      "Epoch 2539, Loss: 0.3999261736\n",
      "0.67\n",
      "Epoch 2540, Loss: 0.1817445626\n",
      "0.67\n",
      "Epoch 2541, Loss: 0.2857154428\n",
      "0.67\n",
      "Epoch 2542, Loss: 0.0986893240\n",
      "0.67\n",
      "Epoch 2543, Loss: 0.2009689417\n",
      "0.67\n",
      "Epoch 2544, Loss: 0.1297415263\n",
      "0.67\n",
      "Epoch 2545, Loss: 0.3400681326\n",
      "0.67\n",
      "Epoch 2546, Loss: 0.1227663920\n",
      "0.67\n",
      "Epoch 2547, Loss: 0.1557144151\n",
      "0.67\n",
      "Epoch 2548, Loss: 0.1077201823\n",
      "0.67\n",
      "Epoch 2549, Loss: 0.2595337472\n",
      "0.67\n",
      "Epoch 2550, Loss: 0.1722169933\n",
      "0.67\n",
      "Epoch 2551, Loss: 0.1197148118\n",
      "0.67\n",
      "Epoch 2552, Loss: 0.1793596083\n",
      "0.67\n",
      "Epoch 2553, Loss: 0.2520549111\n",
      "0.67\n",
      "Epoch 2554, Loss: 0.2162616101\n",
      "0.67\n",
      "Epoch 2555, Loss: 0.1656851080\n",
      "0.67\n",
      "Epoch 2556, Loss: 0.1752496299\n",
      "0.67\n",
      "Epoch 2557, Loss: 0.2809068597\n",
      "0.67\n",
      "Epoch 2558, Loss: 0.4174028353\n",
      "0.67\n",
      "Epoch 2559, Loss: 0.1988562336\n",
      "0.67\n",
      "Epoch 2560, Loss: 0.1702287318\n",
      "0.67\n",
      "Epoch 2561, Loss: 0.3894706545\n",
      "0.67\n",
      "Epoch 2562, Loss: 0.1546200207\n",
      "0.67\n",
      "Epoch 2563, Loss: 0.4047405365\n",
      "0.67\n",
      "Epoch 2564, Loss: 0.1543582859\n",
      "0.67\n",
      "Epoch 2565, Loss: 0.3812246949\n",
      "0.67\n",
      "Epoch 2566, Loss: 0.2156918208\n",
      "0.67\n",
      "Epoch 2567, Loss: 0.3918358245\n",
      "0.67\n",
      "Epoch 2568, Loss: 0.2290129312\n",
      "0.67\n",
      "Epoch 2569, Loss: 0.5207041338\n",
      "0.67\n",
      "Epoch 2570, Loss: 0.3163806086\n",
      "0.67\n",
      "Epoch 2571, Loss: 0.3165148552\n",
      "0.67\n",
      "Epoch 2572, Loss: 0.4060285531\n",
      "0.67\n",
      "Epoch 2573, Loss: 0.5358069637\n",
      "0.67\n",
      "Epoch 2574, Loss: 0.2916313914\n",
      "0.67\n",
      "Epoch 2575, Loss: 1.3211271661\n",
      "0.67\n",
      "Epoch 2576, Loss: 0.4498476930\n",
      "0.67\n",
      "Epoch 2577, Loss: 0.2854277404\n",
      "0.67\n",
      "Epoch 2578, Loss: 0.3231337950\n",
      "0.67\n",
      "Epoch 2579, Loss: 0.1117984955\n",
      "0.67\n",
      "Epoch 2580, Loss: 0.2998267017\n",
      "0.67\n",
      "Epoch 2581, Loss: 0.2635751572\n",
      "0.67\n",
      "Epoch 2582, Loss: 0.3291312778\n",
      "0.67\n",
      "Epoch 2583, Loss: 0.0952545129\n",
      "0.67\n",
      "Epoch 2584, Loss: 0.1399364772\n",
      "0.67\n",
      "Epoch 2585, Loss: 0.0821080948\n",
      "0.67\n",
      "Epoch 2586, Loss: 0.1407905684\n",
      "0.67\n",
      "Epoch 2587, Loss: 0.2198071073\n",
      "0.67\n",
      "Epoch 2588, Loss: 0.0963342268\n",
      "0.67\n",
      "Epoch 2589, Loss: 0.1881600192\n",
      "0.67\n",
      "Epoch 2590, Loss: 0.1498772113\n",
      "0.67\n",
      "Epoch 2591, Loss: 0.0996516297\n",
      "0.67\n",
      "Epoch 2592, Loss: 0.0917215542\n",
      "0.67\n",
      "Epoch 2593, Loss: 0.2793615052\n",
      "0.67\n",
      "Epoch 2594, Loss: 0.2761555282\n",
      "0.67\n",
      "Epoch 2595, Loss: 0.1231319310\n",
      "0.67\n",
      "Epoch 2596, Loss: 0.2623645491\n",
      "0.67\n",
      "Epoch 2597, Loss: 0.2154998118\n",
      "0.67\n",
      "Epoch 2598, Loss: 0.2594729786\n",
      "0.67\n",
      "Epoch 2599, Loss: 0.4258775295\n",
      "0.67\n",
      "Epoch 2600, Loss: 0.5574101642\n",
      "0.67\n",
      "Epoch 2601, Loss: 0.2657617663\n",
      "0.67\n",
      "Epoch 2602, Loss: 0.2800005564\n",
      "0.67\n",
      "Epoch 2603, Loss: 0.1775846088\n",
      "0.67\n",
      "Epoch 2604, Loss: 0.0936997802\n",
      "0.67\n",
      "Epoch 2605, Loss: 0.2699135212\n",
      "0.67\n",
      "Epoch 2606, Loss: 0.2904316077\n",
      "0.67\n",
      "Epoch 2607, Loss: 0.2118689280\n",
      "0.67\n",
      "Epoch 2608, Loss: 0.1460512802\n",
      "0.67\n",
      "Epoch 2609, Loss: 0.0875473771\n",
      "0.67\n",
      "Epoch 2610, Loss: 0.0440665808\n",
      "0.67\n",
      "Epoch 2611, Loss: 0.3358567042\n",
      "0.67\n",
      "Epoch 2612, Loss: 0.1705139836\n",
      "0.67\n",
      "Epoch 2613, Loss: 0.2435635636\n",
      "0.67\n",
      "Epoch 2614, Loss: 0.1600373838\n",
      "0.67\n",
      "Epoch 2615, Loss: 0.1666677163\n",
      "0.67\n",
      "Epoch 2616, Loss: 0.2043484669\n",
      "0.67\n",
      "Epoch 2617, Loss: 0.1294501451\n",
      "0.67\n",
      "Epoch 2618, Loss: 0.1883513856\n",
      "0.67\n",
      "Epoch 2619, Loss: 0.1726063393\n",
      "0.67\n",
      "Epoch 2620, Loss: 0.1376070060\n",
      "0.67\n",
      "Epoch 2621, Loss: 0.1425940159\n",
      "0.67\n",
      "Epoch 2622, Loss: 0.1466416786\n",
      "0.67\n",
      "Epoch 2623, Loss: 0.1651297476\n",
      "0.67\n",
      "Epoch 2624, Loss: 0.1575064610\n",
      "0.67\n",
      "Epoch 2625, Loss: 0.1324442096\n",
      "0.67\n",
      "Epoch 2626, Loss: 0.1187463731\n",
      "0.67\n",
      "Epoch 2627, Loss: 0.0942928710\n",
      "0.67\n",
      "Epoch 2628, Loss: 0.1119833461\n",
      "0.67\n",
      "Epoch 2629, Loss: 0.1475259443\n",
      "0.67\n",
      "Epoch 2630, Loss: 0.1594085342\n",
      "0.67\n",
      "Epoch 2631, Loss: 0.1681779363\n",
      "0.67\n",
      "Epoch 2632, Loss: 0.1334260673\n",
      "0.67\n",
      "Epoch 2633, Loss: 0.1179063195\n",
      "0.67\n",
      "Epoch 2634, Loss: 0.0475149713\n",
      "0.67\n",
      "Epoch 2635, Loss: 0.0267843266\n",
      "0.67\n",
      "Epoch 2636, Loss: 0.0196960930\n",
      "0.67\n",
      "Epoch 2637, Loss: 0.0155312073\n",
      "0.67\n",
      "Epoch 2638, Loss: 0.0120525955\n",
      "0.67\n",
      "Epoch 2639, Loss: 0.0084577718\n",
      "0.64625\n",
      "Epoch 2640, Loss: 0.0070227976\n",
      "0.66125\n",
      "Epoch 2641, Loss: 0.0058854133\n",
      "0.65625\n",
      "Epoch 2642, Loss: 0.0051547309\n",
      "0.66875\n",
      "Epoch 2643, Loss: 0.0047725398\n",
      "0.6675\n",
      "Epoch 2644, Loss: 0.0046393377\n",
      "0.65875\n",
      "Epoch 2645, Loss: 0.0042847575\n",
      "0.66875\n",
      "Epoch 2646, Loss: 0.0044398484\n",
      "0.66875\n",
      "Epoch 2647, Loss: 0.0041976418\n",
      "0.66\n",
      "Epoch 2648, Loss: 0.0039885554\n",
      "0.66125\n",
      "Epoch 2649, Loss: 0.0038584555\n",
      "0.665\n",
      "Epoch 2650, Loss: 0.0037654251\n",
      "0.66375\n",
      "Epoch 2651, Loss: 0.0036650592\n",
      "0.65875\n",
      "Epoch 2652, Loss: 0.0035663668\n",
      "0.6575\n",
      "Epoch 2653, Loss: 0.0034896185\n",
      "0.66\n",
      "Epoch 2654, Loss: 0.0034093157\n",
      "0.6575\n",
      "Epoch 2655, Loss: 0.0033394131\n",
      "0.65625\n",
      "Epoch 2656, Loss: 0.0032730656\n",
      "0.65625\n",
      "Epoch 2657, Loss: 0.0032093510\n",
      "0.65625\n",
      "Epoch 2658, Loss: 0.0031489897\n",
      "0.655\n",
      "Epoch 2659, Loss: 0.0030905400\n",
      "0.655\n",
      "Epoch 2660, Loss: 0.0030337049\n",
      "0.6525\n",
      "Epoch 2661, Loss: 0.0029847753\n",
      "0.65375\n",
      "Epoch 2662, Loss: 0.0029308390\n",
      "0.6525\n",
      "Epoch 2663, Loss: 0.0028818187\n",
      "0.655\n",
      "Epoch 2664, Loss: 0.0028331209\n",
      "0.65625\n",
      "Epoch 2665, Loss: 0.0027893871\n",
      "0.655\n",
      "Epoch 2666, Loss: 0.0027440159\n",
      "0.655\n",
      "Epoch 2667, Loss: 0.0026998748\n",
      "0.65625\n",
      "Epoch 2668, Loss: 0.0026613115\n",
      "0.655\n",
      "Epoch 2669, Loss: 0.0026184483\n",
      "0.65625\n",
      "Epoch 2670, Loss: 0.0025687735\n",
      "0.6575\n",
      "Epoch 2671, Loss: 0.0025291031\n",
      "0.65875\n",
      "Epoch 2672, Loss: 0.0024808637\n",
      "0.65875\n",
      "Epoch 2673, Loss: 0.0024446028\n",
      "0.66\n",
      "Epoch 2674, Loss: 0.0024029533\n",
      "0.65875\n",
      "Epoch 2675, Loss: 0.0023632889\n",
      "0.6575\n",
      "Epoch 2676, Loss: 0.0023266790\n",
      "0.65875\n",
      "Epoch 2677, Loss: 0.0022905977\n",
      "0.65875\n",
      "Epoch 2678, Loss: 0.0022546071\n",
      "0.6575\n",
      "Epoch 2679, Loss: 0.0022248935\n",
      "0.66\n",
      "Epoch 2680, Loss: 0.0021880655\n",
      "0.66\n",
      "Epoch 2681, Loss: 0.0021619689\n",
      "0.6575\n",
      "Epoch 2682, Loss: 0.0021274682\n",
      "0.65875\n",
      "Epoch 2683, Loss: 0.0021012328\n",
      "0.6575\n",
      "Epoch 2684, Loss: 0.0020692883\n",
      "0.66\n",
      "Epoch 2685, Loss: 0.0020426216\n",
      "0.66\n",
      "Epoch 2686, Loss: 0.0020156596\n",
      "0.65875\n",
      "Epoch 2687, Loss: 0.0019873204\n",
      "0.66\n",
      "Epoch 2688, Loss: 0.0019596665\n",
      "0.66\n",
      "Epoch 2689, Loss: 0.0019359964\n",
      "0.6575\n",
      "Epoch 2690, Loss: 0.0019070598\n",
      "0.6575\n",
      "Epoch 2691, Loss: 0.0018797157\n",
      "0.65875\n",
      "Epoch 2692, Loss: 0.0018622350\n",
      "0.6575\n",
      "Epoch 2693, Loss: 0.0018422421\n",
      "0.65625\n",
      "Epoch 2694, Loss: 0.0018151738\n",
      "0.65875\n",
      "Epoch 2695, Loss: 0.0017922896\n",
      "0.65875\n",
      "Epoch 2696, Loss: 0.0017609731\n",
      "0.66\n",
      "Epoch 2697, Loss: 0.0017437893\n",
      "0.6575\n",
      "Epoch 2698, Loss: 0.0017239605\n",
      "0.6575\n",
      "Epoch 2699, Loss: 0.0017099438\n",
      "0.66\n",
      "Epoch 2700, Loss: 0.0016811223\n",
      "0.65875\n",
      "Epoch 2701, Loss: 0.0016616187\n",
      "0.66\n",
      "Epoch 2702, Loss: 0.0016336556\n",
      "0.66375\n",
      "Epoch 2703, Loss: 0.0016103687\n",
      "0.6575\n",
      "Epoch 2704, Loss: 0.0015932194\n",
      "0.66125\n",
      "Epoch 2705, Loss: 0.0015967974\n",
      "0.66125\n",
      "Epoch 2706, Loss: 0.0015556233\n",
      "0.66375\n",
      "Epoch 2707, Loss: 0.0015620734\n",
      "0.66375\n",
      "Epoch 2708, Loss: 0.0015314778\n",
      "0.65875\n",
      "Epoch 2709, Loss: 0.0015235136\n",
      "0.6575\n",
      "Epoch 2710, Loss: 0.0014901720\n",
      "0.66125\n",
      "Epoch 2711, Loss: 0.0014679580\n",
      "0.65875\n",
      "Epoch 2712, Loss: 0.0014564120\n",
      "0.65875\n",
      "Epoch 2713, Loss: 0.0014398124\n",
      "0.66125\n",
      "Epoch 2714, Loss: 0.0014192370\n",
      "0.66125\n",
      "Epoch 2715, Loss: 0.0014030479\n",
      "0.65875\n",
      "Epoch 2716, Loss: 0.0013948675\n",
      "0.65875\n",
      "Epoch 2717, Loss: 0.0013767312\n",
      "0.66375\n",
      "Epoch 2718, Loss: 0.0013668053\n",
      "0.6575\n",
      "Epoch 2719, Loss: 0.0013328470\n",
      "0.66\n",
      "Epoch 2720, Loss: 0.0013178461\n",
      "0.65875\n",
      "Epoch 2721, Loss: 0.0013121817\n",
      "0.66\n",
      "Epoch 2722, Loss: 0.0012985608\n",
      "0.65875\n",
      "Epoch 2723, Loss: 0.0012716970\n",
      "0.66\n",
      "Epoch 2724, Loss: 0.0012606302\n",
      "0.66125\n",
      "Epoch 2725, Loss: 0.0012515268\n",
      "0.66\n",
      "Epoch 2726, Loss: 0.0012457997\n",
      "0.665\n",
      "Epoch 2727, Loss: 0.0012769666\n",
      "0.665\n",
      "Epoch 2728, Loss: 0.0012396906\n",
      "0.66\n",
      "Epoch 2729, Loss: 0.0012430001\n",
      "0.66\n",
      "Epoch 2730, Loss: 0.0012364018\n",
      "0.65625\n",
      "Epoch 2731, Loss: 0.0012075014\n",
      "0.66125\n",
      "Epoch 2732, Loss: 0.0011874145\n",
      "0.6625\n",
      "Epoch 2733, Loss: 0.0011620434\n",
      "0.66\n",
      "Epoch 2734, Loss: 0.0011695492\n",
      "0.66\n",
      "Epoch 2735, Loss: 0.0012022262\n",
      "0.66\n",
      "Epoch 2736, Loss: 0.0011807545\n",
      "0.66\n",
      "Epoch 2737, Loss: 0.0011606044\n",
      "0.66\n",
      "Epoch 2738, Loss: 0.0011266011\n",
      "0.665\n",
      "Epoch 2739, Loss: 0.0011002719\n",
      "0.65375\n",
      "Epoch 2740, Loss: 0.0010725208\n",
      "0.65625\n",
      "Epoch 2741, Loss: 0.0010783656\n",
      "0.65625\n",
      "Epoch 2742, Loss: 0.0010683103\n",
      "0.66\n",
      "Epoch 2743, Loss: 0.0010576017\n",
      "0.65875\n",
      "Epoch 2744, Loss: 0.0010464699\n",
      "0.6575\n",
      "Epoch 2745, Loss: 0.0010281222\n",
      "0.65875\n",
      "Epoch 2746, Loss: 0.0010198873\n",
      "0.655\n",
      "Epoch 2747, Loss: 0.0010033996\n",
      "0.6575\n",
      "Epoch 2748, Loss: 0.0009850121\n",
      "0.66\n",
      "Epoch 2749, Loss: 0.0009809946\n",
      "0.66125\n",
      "Epoch 2750, Loss: 0.0009766973\n",
      "0.66\n",
      "Epoch 2751, Loss: 0.0009595606\n",
      "0.6575\n",
      "Epoch 2752, Loss: 0.0009664617\n",
      "0.6575\n",
      "Epoch 2753, Loss: 0.0009428053\n",
      "0.65875\n",
      "Epoch 2754, Loss: 0.0009502511\n",
      "0.65875\n",
      "Epoch 2755, Loss: 0.0009279429\n",
      "0.6575\n",
      "Epoch 2756, Loss: 0.0009225701\n",
      "0.65875\n",
      "Epoch 2757, Loss: 0.0009016854\n",
      "0.65625\n",
      "Epoch 2758, Loss: 0.0009059472\n",
      "0.65625\n",
      "Epoch 2759, Loss: 0.0009045340\n",
      "0.65625\n",
      "Epoch 2760, Loss: 0.0008843646\n",
      "0.655\n",
      "Epoch 2761, Loss: 0.0008818880\n",
      "0.6575\n",
      "Epoch 2762, Loss: 0.0008729110\n",
      "0.6575\n",
      "Epoch 2763, Loss: 0.0008740214\n",
      "0.6575\n",
      "Epoch 2764, Loss: 0.0008727826\n",
      "0.655\n",
      "Epoch 2765, Loss: 0.0008591952\n",
      "0.65875\n",
      "Epoch 2766, Loss: 0.0008448706\n",
      "0.66\n",
      "Epoch 2767, Loss: 0.0008328488\n",
      "0.6575\n",
      "Epoch 2768, Loss: 0.0008287728\n",
      "0.655\n",
      "Epoch 2769, Loss: 0.0008309046\n",
      "0.655\n",
      "Epoch 2770, Loss: 0.0008244533\n",
      "0.6575\n",
      "Epoch 2771, Loss: 0.0008218844\n",
      "0.6575\n",
      "Epoch 2772, Loss: 0.0008004300\n",
      "0.6525\n",
      "Epoch 2773, Loss: 0.0007901162\n",
      "0.6525\n",
      "Epoch 2774, Loss: 0.0007789716\n",
      "0.65375\n",
      "Epoch 2775, Loss: 0.0007797101\n",
      "0.65375\n",
      "Epoch 2776, Loss: 0.0007775072\n",
      "0.65625\n",
      "Epoch 2777, Loss: 0.0007606847\n",
      "0.6525\n",
      "Epoch 2778, Loss: 0.0007543196\n",
      "0.65625\n",
      "Epoch 2779, Loss: 0.0007509679\n",
      "0.655\n",
      "Epoch 2780, Loss: 0.0007426291\n",
      "0.655\n",
      "Epoch 2781, Loss: 0.0007398700\n",
      "0.6575\n",
      "Epoch 2782, Loss: 0.0007314111\n",
      "0.65625\n",
      "Epoch 2783, Loss: 0.0007225119\n",
      "0.6575\n",
      "Epoch 2784, Loss: 0.0007255379\n",
      "0.6575\n",
      "Epoch 2785, Loss: 0.0007317628\n",
      "0.6575\n",
      "Epoch 2786, Loss: 0.0007130084\n",
      "0.65625\n",
      "Epoch 2787, Loss: 0.0007288607\n",
      "0.65625\n",
      "Epoch 2788, Loss: 0.0007069507\n",
      "0.6575\n",
      "Epoch 2789, Loss: 0.0006941796\n",
      "0.65625\n",
      "Epoch 2790, Loss: 0.0006867829\n",
      "0.65625\n",
      "Epoch 2791, Loss: 0.0006880600\n",
      "0.65625\n",
      "Epoch 2792, Loss: 0.0006770163\n",
      "0.655\n",
      "Epoch 2793, Loss: 0.0006782528\n",
      "0.655\n",
      "Epoch 2794, Loss: 0.0006757702\n",
      "0.65875\n",
      "Epoch 2795, Loss: 0.0006613236\n",
      "0.655\n",
      "Epoch 2796, Loss: 0.0006531057\n",
      "0.655\n",
      "Epoch 2797, Loss: 0.0006595399\n",
      "0.655\n",
      "Epoch 2798, Loss: 0.0006514380\n",
      "0.655\n",
      "Epoch 2799, Loss: 0.0006398492\n",
      "0.6575\n",
      "Epoch 2800, Loss: 0.0006432509\n",
      "0.6575\n",
      "Epoch 2801, Loss: 0.0006436944\n",
      "0.6575\n",
      "Epoch 2802, Loss: 0.0006266822\n",
      "0.6575\n",
      "Epoch 2803, Loss: 0.0006233026\n",
      "0.6525\n",
      "Epoch 2804, Loss: 0.0006130575\n",
      "0.65875\n",
      "Epoch 2805, Loss: 0.0006130605\n",
      "0.65875\n",
      "Epoch 2806, Loss: 0.0006053368\n",
      "0.6575\n",
      "Epoch 2807, Loss: 0.0005990447\n",
      "0.65875\n",
      "Epoch 2808, Loss: 0.0006140806\n",
      "0.65875\n",
      "Epoch 2809, Loss: 0.0006340622\n",
      "0.65875\n",
      "Epoch 2810, Loss: 0.0005983614\n",
      "0.65375\n",
      "Epoch 2811, Loss: 0.0006176494\n",
      "0.65375\n",
      "Epoch 2812, Loss: 0.0005983306\n",
      "0.6575\n",
      "Epoch 2813, Loss: 0.0005862236\n",
      "0.65625\n",
      "Epoch 2814, Loss: 0.0005764571\n",
      "0.65625\n",
      "Epoch 2815, Loss: 0.0005768384\n",
      "0.65625\n",
      "Epoch 2816, Loss: 0.0005759943\n",
      "0.6575\n",
      "Epoch 2817, Loss: 0.0005581990\n",
      "0.655\n",
      "Epoch 2818, Loss: 0.0005513789\n",
      "0.65375\n",
      "Epoch 2819, Loss: 0.0005481942\n",
      "0.6575\n",
      "Epoch 2820, Loss: 0.0005604617\n",
      "0.6575\n",
      "Epoch 2821, Loss: 0.0005667964\n",
      "0.6575\n",
      "Epoch 2822, Loss: 0.0005590780\n",
      "0.6575\n",
      "Epoch 2823, Loss: 0.0005457110\n",
      "0.65375\n",
      "Epoch 2824, Loss: 0.0005453424\n",
      "0.66\n",
      "Epoch 2825, Loss: 0.0005500221\n",
      "0.66\n",
      "Epoch 2826, Loss: 0.0005247303\n",
      "0.6625\n",
      "Epoch 2827, Loss: 0.0005199431\n",
      "0.66\n",
      "Epoch 2828, Loss: 0.0005102484\n",
      "0.65875\n",
      "Epoch 2829, Loss: 0.0005195656\n",
      "0.65875\n",
      "Epoch 2830, Loss: 0.0005290198\n",
      "0.65875\n",
      "Epoch 2831, Loss: 0.0005027663\n",
      "0.66\n",
      "Epoch 2832, Loss: 0.0004947930\n",
      "0.66\n",
      "Epoch 2833, Loss: 0.0004936875\n",
      "0.66375\n",
      "Epoch 2834, Loss: 0.0004909897\n",
      "0.6575\n",
      "Epoch 2835, Loss: 0.0004898714\n",
      "0.65875\n",
      "Epoch 2836, Loss: 0.0004854753\n",
      "0.65875\n",
      "Epoch 2837, Loss: 0.0004811269\n",
      "0.6575\n",
      "Epoch 2838, Loss: 0.0004745652\n",
      "0.66\n",
      "Epoch 2839, Loss: 0.0004701040\n",
      "0.66\n",
      "Epoch 2840, Loss: 0.0004674379\n",
      "0.65875\n",
      "Epoch 2841, Loss: 0.0004650369\n",
      "0.66125\n",
      "Epoch 2842, Loss: 0.0004751691\n",
      "0.66125\n",
      "Epoch 2843, Loss: 0.0004738417\n",
      "0.66125\n",
      "Epoch 2844, Loss: 0.0004636691\n",
      "0.65875\n",
      "Epoch 2845, Loss: 0.0004472726\n",
      "0.66375\n",
      "Epoch 2846, Loss: 0.0004472919\n",
      "0.66375\n",
      "Epoch 2847, Loss: 0.0004432401\n",
      "0.66\n",
      "Epoch 2848, Loss: 0.0004691259\n",
      "0.66\n",
      "Epoch 2849, Loss: 0.0004487579\n",
      "0.66\n",
      "Epoch 2850, Loss: 0.0004370415\n",
      "0.66\n",
      "Epoch 2851, Loss: 0.0004295124\n",
      "0.66\n",
      "Epoch 2852, Loss: 0.0004413881\n",
      "0.66\n",
      "Epoch 2853, Loss: 0.0004420579\n",
      "0.66\n",
      "Epoch 2854, Loss: 0.0004249436\n",
      "0.66\n",
      "Epoch 2855, Loss: 0.0004179046\n",
      "0.65875\n",
      "Epoch 2856, Loss: 0.0004275203\n",
      "0.65875\n",
      "Epoch 2857, Loss: 0.0004294486\n",
      "0.65875\n",
      "Epoch 2858, Loss: 0.0004086239\n",
      "0.6625\n",
      "Epoch 2859, Loss: 0.0004048972\n",
      "0.66\n",
      "Epoch 2860, Loss: 0.0004067406\n",
      "0.66\n",
      "Epoch 2861, Loss: 0.0004084885\n",
      "0.66\n",
      "Epoch 2862, Loss: 0.0003946095\n",
      "0.66\n",
      "Epoch 2863, Loss: 0.0003914744\n",
      "0.65875\n",
      "Epoch 2864, Loss: 0.0003850588\n",
      "0.6625\n",
      "Epoch 2865, Loss: 0.0003872781\n",
      "0.6625\n",
      "Epoch 2866, Loss: 0.0003958387\n",
      "0.6625\n",
      "Epoch 2867, Loss: 0.0003784407\n",
      "0.66125\n",
      "Epoch 2868, Loss: 0.0003955054\n",
      "0.66125\n",
      "Epoch 2869, Loss: 0.0003869216\n",
      "0.66125\n",
      "Epoch 2870, Loss: 0.0003741252\n",
      "0.66125\n",
      "Epoch 2871, Loss: 0.0003751424\n",
      "0.66125\n",
      "Epoch 2872, Loss: 0.0003661032\n",
      "0.66125\n",
      "Epoch 2873, Loss: 0.0003572100\n",
      "0.66125\n",
      "Epoch 2874, Loss: 0.0003570706\n",
      "0.66\n",
      "Epoch 2875, Loss: 0.0003653507\n",
      "0.66\n",
      "Epoch 2876, Loss: 0.0003697176\n",
      "0.66\n",
      "Epoch 2877, Loss: 0.0003545837\n",
      "0.6625\n",
      "Epoch 2878, Loss: 0.0003425227\n",
      "0.66\n",
      "Epoch 2879, Loss: 0.0003393451\n",
      "0.66\n",
      "Epoch 2880, Loss: 0.0003394293\n",
      "0.66\n",
      "Epoch 2881, Loss: 0.0003379261\n",
      "0.665\n",
      "Epoch 2882, Loss: 0.0003403702\n",
      "0.665\n",
      "Epoch 2883, Loss: 0.0003372726\n",
      "0.665\n",
      "Epoch 2884, Loss: 0.0003359850\n",
      "0.665\n",
      "Epoch 2885, Loss: 0.0003395361\n",
      "0.665\n",
      "Epoch 2886, Loss: 0.0003408195\n",
      "0.665\n",
      "Epoch 2887, Loss: 0.0003346420\n",
      "0.665\n",
      "Epoch 2888, Loss: 0.0003411096\n",
      "0.665\n",
      "Epoch 2889, Loss: 0.0003400550\n",
      "0.665\n",
      "Epoch 2890, Loss: 0.0003264213\n",
      "0.65875\n",
      "Epoch 2891, Loss: 0.0003153480\n",
      "0.65875\n",
      "Epoch 2892, Loss: 0.0003152630\n",
      "0.66375\n",
      "Epoch 2893, Loss: 0.0003199685\n",
      "0.66375\n",
      "Epoch 2894, Loss: 0.0003162872\n",
      "0.66375\n",
      "Epoch 2895, Loss: 0.0003093729\n",
      "0.66\n",
      "Epoch 2896, Loss: 0.0003345966\n",
      "0.66\n",
      "Epoch 2897, Loss: 0.0003339501\n",
      "0.66\n",
      "Epoch 2898, Loss: 0.0003313611\n",
      "0.66\n",
      "Epoch 2899, Loss: 0.0003099905\n",
      "0.66\n",
      "Epoch 2900, Loss: 0.0002994192\n",
      "0.665\n",
      "Epoch 2901, Loss: 0.0002968805\n",
      "0.6625\n",
      "Epoch 2902, Loss: 0.0002998855\n",
      "0.6625\n",
      "Epoch 2903, Loss: 0.0002944142\n",
      "0.66375\n",
      "Epoch 2904, Loss: 0.0002846881\n",
      "0.66125\n",
      "Epoch 2905, Loss: 0.0002887152\n",
      "0.66125\n",
      "Epoch 2906, Loss: 0.0002984670\n",
      "0.66125\n",
      "Epoch 2907, Loss: 0.0003073528\n",
      "0.66125\n",
      "Epoch 2908, Loss: 0.0002995739\n",
      "0.66125\n",
      "Epoch 2909, Loss: 0.0002818849\n",
      "0.66375\n",
      "Epoch 2910, Loss: 0.0002728045\n",
      "0.6625\n",
      "Epoch 2911, Loss: 0.0002819874\n",
      "0.6625\n",
      "Epoch 2912, Loss: 0.0002667898\n",
      "0.6625\n",
      "Epoch 2913, Loss: 0.0002739211\n",
      "0.6625\n",
      "Epoch 2914, Loss: 0.0002742855\n",
      "0.6625\n",
      "Epoch 2915, Loss: 0.0002688761\n",
      "0.6625\n",
      "Epoch 2916, Loss: 0.0002567108\n",
      "0.6625\n",
      "Epoch 2917, Loss: 0.0002561082\n",
      "0.66125\n",
      "Epoch 2918, Loss: 0.0002571956\n",
      "0.66125\n",
      "Epoch 2919, Loss: 0.0002630376\n",
      "0.66125\n",
      "Epoch 2920, Loss: 0.0002695392\n",
      "0.66125\n",
      "Epoch 2921, Loss: 0.0002789992\n",
      "0.66125\n",
      "Epoch 2922, Loss: 0.0002843476\n",
      "0.66125\n",
      "Epoch 2923, Loss: 0.0002799738\n",
      "0.66125\n",
      "Epoch 2924, Loss: 0.0002659818\n",
      "0.66125\n",
      "Epoch 2925, Loss: 0.0002487842\n",
      "0.66\n",
      "Epoch 2926, Loss: 0.0002589114\n",
      "0.66\n",
      "Epoch 2927, Loss: 0.0002598573\n",
      "0.66\n",
      "Epoch 2928, Loss: 0.0002589511\n",
      "0.66\n",
      "Epoch 2929, Loss: 0.0002407340\n",
      "0.66125\n",
      "Epoch 2930, Loss: 0.0002385598\n",
      "0.66125\n",
      "Epoch 2931, Loss: 0.0002409558\n",
      "0.66125\n",
      "Epoch 2932, Loss: 0.0002419884\n",
      "0.66125\n",
      "Epoch 2933, Loss: 0.0002444367\n",
      "0.66125\n",
      "Epoch 2934, Loss: 0.0002280958\n",
      "0.66875\n",
      "Epoch 2935, Loss: 0.0002284756\n",
      "0.66875\n",
      "Epoch 2936, Loss: 0.0002222468\n",
      "0.66375\n",
      "Epoch 2937, Loss: 0.0002292648\n",
      "0.66375\n",
      "Epoch 2938, Loss: 0.0002291761\n",
      "0.66375\n",
      "Epoch 2939, Loss: 0.0002264479\n",
      "0.66375\n",
      "Epoch 2940, Loss: 0.0002200622\n",
      "0.665\n",
      "Epoch 2941, Loss: 0.0002137441\n",
      "0.66125\n",
      "Epoch 2942, Loss: 0.0002110131\n",
      "0.66625\n",
      "Epoch 2943, Loss: 0.0002120432\n",
      "0.66625\n",
      "Epoch 2944, Loss: 0.0002173072\n",
      "0.66625\n",
      "Epoch 2945, Loss: 0.0002200286\n",
      "0.66625\n",
      "Epoch 2946, Loss: 0.0002199236\n",
      "0.66625\n",
      "Epoch 2947, Loss: 0.0002113542\n",
      "0.66625\n",
      "Epoch 2948, Loss: 0.0002140040\n",
      "0.66625\n",
      "Epoch 2949, Loss: 0.0002038851\n",
      "0.66375\n",
      "Epoch 2950, Loss: 0.0002006247\n",
      "0.66\n",
      "Epoch 2951, Loss: 0.0002061633\n",
      "0.66\n",
      "Epoch 2952, Loss: 0.0002080178\n",
      "0.66\n",
      "Epoch 2953, Loss: 0.0002067263\n",
      "0.66\n",
      "Epoch 2954, Loss: 0.0002018181\n",
      "0.66\n",
      "Epoch 2955, Loss: 0.0002003657\n",
      "0.665\n",
      "Epoch 2956, Loss: 0.0001929424\n",
      "0.66625\n",
      "Epoch 2957, Loss: 0.0002100013\n",
      "0.66625\n",
      "Epoch 2958, Loss: 0.0002345626\n",
      "0.66625\n",
      "Epoch 2959, Loss: 0.0002185426\n",
      "0.66625\n",
      "Epoch 2960, Loss: 0.0001950239\n",
      "0.66625\n",
      "Epoch 2961, Loss: 0.0002055367\n",
      "0.66625\n",
      "Epoch 2962, Loss: 0.0001887617\n",
      "0.66375\n",
      "Epoch 2963, Loss: 0.0001959566\n",
      "0.66375\n",
      "Epoch 2964, Loss: 0.0002044051\n",
      "0.66375\n",
      "Epoch 2965, Loss: 0.0002059984\n",
      "0.66375\n",
      "Epoch 2966, Loss: 0.0001938276\n",
      "0.66375\n",
      "Epoch 2967, Loss: 0.0001809335\n",
      "0.6625\n",
      "Epoch 2968, Loss: 0.0001796040\n",
      "0.6625\n",
      "Epoch 2969, Loss: 0.0001808949\n",
      "0.6625\n",
      "Epoch 2970, Loss: 0.0001745034\n",
      "0.66125\n",
      "Epoch 2971, Loss: 0.0001695347\n",
      "0.6675\n",
      "Epoch 2972, Loss: 0.0001685708\n",
      "0.665\n",
      "Epoch 2973, Loss: 0.0001698753\n",
      "0.665\n",
      "Epoch 2974, Loss: 0.0001712977\n",
      "0.665\n",
      "Epoch 2975, Loss: 0.0001648834\n",
      "0.65875\n",
      "Epoch 2976, Loss: 0.0001623232\n",
      "0.6625\n",
      "Epoch 2977, Loss: 0.0001594011\n",
      "0.66375\n",
      "Epoch 2978, Loss: 0.0001592008\n",
      "0.665\n",
      "Epoch 2979, Loss: 0.0001597238\n",
      "0.665\n",
      "Epoch 2980, Loss: 0.0001602865\n",
      "0.665\n",
      "Epoch 2981, Loss: 0.0001576208\n",
      "0.66125\n",
      "Epoch 2982, Loss: 0.0001540479\n",
      "0.66625\n",
      "Epoch 2983, Loss: 0.0001519794\n",
      "0.665\n",
      "Epoch 2984, Loss: 0.0001515741\n",
      "0.6625\n",
      "Epoch 2985, Loss: 0.0001519931\n",
      "0.6625\n",
      "Epoch 2986, Loss: 0.0001529660\n",
      "0.6625\n",
      "Epoch 2987, Loss: 0.0001513211\n",
      "0.66375\n",
      "Epoch 2988, Loss: 0.0001474241\n",
      "0.6625\n",
      "Epoch 2989, Loss: 0.0001471125\n",
      "0.66125\n",
      "Epoch 2990, Loss: 0.0001470235\n",
      "0.66125\n",
      "Epoch 2991, Loss: 0.0001481938\n",
      "0.66125\n",
      "Epoch 2992, Loss: 0.0001472056\n",
      "0.66125\n",
      "Epoch 2993, Loss: 0.0001447925\n",
      "0.66125\n",
      "Epoch 2994, Loss: 0.0001422594\n",
      "0.66\n",
      "Epoch 2995, Loss: 0.0001388118\n",
      "0.6625\n",
      "Epoch 2996, Loss: 0.0001405177\n",
      "0.6625\n",
      "Epoch 2997, Loss: 0.0001413490\n",
      "0.6625\n",
      "Epoch 2998, Loss: 0.0001480001\n",
      "0.6625\n",
      "Epoch 2999, Loss: 0.0001512262\n",
      "0.6625\n",
      "Epoch 3000, Loss: 0.0001545515\n",
      "0.6625\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork_Adam' object has no attribute 'best_seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m         best_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mget_best_weights()\n\u001b[1;32m     41\u001b[0m         best_biases \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mget_best_biases()\n\u001b[0;32m---> 42\u001b[0m         best_seed \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_loss)\n",
      "Cell \u001b[0;32mIn[32], line 195\u001b[0m, in \u001b[0;36mNeuralNetwork_Adam.get_best_seed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_best_seed\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_seed\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NeuralNetwork_Adam' object has no attribute 'best_seed'"
     ]
    }
   ],
   "source": [
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_weights_init = []\n",
    "best_biases_init = []\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "best_seed = 0\n",
    "\n",
    "for _ in range (1):\n",
    "    nn = NeuralNetwork_Adam(625, [256,128,64], 8, beta1=0.95, beta2=0.999)\n",
    "    nn.train(batches,14,learning_rate=0.0015)\n",
    "    if nn.get_best_loss() < best_loss:\n",
    "        best_loss = nn.get_best_loss()\n",
    "        best_weights = nn.get_best_weights()\n",
    "        best_biases = nn.get_best_biases()\n",
    "        # best_seed = nn.get_best_seed()\n",
    "\n",
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = range(len(accu))\n",
    "\n",
    "# Plot the array\n",
    "plt.plot(x, accu, marker='o',markersize=1)  # 'o' adds markers at each data point\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plot')\n",
    "\n",
    "# Show grid\n",
    "plt.grid()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00013881181011596156\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "# print(best_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8, init_weights=best_weights, init_biases=best_biases, init_seed=best_seed)\n",
    "# nn.train(batches, 3, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.get_best_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Number of layers in the Neural Network\n",
    "N = 4  # Example value, replace with the actual number of layers\n",
    "\n",
    "# Initialize the dictionary\n",
    "weights_dict = {\n",
    "    'weights': {},\n",
    "    'bias': {}\n",
    "}\n",
    "\n",
    "weights = nn.get_best_weights()\n",
    "biases = nn.get_best_biases()\n",
    "\n",
    "# Populate the weights and bias dictionaries\n",
    "for i in range(N):\n",
    "    weights_dict['weights'][f'fc{i+1}'] = weights[i]\n",
    "    weights_dict['bias'][f'fc{i+1}'] = biases[i].flatten()\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.366776515690029\n",
      "0.6625\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'val' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))\n",
    "\n",
    "for X_val, Y_val in batches:\n",
    "    Y_pred= nn.predict(X_val)\n",
    "    print(cross_entropy_loss(Y_val,Y_pred)/len(dataset))\n",
    "    print(accuracy(Y_val, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
