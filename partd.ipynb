{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Remember to import \"numpy_transforms\" functions if you wish to import these two classes in a different script.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "        label = row[\"class\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image), label\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image, label = self.dataset[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "        batch_labels = np.array(labels)\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(128, 625)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    print(labels.shape)  # Should be [batch_size]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# # ReLU activation and its derivative\n",
    "# def relu(x):\n",
    "#     return np.maximum(0, x)\n",
    "\n",
    "# def relu_derivative(x):\n",
    "#     return np.where(x > 0, 1, 0)\n",
    "\n",
    "# # Mean Squared Error loss\n",
    "# def mean_squared_error(y_true, y_pred):\n",
    "#     return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "# # Neural Network Class with ReLU in the Output Layer and Hidden Layers\n",
    "# class NeuralNetwork_Adam:\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "#         if init_seed is None:\n",
    "#             self.best_seed = int(time.time())\n",
    "#             np.random.seed(self.best_seed)\n",
    "#         else:\n",
    "#             np.random.seed(init_seed)\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "#         self.m_w = []\n",
    "#         self.v_w = []\n",
    "#         self.m_b = []\n",
    "#         self.v_b = []\n",
    "#         self.beta1 = beta1\n",
    "#         self.beta2 = beta2\n",
    "#         self.epsilon = epsilon\n",
    "#         self.t = 0  # Time step for Adam\n",
    "#         self.best_weights = []\n",
    "#         self.best_biases = []\n",
    "#         self.best_loss = float(\"inf\")\n",
    "\n",
    "#         layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "#         # Initialize weights, biases, and Adam parameters (m, v)\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             if (init_weights is not None) and (init_biases is not None):\n",
    "#                 self.weights.append(init_weights[i])\n",
    "#                 self.biases.append(init_biases[i])\n",
    "#             else:\n",
    "#                 self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "#                 self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "#             self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.best_weights = self.weights\n",
    "#             self.best_biases = self.biases\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         pre_activations = []\n",
    "\n",
    "#         # Pass through each layer except the output layer\n",
    "#         for i in range(len(self.weights) - 1):\n",
    "#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             pre_activations.append(z)\n",
    "#             a = relu(z)  # ReLU for hidden layers\n",
    "#             activations.append(a)\n",
    "\n",
    "#         # Pass through the output layer with ReLU\n",
    "#         z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         pre_activations.append(z)\n",
    "#         a = relu(z)  # ReLU for the output layer\n",
    "#         activations.append(a)\n",
    "\n",
    "#         return activations, pre_activations\n",
    "\n",
    "#     def backward(self, X, y, activations, pre_activations):\n",
    "#         grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "#         grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "#         # Start with output layer error\n",
    "#         delta = activations[-1] - y\n",
    "#         delta *= relu_derivative(pre_activations[-1])  # ReLU derivative for the output layer\n",
    "\n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "#             grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "#             if i > 0:\n",
    "#                 delta = np.dot(delta, self.weights[i].T) * relu_derivative(pre_activations[i - 1])\n",
    "\n",
    "#         return grad_w, grad_b\n",
    "\n",
    "#     def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "#         self.t += 1  # Increment time step for Adam\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             # Update biased first moment estimate\n",
    "#             self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "#             self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "#             # Update biased second moment estimate\n",
    "#             self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "#             self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "#             # Compute bias-corrected first moment estimate\n",
    "#             m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "#             m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "#             # Compute bias-corrected second moment estimate\n",
    "#             v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "#             v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "#             # Update weights and biases\n",
    "#             self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "#             self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "#     def train(self, batches, time_of_running, learning_rate):\n",
    "#         start_time = time.time()\n",
    "#         epoch = 0\n",
    "#         while True:\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 activations, pre_activations = self.forward(X_batch)\n",
    "#                 grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "#                 self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "#             # Calculate average loss over batches\n",
    "#             loss = 0\n",
    "#             z = 0\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 y_pred, _ = self.forward(X_batch)\n",
    "#                 loss += mean_squared_error(y_batch, y_pred[-1])\n",
    "#                 z += len(y_pred[-1])\n",
    "#             loss /= z\n",
    "            \n",
    "#             if loss < self.best_loss:\n",
    "#                 self.best_loss = loss\n",
    "#                 self.best_weights = self.weights\n",
    "#                 self.best_biases = self.biases\n",
    "#             print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "#             epoch += 1\n",
    "#             # if time elapsed is greater than 1 minute, break the loop\n",
    "#             if time.time() - start_time > 60 * time_of_running:\n",
    "#                 break\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "#     def get_best_weights(self):\n",
    "#         return self.best_weights\n",
    "\n",
    "#     def get_best_biases(self):\n",
    "#         return self.best_biases\n",
    "\n",
    "#     def get_best_loss(self):\n",
    "#         return self.best_loss\n",
    "\n",
    "#     def get_best_seed(self):\n",
    "#         return self.best_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Leaky ReLU activation and its derivative\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Neural Network Class with Leaky ReLU and Adam Optimizer\n",
    "class NeuralNetwork_Adam:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, alpha=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        if init_seed is None:\n",
    "            self.best_seed = int(time.time())\n",
    "            np.random.seed(self.best_seed)\n",
    "        else:\n",
    "            np.random.seed(init_seed)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.m_w = []\n",
    "        self.v_w = []\n",
    "        self.m_b = []\n",
    "        self.v_b = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step for Adam\n",
    "        self.alpha = alpha  # Leaky ReLU parameter\n",
    "        self.best_weights = []\n",
    "        self.best_biases = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if init_weights is not None and init_biases is not None:\n",
    "                self.weights.append(init_weights[i])\n",
    "                self.biases.append(init_biases[i])\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "                self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "            self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "        self.best_weights = self.weights\n",
    "        self.best_biases = self.biases\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each hidden layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = leaky_relu(z, alpha=self.alpha)  # Leaky ReLU for hidden layers\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * leaky_relu_derivative(pre_activations[i - 1], alpha=self.alpha)\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        self.t += 1  # Increment time step for Adam\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def train(self, batches, time_of_running, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while True:\n",
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
    "            loss /= z\n",
    "            \n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = self.weights\n",
    "                self.best_biases = self.biases\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            epoch += 1\n",
    "            # if time elapsed is greater than 1 minute, break the loop\n",
    "            if time.time() - start_time > 60 * time_of_running:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    \n",
    "    def get_best_biases(self):\n",
    "        return self.best_biases\n",
    "    \n",
    "    def get_best_loss(self):\n",
    "        return self.best_loss\n",
    "    \n",
    "    def get_best_seed(self):\n",
    "        return self.best_seed\n",
    "\n",
    "# Example usage:\n",
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8)\n",
    "# nn.train(batches, 1, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0639233793\n",
      "Epoch 2, Loss: 2.0346738039\n",
      "Epoch 3, Loss: 1.9980738901\n",
      "Epoch 4, Loss: 1.9406382383\n",
      "Epoch 5, Loss: 1.8905575864\n",
      "Epoch 6, Loss: 1.8424504922\n",
      "Epoch 7, Loss: 1.8620622831\n",
      "Epoch 8, Loss: 1.7789529820\n",
      "Epoch 9, Loss: 1.7578560990\n",
      "Epoch 10, Loss: 1.7097578432\n",
      "Epoch 11, Loss: 1.6751316888\n",
      "Epoch 12, Loss: 1.6674058140\n",
      "Epoch 13, Loss: 1.6444224643\n",
      "Epoch 14, Loss: 1.6225600485\n",
      "Epoch 15, Loss: 1.5939783699\n",
      "Epoch 16, Loss: 1.5697895953\n",
      "Epoch 17, Loss: 1.5924414856\n",
      "Epoch 18, Loss: 1.5275227125\n",
      "Epoch 19, Loss: 1.5224598312\n",
      "Epoch 20, Loss: 1.4999123540\n",
      "Epoch 21, Loss: 1.4826440594\n",
      "Epoch 22, Loss: 1.4619952098\n",
      "Epoch 23, Loss: 1.4453254570\n",
      "Epoch 24, Loss: 1.4588823545\n",
      "Epoch 25, Loss: 1.5582065906\n",
      "Epoch 26, Loss: 1.4234411385\n",
      "Epoch 27, Loss: 1.3974205144\n",
      "Epoch 28, Loss: 1.4233372778\n",
      "Epoch 29, Loss: 1.3734945685\n",
      "Epoch 30, Loss: 1.3631840099\n",
      "Epoch 31, Loss: 1.3212050710\n",
      "Epoch 32, Loss: 1.3088426497\n",
      "Epoch 33, Loss: 1.3067931691\n",
      "Epoch 34, Loss: 1.2858990299\n",
      "Epoch 35, Loss: 1.2871065206\n",
      "Epoch 36, Loss: 1.2775997144\n",
      "Epoch 37, Loss: 1.2659723653\n",
      "Epoch 38, Loss: 1.2447119187\n",
      "Epoch 39, Loss: 1.2525771076\n",
      "Epoch 40, Loss: 1.2503429065\n",
      "Epoch 41, Loss: 1.2669069461\n",
      "Epoch 42, Loss: 1.4517904827\n",
      "Epoch 43, Loss: 1.2510433400\n",
      "Epoch 44, Loss: 1.1954685847\n",
      "Epoch 45, Loss: 1.2110022554\n",
      "Epoch 46, Loss: 1.2178357676\n",
      "Epoch 47, Loss: 1.2108397124\n",
      "Epoch 48, Loss: 1.1811933762\n",
      "Epoch 49, Loss: 1.1885756682\n",
      "Epoch 50, Loss: 1.1634578617\n",
      "Epoch 51, Loss: 1.1283642960\n",
      "Epoch 52, Loss: 1.1133827364\n",
      "Epoch 53, Loss: 1.1327520336\n",
      "Epoch 54, Loss: 1.1221396534\n",
      "Epoch 55, Loss: 1.1087857543\n",
      "Epoch 56, Loss: 1.0961397877\n",
      "Epoch 57, Loss: 1.0953405656\n",
      "Epoch 58, Loss: 1.0811322244\n",
      "Epoch 59, Loss: 1.1131473918\n",
      "Epoch 60, Loss: 1.0960547498\n",
      "Epoch 61, Loss: 1.0975013934\n",
      "Epoch 62, Loss: 1.1269972814\n",
      "Epoch 63, Loss: 1.0749476615\n",
      "Epoch 64, Loss: 1.0578708690\n",
      "Epoch 65, Loss: 1.0568757691\n",
      "Epoch 66, Loss: 1.0494727605\n",
      "Epoch 67, Loss: 1.0386045506\n",
      "Epoch 68, Loss: 1.0394179535\n",
      "Epoch 69, Loss: 1.0516996486\n",
      "Epoch 70, Loss: 1.0709647598\n",
      "Epoch 71, Loss: 1.1105037904\n",
      "Epoch 72, Loss: 1.0930003700\n",
      "Epoch 73, Loss: 1.0953437254\n",
      "Epoch 74, Loss: 1.1637999115\n",
      "Epoch 75, Loss: 1.1593893359\n",
      "Epoch 76, Loss: 1.0511140177\n",
      "Epoch 77, Loss: 1.1144064616\n",
      "Epoch 78, Loss: 0.9989139649\n",
      "Epoch 79, Loss: 0.9888416791\n",
      "Epoch 80, Loss: 0.9991499926\n",
      "Epoch 81, Loss: 1.0178217364\n",
      "Epoch 82, Loss: 1.0525203040\n",
      "Epoch 83, Loss: 1.0026715178\n",
      "Epoch 84, Loss: 1.0162236492\n",
      "Epoch 85, Loss: 1.0010019682\n",
      "Epoch 86, Loss: 1.0429326853\n",
      "Epoch 87, Loss: 1.0054118046\n",
      "Epoch 88, Loss: 0.9761675403\n",
      "Epoch 89, Loss: 0.9431208157\n",
      "Epoch 90, Loss: 0.9269865245\n",
      "Epoch 91, Loss: 0.9326254626\n",
      "Epoch 92, Loss: 0.9576876059\n",
      "Epoch 93, Loss: 0.9354131525\n",
      "Epoch 94, Loss: 1.0008743936\n",
      "Epoch 95, Loss: 0.9214899613\n",
      "Epoch 96, Loss: 0.9138873061\n",
      "Epoch 97, Loss: 0.9551957227\n",
      "Epoch 98, Loss: 0.9599267299\n",
      "Epoch 99, Loss: 0.9543394037\n",
      "Epoch 100, Loss: 0.9718383083\n",
      "Epoch 101, Loss: 0.8757565215\n",
      "Epoch 102, Loss: 0.8858959216\n",
      "Epoch 103, Loss: 0.8885260829\n",
      "Epoch 104, Loss: 0.8858309463\n",
      "Epoch 105, Loss: 0.9085155871\n",
      "Epoch 106, Loss: 0.9428630815\n",
      "Epoch 107, Loss: 0.8857204275\n",
      "Epoch 108, Loss: 0.8402319397\n",
      "Epoch 109, Loss: 0.8387931501\n",
      "Epoch 110, Loss: 0.8513621812\n",
      "Epoch 111, Loss: 0.8757879768\n",
      "Epoch 112, Loss: 0.8781112037\n",
      "Epoch 113, Loss: 0.8840282652\n",
      "Epoch 114, Loss: 0.8385132710\n",
      "Epoch 115, Loss: 0.8281632980\n",
      "Epoch 116, Loss: 0.8151448600\n",
      "Epoch 117, Loss: 0.8011409987\n",
      "Epoch 118, Loss: 0.8056420326\n",
      "Epoch 119, Loss: 0.8012928529\n",
      "Epoch 120, Loss: 0.8120708755\n",
      "Epoch 121, Loss: 0.8447528109\n",
      "Epoch 122, Loss: 0.8118258729\n",
      "Epoch 123, Loss: 0.8755571999\n",
      "Epoch 124, Loss: 0.7834303516\n",
      "Epoch 125, Loss: 0.8430800028\n",
      "Epoch 126, Loss: 0.8560992582\n",
      "Epoch 127, Loss: 1.0709923846\n",
      "Epoch 128, Loss: 0.9414065128\n",
      "Epoch 129, Loss: 0.8745489928\n",
      "Epoch 130, Loss: 0.7921905736\n",
      "Epoch 131, Loss: 0.9324823580\n",
      "Epoch 132, Loss: 0.8605198351\n",
      "Epoch 133, Loss: 0.8241631510\n",
      "Epoch 134, Loss: 1.0559292673\n",
      "Epoch 135, Loss: 1.0805071820\n",
      "Epoch 136, Loss: 1.3074666249\n",
      "Epoch 137, Loss: 0.9014662143\n",
      "Epoch 138, Loss: 0.8915515832\n",
      "Epoch 139, Loss: 0.9424086619\n",
      "Epoch 140, Loss: 0.8962426266\n",
      "Epoch 141, Loss: 0.9264428769\n",
      "Epoch 142, Loss: 0.8570455310\n",
      "Epoch 143, Loss: 0.8048503413\n",
      "Epoch 144, Loss: 0.9141356860\n",
      "Epoch 145, Loss: 0.8776642073\n",
      "Epoch 146, Loss: 0.7928734717\n",
      "Epoch 147, Loss: 0.8386547796\n",
      "Epoch 148, Loss: 0.8510482788\n",
      "Epoch 149, Loss: 0.7701330019\n",
      "Epoch 150, Loss: 0.7928031690\n",
      "Epoch 151, Loss: 0.7570001577\n",
      "Epoch 152, Loss: 0.7716808321\n",
      "Epoch 153, Loss: 0.7510542542\n",
      "Epoch 154, Loss: 0.7724216493\n",
      "Epoch 155, Loss: 0.8040941081\n",
      "Epoch 156, Loss: 0.8104281524\n",
      "Epoch 157, Loss: 0.7868761890\n",
      "Epoch 158, Loss: 0.7659000216\n",
      "Epoch 159, Loss: 0.7644437984\n",
      "Epoch 160, Loss: 0.8075695563\n",
      "Epoch 161, Loss: 0.7240589016\n",
      "Epoch 162, Loss: 0.7637478495\n",
      "Epoch 163, Loss: 0.7481148250\n",
      "Epoch 164, Loss: 0.7244256628\n",
      "Epoch 165, Loss: 0.7356975824\n",
      "Epoch 166, Loss: 0.7546248597\n",
      "Epoch 167, Loss: 0.8046735628\n",
      "Epoch 168, Loss: 0.7563436614\n",
      "Epoch 169, Loss: 0.7233926772\n",
      "Epoch 170, Loss: 0.7439273644\n",
      "Epoch 171, Loss: 0.7950293301\n",
      "Epoch 172, Loss: 0.7592720598\n",
      "Epoch 173, Loss: 0.7179377964\n",
      "Epoch 174, Loss: 0.6829002721\n",
      "Epoch 175, Loss: 0.7025950029\n",
      "Epoch 176, Loss: 0.7215402003\n",
      "Epoch 177, Loss: 0.8091797235\n",
      "Epoch 178, Loss: 0.9046359796\n",
      "Epoch 179, Loss: 1.0421636258\n",
      "Epoch 180, Loss: 0.8553002614\n",
      "Epoch 181, Loss: 0.7887954175\n",
      "Epoch 182, Loss: 0.7876050499\n",
      "Epoch 183, Loss: 0.7077858734\n",
      "Epoch 184, Loss: 0.6538776018\n",
      "Epoch 185, Loss: 0.7010662134\n",
      "Epoch 186, Loss: 0.7196438111\n",
      "Epoch 187, Loss: 0.7466759295\n",
      "Epoch 188, Loss: 0.7484046438\n",
      "Epoch 189, Loss: 0.6981378341\n",
      "Epoch 190, Loss: 0.7056712351\n",
      "Epoch 191, Loss: 0.7531951026\n",
      "Epoch 192, Loss: 0.7475747945\n",
      "Epoch 193, Loss: 0.6742304509\n",
      "Epoch 194, Loss: 0.6692740350\n",
      "Epoch 195, Loss: 0.6306378624\n",
      "Epoch 196, Loss: 0.6281570862\n",
      "Epoch 197, Loss: 0.6373558599\n",
      "Epoch 198, Loss: 0.6608792186\n",
      "Epoch 199, Loss: 0.6936052832\n",
      "Epoch 200, Loss: 0.6889362518\n",
      "Epoch 201, Loss: 0.7236792967\n",
      "Epoch 202, Loss: 0.7739535165\n",
      "Epoch 203, Loss: 0.7570374658\n",
      "Epoch 204, Loss: 0.6818399734\n",
      "Epoch 205, Loss: 0.6405631822\n",
      "Epoch 206, Loss: 0.6598963093\n",
      "Epoch 207, Loss: 0.6568149279\n",
      "Epoch 208, Loss: 0.6412529721\n",
      "Epoch 209, Loss: 0.6050901712\n",
      "Epoch 210, Loss: 0.6308346790\n",
      "Epoch 211, Loss: 0.7243833328\n",
      "Epoch 212, Loss: 0.6529334710\n",
      "Epoch 213, Loss: 0.6002819071\n",
      "Epoch 214, Loss: 0.6091398433\n",
      "Epoch 215, Loss: 0.6122763151\n",
      "Epoch 216, Loss: 0.6562942850\n",
      "Epoch 217, Loss: 0.7190286932\n",
      "Epoch 218, Loss: 0.7610057630\n",
      "Epoch 219, Loss: 0.7117120355\n",
      "Epoch 220, Loss: 0.6100406223\n",
      "Epoch 221, Loss: 0.6116413042\n",
      "Epoch 222, Loss: 0.6829454961\n",
      "Epoch 223, Loss: 0.6134415513\n",
      "Epoch 224, Loss: 0.5822441970\n",
      "Epoch 225, Loss: 0.6163548368\n",
      "Epoch 226, Loss: 0.6181213888\n",
      "Epoch 227, Loss: 0.6817049907\n",
      "Epoch 228, Loss: 0.6906403588\n",
      "Epoch 229, Loss: 0.6231305831\n",
      "Epoch 230, Loss: 0.6759911581\n",
      "Epoch 231, Loss: 0.5910100309\n",
      "Epoch 232, Loss: 0.5726126910\n",
      "Epoch 233, Loss: 0.5609675941\n",
      "Epoch 234, Loss: 0.5870515338\n",
      "Epoch 235, Loss: 0.5502494463\n",
      "Epoch 236, Loss: 0.5639178475\n",
      "Epoch 237, Loss: 0.5356742257\n",
      "Epoch 238, Loss: 0.5351940023\n",
      "Epoch 239, Loss: 0.5523691822\n",
      "Epoch 240, Loss: 0.5332930895\n",
      "Epoch 241, Loss: 0.5215357713\n",
      "Epoch 242, Loss: 0.5287555849\n",
      "Epoch 243, Loss: 0.5357888088\n",
      "Epoch 244, Loss: 0.5250385956\n",
      "Epoch 245, Loss: 0.6122181784\n",
      "Epoch 246, Loss: 0.7241646810\n",
      "Epoch 247, Loss: 0.6191981165\n",
      "Epoch 248, Loss: 0.5748022045\n",
      "Epoch 249, Loss: 0.7145112639\n",
      "Epoch 250, Loss: 0.5988790699\n",
      "Epoch 251, Loss: 0.5469765254\n",
      "Epoch 252, Loss: 0.6227504300\n",
      "Epoch 253, Loss: 0.5487620597\n",
      "Epoch 254, Loss: 0.5648567277\n",
      "Epoch 255, Loss: 0.5354107302\n",
      "Epoch 256, Loss: 0.6493984205\n",
      "Epoch 257, Loss: 0.5946716873\n",
      "Epoch 258, Loss: 0.6273749809\n",
      "Epoch 259, Loss: 0.5466453943\n",
      "Epoch 260, Loss: 0.5539674594\n",
      "Epoch 261, Loss: 0.5744288981\n",
      "Epoch 262, Loss: 0.5444031415\n",
      "Epoch 263, Loss: 0.5750804953\n",
      "Epoch 264, Loss: 0.6349577575\n",
      "Epoch 265, Loss: 0.5820238551\n",
      "Epoch 266, Loss: 0.6797562798\n",
      "Epoch 267, Loss: 0.7105448858\n",
      "Epoch 268, Loss: 0.6431454565\n",
      "Epoch 269, Loss: 0.5622175343\n",
      "Epoch 270, Loss: 0.6335247777\n",
      "Epoch 271, Loss: 0.5775574664\n",
      "Epoch 272, Loss: 0.6002273379\n",
      "Epoch 273, Loss: 0.6335953455\n",
      "Epoch 274, Loss: 0.5757978097\n",
      "Epoch 275, Loss: 0.5087786989\n",
      "Epoch 276, Loss: 0.5141539664\n",
      "Epoch 277, Loss: 0.5421110307\n",
      "Epoch 278, Loss: 0.5074037976\n",
      "Epoch 279, Loss: 0.5375134298\n",
      "Epoch 280, Loss: 0.7291861730\n",
      "Epoch 281, Loss: 0.7288421920\n",
      "Epoch 282, Loss: 0.6567091393\n",
      "Epoch 283, Loss: 0.6068196553\n",
      "Epoch 284, Loss: 0.5954125411\n",
      "Epoch 285, Loss: 0.6319268765\n",
      "Epoch 286, Loss: 0.7567248962\n",
      "Epoch 287, Loss: 0.6365973460\n",
      "Epoch 288, Loss: 0.6129880577\n",
      "Epoch 289, Loss: 0.5349466708\n",
      "Epoch 290, Loss: 0.5651772126\n",
      "Epoch 291, Loss: 0.6890719902\n",
      "Epoch 292, Loss: 0.5844177053\n",
      "Epoch 293, Loss: 0.6063681028\n",
      "Epoch 294, Loss: 0.7475642057\n",
      "Epoch 295, Loss: 0.7737789086\n",
      "Epoch 296, Loss: 0.7531537684\n",
      "Epoch 297, Loss: 0.6267788416\n",
      "Epoch 298, Loss: 0.7563166364\n",
      "Epoch 299, Loss: 0.7236968978\n",
      "Epoch 300, Loss: 0.6769184185\n",
      "Epoch 301, Loss: 0.7214270624\n",
      "Epoch 302, Loss: 0.7548113988\n",
      "Epoch 303, Loss: 0.7825419195\n",
      "Epoch 304, Loss: 0.6555010306\n",
      "Epoch 305, Loss: 0.6986731963\n",
      "Epoch 306, Loss: 0.7195843099\n",
      "Epoch 307, Loss: 0.6517422800\n",
      "Epoch 308, Loss: 0.6941537697\n",
      "Epoch 309, Loss: 0.8771855461\n",
      "Epoch 310, Loss: 0.6495842461\n",
      "Epoch 311, Loss: 0.6817342755\n",
      "Epoch 312, Loss: 0.6506859288\n",
      "Epoch 313, Loss: 0.5423548598\n",
      "Epoch 314, Loss: 0.6029565633\n",
      "Epoch 315, Loss: 0.5715340396\n",
      "Epoch 316, Loss: 0.5882724062\n",
      "Epoch 317, Loss: 0.5775119144\n",
      "Epoch 318, Loss: 0.5897354359\n",
      "Epoch 319, Loss: 0.6213502686\n",
      "Epoch 320, Loss: 0.6344821289\n",
      "Epoch 321, Loss: 0.5484279329\n",
      "Epoch 322, Loss: 0.5418357886\n",
      "Epoch 323, Loss: 0.6087732332\n",
      "Epoch 324, Loss: 0.6212169861\n",
      "Epoch 325, Loss: 0.5532274488\n",
      "Epoch 326, Loss: 0.5262890001\n",
      "Epoch 327, Loss: 0.5420208120\n",
      "Epoch 328, Loss: 0.5784188473\n",
      "Epoch 329, Loss: 0.5447329873\n",
      "Epoch 330, Loss: 0.5707103285\n",
      "Epoch 331, Loss: 0.5129959065\n",
      "Epoch 332, Loss: 0.6137827496\n",
      "Epoch 333, Loss: 0.5647272938\n",
      "Epoch 334, Loss: 0.5781303946\n",
      "Epoch 335, Loss: 0.5658557261\n",
      "Epoch 336, Loss: 0.5880807417\n",
      "Epoch 337, Loss: 0.6503942133\n",
      "Epoch 338, Loss: 0.6713449572\n",
      "Epoch 339, Loss: 0.7170678827\n",
      "Epoch 340, Loss: 0.5674483333\n",
      "Epoch 341, Loss: 0.6084748739\n",
      "Epoch 342, Loss: 0.5244471928\n",
      "Epoch 343, Loss: 0.5830350295\n",
      "Epoch 344, Loss: 0.4907082169\n",
      "Epoch 345, Loss: 0.4801225269\n",
      "Epoch 346, Loss: 0.5222694663\n",
      "Epoch 347, Loss: 0.5059810177\n",
      "Epoch 348, Loss: 0.6147345757\n",
      "Epoch 349, Loss: 0.5621199506\n",
      "Epoch 350, Loss: 0.5797299913\n",
      "Epoch 351, Loss: 0.5109404116\n",
      "Epoch 352, Loss: 0.4678492648\n",
      "Epoch 353, Loss: 0.5267595465\n",
      "Epoch 354, Loss: 0.5670166476\n",
      "Epoch 355, Loss: 0.5562354896\n",
      "Epoch 356, Loss: 0.6424714128\n",
      "Epoch 357, Loss: 0.6193934118\n",
      "Epoch 358, Loss: 0.5726501612\n",
      "Epoch 359, Loss: 0.5069771302\n",
      "Epoch 360, Loss: 0.5393156884\n",
      "Epoch 361, Loss: 0.7217070355\n",
      "Epoch 362, Loss: 0.7490769298\n",
      "Epoch 363, Loss: 0.6666156413\n",
      "Epoch 364, Loss: 0.6366012310\n",
      "Epoch 365, Loss: 0.6309029084\n",
      "Epoch 366, Loss: 0.6357704638\n",
      "Epoch 367, Loss: 0.4728526025\n",
      "Epoch 368, Loss: 0.5494965295\n",
      "Epoch 369, Loss: 0.4715088917\n",
      "Epoch 370, Loss: 0.4721076223\n",
      "Epoch 371, Loss: 0.5560958564\n",
      "Epoch 372, Loss: 0.5758822405\n",
      "Epoch 373, Loss: 0.5045611651\n",
      "Epoch 374, Loss: 0.4433324874\n",
      "Epoch 375, Loss: 0.4353296161\n",
      "Epoch 376, Loss: 0.5498677800\n",
      "Epoch 377, Loss: 0.5004876072\n",
      "Epoch 378, Loss: 0.4969264370\n",
      "Epoch 379, Loss: 0.4808342540\n",
      "Epoch 380, Loss: 0.5583245688\n",
      "Epoch 381, Loss: 0.5721944761\n",
      "Epoch 382, Loss: 0.5718444632\n",
      "Epoch 383, Loss: 0.5390579267\n",
      "Epoch 384, Loss: 0.5883163278\n",
      "Epoch 385, Loss: 0.4918909630\n",
      "Epoch 386, Loss: 0.5198807019\n",
      "Epoch 387, Loss: 0.4568220861\n",
      "Epoch 388, Loss: 0.4706803788\n",
      "Epoch 389, Loss: 0.5110005231\n",
      "Epoch 390, Loss: 0.5449632434\n",
      "Epoch 391, Loss: 0.5422101004\n",
      "Epoch 392, Loss: 0.5269201422\n",
      "Epoch 393, Loss: 0.5173095090\n",
      "Epoch 394, Loss: 0.4833212536\n",
      "Epoch 395, Loss: 0.5560831974\n",
      "Epoch 396, Loss: 0.5421486558\n",
      "Epoch 397, Loss: 0.5428458877\n",
      "Epoch 398, Loss: 0.6494993542\n",
      "Epoch 399, Loss: 0.5802742987\n",
      "Epoch 400, Loss: 0.5479272392\n",
      "Epoch 401, Loss: 0.5173204945\n",
      "Epoch 402, Loss: 0.4826386150\n",
      "Epoch 403, Loss: 0.4347997259\n",
      "Epoch 404, Loss: 0.4296271944\n",
      "Epoch 405, Loss: 0.4111765381\n",
      "Epoch 406, Loss: 0.4457662358\n",
      "Epoch 407, Loss: 0.4499627112\n",
      "Epoch 408, Loss: 0.4729175696\n",
      "Epoch 409, Loss: 0.4785176115\n",
      "Epoch 410, Loss: 0.4525778244\n",
      "Epoch 411, Loss: 0.4322700964\n",
      "Epoch 412, Loss: 0.4172222252\n",
      "Epoch 413, Loss: 0.4098339984\n",
      "Epoch 414, Loss: 0.4069475689\n",
      "Epoch 415, Loss: 0.4722953243\n",
      "Epoch 416, Loss: 0.4820718609\n",
      "Epoch 417, Loss: 0.4650817450\n",
      "Epoch 418, Loss: 0.4896318350\n",
      "Epoch 419, Loss: 0.5230444470\n",
      "Epoch 420, Loss: 0.4452741431\n",
      "Epoch 421, Loss: 0.4733325715\n",
      "Epoch 422, Loss: 0.4911526546\n",
      "Epoch 423, Loss: 0.4360469821\n",
      "Epoch 424, Loss: 0.3844067124\n",
      "Epoch 425, Loss: 0.4167715844\n",
      "Epoch 426, Loss: 0.4529745260\n",
      "Epoch 427, Loss: 0.5993345799\n",
      "Epoch 428, Loss: 0.4149383653\n",
      "Epoch 429, Loss: 0.4564215546\n",
      "Epoch 430, Loss: 0.4944665591\n",
      "Epoch 431, Loss: 0.4389206524\n",
      "Epoch 432, Loss: 0.5263049641\n",
      "Epoch 433, Loss: 0.6992725238\n",
      "Epoch 434, Loss: 0.4624860379\n",
      "Epoch 435, Loss: 0.5308796160\n",
      "Epoch 436, Loss: 0.6145303828\n",
      "Epoch 437, Loss: 0.5719188960\n",
      "Epoch 438, Loss: 0.4544459413\n",
      "Epoch 439, Loss: 0.3971647277\n",
      "Epoch 440, Loss: 0.4380001472\n",
      "Epoch 441, Loss: 0.4464667501\n",
      "Epoch 442, Loss: 0.5228451190\n",
      "Epoch 443, Loss: 0.4623155044\n",
      "Epoch 444, Loss: 0.4760864456\n",
      "Epoch 445, Loss: 0.4959231323\n",
      "Epoch 446, Loss: 0.5930812485\n",
      "Epoch 447, Loss: 0.5482698778\n",
      "Epoch 448, Loss: 0.5387407472\n",
      "Epoch 449, Loss: 0.5065991720\n",
      "Epoch 450, Loss: 0.5245630248\n",
      "Epoch 451, Loss: 0.5417737442\n",
      "Epoch 452, Loss: 0.5232233201\n",
      "Epoch 453, Loss: 0.5931948053\n",
      "Epoch 454, Loss: 0.5307912181\n",
      "Epoch 455, Loss: 0.5158885015\n",
      "Epoch 456, Loss: 0.5843830779\n",
      "Epoch 457, Loss: 0.4803091652\n",
      "Epoch 458, Loss: 0.4970613117\n",
      "Epoch 459, Loss: 0.4497972069\n",
      "Epoch 460, Loss: 0.4676430090\n",
      "Epoch 461, Loss: 0.4609687077\n",
      "Epoch 462, Loss: 0.7027643277\n",
      "Epoch 463, Loss: 0.7532672685\n",
      "Epoch 464, Loss: 0.5716969286\n",
      "Epoch 465, Loss: 0.4649403941\n",
      "Epoch 466, Loss: 0.4386087363\n",
      "Epoch 467, Loss: 0.4488867728\n",
      "Epoch 468, Loss: 0.4971440549\n",
      "Epoch 469, Loss: 0.5149269425\n",
      "Epoch 470, Loss: 0.4888379789\n",
      "Epoch 471, Loss: 0.5029469887\n",
      "Epoch 472, Loss: 0.5184252620\n",
      "Epoch 473, Loss: 0.3689829408\n",
      "Epoch 474, Loss: 0.4135800715\n",
      "Epoch 475, Loss: 0.4710031794\n",
      "Epoch 476, Loss: 0.6320668273\n",
      "Epoch 477, Loss: 0.8156749039\n",
      "Epoch 478, Loss: 0.4868077527\n",
      "Epoch 479, Loss: 0.5252143191\n",
      "Epoch 480, Loss: 0.5329438692\n",
      "Epoch 481, Loss: 0.4670417990\n",
      "Epoch 482, Loss: 0.4843934335\n",
      "Epoch 483, Loss: 0.3989624805\n",
      "Epoch 484, Loss: 0.3507512459\n",
      "Epoch 485, Loss: 0.3351785816\n",
      "Epoch 486, Loss: 0.3960102946\n",
      "Epoch 487, Loss: 0.4546415564\n",
      "Epoch 488, Loss: 0.5697588171\n",
      "Epoch 489, Loss: 0.5719713488\n",
      "Epoch 490, Loss: 0.5299722812\n",
      "Epoch 491, Loss: 0.4843191104\n",
      "Epoch 492, Loss: 0.3761069174\n",
      "Epoch 493, Loss: 0.3512269282\n",
      "Epoch 494, Loss: 0.4266524006\n",
      "Epoch 495, Loss: 0.4861723816\n",
      "Epoch 496, Loss: 0.5481823452\n",
      "Epoch 497, Loss: 0.5951976013\n",
      "Epoch 498, Loss: 0.7921021237\n",
      "Epoch 499, Loss: 0.4397481441\n",
      "Epoch 500, Loss: 0.4518949946\n",
      "Epoch 501, Loss: 0.4976727299\n",
      "Epoch 502, Loss: 0.4913211030\n",
      "Epoch 503, Loss: 0.4731505733\n",
      "Epoch 504, Loss: 0.3941559240\n",
      "Epoch 505, Loss: 0.3331357397\n",
      "Epoch 506, Loss: 0.3879685066\n",
      "Epoch 507, Loss: 0.4660674684\n",
      "Epoch 508, Loss: 0.5127252177\n",
      "Epoch 509, Loss: 0.4478299794\n",
      "Epoch 510, Loss: 0.3615042159\n",
      "Epoch 511, Loss: 0.3533562166\n",
      "Epoch 512, Loss: 0.3522149902\n",
      "Epoch 513, Loss: 0.3802026437\n",
      "Epoch 514, Loss: 0.5112322082\n",
      "Epoch 515, Loss: 0.6271735183\n",
      "Epoch 516, Loss: 0.5757278809\n",
      "Epoch 517, Loss: 0.4087894521\n",
      "Epoch 518, Loss: 0.4104569631\n",
      "Epoch 519, Loss: 0.3344393921\n",
      "Epoch 520, Loss: 0.5097069949\n",
      "Epoch 521, Loss: 0.4621299250\n",
      "Epoch 522, Loss: 0.7257804998\n",
      "Epoch 523, Loss: 0.5383291357\n",
      "Epoch 524, Loss: 0.4766453675\n",
      "Epoch 525, Loss: 0.5167084600\n",
      "Epoch 526, Loss: 0.6832445476\n",
      "Epoch 527, Loss: 0.6274955017\n",
      "Epoch 528, Loss: 0.4436089852\n",
      "Epoch 529, Loss: 0.4669685059\n",
      "Epoch 530, Loss: 0.4635271406\n",
      "Epoch 531, Loss: 0.4909718701\n",
      "Epoch 532, Loss: 0.5150464170\n",
      "Epoch 533, Loss: 0.4780259869\n",
      "Epoch 534, Loss: 0.4213233785\n",
      "Epoch 535, Loss: 0.5887139366\n",
      "Epoch 536, Loss: 0.5420204977\n",
      "Epoch 537, Loss: 0.4334584461\n",
      "Epoch 538, Loss: 0.4883470571\n",
      "Epoch 539, Loss: 0.7523431982\n",
      "Epoch 540, Loss: 0.6655717460\n",
      "Epoch 541, Loss: 0.6296581281\n",
      "Epoch 542, Loss: 0.4440026369\n",
      "Epoch 543, Loss: 0.5725936328\n",
      "Epoch 544, Loss: 0.4659575162\n",
      "Epoch 545, Loss: 0.4726746590\n",
      "Epoch 546, Loss: 0.6001952741\n",
      "Epoch 547, Loss: 0.6922488847\n",
      "Epoch 548, Loss: 0.4790560637\n",
      "Epoch 549, Loss: 0.5036221893\n",
      "Epoch 550, Loss: 0.5220726758\n",
      "Epoch 551, Loss: 0.5654569723\n",
      "Epoch 552, Loss: 0.5101278315\n",
      "Epoch 553, Loss: 0.5371944299\n",
      "Epoch 554, Loss: 0.5451292583\n",
      "Epoch 555, Loss: 0.5242683859\n",
      "Epoch 556, Loss: 0.4018966150\n",
      "Epoch 557, Loss: 0.5320238044\n",
      "Epoch 558, Loss: 0.5194363281\n",
      "Epoch 559, Loss: 0.4471963687\n",
      "Epoch 560, Loss: 0.6267201444\n",
      "Epoch 561, Loss: 0.4470139832\n",
      "Epoch 562, Loss: 0.4902348904\n",
      "Epoch 563, Loss: 0.5097284023\n",
      "Epoch 564, Loss: 0.4936074864\n",
      "Epoch 565, Loss: 0.6941467544\n",
      "Epoch 566, Loss: 0.6049079287\n",
      "Epoch 567, Loss: 0.6631629141\n",
      "Epoch 568, Loss: 0.6426481531\n",
      "Epoch 569, Loss: 0.5281351243\n",
      "Epoch 570, Loss: 0.4172136311\n",
      "Epoch 571, Loss: 0.4436915720\n",
      "Epoch 572, Loss: 0.5922537478\n",
      "Epoch 573, Loss: 0.4307818448\n",
      "Epoch 574, Loss: 0.4185616027\n",
      "Epoch 575, Loss: 0.5609873379\n",
      "Epoch 576, Loss: 0.3563382423\n",
      "Epoch 577, Loss: 0.4761126401\n",
      "Epoch 578, Loss: 0.3424853875\n",
      "Epoch 579, Loss: 0.4008155454\n",
      "Epoch 580, Loss: 0.4546643835\n",
      "Epoch 581, Loss: 0.4828644121\n",
      "Epoch 582, Loss: 0.3905972997\n",
      "Epoch 583, Loss: 0.3863092501\n",
      "Epoch 584, Loss: 0.4176026331\n",
      "Epoch 585, Loss: 0.3473502691\n",
      "Epoch 586, Loss: 0.3032774289\n",
      "Epoch 587, Loss: 0.3461014571\n",
      "Epoch 588, Loss: 0.3382570318\n",
      "Epoch 589, Loss: 0.3896961326\n",
      "Epoch 590, Loss: 0.3675544984\n",
      "Epoch 591, Loss: 0.3824944087\n",
      "Epoch 592, Loss: 0.3913535928\n",
      "Epoch 593, Loss: 0.3485559430\n",
      "Epoch 594, Loss: 0.3046008866\n",
      "Epoch 595, Loss: 0.3458621115\n",
      "Epoch 596, Loss: 0.3675397619\n",
      "Epoch 597, Loss: 0.3565722065\n",
      "Epoch 598, Loss: 0.4987431528\n",
      "Epoch 599, Loss: 0.4107139497\n",
      "Epoch 600, Loss: 0.4689078062\n",
      "Epoch 601, Loss: 0.3684897571\n",
      "Epoch 602, Loss: 0.3261520704\n",
      "Epoch 603, Loss: 0.3189146390\n",
      "Epoch 604, Loss: 0.3822679840\n",
      "Epoch 605, Loss: 0.3315524444\n",
      "Epoch 606, Loss: 0.2784341699\n",
      "Epoch 607, Loss: 0.2696519096\n",
      "Epoch 608, Loss: 0.2659908260\n",
      "Epoch 609, Loss: 0.2404003082\n",
      "Epoch 610, Loss: 0.2714077857\n",
      "Epoch 611, Loss: 0.2646422226\n",
      "Epoch 612, Loss: 0.2905587388\n",
      "Epoch 613, Loss: 0.3489157552\n",
      "Epoch 614, Loss: 0.3275586013\n",
      "Epoch 615, Loss: 0.2988643684\n",
      "Epoch 616, Loss: 0.3423596587\n",
      "Epoch 617, Loss: 0.3402368081\n",
      "Epoch 618, Loss: 0.2844806743\n",
      "Epoch 619, Loss: 0.2373092691\n",
      "Epoch 620, Loss: 0.2363155027\n",
      "Epoch 621, Loss: 0.2688230458\n",
      "Epoch 622, Loss: 0.2695128261\n",
      "Epoch 623, Loss: 0.2317642576\n",
      "Epoch 624, Loss: 0.2198211316\n",
      "Epoch 625, Loss: 0.2627870643\n",
      "Epoch 626, Loss: 0.2843591081\n",
      "Epoch 627, Loss: 0.2774763199\n",
      "Epoch 628, Loss: 0.2816440507\n",
      "Epoch 629, Loss: 0.3412421499\n",
      "Epoch 630, Loss: 0.2976011415\n",
      "Epoch 631, Loss: 0.2775938518\n",
      "Epoch 632, Loss: 0.2975659912\n",
      "Epoch 633, Loss: 0.2737438023\n",
      "Epoch 634, Loss: 0.2467252169\n",
      "Epoch 635, Loss: 0.2206883992\n",
      "Epoch 636, Loss: 0.2497600650\n",
      "Epoch 637, Loss: 0.2431405089\n",
      "Epoch 638, Loss: 0.2414529130\n",
      "Epoch 639, Loss: 0.2606660909\n",
      "Epoch 640, Loss: 0.2919547105\n",
      "Epoch 641, Loss: 0.3643264110\n",
      "Epoch 642, Loss: 0.4006483238\n",
      "Epoch 643, Loss: 0.3452479840\n",
      "Epoch 644, Loss: 0.2527580920\n",
      "Epoch 645, Loss: 0.3695454018\n",
      "Epoch 646, Loss: 0.3334042205\n",
      "Epoch 647, Loss: 0.3016587512\n",
      "Epoch 648, Loss: 0.2599015527\n",
      "Epoch 649, Loss: 0.2047809866\n",
      "Epoch 650, Loss: 0.2022238773\n",
      "Epoch 651, Loss: 0.2109263732\n",
      "Epoch 652, Loss: 0.2284489624\n",
      "Epoch 653, Loss: 0.1942937333\n",
      "Epoch 654, Loss: 0.2104484243\n",
      "Epoch 655, Loss: 0.2542449135\n",
      "Epoch 656, Loss: 0.2902816535\n",
      "Epoch 657, Loss: 0.3403393973\n",
      "Epoch 658, Loss: 0.3667487786\n",
      "Epoch 659, Loss: 0.4793161849\n",
      "Epoch 660, Loss: 0.2789636072\n",
      "Epoch 661, Loss: 0.2722981584\n",
      "Epoch 662, Loss: 0.2745789635\n",
      "Epoch 663, Loss: 0.3179974148\n",
      "Epoch 664, Loss: 0.2697798802\n",
      "Epoch 665, Loss: 0.2923714978\n",
      "Epoch 666, Loss: 0.3633995749\n",
      "Epoch 667, Loss: 0.3325982177\n",
      "Epoch 668, Loss: 0.2665218657\n",
      "Epoch 669, Loss: 0.2329930362\n",
      "Epoch 670, Loss: 0.2565793530\n",
      "Epoch 671, Loss: 0.3455555998\n",
      "Epoch 672, Loss: 0.2937220082\n",
      "Epoch 673, Loss: 0.2395779926\n",
      "Epoch 674, Loss: 0.2794818656\n",
      "Epoch 675, Loss: 0.2804967222\n",
      "Epoch 676, Loss: 0.3562532781\n",
      "Epoch 677, Loss: 0.3361826154\n",
      "Epoch 678, Loss: 0.2905769888\n",
      "Epoch 679, Loss: 0.3452794035\n",
      "Epoch 680, Loss: 0.3046534467\n",
      "Epoch 681, Loss: 0.3002630100\n",
      "Epoch 682, Loss: 0.2248474117\n",
      "Epoch 683, Loss: 0.2576963521\n",
      "Epoch 684, Loss: 0.2194621924\n",
      "Epoch 685, Loss: 0.2148164954\n",
      "Epoch 686, Loss: 0.2461599816\n",
      "Epoch 687, Loss: 0.2879592533\n",
      "Epoch 688, Loss: 0.2883456834\n",
      "Epoch 689, Loss: 0.2586661005\n",
      "Epoch 690, Loss: 0.2412713871\n",
      "Epoch 691, Loss: 0.2667554185\n",
      "Epoch 692, Loss: 0.2644208519\n",
      "Epoch 693, Loss: 0.2087064390\n",
      "Epoch 694, Loss: 0.4390970675\n",
      "Epoch 695, Loss: 0.4339151054\n",
      "Epoch 696, Loss: 0.4446207351\n",
      "Epoch 697, Loss: 0.4790511488\n",
      "Epoch 698, Loss: 0.3406814234\n",
      "Epoch 699, Loss: 0.2445894654\n",
      "Epoch 700, Loss: 0.2128427616\n",
      "Epoch 701, Loss: 0.3032405007\n",
      "Epoch 702, Loss: 0.3832462593\n",
      "Epoch 703, Loss: 0.4878373303\n",
      "Epoch 704, Loss: 0.4265809201\n",
      "Epoch 705, Loss: 0.2698181439\n",
      "Epoch 706, Loss: 0.4056386825\n",
      "Epoch 707, Loss: 0.2936238717\n",
      "Epoch 708, Loss: 0.2318609152\n",
      "Epoch 709, Loss: 0.2568702093\n",
      "Epoch 710, Loss: 0.2655156386\n",
      "Epoch 711, Loss: 0.1929544893\n",
      "Epoch 712, Loss: 0.2518824050\n",
      "Epoch 713, Loss: 0.2888652194\n",
      "Epoch 714, Loss: 0.2608153477\n",
      "Epoch 715, Loss: 0.2728098160\n",
      "Epoch 716, Loss: 0.3072724212\n",
      "Epoch 717, Loss: 0.3563456991\n",
      "Epoch 718, Loss: 0.3260194081\n",
      "Epoch 719, Loss: 0.2655005922\n",
      "Epoch 720, Loss: 0.2518994001\n",
      "Epoch 721, Loss: 0.2114011648\n",
      "Epoch 722, Loss: 0.2890782593\n",
      "Epoch 723, Loss: 0.3197158476\n",
      "Epoch 724, Loss: 0.2437054667\n",
      "Epoch 725, Loss: 0.2591694307\n",
      "Epoch 726, Loss: 0.2558508604\n",
      "Epoch 727, Loss: 0.4423489938\n",
      "Epoch 728, Loss: 0.5385396289\n",
      "Epoch 729, Loss: 0.4023714538\n",
      "Epoch 730, Loss: 0.2220952037\n",
      "Epoch 731, Loss: 0.4259221704\n",
      "Epoch 732, Loss: 0.3827008978\n",
      "Epoch 733, Loss: 0.5440399668\n",
      "Epoch 734, Loss: 0.4963198252\n",
      "Epoch 735, Loss: 0.3622657776\n",
      "Epoch 736, Loss: 0.2839177516\n",
      "Epoch 737, Loss: 0.2894335883\n",
      "Epoch 738, Loss: 0.2208849352\n",
      "Epoch 739, Loss: 0.1741091269\n",
      "Epoch 740, Loss: 0.1737800700\n",
      "Epoch 741, Loss: 0.1965647761\n",
      "Epoch 742, Loss: 0.1782335841\n",
      "Epoch 743, Loss: 0.1815795250\n",
      "Epoch 744, Loss: 0.1449792709\n",
      "Epoch 745, Loss: 0.1523826120\n",
      "Epoch 746, Loss: 0.1708928874\n",
      "Epoch 747, Loss: 0.1599345859\n",
      "Epoch 748, Loss: 0.2054051749\n",
      "Epoch 749, Loss: 0.1708499755\n",
      "Epoch 750, Loss: 0.1673014089\n",
      "Epoch 751, Loss: 0.1672118512\n",
      "Epoch 752, Loss: 0.1854036439\n",
      "Epoch 753, Loss: 0.2053186897\n",
      "Epoch 754, Loss: 0.1676356111\n",
      "Epoch 755, Loss: 0.1527501115\n",
      "Epoch 756, Loss: 0.1843555269\n",
      "Epoch 757, Loss: 0.2202920992\n",
      "Epoch 758, Loss: 0.2359920687\n",
      "Epoch 759, Loss: 0.2714063646\n",
      "Epoch 760, Loss: 0.3840053548\n",
      "Epoch 761, Loss: 0.7123268565\n",
      "Epoch 762, Loss: 0.3190222235\n",
      "Epoch 763, Loss: 0.3320838140\n",
      "Epoch 764, Loss: 0.4791984859\n",
      "Epoch 765, Loss: 0.3706296284\n",
      "Epoch 766, Loss: 0.2677812185\n",
      "Epoch 767, Loss: 0.2232069237\n",
      "Epoch 768, Loss: 0.2015079020\n",
      "Epoch 769, Loss: 0.2878028472\n",
      "Epoch 770, Loss: 0.1972863782\n",
      "Epoch 771, Loss: 0.2393013168\n",
      "Epoch 772, Loss: 0.1837968064\n",
      "Epoch 773, Loss: 0.1521655933\n",
      "Epoch 774, Loss: 0.1743628200\n",
      "Epoch 775, Loss: 0.2408532293\n",
      "Epoch 776, Loss: 0.1960458654\n",
      "Epoch 777, Loss: 0.1787151465\n",
      "Epoch 778, Loss: 0.1785985274\n",
      "Epoch 779, Loss: 0.1812216414\n",
      "Epoch 780, Loss: 0.1965961137\n",
      "Epoch 781, Loss: 0.2308176127\n",
      "Epoch 782, Loss: 0.2145697523\n",
      "Epoch 783, Loss: 0.2023466675\n",
      "Epoch 784, Loss: 0.4389938484\n",
      "Epoch 785, Loss: 0.3835991084\n",
      "Epoch 786, Loss: 0.4988066130\n",
      "Epoch 787, Loss: 0.2060916461\n",
      "Epoch 788, Loss: 0.2133078744\n",
      "Epoch 789, Loss: 0.2899553080\n",
      "Epoch 790, Loss: 0.1699720358\n",
      "Epoch 791, Loss: 0.1659490395\n",
      "Epoch 792, Loss: 0.1704091712\n",
      "Epoch 793, Loss: 0.1966855149\n",
      "Epoch 794, Loss: 0.1622682980\n",
      "Epoch 795, Loss: 0.1987254171\n",
      "Epoch 796, Loss: 0.2090790310\n",
      "Epoch 797, Loss: 0.2299388120\n",
      "Epoch 798, Loss: 0.2235157190\n",
      "Epoch 799, Loss: 0.2231486244\n",
      "Epoch 800, Loss: 0.1879886148\n",
      "Epoch 801, Loss: 0.3039228270\n",
      "Epoch 802, Loss: 0.2505040467\n",
      "Epoch 803, Loss: 0.3343937728\n",
      "Epoch 804, Loss: 0.2953012630\n",
      "Epoch 805, Loss: 0.2387071305\n",
      "Epoch 806, Loss: 0.2442732135\n",
      "Epoch 807, Loss: 0.2477418110\n",
      "Epoch 808, Loss: 0.2549675551\n",
      "Epoch 809, Loss: 0.2362701915\n",
      "Epoch 810, Loss: 0.2404211579\n",
      "Epoch 811, Loss: 0.2291329470\n",
      "Epoch 812, Loss: 0.2789122543\n",
      "Epoch 813, Loss: 0.3154244356\n",
      "Epoch 814, Loss: 0.3268523048\n",
      "Epoch 815, Loss: 0.3568423126\n",
      "Epoch 816, Loss: 0.5167901866\n",
      "Epoch 817, Loss: 0.3200921061\n",
      "Epoch 818, Loss: 0.2167025598\n",
      "Epoch 819, Loss: 0.2369281339\n",
      "Epoch 820, Loss: 0.2542178980\n",
      "Epoch 821, Loss: 0.2502899906\n",
      "Epoch 822, Loss: 0.2057041191\n",
      "Epoch 823, Loss: 0.2929555929\n",
      "Epoch 824, Loss: 0.5052239608\n",
      "Epoch 825, Loss: 0.2727785626\n",
      "Epoch 826, Loss: 0.2711482178\n",
      "Epoch 827, Loss: 0.2067194060\n",
      "Epoch 828, Loss: 0.2175477698\n",
      "Epoch 829, Loss: 0.2521716090\n",
      "Epoch 830, Loss: 0.2961105450\n",
      "Epoch 831, Loss: 0.3237388421\n",
      "Epoch 832, Loss: 0.3426411481\n",
      "Epoch 833, Loss: 0.2616559208\n",
      "Epoch 834, Loss: 0.2411829959\n",
      "Epoch 835, Loss: 0.3125728309\n",
      "Epoch 836, Loss: 0.3046915967\n",
      "Epoch 837, Loss: 0.2989512841\n",
      "Epoch 838, Loss: 0.2913477371\n",
      "Epoch 839, Loss: 0.2496911434\n",
      "Epoch 840, Loss: 0.2965519028\n",
      "Epoch 841, Loss: 0.1844409564\n",
      "Epoch 842, Loss: 0.2557861084\n",
      "Epoch 843, Loss: 0.2491979273\n",
      "Epoch 844, Loss: 0.2425108116\n",
      "Epoch 845, Loss: 0.3229373062\n",
      "Epoch 846, Loss: 0.3058940136\n",
      "Epoch 847, Loss: 0.1624516649\n",
      "Epoch 848, Loss: 0.2123478247\n",
      "Epoch 849, Loss: 0.1759494029\n",
      "Epoch 850, Loss: 0.1932092530\n",
      "Epoch 851, Loss: 0.2169071614\n",
      "Epoch 852, Loss: 0.1694799202\n",
      "Epoch 853, Loss: 0.1764971791\n",
      "Epoch 854, Loss: 0.2238258460\n",
      "Epoch 855, Loss: 0.2307421692\n",
      "Epoch 856, Loss: 0.2759294556\n",
      "Epoch 857, Loss: 0.2244457799\n",
      "Epoch 858, Loss: 0.2848405474\n",
      "Epoch 859, Loss: 0.2188195043\n",
      "Epoch 860, Loss: 0.2085945911\n",
      "Epoch 861, Loss: 0.3275749036\n",
      "Epoch 862, Loss: 0.3696598766\n",
      "Epoch 863, Loss: 0.3313495944\n",
      "Epoch 864, Loss: 0.3541257783\n",
      "Epoch 865, Loss: 0.3301555140\n",
      "Epoch 866, Loss: 0.7291999826\n",
      "Epoch 867, Loss: 0.5244322654\n",
      "Epoch 868, Loss: 0.2565183763\n",
      "Epoch 869, Loss: 0.2006486960\n",
      "Epoch 870, Loss: 0.2784330202\n",
      "Epoch 871, Loss: 0.4570544443\n",
      "Epoch 872, Loss: 0.2715701362\n",
      "Epoch 873, Loss: 0.2492855126\n",
      "Epoch 874, Loss: 0.2701072191\n",
      "Epoch 875, Loss: 0.2528690570\n",
      "Epoch 876, Loss: 0.1776553888\n",
      "Epoch 877, Loss: 0.3477065423\n",
      "Epoch 878, Loss: 0.2248059114\n",
      "Epoch 879, Loss: 0.2723869697\n",
      "Epoch 880, Loss: 0.2635564757\n",
      "Epoch 881, Loss: 0.3035015541\n",
      "Epoch 882, Loss: 0.3664935704\n",
      "Epoch 883, Loss: 0.3833088911\n",
      "Epoch 884, Loss: 0.1915233122\n",
      "Epoch 885, Loss: 0.2924633397\n",
      "Epoch 886, Loss: 0.3681659491\n",
      "Epoch 887, Loss: 0.4260647926\n",
      "Epoch 888, Loss: 0.2735101436\n",
      "Epoch 889, Loss: 0.2308534242\n",
      "Epoch 890, Loss: 0.2425685865\n",
      "Epoch 891, Loss: 0.2436108314\n",
      "Epoch 892, Loss: 0.2461152745\n",
      "Epoch 893, Loss: 0.2509637192\n",
      "Epoch 894, Loss: 0.2108816152\n",
      "Epoch 895, Loss: 0.3151022935\n",
      "Epoch 896, Loss: 0.2649407806\n",
      "Epoch 897, Loss: 0.1882476888\n",
      "Epoch 898, Loss: 0.3812179202\n",
      "Epoch 899, Loss: 0.2589757894\n",
      "Epoch 900, Loss: 0.3277317670\n",
      "Epoch 901, Loss: 0.3564225326\n",
      "Epoch 902, Loss: 0.4739409776\n",
      "Epoch 903, Loss: 0.3363407900\n",
      "Epoch 904, Loss: 0.2755963861\n",
      "Epoch 905, Loss: 0.1771686276\n",
      "Epoch 906, Loss: 0.2409816428\n",
      "Epoch 907, Loss: 0.3121526431\n",
      "Epoch 908, Loss: 0.1751212721\n",
      "Epoch 909, Loss: 0.1536346350\n",
      "Epoch 910, Loss: 0.2780920582\n",
      "Epoch 911, Loss: 0.2476660835\n",
      "Epoch 912, Loss: 0.2466079766\n",
      "Epoch 913, Loss: 0.3453857071\n",
      "Epoch 914, Loss: 0.3050725802\n",
      "Epoch 915, Loss: 0.2020806256\n",
      "Epoch 916, Loss: 0.2255460129\n",
      "Epoch 917, Loss: 0.2365661880\n",
      "Epoch 918, Loss: 0.2411868356\n",
      "Epoch 919, Loss: 0.3234529304\n",
      "Epoch 920, Loss: 0.2924810131\n",
      "Epoch 921, Loss: 0.3898007022\n",
      "Epoch 922, Loss: 0.4242909982\n",
      "Epoch 923, Loss: 0.3964374652\n",
      "Epoch 924, Loss: 0.2899524509\n",
      "Epoch 925, Loss: 0.2026976858\n",
      "Epoch 926, Loss: 0.1907020533\n",
      "Epoch 927, Loss: 0.2513488820\n",
      "Epoch 928, Loss: 0.2022387463\n",
      "Epoch 929, Loss: 0.2525949535\n",
      "Epoch 930, Loss: 0.2146686504\n",
      "Epoch 931, Loss: 0.1904102846\n",
      "Epoch 932, Loss: 0.1648050638\n",
      "Epoch 933, Loss: 0.3054404989\n",
      "Epoch 934, Loss: 0.3183328788\n",
      "Epoch 935, Loss: 0.2654527323\n",
      "Epoch 936, Loss: 0.1710903432\n",
      "Epoch 937, Loss: 0.3808534536\n",
      "Epoch 938, Loss: 0.2353022579\n",
      "Epoch 939, Loss: 0.4005640726\n",
      "Epoch 940, Loss: 0.4174748847\n",
      "Epoch 941, Loss: 0.3020670080\n",
      "Epoch 942, Loss: 0.3307648238\n",
      "Epoch 943, Loss: 0.3041211202\n",
      "Epoch 944, Loss: 0.2476035959\n",
      "Epoch 945, Loss: 0.1838128449\n",
      "Epoch 946, Loss: 0.1999124077\n",
      "Epoch 947, Loss: 0.3638716303\n",
      "Epoch 948, Loss: 0.1491763897\n",
      "Epoch 949, Loss: 0.2555664011\n",
      "Epoch 950, Loss: 0.2603437810\n",
      "Epoch 951, Loss: 0.2112590001\n",
      "Epoch 952, Loss: 0.1825344694\n",
      "Epoch 953, Loss: 0.4437594057\n",
      "Epoch 954, Loss: 0.3983386838\n",
      "Epoch 955, Loss: 0.4005828577\n",
      "Epoch 956, Loss: 0.2970770351\n",
      "Epoch 957, Loss: 0.4627920199\n",
      "Epoch 958, Loss: 0.2888752703\n",
      "Epoch 959, Loss: 0.1911633913\n",
      "Epoch 960, Loss: 0.3030312014\n",
      "Epoch 961, Loss: 0.2411903985\n",
      "Epoch 962, Loss: 0.3434279567\n",
      "Epoch 963, Loss: 0.3255600083\n",
      "Epoch 964, Loss: 0.6058566981\n",
      "Epoch 965, Loss: 0.4826178132\n",
      "Epoch 966, Loss: 0.2943266324\n",
      "Epoch 967, Loss: 0.2869506112\n",
      "Epoch 968, Loss: 0.6020007148\n",
      "Epoch 969, Loss: 0.6426803841\n",
      "Epoch 970, Loss: 0.8048737000\n",
      "Epoch 971, Loss: 0.5439653755\n",
      "Epoch 972, Loss: 0.3535866372\n",
      "Epoch 973, Loss: 0.6514792709\n",
      "Epoch 974, Loss: 0.3587765900\n",
      "Epoch 975, Loss: 0.2742165493\n",
      "Epoch 976, Loss: 0.2399165006\n",
      "Epoch 977, Loss: 0.3114049732\n",
      "Epoch 978, Loss: 0.3301518591\n",
      "Epoch 979, Loss: 0.3020188376\n",
      "Epoch 980, Loss: 0.3588025642\n",
      "Epoch 981, Loss: 0.3024295526\n",
      "Epoch 982, Loss: 0.2362453976\n",
      "Epoch 983, Loss: 0.2989264948\n",
      "Epoch 984, Loss: 0.2546574841\n",
      "Epoch 985, Loss: 0.1841389515\n",
      "Epoch 986, Loss: 0.1957146156\n",
      "Epoch 987, Loss: 0.2143438049\n",
      "Epoch 988, Loss: 0.1953467447\n",
      "Epoch 989, Loss: 0.2123346023\n",
      "Epoch 990, Loss: 0.2984533761\n",
      "Epoch 991, Loss: 0.2412997331\n",
      "Epoch 992, Loss: 0.2361840092\n",
      "Epoch 993, Loss: 0.1963707823\n",
      "Epoch 994, Loss: 0.1615520111\n",
      "Epoch 995, Loss: 0.2361296087\n",
      "Epoch 996, Loss: 0.2020463093\n",
      "Epoch 997, Loss: 0.2572450056\n",
      "Epoch 998, Loss: 0.2326126901\n",
      "Epoch 999, Loss: 0.3297519387\n",
      "Epoch 1000, Loss: 0.2566428804\n",
      "Epoch 1001, Loss: 0.1670979704\n",
      "Epoch 1002, Loss: 0.2835551936\n",
      "Epoch 1003, Loss: 0.4116452892\n",
      "Epoch 1004, Loss: 0.4026948335\n",
      "Epoch 1005, Loss: 0.2393980726\n",
      "Epoch 1006, Loss: 0.2007808874\n",
      "Epoch 1007, Loss: 0.2040959245\n",
      "Epoch 1008, Loss: 0.3473553219\n",
      "Epoch 1009, Loss: 0.2571408184\n",
      "Epoch 1010, Loss: 0.2562441276\n",
      "Epoch 1011, Loss: 0.3330812484\n",
      "Epoch 1012, Loss: 0.2466832963\n",
      "Epoch 1013, Loss: 0.1785239935\n",
      "Epoch 1014, Loss: 0.2385550879\n",
      "Epoch 1015, Loss: 0.2329918703\n",
      "Epoch 1016, Loss: 0.3062045031\n",
      "Epoch 1017, Loss: 0.2725887058\n",
      "Epoch 1018, Loss: 0.1897613687\n",
      "Epoch 1019, Loss: 0.1555320821\n",
      "Epoch 1020, Loss: 0.2269618717\n",
      "Epoch 1021, Loss: 0.3325653337\n",
      "Epoch 1022, Loss: 0.3680127243\n",
      "Epoch 1023, Loss: 0.3811255458\n",
      "Epoch 1024, Loss: 0.3400101737\n",
      "Epoch 1025, Loss: 0.3206571600\n",
      "Epoch 1026, Loss: 0.2429816807\n",
      "Epoch 1027, Loss: 0.2177085810\n",
      "Epoch 1028, Loss: 0.2009451416\n",
      "Epoch 1029, Loss: 0.2260779979\n",
      "Epoch 1030, Loss: 0.2224914329\n",
      "Epoch 1031, Loss: 0.1582102274\n",
      "Epoch 1032, Loss: 0.5995093888\n",
      "Epoch 1033, Loss: 0.3243644540\n",
      "Epoch 1034, Loss: 0.4895322378\n",
      "Epoch 1035, Loss: 0.3258024337\n",
      "Epoch 1036, Loss: 0.3686251150\n",
      "Epoch 1037, Loss: 0.5941072025\n",
      "Epoch 1038, Loss: 0.5032091402\n",
      "Epoch 1039, Loss: 0.6678033667\n",
      "Epoch 1040, Loss: 0.4472889094\n",
      "Epoch 1041, Loss: 0.4176696153\n",
      "Epoch 1042, Loss: 0.2499342132\n",
      "Epoch 1043, Loss: 0.2889448450\n",
      "Epoch 1044, Loss: 0.4306519494\n",
      "Epoch 1045, Loss: 0.2520185484\n",
      "Epoch 1046, Loss: 0.2991939095\n",
      "Epoch 1047, Loss: 0.2895254708\n",
      "Epoch 1048, Loss: 0.2397051343\n",
      "Epoch 1049, Loss: 0.3647407546\n",
      "Epoch 1050, Loss: 0.2986258615\n",
      "Epoch 1051, Loss: 0.3203026674\n",
      "Epoch 1052, Loss: 0.3796002725\n",
      "Epoch 1053, Loss: 0.4506512124\n",
      "Epoch 1054, Loss: 0.4304127726\n",
      "Epoch 1055, Loss: 0.5428506962\n",
      "Epoch 1056, Loss: 0.4091522587\n",
      "Epoch 1057, Loss: 0.3998467682\n",
      "Epoch 1058, Loss: 0.3737211888\n",
      "Epoch 1059, Loss: 0.3423204848\n",
      "Epoch 1060, Loss: 0.5111287863\n",
      "Epoch 1061, Loss: 0.3470883829\n",
      "Epoch 1062, Loss: 0.5588451129\n",
      "Epoch 1063, Loss: 0.3474719399\n",
      "Epoch 1064, Loss: 0.4834640978\n",
      "Epoch 1065, Loss: 0.5156586391\n",
      "Epoch 1066, Loss: 0.1769841742\n",
      "Epoch 1067, Loss: 0.3481776034\n",
      "Epoch 1068, Loss: 0.2013008660\n",
      "Epoch 1069, Loss: 0.3775958789\n",
      "Epoch 1070, Loss: 0.4254631376\n",
      "Epoch 1071, Loss: 0.2178009447\n",
      "Epoch 1072, Loss: 0.2669184674\n",
      "Epoch 1073, Loss: 0.2633446855\n",
      "Epoch 1074, Loss: 0.3872437401\n",
      "Epoch 1075, Loss: 0.3600052126\n",
      "Epoch 1076, Loss: 0.4161204737\n",
      "Epoch 1077, Loss: 0.4200933370\n",
      "Epoch 1078, Loss: 0.4783897999\n",
      "Epoch 1079, Loss: 0.2795318917\n",
      "Epoch 1080, Loss: 0.2612940338\n",
      "Epoch 1081, Loss: 0.2944909616\n",
      "Epoch 1082, Loss: 0.2344298552\n",
      "Epoch 1083, Loss: 0.3334025883\n",
      "Epoch 1084, Loss: 0.2400572135\n",
      "Epoch 1085, Loss: 0.1828847283\n",
      "Epoch 1086, Loss: 0.2001308531\n",
      "Epoch 1087, Loss: 0.2475855788\n",
      "Epoch 1088, Loss: 0.2291900603\n",
      "Epoch 1089, Loss: 0.2221365329\n",
      "Epoch 1090, Loss: 0.1789489362\n",
      "Epoch 1091, Loss: 0.1962777798\n",
      "Epoch 1092, Loss: 0.1632636953\n",
      "Epoch 1093, Loss: 0.1633320069\n",
      "Epoch 1094, Loss: 0.1784905142\n",
      "Epoch 1095, Loss: 0.1556699229\n",
      "Epoch 1096, Loss: 0.1651828053\n",
      "Epoch 1097, Loss: 0.1454146861\n",
      "Epoch 1098, Loss: 0.1545068032\n",
      "Epoch 1099, Loss: 0.1833104667\n",
      "Epoch 1100, Loss: 0.2384662427\n",
      "Epoch 1101, Loss: 0.2580683087\n",
      "Epoch 1102, Loss: 0.2527076261\n",
      "Epoch 1103, Loss: 0.2619954189\n",
      "Epoch 1104, Loss: 0.1975117178\n",
      "Epoch 1105, Loss: 0.1485898281\n",
      "Epoch 1106, Loss: 0.1756342717\n",
      "Epoch 1107, Loss: 0.2036815421\n",
      "Epoch 1108, Loss: 0.2700594538\n",
      "Epoch 1109, Loss: 0.2617264511\n",
      "Epoch 1110, Loss: 0.2867132212\n",
      "Epoch 1111, Loss: 0.3106818108\n",
      "Epoch 1112, Loss: 0.3602324135\n",
      "Epoch 1113, Loss: 0.3017853725\n",
      "Epoch 1114, Loss: 0.3097872984\n",
      "Epoch 1115, Loss: 0.3443565565\n",
      "Epoch 1116, Loss: 0.1679719608\n",
      "Epoch 1117, Loss: 0.2663258537\n",
      "Epoch 1118, Loss: 0.2376216425\n",
      "Epoch 1119, Loss: 0.2070837445\n",
      "Epoch 1120, Loss: 0.3117690788\n",
      "Epoch 1121, Loss: 0.2137852263\n",
      "Epoch 1122, Loss: 0.2159658275\n",
      "Epoch 1123, Loss: 0.2171645551\n",
      "Epoch 1124, Loss: 0.2538320084\n",
      "Epoch 1125, Loss: 0.2901648960\n",
      "Epoch 1126, Loss: 0.1995832588\n",
      "Epoch 1127, Loss: 0.1629659179\n",
      "Epoch 1128, Loss: 0.2273438907\n",
      "Epoch 1129, Loss: 0.2505911540\n",
      "Epoch 1130, Loss: 0.2153414618\n",
      "Epoch 1131, Loss: 0.2400538950\n",
      "Epoch 1132, Loss: 0.1930467636\n",
      "Epoch 1133, Loss: 0.1888467207\n",
      "Epoch 1134, Loss: 0.1434199660\n",
      "Epoch 1135, Loss: 0.1830634751\n",
      "Epoch 1136, Loss: 0.2057959174\n",
      "Epoch 1137, Loss: 0.1890003404\n",
      "Epoch 1138, Loss: 0.1962991168\n",
      "Epoch 1139, Loss: 0.2174384816\n",
      "Epoch 1140, Loss: 0.1503921016\n",
      "Epoch 1141, Loss: 0.2099738484\n",
      "Epoch 1142, Loss: 0.1356348453\n",
      "Epoch 1143, Loss: 0.1716836136\n",
      "Epoch 1144, Loss: 0.1580203711\n",
      "Epoch 1145, Loss: 0.2253032390\n",
      "Epoch 1146, Loss: 0.2574915607\n",
      "Epoch 1147, Loss: 0.2714979483\n",
      "Epoch 1148, Loss: 0.2416635305\n",
      "Epoch 1149, Loss: 0.3080875459\n",
      "Epoch 1150, Loss: 0.3049945161\n",
      "Epoch 1151, Loss: 0.3919012340\n",
      "Epoch 1152, Loss: 0.2689639536\n",
      "Epoch 1153, Loss: 0.1299040548\n",
      "Epoch 1154, Loss: 0.1929146509\n",
      "Epoch 1155, Loss: 0.2461016386\n",
      "Epoch 1156, Loss: 0.2031514614\n",
      "Epoch 1157, Loss: 0.2256548948\n",
      "Epoch 1158, Loss: 0.1213124762\n",
      "Epoch 1159, Loss: 0.2650480813\n",
      "Epoch 1160, Loss: 0.1961766013\n",
      "Epoch 1161, Loss: 0.1140377892\n",
      "Epoch 1162, Loss: 0.1526242599\n",
      "Epoch 1163, Loss: 0.1408162544\n",
      "Epoch 1164, Loss: 0.3032545417\n",
      "Epoch 1165, Loss: 0.1855805913\n",
      "Epoch 1166, Loss: 0.2145263433\n",
      "Epoch 1167, Loss: 0.2223809709\n",
      "Epoch 1168, Loss: 0.2675561968\n",
      "Epoch 1169, Loss: 0.1619559669\n",
      "Epoch 1170, Loss: 0.2145321285\n",
      "Epoch 1171, Loss: 0.2375572721\n",
      "Epoch 1172, Loss: 0.1913869802\n",
      "Epoch 1173, Loss: 0.3060007566\n",
      "Epoch 1174, Loss: 0.1747124295\n",
      "Epoch 1175, Loss: 0.1843912715\n",
      "Epoch 1176, Loss: 0.2827675868\n",
      "Epoch 1177, Loss: 0.2719368634\n",
      "Epoch 1178, Loss: 0.3246875118\n",
      "Epoch 1179, Loss: 0.4107319004\n",
      "Epoch 1180, Loss: 0.3650660170\n",
      "Epoch 1181, Loss: 0.3306758557\n",
      "Epoch 1182, Loss: 0.3604953671\n",
      "Epoch 1183, Loss: 0.3055789974\n",
      "Epoch 1184, Loss: 0.3693688128\n",
      "Epoch 1185, Loss: 0.3462339763\n",
      "Epoch 1186, Loss: 0.3313524103\n",
      "Epoch 1187, Loss: 0.8521156179\n",
      "Epoch 1188, Loss: 0.3889750916\n",
      "Epoch 1189, Loss: 0.5482465656\n",
      "Epoch 1190, Loss: 0.3010047127\n",
      "Epoch 1191, Loss: 0.3628760859\n",
      "Epoch 1192, Loss: 0.2422978331\n",
      "Epoch 1193, Loss: 0.1989525118\n",
      "Epoch 1194, Loss: 0.2721427635\n",
      "Epoch 1195, Loss: 0.1806791611\n",
      "Epoch 1196, Loss: 0.2468451950\n",
      "Epoch 1197, Loss: 0.2510650767\n",
      "Epoch 1198, Loss: 0.2278746594\n",
      "Epoch 1199, Loss: 0.3453825802\n",
      "Epoch 1200, Loss: 0.3365712949\n",
      "Epoch 1201, Loss: 0.3873745605\n",
      "Epoch 1202, Loss: 0.5453750040\n",
      "Epoch 1203, Loss: 0.3002248083\n",
      "Epoch 1204, Loss: 0.3772993586\n",
      "Epoch 1205, Loss: 0.5748108587\n",
      "Epoch 1206, Loss: 0.4049341363\n",
      "Epoch 1207, Loss: 0.2925465941\n",
      "Epoch 1208, Loss: 0.3391584320\n",
      "Epoch 1209, Loss: 0.3554348701\n",
      "Epoch 1210, Loss: 0.4129254289\n",
      "Epoch 1211, Loss: 0.3094669275\n",
      "Epoch 1212, Loss: 0.2366761372\n",
      "Epoch 1213, Loss: 0.3659459972\n",
      "Epoch 1214, Loss: 0.3738622141\n",
      "Epoch 1215, Loss: 0.4633287378\n",
      "Epoch 1216, Loss: 0.3819196906\n",
      "Epoch 1217, Loss: 0.3697952203\n",
      "Epoch 1218, Loss: 0.4066674812\n",
      "Epoch 1219, Loss: 0.2626819549\n",
      "Epoch 1220, Loss: 0.2661819722\n",
      "Epoch 1221, Loss: 0.2193312851\n",
      "Epoch 1222, Loss: 0.2400304623\n",
      "Epoch 1223, Loss: 0.2001663756\n",
      "Epoch 1224, Loss: 0.2517873689\n",
      "Epoch 1225, Loss: 0.2155999648\n",
      "Epoch 1226, Loss: 0.1977981219\n",
      "Epoch 1227, Loss: 0.3042095298\n",
      "Epoch 1228, Loss: 0.2213943191\n",
      "Epoch 1229, Loss: 0.2249983247\n",
      "Epoch 1230, Loss: 0.2482511811\n",
      "Epoch 1231, Loss: 0.2245509621\n",
      "Epoch 1232, Loss: 0.2139064582\n",
      "Epoch 1233, Loss: 0.1676474365\n",
      "Epoch 1234, Loss: 0.1941278064\n",
      "Epoch 1235, Loss: 0.2359658141\n",
      "Epoch 1236, Loss: 0.2935013387\n",
      "Epoch 1237, Loss: 0.3052269402\n",
      "Epoch 1238, Loss: 0.1042235448\n",
      "Epoch 1239, Loss: 0.2193872916\n",
      "Epoch 1240, Loss: 0.2438750953\n",
      "Epoch 1241, Loss: 0.2573850580\n",
      "Epoch 1242, Loss: 0.2963548128\n",
      "Epoch 1243, Loss: 0.2221520760\n",
      "Epoch 1244, Loss: 0.2786085526\n",
      "Epoch 1245, Loss: 0.4057704144\n",
      "Epoch 1246, Loss: 0.3525541292\n",
      "Epoch 1247, Loss: 0.3099978010\n",
      "Epoch 1248, Loss: 0.3071942175\n",
      "Epoch 1249, Loss: 0.2422871095\n",
      "Epoch 1250, Loss: 0.2687159672\n",
      "Epoch 1251, Loss: 0.1685235576\n",
      "Epoch 1252, Loss: 0.1551938488\n",
      "Epoch 1253, Loss: 0.2559656757\n",
      "Epoch 1254, Loss: 0.4957882566\n",
      "Epoch 1255, Loss: 0.2069682765\n",
      "Epoch 1256, Loss: 0.1160082384\n",
      "Epoch 1257, Loss: 0.1983617094\n",
      "Epoch 1258, Loss: 0.0988208545\n",
      "Epoch 1259, Loss: 0.1016597047\n",
      "Epoch 1260, Loss: 0.1760502326\n",
      "Epoch 1261, Loss: 0.1015347054\n",
      "Epoch 1262, Loss: 0.0875029510\n",
      "Epoch 1263, Loss: 0.1775956120\n",
      "Epoch 1264, Loss: 0.1242632690\n",
      "Epoch 1265, Loss: 0.0929786154\n",
      "Epoch 1266, Loss: 0.0832165839\n",
      "Epoch 1267, Loss: 0.0793035492\n",
      "Epoch 1268, Loss: 0.0589557569\n",
      "Epoch 1269, Loss: 0.0658655774\n",
      "Epoch 1270, Loss: 0.0549536234\n",
      "Epoch 1271, Loss: 0.0624145464\n",
      "Epoch 1272, Loss: 0.0569109127\n",
      "Epoch 1273, Loss: 0.0613499958\n",
      "Epoch 1274, Loss: 0.0513329080\n",
      "Epoch 1275, Loss: 0.0672169897\n",
      "Epoch 1276, Loss: 0.0609251404\n",
      "Epoch 1277, Loss: 0.0626737351\n",
      "Epoch 1278, Loss: 0.0575812314\n",
      "Epoch 1279, Loss: 0.0639234313\n",
      "Epoch 1280, Loss: 0.0577832568\n",
      "Epoch 1281, Loss: 0.0666130598\n",
      "Epoch 1282, Loss: 0.0562722545\n",
      "Epoch 1283, Loss: 0.0701255718\n",
      "Epoch 1284, Loss: 0.0560721703\n",
      "Epoch 1285, Loss: 0.0785432012\n",
      "Epoch 1286, Loss: 0.0506290981\n",
      "Epoch 1287, Loss: 0.0725098882\n",
      "Epoch 1288, Loss: 0.0484200520\n",
      "Epoch 1289, Loss: 0.0601098181\n",
      "Epoch 1290, Loss: 0.0570278838\n",
      "Epoch 1291, Loss: 0.0593548615\n",
      "Epoch 1292, Loss: 0.0562426604\n",
      "Epoch 1293, Loss: 0.0722303248\n",
      "Epoch 1294, Loss: 0.0996307903\n",
      "Epoch 1295, Loss: 0.1084649711\n",
      "Epoch 1296, Loss: 0.1022409425\n",
      "Epoch 1297, Loss: 0.0847608154\n",
      "Epoch 1298, Loss: 0.0994009976\n",
      "Epoch 1299, Loss: 0.1159302114\n",
      "Epoch 1300, Loss: 0.1156501876\n",
      "Epoch 1301, Loss: 0.1061356976\n",
      "Epoch 1302, Loss: 0.1260620001\n",
      "Epoch 1303, Loss: 0.1057818546\n",
      "Epoch 1304, Loss: 0.1056240055\n",
      "Epoch 1305, Loss: 0.1086394887\n",
      "Epoch 1306, Loss: 0.0945920698\n",
      "Epoch 1307, Loss: 0.0969856322\n",
      "Epoch 1308, Loss: 0.1317250507\n",
      "Epoch 1309, Loss: 0.1517977030\n",
      "Epoch 1310, Loss: 0.0971660720\n",
      "Epoch 1311, Loss: 0.1191305191\n",
      "Epoch 1312, Loss: 0.1435862660\n",
      "Epoch 1313, Loss: 0.1509198221\n",
      "Epoch 1314, Loss: 0.0980488919\n",
      "Epoch 1315, Loss: 0.1355842765\n",
      "Epoch 1316, Loss: 0.1636935015\n",
      "Epoch 1317, Loss: 0.0908857699\n",
      "Epoch 1318, Loss: 0.1302040822\n",
      "Epoch 1319, Loss: 0.1305736009\n",
      "Epoch 1320, Loss: 0.1183345113\n",
      "Epoch 1321, Loss: 0.0994844867\n",
      "Epoch 1322, Loss: 0.1049768222\n",
      "Epoch 1323, Loss: 0.1045227299\n",
      "Epoch 1324, Loss: 0.1116464326\n",
      "Epoch 1325, Loss: 0.2481932353\n",
      "Epoch 1326, Loss: 0.3017960851\n",
      "Epoch 1327, Loss: 0.2353182903\n",
      "Epoch 1328, Loss: 0.3169511335\n",
      "Epoch 1329, Loss: 0.3986572823\n",
      "Epoch 1330, Loss: 0.1801600313\n",
      "Epoch 1331, Loss: 0.3172503309\n",
      "Epoch 1332, Loss: 0.3015042681\n",
      "Epoch 1333, Loss: 0.3535124710\n",
      "Epoch 1334, Loss: 0.4713641502\n",
      "Epoch 1335, Loss: 0.2406431336\n",
      "Epoch 1336, Loss: 0.1906932485\n",
      "Epoch 1337, Loss: 0.3829415588\n",
      "Epoch 1338, Loss: 0.1767019076\n",
      "Epoch 1339, Loss: 0.1557545147\n",
      "Epoch 1340, Loss: 0.1431765994\n",
      "Epoch 1341, Loss: 0.2765649636\n",
      "Epoch 1342, Loss: 0.3707822176\n",
      "Epoch 1343, Loss: 0.6927431116\n",
      "Epoch 1344, Loss: 0.3785766877\n",
      "Epoch 1345, Loss: 0.1916886120\n",
      "Epoch 1346, Loss: 0.1809407186\n",
      "Epoch 1347, Loss: 0.2608773887\n",
      "Epoch 1348, Loss: 0.2858921891\n",
      "Epoch 1349, Loss: 0.5091928078\n",
      "Epoch 1350, Loss: 0.1539737777\n",
      "Epoch 1351, Loss: 0.3530077129\n",
      "Epoch 1352, Loss: 0.2327959412\n",
      "Epoch 1353, Loss: 0.2347122589\n",
      "Epoch 1354, Loss: 0.2743137865\n",
      "Epoch 1355, Loss: 0.5178752068\n",
      "Epoch 1356, Loss: 0.4304232050\n",
      "Epoch 1357, Loss: 0.1668480024\n",
      "Epoch 1358, Loss: 0.2353667722\n",
      "Epoch 1359, Loss: 0.1438848825\n",
      "Epoch 1360, Loss: 0.1064162476\n",
      "Epoch 1361, Loss: 0.1101942787\n",
      "Epoch 1362, Loss: 0.1292987866\n",
      "Epoch 1363, Loss: 0.2201028929\n",
      "Epoch 1364, Loss: 0.1532071873\n",
      "Epoch 1365, Loss: 0.1544473301\n",
      "Epoch 1366, Loss: 0.3059475149\n",
      "Epoch 1367, Loss: 0.2042236780\n",
      "Epoch 1368, Loss: 0.2133332529\n",
      "Epoch 1369, Loss: 0.3357834109\n",
      "Epoch 1370, Loss: 0.4042059379\n",
      "Epoch 1371, Loss: 0.2035175572\n",
      "Epoch 1372, Loss: 0.3286176759\n",
      "Epoch 1373, Loss: 0.1556182143\n",
      "Epoch 1374, Loss: 0.1469115419\n",
      "Epoch 1375, Loss: 0.1851367558\n",
      "Epoch 1376, Loss: 0.2019434917\n",
      "Epoch 1377, Loss: 0.2172792010\n",
      "Epoch 1378, Loss: 0.3462871182\n",
      "Epoch 1379, Loss: 0.3212000293\n",
      "Epoch 1380, Loss: 0.3491356455\n",
      "Epoch 1381, Loss: 0.2718358650\n",
      "Epoch 1382, Loss: 0.1853468149\n",
      "Epoch 1383, Loss: 0.0987612338\n",
      "Epoch 1384, Loss: 0.1462633751\n",
      "Epoch 1385, Loss: 0.1390431322\n",
      "Epoch 1386, Loss: 0.0946942480\n",
      "Epoch 1387, Loss: 0.1312619138\n",
      "Epoch 1388, Loss: 0.1688399834\n",
      "Epoch 1389, Loss: 0.1630921038\n",
      "Epoch 1390, Loss: 0.2684411177\n",
      "Epoch 1391, Loss: 0.2745467382\n",
      "Epoch 1392, Loss: 0.2703375388\n",
      "Epoch 1393, Loss: 0.2082339083\n",
      "Epoch 1394, Loss: 0.1635852645\n",
      "Epoch 1395, Loss: 0.1473884899\n",
      "Epoch 1396, Loss: 0.2020716349\n",
      "Epoch 1397, Loss: 0.1193774015\n",
      "Epoch 1398, Loss: 0.1130006866\n",
      "Epoch 1399, Loss: 0.0664542396\n",
      "Epoch 1400, Loss: 0.1901887621\n",
      "Epoch 1401, Loss: 0.1297581121\n",
      "Epoch 1402, Loss: 0.2625889963\n",
      "Epoch 1403, Loss: 0.5460646581\n",
      "Epoch 1404, Loss: 0.3769607952\n",
      "Epoch 1405, Loss: 0.1105418408\n",
      "Epoch 1406, Loss: 0.2097802940\n",
      "Epoch 1407, Loss: 0.1445984910\n",
      "Epoch 1408, Loss: 0.1171830012\n",
      "Epoch 1409, Loss: 0.2029639308\n",
      "Epoch 1410, Loss: 0.0989301948\n",
      "Epoch 1411, Loss: 0.1533888211\n",
      "Epoch 1412, Loss: 0.1820425942\n",
      "Epoch 1413, Loss: 0.1332499038\n",
      "Epoch 1414, Loss: 0.1657701210\n",
      "Epoch 1415, Loss: 0.1524202618\n",
      "Epoch 1416, Loss: 0.1612646933\n",
      "Epoch 1417, Loss: 0.2299232365\n",
      "Epoch 1418, Loss: 0.1608672477\n",
      "Epoch 1419, Loss: 0.1210794136\n",
      "Epoch 1420, Loss: 0.0695129560\n",
      "Epoch 1421, Loss: 0.1024431631\n",
      "Epoch 1422, Loss: 0.4965700514\n",
      "Epoch 1423, Loss: 0.3091362017\n",
      "Epoch 1424, Loss: 0.4320808465\n",
      "Epoch 1425, Loss: 0.1080122228\n",
      "Epoch 1426, Loss: 0.1759399547\n",
      "Epoch 1427, Loss: 0.1933317600\n",
      "Epoch 1428, Loss: 0.2628407035\n",
      "Epoch 1429, Loss: 0.0920984798\n",
      "Epoch 1430, Loss: 0.1983257718\n",
      "Epoch 1431, Loss: 0.0777493321\n",
      "Epoch 1432, Loss: 0.1641319935\n",
      "Epoch 1433, Loss: 0.1748356284\n",
      "Epoch 1434, Loss: 0.1887859400\n",
      "Epoch 1435, Loss: 0.2081885653\n",
      "Epoch 1436, Loss: 0.2342926960\n",
      "Epoch 1437, Loss: 0.2377730879\n",
      "Epoch 1438, Loss: 0.1509885002\n",
      "Epoch 1439, Loss: 0.2790935970\n",
      "Epoch 1440, Loss: 0.2429663165\n",
      "Epoch 1441, Loss: 0.0782666175\n",
      "Epoch 1442, Loss: 0.1454856413\n",
      "Epoch 1443, Loss: 0.2976266763\n",
      "Epoch 1444, Loss: 0.2063617378\n",
      "Epoch 1445, Loss: 0.2506096540\n",
      "Epoch 1446, Loss: 0.2296481533\n",
      "Epoch 1447, Loss: 0.2057869001\n",
      "Epoch 1448, Loss: 0.2209455184\n",
      "Epoch 1449, Loss: 0.2310257125\n",
      "Epoch 1450, Loss: 0.1420465862\n",
      "Epoch 1451, Loss: 0.1172613273\n",
      "Epoch 1452, Loss: 0.1634888792\n",
      "Epoch 1453, Loss: 0.1555646032\n",
      "Epoch 1454, Loss: 0.2005229351\n",
      "Epoch 1455, Loss: 0.2733807061\n",
      "Epoch 1456, Loss: 0.2808382212\n",
      "Epoch 1457, Loss: 0.2067950472\n",
      "Epoch 1458, Loss: 0.3235203547\n",
      "Epoch 1459, Loss: 0.5489179368\n",
      "Epoch 1460, Loss: 0.4247546760\n",
      "Epoch 1461, Loss: 0.2282522659\n",
      "Epoch 1462, Loss: 0.1425106613\n",
      "Epoch 1463, Loss: 0.1936162977\n",
      "Epoch 1464, Loss: 0.2660598305\n",
      "Epoch 1465, Loss: 0.1213854718\n",
      "Epoch 1466, Loss: 0.1392239871\n",
      "Epoch 1467, Loss: 0.1218841045\n",
      "Epoch 1468, Loss: 0.2170434265\n",
      "Epoch 1469, Loss: 0.2629685256\n",
      "Epoch 1470, Loss: 0.1759924481\n",
      "Epoch 1471, Loss: 0.1594698479\n",
      "Epoch 1472, Loss: 0.3460963723\n",
      "Epoch 1473, Loss: 0.3603383789\n",
      "Epoch 1474, Loss: 0.1448144241\n",
      "Epoch 1475, Loss: 0.1342972122\n",
      "Epoch 1476, Loss: 0.1557246798\n",
      "Epoch 1477, Loss: 0.2002665194\n",
      "Epoch 1478, Loss: 0.1994199412\n",
      "Epoch 1479, Loss: 0.1510012229\n",
      "Epoch 1480, Loss: 0.2129071138\n",
      "Epoch 1481, Loss: 0.2074387812\n",
      "Epoch 1482, Loss: 0.1974277264\n",
      "Epoch 1483, Loss: 0.1611576986\n",
      "Epoch 1484, Loss: 0.0829010648\n",
      "Epoch 1485, Loss: 0.0879791553\n",
      "Epoch 1486, Loss: 0.1353404359\n",
      "Epoch 1487, Loss: 0.1546363927\n",
      "Epoch 1488, Loss: 0.3033027629\n",
      "Epoch 1489, Loss: 0.1581188767\n",
      "Epoch 1490, Loss: 0.2511340455\n",
      "Epoch 1491, Loss: 0.1308963760\n",
      "Epoch 1492, Loss: 0.1311397626\n",
      "Epoch 1493, Loss: 0.1953868682\n",
      "Epoch 1494, Loss: 0.1973218020\n",
      "Epoch 1495, Loss: 0.2784898385\n",
      "Epoch 1496, Loss: 0.1108969376\n",
      "Epoch 1497, Loss: 0.0973608657\n",
      "Epoch 1498, Loss: 0.1208212739\n",
      "Epoch 1499, Loss: 0.1100635017\n",
      "Epoch 1500, Loss: 0.0551553762\n",
      "Epoch 1501, Loss: 0.0791586030\n",
      "Epoch 1502, Loss: 0.0824847288\n",
      "Epoch 1503, Loss: 0.0773450276\n",
      "Epoch 1504, Loss: 0.0841161602\n",
      "Epoch 1505, Loss: 0.0670312633\n",
      "Epoch 1506, Loss: 0.1691634966\n",
      "Epoch 1507, Loss: 0.2620733908\n",
      "Epoch 1508, Loss: 0.1455156204\n",
      "Epoch 1509, Loss: 0.1186437873\n",
      "Epoch 1510, Loss: 0.0466286495\n",
      "Epoch 1511, Loss: 0.1540310481\n",
      "Epoch 1512, Loss: 0.0744315165\n",
      "Epoch 1513, Loss: 0.0670636171\n",
      "Epoch 1514, Loss: 0.0832537530\n",
      "Epoch 1515, Loss: 0.0394221311\n",
      "Epoch 1516, Loss: 0.0958344210\n",
      "Epoch 1517, Loss: 0.0919614425\n",
      "Epoch 1518, Loss: 0.0442794985\n",
      "Epoch 1519, Loss: 0.2026656704\n",
      "Epoch 1520, Loss: 0.0513602377\n",
      "Epoch 1521, Loss: 0.1075128961\n",
      "Epoch 1522, Loss: 0.0346271824\n",
      "Epoch 1523, Loss: 0.1475714271\n",
      "Epoch 1524, Loss: 0.0325224926\n",
      "Epoch 1525, Loss: 0.1775038139\n",
      "Epoch 1526, Loss: 0.0450443041\n",
      "Epoch 1527, Loss: 0.0651565775\n",
      "Epoch 1528, Loss: 0.1015588657\n",
      "Epoch 1529, Loss: 0.0276934759\n",
      "Epoch 1530, Loss: 0.1542229208\n",
      "Epoch 1531, Loss: 0.0632505772\n",
      "Epoch 1532, Loss: 0.0557184701\n",
      "Epoch 1533, Loss: 0.1188234757\n",
      "Epoch 1534, Loss: 0.0587905223\n",
      "Epoch 1535, Loss: 0.1836085195\n",
      "Epoch 1536, Loss: 0.1979818158\n",
      "Epoch 1537, Loss: 0.1120994659\n",
      "Epoch 1538, Loss: 0.1174423322\n",
      "Epoch 1539, Loss: 0.1493580481\n",
      "Epoch 1540, Loss: 0.0946033396\n",
      "Epoch 1541, Loss: 0.1646972557\n",
      "Epoch 1542, Loss: 0.0412959109\n",
      "Epoch 1543, Loss: 0.1303419899\n",
      "Epoch 1544, Loss: 0.0601061050\n",
      "Epoch 1545, Loss: 0.0656898199\n",
      "Epoch 1546, Loss: 0.1140932855\n",
      "Epoch 1547, Loss: 0.0558998634\n",
      "Epoch 1548, Loss: 0.1155462105\n",
      "Epoch 1549, Loss: 0.0661300036\n",
      "Epoch 1550, Loss: 0.2089769402\n",
      "Epoch 1551, Loss: 0.0351744981\n",
      "Epoch 1552, Loss: 0.1404617237\n",
      "Epoch 1553, Loss: 0.0793201503\n",
      "Epoch 1554, Loss: 0.1149389349\n",
      "Epoch 1555, Loss: 0.1289031966\n",
      "Epoch 1556, Loss: 0.1266897415\n",
      "Epoch 1557, Loss: 0.0934205965\n",
      "Epoch 1558, Loss: 0.3911129506\n",
      "Epoch 1559, Loss: 1.1259910889\n",
      "Epoch 1560, Loss: 0.8182910532\n",
      "Epoch 1561, Loss: 0.4872737143\n",
      "Epoch 1562, Loss: 0.3202943846\n",
      "Epoch 1563, Loss: 0.2028173250\n",
      "Epoch 1564, Loss: 0.2010588647\n",
      "Epoch 1565, Loss: 0.1762587068\n",
      "Epoch 1566, Loss: 0.1281608616\n",
      "Epoch 1567, Loss: 0.2949590548\n",
      "Epoch 1568, Loss: 0.2251977132\n",
      "Epoch 1569, Loss: 0.1150283512\n",
      "Epoch 1570, Loss: 0.1209178058\n",
      "Epoch 1571, Loss: 0.1437655467\n",
      "Epoch 1572, Loss: 0.2106343378\n",
      "Epoch 1573, Loss: 0.2182490477\n",
      "Epoch 1574, Loss: 0.1424078301\n",
      "Epoch 1575, Loss: 0.2502461320\n",
      "Epoch 1576, Loss: 0.1924792694\n",
      "Epoch 1577, Loss: 0.1200356674\n",
      "Epoch 1578, Loss: 0.1429495353\n",
      "Epoch 1579, Loss: 0.1891888206\n",
      "Epoch 1580, Loss: 0.2545365730\n",
      "Epoch 1581, Loss: 0.2285184456\n",
      "Epoch 1582, Loss: 0.1880249894\n",
      "Epoch 1583, Loss: 0.1165346986\n",
      "Epoch 1584, Loss: 0.1281165949\n",
      "Epoch 1585, Loss: 0.3266419161\n",
      "Epoch 1586, Loss: 0.1446979913\n",
      "Epoch 1587, Loss: 0.0873653847\n",
      "Epoch 1588, Loss: 0.1344992079\n",
      "Epoch 1589, Loss: 0.3925578351\n",
      "Epoch 1590, Loss: 0.1498894729\n",
      "Epoch 1591, Loss: 0.1361164871\n",
      "Epoch 1592, Loss: 0.1317018481\n",
      "Epoch 1593, Loss: 0.1650205077\n",
      "Epoch 1594, Loss: 0.2213949759\n",
      "Epoch 1595, Loss: 0.1212575925\n",
      "Epoch 1596, Loss: 0.0858920513\n",
      "Epoch 1597, Loss: 0.2437708950\n",
      "Epoch 1598, Loss: 0.1940023052\n",
      "Epoch 1599, Loss: 0.1322413517\n",
      "Epoch 1600, Loss: 0.1731254930\n",
      "Epoch 1601, Loss: 0.0849544436\n",
      "Epoch 1602, Loss: 0.1385338613\n",
      "Epoch 1603, Loss: 0.1610005492\n",
      "Epoch 1604, Loss: 0.1942103838\n",
      "Epoch 1605, Loss: 0.0859889516\n",
      "Epoch 1606, Loss: 0.1939756442\n",
      "Epoch 1607, Loss: 0.3940665774\n",
      "Epoch 1608, Loss: 0.7216634519\n",
      "Epoch 1609, Loss: 0.2724740073\n",
      "Epoch 1610, Loss: 0.3674304141\n",
      "Epoch 1611, Loss: 0.1439089517\n",
      "Epoch 1612, Loss: 0.1450598807\n",
      "Epoch 1613, Loss: 0.1771360985\n",
      "Epoch 1614, Loss: 0.0887207837\n",
      "Epoch 1615, Loss: 0.1302125056\n",
      "Epoch 1616, Loss: 0.1119741679\n",
      "Epoch 1617, Loss: 0.1600166713\n",
      "Epoch 1618, Loss: 0.0713418443\n",
      "Epoch 1619, Loss: 0.1258017131\n",
      "Epoch 1620, Loss: 0.0434885815\n",
      "Epoch 1621, Loss: 0.1391571188\n",
      "Epoch 1622, Loss: 0.1248619939\n",
      "Epoch 1623, Loss: 0.0941163784\n",
      "Epoch 1624, Loss: 0.0484086838\n",
      "Epoch 1625, Loss: 0.1682944538\n",
      "Epoch 1626, Loss: 0.0505903165\n",
      "Epoch 1627, Loss: 0.0854782765\n",
      "Epoch 1628, Loss: 0.1076533144\n",
      "Epoch 1629, Loss: 0.0518757016\n",
      "Epoch 1630, Loss: 0.1674976041\n",
      "Epoch 1631, Loss: 0.0621581950\n",
      "Epoch 1632, Loss: 0.1714163905\n",
      "Epoch 1633, Loss: 0.0434489904\n",
      "Epoch 1634, Loss: 0.0664612459\n",
      "Epoch 1635, Loss: 0.1933609737\n",
      "Epoch 1636, Loss: 0.1589834987\n",
      "Epoch 1637, Loss: 0.2543839729\n",
      "Epoch 1638, Loss: 0.0856967532\n",
      "Epoch 1639, Loss: 0.1545393797\n",
      "Epoch 1640, Loss: 0.1220255634\n",
      "Epoch 1641, Loss: 0.1158286441\n",
      "Epoch 1642, Loss: 0.1212556870\n",
      "Epoch 1643, Loss: 0.0490581496\n",
      "Epoch 1644, Loss: 0.0945779307\n",
      "Epoch 1645, Loss: 0.0510515454\n",
      "Epoch 1646, Loss: 0.1262702562\n",
      "Epoch 1647, Loss: 0.0846510218\n",
      "Epoch 1648, Loss: 0.0604007951\n",
      "Epoch 1649, Loss: 0.1837046459\n",
      "Epoch 1650, Loss: 0.1986291296\n",
      "Epoch 1651, Loss: 0.1360959830\n",
      "Epoch 1652, Loss: 0.1463988337\n",
      "Epoch 1653, Loss: 0.2770480541\n",
      "Epoch 1654, Loss: 0.0819153724\n",
      "Epoch 1655, Loss: 0.0890393266\n",
      "Epoch 1656, Loss: 0.1506934094\n",
      "Epoch 1657, Loss: 0.1367230201\n",
      "Epoch 1658, Loss: 0.1246339733\n",
      "Epoch 1659, Loss: 0.1176351202\n",
      "Epoch 1660, Loss: 0.1208352905\n",
      "Epoch 1661, Loss: 0.1820638781\n",
      "Epoch 1662, Loss: 0.0966148262\n",
      "Epoch 1663, Loss: 0.1782116979\n",
      "Epoch 1664, Loss: 0.2829862514\n",
      "Epoch 1665, Loss: 0.2866627284\n",
      "Epoch 1666, Loss: 0.0851427951\n",
      "Epoch 1667, Loss: 0.1672327518\n",
      "Epoch 1668, Loss: 1.2687769363\n",
      "Epoch 1669, Loss: 0.6103894890\n",
      "Epoch 1670, Loss: 0.3977572529\n",
      "Epoch 1671, Loss: 0.1417614364\n",
      "Epoch 1672, Loss: 0.2251873572\n",
      "Epoch 1673, Loss: 0.2041164700\n",
      "Epoch 1674, Loss: 0.3128237885\n",
      "Epoch 1675, Loss: 0.2800385543\n",
      "Epoch 1676, Loss: 0.1205559230\n",
      "Epoch 1677, Loss: 0.0907728564\n",
      "Epoch 1678, Loss: 0.1280225955\n",
      "Epoch 1679, Loss: 0.1096244882\n",
      "Epoch 1680, Loss: 0.1616986694\n",
      "Epoch 1681, Loss: 0.1184961116\n",
      "Epoch 1682, Loss: 0.1498915350\n",
      "Epoch 1683, Loss: 0.1577180630\n",
      "Epoch 1684, Loss: 0.1227124559\n",
      "Epoch 1685, Loss: 0.3212016531\n",
      "Epoch 1686, Loss: 0.1303658636\n",
      "Epoch 1687, Loss: 0.2497513895\n",
      "Epoch 1688, Loss: 0.1710924141\n",
      "Epoch 1689, Loss: 0.3610120600\n",
      "Epoch 1690, Loss: 0.4337546659\n",
      "Epoch 1691, Loss: 0.2520752256\n",
      "Epoch 1692, Loss: 0.2463706321\n",
      "Epoch 1693, Loss: 0.4069101079\n",
      "Epoch 1694, Loss: 0.3422156723\n",
      "Epoch 1695, Loss: 0.0946659379\n",
      "Epoch 1696, Loss: 0.1843698754\n",
      "Epoch 1697, Loss: 0.2365067216\n",
      "Epoch 1698, Loss: 0.1604705073\n",
      "Epoch 1699, Loss: 0.0647559624\n",
      "Epoch 1700, Loss: 0.1592743082\n",
      "Epoch 1701, Loss: 0.1782317182\n",
      "Epoch 1702, Loss: 0.2000824083\n",
      "Epoch 1703, Loss: 0.2434834270\n",
      "Epoch 1704, Loss: 0.1654499039\n",
      "Epoch 1705, Loss: 0.3005445007\n",
      "Epoch 1706, Loss: 0.3283464944\n",
      "Epoch 1707, Loss: 0.2094072666\n",
      "Epoch 1708, Loss: 0.1575669427\n",
      "Epoch 1709, Loss: 0.2224928385\n",
      "Epoch 1710, Loss: 0.1954315264\n",
      "Epoch 1711, Loss: 0.1612053281\n",
      "Epoch 1712, Loss: 0.1261357425\n",
      "Epoch 1713, Loss: 0.1921050065\n",
      "Epoch 1714, Loss: 0.2351724174\n",
      "Epoch 1715, Loss: 0.3432356702\n",
      "Epoch 1716, Loss: 0.1469262065\n",
      "Epoch 1717, Loss: 0.2224129051\n",
      "Epoch 1718, Loss: 0.1666858289\n",
      "Epoch 1719, Loss: 0.2181460519\n",
      "Epoch 1720, Loss: 0.4962481464\n",
      "Epoch 1721, Loss: 0.2974692872\n",
      "Epoch 1722, Loss: 0.1685002539\n",
      "Epoch 1723, Loss: 0.1952095514\n",
      "Epoch 1724, Loss: 0.1535049463\n",
      "Epoch 1725, Loss: 0.1855677574\n",
      "Epoch 1726, Loss: 0.1554367475\n",
      "Epoch 1727, Loss: 0.1456872245\n",
      "Epoch 1728, Loss: 0.1074768878\n",
      "Epoch 1729, Loss: 0.4393943003\n",
      "Epoch 1730, Loss: 0.2774143924\n",
      "Epoch 1731, Loss: 0.3661039433\n",
      "Epoch 1732, Loss: 0.2313461500\n",
      "Epoch 1733, Loss: 0.2981236427\n",
      "Epoch 1734, Loss: 0.1828109794\n",
      "Epoch 1735, Loss: 0.3291774240\n",
      "Epoch 1736, Loss: 0.0924271675\n",
      "Epoch 1737, Loss: 0.1297621870\n",
      "Epoch 1738, Loss: 0.2275185680\n",
      "Epoch 1739, Loss: 0.2022371457\n",
      "Epoch 1740, Loss: 0.1481537985\n",
      "Epoch 1741, Loss: 0.1673169696\n",
      "Epoch 1742, Loss: 0.1139379540\n",
      "Epoch 1743, Loss: 0.1489210468\n",
      "Epoch 1744, Loss: 0.1618695480\n",
      "Epoch 1745, Loss: 0.1442104114\n",
      "Epoch 1746, Loss: 0.1879911518\n",
      "Epoch 1747, Loss: 0.2405380440\n",
      "Epoch 1748, Loss: 0.1313966270\n",
      "Epoch 1749, Loss: 0.1503316542\n",
      "Epoch 1750, Loss: 0.1607679353\n",
      "Epoch 1751, Loss: 0.2011962279\n",
      "Epoch 1752, Loss: 0.1457134159\n",
      "Epoch 1753, Loss: 0.2824424068\n",
      "Epoch 1754, Loss: 0.1705007414\n",
      "Epoch 1755, Loss: 0.1808472390\n",
      "Epoch 1756, Loss: 0.1334703295\n",
      "Epoch 1757, Loss: 0.0794573456\n",
      "Epoch 1758, Loss: 0.1774101399\n",
      "Epoch 1759, Loss: 0.0729232596\n",
      "Epoch 1760, Loss: 0.1633519163\n",
      "Epoch 1761, Loss: 0.1328425017\n",
      "Epoch 1762, Loss: 0.0850053356\n",
      "Epoch 1763, Loss: 0.1789354865\n",
      "Epoch 1764, Loss: 0.1365644154\n",
      "Epoch 1765, Loss: 0.0580224792\n",
      "Epoch 1766, Loss: 0.1050566362\n",
      "Epoch 1767, Loss: 0.1134060511\n",
      "Epoch 1768, Loss: 0.1449555857\n",
      "Epoch 1769, Loss: 0.1103693365\n",
      "Epoch 1770, Loss: 0.0945428927\n",
      "Epoch 1771, Loss: 0.1247385789\n",
      "Epoch 1772, Loss: 0.4538657531\n",
      "Epoch 1773, Loss: 0.0746899430\n",
      "Epoch 1774, Loss: 0.2093413822\n",
      "Epoch 1775, Loss: 0.0835958746\n",
      "Epoch 1776, Loss: 0.1750955188\n",
      "Epoch 1777, Loss: 0.2904350469\n",
      "Epoch 1778, Loss: 0.1199706713\n",
      "Epoch 1779, Loss: 0.2265646585\n",
      "Epoch 1780, Loss: 0.1036495218\n",
      "Epoch 1781, Loss: 0.1220934590\n",
      "Epoch 1782, Loss: 0.2218167818\n",
      "Epoch 1783, Loss: 0.1782553306\n",
      "Epoch 1784, Loss: 0.1809556234\n",
      "Epoch 1785, Loss: 0.2614715330\n",
      "Epoch 1786, Loss: 0.1264178494\n",
      "Epoch 1787, Loss: 0.3190383477\n",
      "Epoch 1788, Loss: 0.2447995124\n",
      "Epoch 1789, Loss: 0.3128279694\n",
      "Epoch 1790, Loss: 0.2437401896\n",
      "Epoch 1791, Loss: 0.2098870311\n",
      "Epoch 1792, Loss: 0.3056463804\n",
      "Epoch 1793, Loss: 0.1588827602\n",
      "Epoch 1794, Loss: 0.2128962683\n",
      "Epoch 1795, Loss: 0.1772417490\n",
      "Epoch 1796, Loss: 0.2034063422\n",
      "Epoch 1797, Loss: 0.2079241569\n",
      "Epoch 1798, Loss: 0.1830271937\n",
      "Epoch 1799, Loss: 0.4000067634\n",
      "Epoch 1800, Loss: 0.2234214525\n",
      "Epoch 1801, Loss: 0.0779830253\n",
      "Epoch 1802, Loss: 0.1094078312\n",
      "Epoch 1803, Loss: 0.1617470681\n",
      "Epoch 1804, Loss: 0.3259276870\n",
      "Epoch 1805, Loss: 0.1898716301\n",
      "Epoch 1806, Loss: 0.1517489383\n",
      "Epoch 1807, Loss: 0.2295897215\n",
      "Epoch 1808, Loss: 0.1378441426\n",
      "Epoch 1809, Loss: 0.0674934674\n",
      "Epoch 1810, Loss: 0.1161249298\n",
      "Epoch 1811, Loss: 0.1286931952\n",
      "Epoch 1812, Loss: 0.3328322632\n",
      "Epoch 1813, Loss: 0.1740061752\n",
      "Epoch 1814, Loss: 0.1662806307\n",
      "Epoch 1815, Loss: 0.0742556755\n",
      "Epoch 1816, Loss: 0.1173335178\n",
      "Epoch 1817, Loss: 0.1477879548\n",
      "Epoch 1818, Loss: 0.2353325058\n",
      "Epoch 1819, Loss: 0.2631044906\n",
      "Epoch 1820, Loss: 0.1717280436\n",
      "Epoch 1821, Loss: 0.0918133016\n",
      "Epoch 1822, Loss: 0.1193690487\n",
      "Epoch 1823, Loss: 0.1866092456\n",
      "Epoch 1824, Loss: 0.1578387858\n",
      "Epoch 1825, Loss: 0.1787406682\n",
      "Epoch 1826, Loss: 0.1940461529\n",
      "Epoch 1827, Loss: 0.1276619037\n",
      "Epoch 1828, Loss: 0.1076731605\n",
      "Epoch 1829, Loss: 0.2812419175\n",
      "Epoch 1830, Loss: 0.4542937017\n",
      "Epoch 1831, Loss: 0.4239124919\n",
      "Epoch 1832, Loss: 0.4333363578\n",
      "Epoch 1833, Loss: 0.3505035451\n",
      "Epoch 1834, Loss: 0.2657140313\n",
      "Epoch 1835, Loss: 0.1620031698\n",
      "Epoch 1836, Loss: 0.1627352076\n",
      "Epoch 1837, Loss: 0.3289874004\n",
      "Epoch 1838, Loss: 0.2265576423\n",
      "Epoch 1839, Loss: 0.3268461692\n",
      "Epoch 1840, Loss: 0.1997402139\n",
      "Epoch 1841, Loss: 0.2058833124\n",
      "Epoch 1842, Loss: 0.1892589155\n",
      "Epoch 1843, Loss: 0.1901466046\n",
      "Epoch 1844, Loss: 0.2158015580\n",
      "Epoch 1845, Loss: 0.2389257317\n",
      "Epoch 1846, Loss: 0.1597641820\n",
      "Epoch 1847, Loss: 0.3762942391\n",
      "Epoch 1848, Loss: 0.1759814706\n",
      "Epoch 1849, Loss: 0.1550776722\n",
      "Epoch 1850, Loss: 0.1777949862\n",
      "Epoch 1851, Loss: 0.1243227788\n",
      "Epoch 1852, Loss: 0.1203540647\n",
      "Epoch 1853, Loss: 0.1526631731\n",
      "Epoch 1854, Loss: 0.1070385952\n",
      "Epoch 1855, Loss: 0.0971209916\n",
      "Epoch 1856, Loss: 0.5103657600\n",
      "Epoch 1857, Loss: 0.0809826278\n",
      "Epoch 1858, Loss: 0.2470559517\n",
      "Epoch 1859, Loss: 0.1173445437\n",
      "Epoch 1860, Loss: 0.1085414101\n",
      "Epoch 1861, Loss: 0.1443203699\n",
      "Epoch 1862, Loss: 0.1904299143\n",
      "Epoch 1863, Loss: 0.1441938203\n",
      "Epoch 1864, Loss: 0.1890440425\n",
      "Epoch 1865, Loss: 0.1779090284\n",
      "Epoch 1866, Loss: 0.3659061575\n",
      "Epoch 1867, Loss: 0.0999822904\n",
      "Epoch 1868, Loss: 0.2027317580\n",
      "Epoch 1869, Loss: 0.2841151599\n",
      "Epoch 1870, Loss: 0.2236792285\n",
      "Epoch 1871, Loss: 0.2544397566\n",
      "Epoch 1872, Loss: 0.2836322871\n",
      "Epoch 1873, Loss: 0.1915572398\n",
      "Epoch 1874, Loss: 0.2645565682\n",
      "Epoch 1875, Loss: 0.2334891059\n",
      "Epoch 1876, Loss: 0.2462264170\n",
      "Epoch 1877, Loss: 0.1321055400\n",
      "Epoch 1878, Loss: 0.1338970453\n",
      "Epoch 1879, Loss: 0.2662128091\n",
      "Epoch 1880, Loss: 0.1868173791\n",
      "Epoch 1881, Loss: 0.1740969731\n",
      "Epoch 1882, Loss: 0.1433950856\n",
      "Epoch 1883, Loss: 0.2383383980\n",
      "Epoch 1884, Loss: 0.3987864601\n",
      "Epoch 1885, Loss: 0.2123759780\n",
      "Epoch 1886, Loss: 0.2624698244\n",
      "Epoch 1887, Loss: 0.2356024903\n",
      "Epoch 1888, Loss: 0.1142734554\n",
      "Epoch 1889, Loss: 0.1028432700\n",
      "Epoch 1890, Loss: 0.1442460227\n",
      "Epoch 1891, Loss: 0.1046587132\n",
      "Epoch 1892, Loss: 0.1283250869\n",
      "Epoch 1893, Loss: 0.0869368413\n",
      "Epoch 1894, Loss: 0.1035390376\n",
      "Epoch 1895, Loss: 0.1293188915\n",
      "Epoch 1896, Loss: 0.2292830101\n",
      "Epoch 1897, Loss: 0.1275725079\n",
      "Epoch 1898, Loss: 0.1872934108\n",
      "Epoch 1899, Loss: 0.1800994167\n",
      "Epoch 1900, Loss: 0.2627675746\n",
      "Epoch 1901, Loss: 0.3120958604\n",
      "Epoch 1902, Loss: 0.3029099582\n",
      "Epoch 1903, Loss: 0.2298272400\n",
      "Epoch 1904, Loss: 0.3030848856\n",
      "Epoch 1905, Loss: 0.4380103362\n",
      "Epoch 1906, Loss: 0.1912805637\n",
      "Epoch 1907, Loss: 0.2748327249\n",
      "Epoch 1908, Loss: 0.2849766565\n",
      "Epoch 1909, Loss: 0.2992513357\n",
      "Epoch 1910, Loss: 0.2888574176\n",
      "Epoch 1911, Loss: 0.2243733864\n",
      "Epoch 1912, Loss: 0.2712986968\n",
      "Epoch 1913, Loss: 0.3494371735\n",
      "Epoch 1914, Loss: 0.3317975215\n",
      "Epoch 1915, Loss: 0.3335154476\n",
      "Epoch 1916, Loss: 0.3603701353\n",
      "Epoch 1917, Loss: 0.3461207134\n",
      "Epoch 1918, Loss: 0.4054384390\n",
      "Epoch 1919, Loss: 0.5187774440\n",
      "Epoch 1920, Loss: 0.3857800988\n",
      "Epoch 1921, Loss: 0.1759485093\n",
      "Epoch 1922, Loss: 0.2172741686\n",
      "Epoch 1923, Loss: 0.1751664184\n",
      "Epoch 1924, Loss: 0.1095343259\n",
      "Epoch 1925, Loss: 0.0791790136\n",
      "Epoch 1926, Loss: 0.0639687059\n",
      "Epoch 1927, Loss: 0.0926784949\n",
      "Epoch 1928, Loss: 0.0780644310\n",
      "Epoch 1929, Loss: 0.0723060408\n",
      "Epoch 1930, Loss: 0.0620495687\n",
      "Epoch 1931, Loss: 0.0545943824\n",
      "Epoch 1932, Loss: 0.1433109721\n",
      "Epoch 1933, Loss: 0.1562521455\n",
      "Epoch 1934, Loss: 0.0854816997\n",
      "Epoch 1935, Loss: 0.0939393374\n",
      "Epoch 1936, Loss: 0.2028978276\n",
      "Epoch 1937, Loss: 0.2903435226\n",
      "Epoch 1938, Loss: 0.2798051502\n",
      "Epoch 1939, Loss: 0.1965128721\n",
      "Epoch 1940, Loss: 0.2016484068\n",
      "Epoch 1941, Loss: 0.1099450485\n",
      "Epoch 1942, Loss: 0.2044499897\n",
      "Epoch 1943, Loss: 0.1287890654\n",
      "Epoch 1944, Loss: 0.2127869610\n",
      "Epoch 1945, Loss: 0.0951341191\n",
      "Epoch 1946, Loss: 0.1894225746\n",
      "Epoch 1947, Loss: 0.2891867073\n",
      "Epoch 1948, Loss: 0.2704205179\n",
      "Epoch 1949, Loss: 0.3082746373\n",
      "Epoch 1950, Loss: 0.3442000116\n",
      "Epoch 1951, Loss: 0.3349625905\n",
      "Epoch 1952, Loss: 0.2112283360\n",
      "Epoch 1953, Loss: 0.3027989211\n",
      "Epoch 1954, Loss: 0.3955229003\n",
      "Epoch 1955, Loss: 0.4492947632\n",
      "Epoch 1956, Loss: 0.5237703962\n",
      "Epoch 1957, Loss: 0.2428233363\n",
      "Epoch 1958, Loss: 0.5482780301\n",
      "Epoch 1959, Loss: 0.4455051200\n",
      "Epoch 1960, Loss: 0.2036536375\n",
      "Epoch 1961, Loss: 0.2349167371\n",
      "Epoch 1962, Loss: 0.1924800003\n",
      "Epoch 1963, Loss: 0.2638527612\n",
      "Epoch 1964, Loss: 0.3010288621\n",
      "Epoch 1965, Loss: 0.4789445113\n",
      "Epoch 1966, Loss: 0.4685311630\n",
      "Epoch 1967, Loss: 0.3169831197\n",
      "Epoch 1968, Loss: 0.2823634886\n",
      "Epoch 1969, Loss: 0.3683976358\n",
      "Epoch 1970, Loss: 0.2866611533\n",
      "Epoch 1971, Loss: 0.4915990707\n",
      "Epoch 1972, Loss: 0.3748745199\n",
      "Epoch 1973, Loss: 0.2636101627\n",
      "Epoch 1974, Loss: 0.2570599001\n",
      "Epoch 1975, Loss: 0.2003523444\n",
      "Epoch 1976, Loss: 0.3796545373\n",
      "Epoch 1977, Loss: 0.2163674267\n",
      "Epoch 1978, Loss: 0.1916811016\n",
      "Epoch 1979, Loss: 0.1702996739\n",
      "Epoch 1980, Loss: 0.2823621076\n",
      "Epoch 1981, Loss: 0.2729529511\n",
      "Epoch 1982, Loss: 0.2028562414\n",
      "Epoch 1983, Loss: 0.2788145294\n",
      "Epoch 1984, Loss: 0.3147102214\n",
      "Epoch 1985, Loss: 0.2003428491\n",
      "Epoch 1986, Loss: 0.1786361645\n",
      "Epoch 1987, Loss: 0.2108961504\n",
      "Epoch 1988, Loss: 0.1904423851\n",
      "Epoch 1989, Loss: 0.1460054000\n",
      "Epoch 1990, Loss: 0.2062664344\n",
      "Epoch 1991, Loss: 0.2051108293\n",
      "Epoch 1992, Loss: 0.1222517321\n",
      "Epoch 1993, Loss: 0.1019354438\n",
      "Epoch 1994, Loss: 0.1118670509\n",
      "Epoch 1995, Loss: 0.0440191993\n",
      "Epoch 1996, Loss: 0.0445524080\n",
      "Epoch 1997, Loss: 0.0315738286\n",
      "Epoch 1998, Loss: 0.0348908571\n",
      "Epoch 1999, Loss: 0.0290893712\n",
      "Epoch 2000, Loss: 0.0374470277\n",
      "Epoch 2001, Loss: 0.0385170993\n",
      "Epoch 2002, Loss: 0.0443067691\n",
      "Epoch 2003, Loss: 0.0306687620\n",
      "Epoch 2004, Loss: 0.0480763812\n",
      "Epoch 2005, Loss: 0.0379105120\n",
      "Epoch 2006, Loss: 0.0364059802\n",
      "Epoch 2007, Loss: 0.0411958741\n",
      "Epoch 2008, Loss: 0.0388261258\n",
      "Epoch 2009, Loss: 0.0386044457\n",
      "Epoch 2010, Loss: 0.0295387195\n",
      "Epoch 2011, Loss: 0.0320754107\n",
      "Epoch 2012, Loss: 0.0321262849\n",
      "Epoch 2013, Loss: 0.0257583600\n",
      "Epoch 2014, Loss: 0.0268178011\n",
      "Epoch 2015, Loss: 0.0222143916\n",
      "Epoch 2016, Loss: 0.0268642886\n",
      "Epoch 2017, Loss: 0.0333757839\n",
      "Epoch 2018, Loss: 0.0306833378\n",
      "Epoch 2019, Loss: 0.0383265380\n",
      "Epoch 2020, Loss: 0.0257327534\n",
      "Epoch 2021, Loss: 0.0450769799\n",
      "Epoch 2022, Loss: 0.0245063089\n",
      "Epoch 2023, Loss: 0.0374068165\n",
      "Epoch 2024, Loss: 0.0298548357\n",
      "Epoch 2025, Loss: 0.0372821010\n",
      "Epoch 2026, Loss: 0.0373394429\n",
      "Epoch 2027, Loss: 0.0311561696\n",
      "Epoch 2028, Loss: 0.0372935325\n",
      "Epoch 2029, Loss: 0.0341480677\n",
      "Epoch 2030, Loss: 0.0348853507\n",
      "Epoch 2031, Loss: 0.0476644303\n",
      "Epoch 2032, Loss: 0.0399091603\n",
      "Epoch 2033, Loss: 0.0419041081\n",
      "Epoch 2034, Loss: 0.0562315702\n",
      "Epoch 2035, Loss: 0.0496002642\n",
      "Epoch 2036, Loss: 0.0433426131\n",
      "Epoch 2037, Loss: 0.0450202878\n",
      "Epoch 2038, Loss: 0.0570899707\n",
      "Epoch 2039, Loss: 0.0420763729\n",
      "Epoch 2040, Loss: 0.0704139153\n",
      "Epoch 2041, Loss: 0.0472789384\n",
      "Epoch 2042, Loss: 0.0985203540\n",
      "Epoch 2043, Loss: 0.0555855480\n",
      "Epoch 2044, Loss: 0.1463121623\n",
      "Epoch 2045, Loss: 0.0673798586\n",
      "Epoch 2046, Loss: 0.1571462950\n",
      "Epoch 2047, Loss: 0.0652233811\n",
      "Epoch 2048, Loss: 0.0921795342\n",
      "Epoch 2049, Loss: 0.1290722336\n",
      "Epoch 2050, Loss: 0.1708507309\n",
      "Epoch 2051, Loss: 0.2954635974\n",
      "Epoch 2052, Loss: 0.2706834133\n",
      "Epoch 2053, Loss: 0.1453358428\n",
      "Epoch 2054, Loss: 0.3969094999\n",
      "Epoch 2055, Loss: 0.3200678250\n",
      "Epoch 2056, Loss: 0.3749754050\n",
      "Epoch 2057, Loss: 0.1985099842\n",
      "Epoch 2058, Loss: 0.1067990706\n",
      "Epoch 2059, Loss: 0.0650533561\n",
      "Epoch 2060, Loss: 0.4744983008\n",
      "Epoch 2061, Loss: 0.2958141185\n",
      "Epoch 2062, Loss: 0.1211908704\n",
      "Epoch 2063, Loss: 0.1669701801\n",
      "Epoch 2064, Loss: 0.1187100674\n",
      "Epoch 2065, Loss: 0.1144333100\n",
      "Epoch 2066, Loss: 0.1821817610\n",
      "Epoch 2067, Loss: 0.2532197917\n",
      "Epoch 2068, Loss: 0.1840669716\n",
      "Epoch 2069, Loss: 0.1597846924\n",
      "Epoch 2070, Loss: 0.1116551024\n",
      "Epoch 2071, Loss: 0.1846165246\n",
      "Epoch 2072, Loss: 0.1654002758\n",
      "Epoch 2073, Loss: 0.2304806949\n",
      "Epoch 2074, Loss: 0.1627475121\n",
      "Epoch 2075, Loss: 0.1826906631\n",
      "Epoch 2076, Loss: 0.1170966202\n",
      "Epoch 2077, Loss: 0.1361900305\n",
      "Epoch 2078, Loss: 0.1568841975\n",
      "Epoch 2079, Loss: 0.1303305329\n",
      "Epoch 2080, Loss: 0.1523271689\n",
      "Epoch 2081, Loss: 0.1777181424\n",
      "Epoch 2082, Loss: 0.1590877707\n",
      "Epoch 2083, Loss: 0.1203555421\n",
      "Epoch 2084, Loss: 0.3215120147\n",
      "Epoch 2085, Loss: 0.4194427593\n",
      "Epoch 2086, Loss: 0.2710463497\n",
      "Epoch 2087, Loss: 0.1259166544\n",
      "Epoch 2088, Loss: 0.2128592025\n",
      "Epoch 2089, Loss: 0.4228332651\n",
      "Epoch 2090, Loss: 0.6260305221\n",
      "Epoch 2091, Loss: 0.1455516236\n",
      "Epoch 2092, Loss: 0.2462226780\n",
      "Epoch 2093, Loss: 0.2432734034\n",
      "Epoch 2094, Loss: 0.3728608302\n",
      "Epoch 2095, Loss: 0.1415318160\n",
      "Epoch 2096, Loss: 0.1172782321\n",
      "Epoch 2097, Loss: 0.1560446130\n",
      "Epoch 2098, Loss: 0.1314823285\n",
      "Epoch 2099, Loss: 0.0599487020\n",
      "Epoch 2100, Loss: 0.0452321951\n",
      "Epoch 2101, Loss: 0.0434040748\n",
      "Epoch 2102, Loss: 0.0518975383\n",
      "Epoch 2103, Loss: 0.0217750313\n",
      "Epoch 2104, Loss: 0.0410301547\n",
      "Epoch 2105, Loss: 0.0472818828\n",
      "Epoch 2106, Loss: 0.0272749379\n",
      "Epoch 2107, Loss: 0.0679282067\n",
      "Epoch 2108, Loss: 0.0495406905\n",
      "Epoch 2109, Loss: 0.0742706877\n",
      "Epoch 2110, Loss: 0.0202328805\n",
      "Epoch 2111, Loss: 0.0572935035\n",
      "Epoch 2112, Loss: 0.0185362997\n",
      "Epoch 2113, Loss: 0.0562146586\n",
      "Epoch 2114, Loss: 0.0243492835\n",
      "Epoch 2115, Loss: 0.0490885397\n",
      "Epoch 2116, Loss: 0.0492868046\n",
      "Epoch 2117, Loss: 0.0410893413\n",
      "Epoch 2118, Loss: 0.0680710817\n",
      "Epoch 2119, Loss: 0.0323155913\n",
      "Epoch 2120, Loss: 0.0485420734\n",
      "Epoch 2121, Loss: 0.0457864152\n",
      "Epoch 2122, Loss: 0.0455608177\n",
      "Epoch 2123, Loss: 0.0596210583\n",
      "Epoch 2124, Loss: 0.0353269138\n",
      "Epoch 2125, Loss: 0.0227538450\n",
      "Epoch 2126, Loss: 0.0592015204\n",
      "Epoch 2127, Loss: 0.0446452912\n",
      "Epoch 2128, Loss: 0.0300856115\n",
      "Epoch 2129, Loss: 0.0372009687\n",
      "Epoch 2130, Loss: 0.0780855948\n",
      "Epoch 2131, Loss: 0.0287906738\n",
      "Epoch 2132, Loss: 0.0246272214\n",
      "Epoch 2133, Loss: 0.0616799404\n",
      "Epoch 2134, Loss: 0.0410088861\n",
      "Epoch 2135, Loss: 0.0390077851\n",
      "Epoch 2136, Loss: 0.0355860461\n",
      "Epoch 2137, Loss: 0.0840167694\n",
      "Epoch 2138, Loss: 0.0215604285\n",
      "Epoch 2139, Loss: 0.0352830519\n",
      "Epoch 2140, Loss: 0.0426185721\n",
      "Epoch 2141, Loss: 0.0309155883\n",
      "Epoch 2142, Loss: 0.0909464274\n",
      "Epoch 2143, Loss: 0.0455086062\n",
      "Epoch 2144, Loss: 0.0375097017\n",
      "Epoch 2145, Loss: 0.0702086981\n",
      "Epoch 2146, Loss: 0.0844083159\n",
      "Epoch 2147, Loss: 0.0378356837\n",
      "Epoch 2148, Loss: 0.1640892362\n",
      "Epoch 2149, Loss: 0.0650312687\n",
      "Epoch 2150, Loss: 0.3196624979\n",
      "Epoch 2151, Loss: 0.2227075990\n",
      "Epoch 2152, Loss: 0.0491712066\n",
      "Epoch 2153, Loss: 0.0692043466\n",
      "Epoch 2154, Loss: 0.0433552522\n",
      "Epoch 2155, Loss: 0.0891647573\n",
      "Epoch 2156, Loss: 0.0424028526\n",
      "Epoch 2157, Loss: 0.0973951276\n",
      "Epoch 2158, Loss: 0.0581788868\n",
      "Epoch 2159, Loss: 0.0899352469\n",
      "Epoch 2160, Loss: 0.0944231165\n",
      "Epoch 2161, Loss: 0.4653498745\n",
      "Epoch 2162, Loss: 0.3514216731\n",
      "Epoch 2163, Loss: 0.4097004161\n",
      "Epoch 2164, Loss: 0.1267616270\n",
      "Epoch 2165, Loss: 0.7907577849\n",
      "Epoch 2166, Loss: 0.4752003047\n",
      "Epoch 2167, Loss: 0.3032078399\n",
      "Epoch 2168, Loss: 0.0998486261\n",
      "Epoch 2169, Loss: 0.1069329298\n",
      "Epoch 2170, Loss: 0.1973052150\n",
      "Epoch 2171, Loss: 0.1031195064\n",
      "Epoch 2172, Loss: 0.1308744684\n",
      "Epoch 2173, Loss: 0.1435267820\n",
      "Epoch 2174, Loss: 0.0963141125\n",
      "Epoch 2175, Loss: 0.0519938175\n",
      "Epoch 2176, Loss: 0.0976494442\n",
      "Epoch 2177, Loss: 0.0755856608\n",
      "Epoch 2178, Loss: 0.1120743493\n",
      "Epoch 2179, Loss: 0.0717846548\n",
      "Epoch 2180, Loss: 0.0999492986\n",
      "Epoch 2181, Loss: 0.0919693701\n",
      "Epoch 2182, Loss: 0.1092756996\n",
      "Epoch 2183, Loss: 0.0680682784\n",
      "Epoch 2184, Loss: 0.0814838036\n",
      "Epoch 2185, Loss: 0.0309108261\n",
      "Epoch 2186, Loss: 0.0350827596\n",
      "Epoch 2187, Loss: 0.0706466646\n",
      "Epoch 2188, Loss: 0.0209246767\n",
      "Epoch 2189, Loss: 0.0971844366\n",
      "Epoch 2190, Loss: 0.0719909376\n",
      "Epoch 2191, Loss: 0.0167744965\n",
      "Epoch 2192, Loss: 0.0547313084\n",
      "Epoch 2193, Loss: 0.0317709173\n",
      "Epoch 2194, Loss: 0.0209235140\n",
      "Epoch 2195, Loss: 0.0222477274\n",
      "Epoch 2196, Loss: 0.0418644772\n",
      "Epoch 2197, Loss: 0.0485696181\n",
      "Epoch 2198, Loss: 0.0403772336\n",
      "Epoch 2199, Loss: 0.0304894651\n",
      "Epoch 2200, Loss: 0.0664750311\n",
      "Epoch 2201, Loss: 0.0119126590\n",
      "Epoch 2202, Loss: 0.0248081293\n",
      "Epoch 2203, Loss: 0.0130969849\n",
      "Epoch 2204, Loss: 0.0705647309\n",
      "Epoch 2205, Loss: 0.0072908030\n",
      "Epoch 2206, Loss: 0.0626092194\n",
      "Epoch 2207, Loss: 0.0099529795\n",
      "Epoch 2208, Loss: 0.0222667603\n",
      "Epoch 2209, Loss: 0.0185185233\n",
      "Epoch 2210, Loss: 0.0561798315\n",
      "Epoch 2211, Loss: 0.0085750206\n",
      "Epoch 2212, Loss: 0.0887771611\n",
      "Epoch 2213, Loss: 0.0064731173\n",
      "Epoch 2214, Loss: 0.0789299539\n",
      "Epoch 2215, Loss: 0.0208331432\n",
      "Epoch 2216, Loss: 0.0096481440\n",
      "Epoch 2217, Loss: 0.0262799779\n",
      "Epoch 2218, Loss: 0.0457968834\n",
      "Epoch 2219, Loss: 0.0182416767\n",
      "Epoch 2220, Loss: 0.0209532897\n",
      "Epoch 2221, Loss: 0.0379637633\n",
      "Epoch 2222, Loss: 0.0418591958\n",
      "Epoch 2223, Loss: 0.0151997440\n",
      "Epoch 2224, Loss: 0.0349597296\n",
      "Epoch 2225, Loss: 0.0147709839\n",
      "Epoch 2226, Loss: 0.0534577763\n",
      "Epoch 2227, Loss: 0.0183334428\n",
      "Epoch 2228, Loss: 0.0454648619\n",
      "Epoch 2229, Loss: 0.0184197204\n",
      "Epoch 2230, Loss: 0.0718487432\n",
      "Epoch 2231, Loss: 0.0190698977\n",
      "Epoch 2232, Loss: 0.1118798023\n",
      "Epoch 2233, Loss: 0.0093838271\n",
      "Epoch 2234, Loss: 0.0590104165\n",
      "Epoch 2235, Loss: 0.0222110402\n",
      "Epoch 2236, Loss: 0.0667708873\n",
      "Epoch 2237, Loss: 0.0124983686\n",
      "Epoch 2238, Loss: 0.0411602974\n",
      "Epoch 2239, Loss: 0.0915626064\n",
      "Epoch 2240, Loss: 0.0370246502\n",
      "Epoch 2241, Loss: 0.0956202214\n",
      "Epoch 2242, Loss: 0.1289249215\n",
      "Epoch 2243, Loss: 0.0578911504\n",
      "Epoch 2244, Loss: 0.0299661004\n",
      "Epoch 2245, Loss: 0.0176261539\n",
      "Epoch 2246, Loss: 0.0241026169\n",
      "Epoch 2247, Loss: 0.0429066108\n",
      "Epoch 2248, Loss: 0.0300429541\n",
      "Epoch 2249, Loss: 0.0809545269\n",
      "Epoch 2250, Loss: 0.0560742735\n",
      "Epoch 2251, Loss: 0.1922096801\n",
      "Epoch 2252, Loss: 0.1075573590\n",
      "Epoch 2253, Loss: 0.1198323690\n",
      "Epoch 2254, Loss: 0.0807902442\n",
      "Epoch 2255, Loss: 0.0396948841\n",
      "Epoch 2256, Loss: 0.2621397363\n",
      "Epoch 2257, Loss: 0.1048666594\n",
      "Epoch 2258, Loss: 0.0503414007\n",
      "Epoch 2259, Loss: 0.0462765189\n",
      "Epoch 2260, Loss: 0.0287778395\n",
      "Epoch 2261, Loss: 0.0346630851\n",
      "Epoch 2262, Loss: 0.0419827682\n",
      "Epoch 2263, Loss: 0.0408997243\n",
      "Epoch 2264, Loss: 0.0367514184\n",
      "Epoch 2265, Loss: 0.0123552380\n",
      "Epoch 2266, Loss: 0.0470550109\n",
      "Epoch 2267, Loss: 0.0141568111\n",
      "Epoch 2268, Loss: 0.0462763909\n",
      "Epoch 2269, Loss: 0.0226750015\n",
      "Epoch 2270, Loss: 0.0329963090\n",
      "Epoch 2271, Loss: 0.0448055516\n",
      "Epoch 2272, Loss: 0.0600746041\n",
      "Epoch 2273, Loss: 0.0124529351\n",
      "Epoch 2274, Loss: 0.0343104233\n",
      "Epoch 2275, Loss: 0.0483447883\n",
      "Epoch 2276, Loss: 0.0327629286\n",
      "Epoch 2277, Loss: 0.0279477603\n",
      "Epoch 2278, Loss: 0.0393769898\n",
      "Epoch 2279, Loss: 0.0187863062\n",
      "Epoch 2280, Loss: 0.0346486507\n",
      "Epoch 2281, Loss: 0.0298122291\n",
      "Epoch 2282, Loss: 0.0664233221\n",
      "Epoch 2283, Loss: 0.0297046754\n",
      "Epoch 2284, Loss: 0.0485731165\n",
      "Epoch 2285, Loss: 0.0710324734\n",
      "Epoch 2286, Loss: 0.0358018292\n",
      "Epoch 2287, Loss: 0.0275572889\n",
      "Epoch 2288, Loss: 0.0607333106\n",
      "Epoch 2289, Loss: 0.1177233706\n",
      "Epoch 2290, Loss: 1.8023946967\n",
      "Epoch 2291, Loss: 1.1870019055\n",
      "Epoch 2292, Loss: 1.3340823255\n",
      "Epoch 2293, Loss: 0.7907275376\n",
      "Epoch 2294, Loss: 0.5771382272\n",
      "Epoch 2295, Loss: 0.3520718107\n",
      "Epoch 2296, Loss: 0.2920754167\n",
      "Epoch 2297, Loss: 0.1960197689\n",
      "Epoch 2298, Loss: 0.1930040001\n",
      "Epoch 2299, Loss: 0.1810759170\n",
      "Epoch 2300, Loss: 0.1247711312\n",
      "Epoch 2301, Loss: 0.1704883592\n",
      "Epoch 2302, Loss: 0.2000866914\n",
      "Epoch 2303, Loss: 0.3593834315\n",
      "Epoch 2304, Loss: 0.3290373208\n",
      "Epoch 2305, Loss: 0.1400428980\n",
      "Epoch 2306, Loss: 0.1159395051\n",
      "Epoch 2307, Loss: 0.1932207245\n",
      "Epoch 2308, Loss: 0.1686043367\n",
      "Epoch 2309, Loss: 0.2585961017\n",
      "Epoch 2310, Loss: 0.1404697889\n",
      "Epoch 2311, Loss: 0.1200583444\n",
      "Epoch 2312, Loss: 0.1772931508\n",
      "Epoch 2313, Loss: 0.1576822825\n",
      "Epoch 2314, Loss: 0.1703088918\n",
      "Epoch 2315, Loss: 0.0937000743\n",
      "Epoch 2316, Loss: 0.0513033588\n",
      "Epoch 2317, Loss: 0.0851820126\n",
      "Epoch 2318, Loss: 0.1178033558\n",
      "Epoch 2319, Loss: 0.0927394513\n",
      "Epoch 2320, Loss: 0.0546700282\n",
      "Epoch 2321, Loss: 0.0692290210\n",
      "Epoch 2322, Loss: 0.0868336007\n",
      "Epoch 2323, Loss: 0.0813238771\n",
      "Epoch 2324, Loss: 0.0769176991\n",
      "Epoch 2325, Loss: 0.0618685698\n",
      "Epoch 2326, Loss: 0.0929671127\n",
      "Epoch 2327, Loss: 0.1065654447\n",
      "Epoch 2328, Loss: 0.0699554818\n",
      "Epoch 2329, Loss: 0.1317842225\n",
      "Epoch 2330, Loss: 0.0732315532\n",
      "Epoch 2331, Loss: 0.1127071706\n",
      "Epoch 2332, Loss: 0.0801895522\n",
      "Epoch 2333, Loss: 0.1020263792\n",
      "Epoch 2334, Loss: 0.1021962473\n",
      "Epoch 2335, Loss: 0.0727957088\n",
      "Epoch 2336, Loss: 0.0739714396\n",
      "Epoch 2337, Loss: 0.0671626123\n",
      "Epoch 2338, Loss: 0.0809424066\n",
      "Epoch 2339, Loss: 0.0835196965\n",
      "Epoch 2340, Loss: 0.1062037973\n",
      "Epoch 2341, Loss: 0.0724839170\n",
      "Epoch 2342, Loss: 0.1001672562\n",
      "Epoch 2343, Loss: 0.0451559910\n",
      "Epoch 2344, Loss: 0.1068980976\n",
      "Epoch 2345, Loss: 0.0898470211\n",
      "Epoch 2346, Loss: 0.1151922750\n",
      "Epoch 2347, Loss: 0.0839014033\n",
      "Epoch 2348, Loss: 0.1236890714\n",
      "Epoch 2349, Loss: 0.1519355566\n",
      "Epoch 2350, Loss: 0.1010305845\n",
      "Epoch 2351, Loss: 0.0683058700\n",
      "Epoch 2352, Loss: 0.0308923066\n",
      "Epoch 2353, Loss: 0.1002645967\n",
      "Epoch 2354, Loss: 0.1046096383\n",
      "Epoch 2355, Loss: 0.1036856210\n",
      "Epoch 2356, Loss: 0.2539827000\n",
      "Epoch 2357, Loss: 0.2623863172\n",
      "Epoch 2358, Loss: 0.1546031943\n",
      "Epoch 2359, Loss: 0.2728589111\n",
      "Epoch 2360, Loss: 0.1611625921\n",
      "Epoch 2361, Loss: 0.1428524592\n",
      "Epoch 2362, Loss: 0.1321907608\n",
      "Epoch 2363, Loss: 0.1602735098\n",
      "Epoch 2364, Loss: 0.2763985692\n",
      "Epoch 2365, Loss: 0.1531288784\n",
      "Epoch 2366, Loss: 0.1107469883\n",
      "Epoch 2367, Loss: 0.1779903126\n",
      "Epoch 2368, Loss: 0.0551406294\n",
      "Epoch 2369, Loss: 0.0829262905\n",
      "Epoch 2370, Loss: 0.0329405873\n",
      "Epoch 2371, Loss: 0.1729324470\n",
      "Epoch 2372, Loss: 0.1155121592\n",
      "Epoch 2373, Loss: 0.0894940512\n",
      "Epoch 2374, Loss: 0.0855568533\n",
      "Epoch 2375, Loss: 0.0859114964\n",
      "Epoch 2376, Loss: 0.0601632410\n",
      "Epoch 2377, Loss: 0.1642231954\n",
      "Epoch 2378, Loss: 0.1087057652\n",
      "Epoch 2379, Loss: 0.0862144306\n",
      "Epoch 2380, Loss: 0.1084992018\n",
      "Epoch 2381, Loss: 0.1561909010\n",
      "Epoch 2382, Loss: 0.0463475720\n",
      "Epoch 2383, Loss: 0.0690769862\n",
      "Epoch 2384, Loss: 0.0695598832\n",
      "Epoch 2385, Loss: 0.0418305275\n",
      "Epoch 2386, Loss: 0.1036683593\n",
      "Epoch 2387, Loss: 0.0547482945\n",
      "Epoch 2388, Loss: 0.0739200040\n",
      "Epoch 2389, Loss: 0.0706905892\n",
      "Epoch 2390, Loss: 0.0528605722\n",
      "Epoch 2391, Loss: 0.0404254003\n",
      "Epoch 2392, Loss: 0.0549914823\n",
      "Epoch 2393, Loss: 0.0682548257\n",
      "Epoch 2394, Loss: 0.0793448014\n",
      "Epoch 2395, Loss: 0.0684417292\n",
      "Epoch 2396, Loss: 0.0535031766\n",
      "Epoch 2397, Loss: 0.1075326735\n",
      "Epoch 2398, Loss: 0.1021841480\n",
      "Epoch 2399, Loss: 0.0867717883\n",
      "Epoch 2400, Loss: 0.1180100065\n",
      "Epoch 2401, Loss: 0.0547308249\n",
      "Epoch 2402, Loss: 0.0699901348\n",
      "Epoch 2403, Loss: 0.1261950142\n",
      "Epoch 2404, Loss: 0.0737084183\n",
      "Epoch 2405, Loss: 0.0539110018\n",
      "Epoch 2406, Loss: 0.0433022689\n",
      "Epoch 2407, Loss: 0.0750722113\n",
      "Epoch 2408, Loss: 0.1145563009\n",
      "Epoch 2409, Loss: 0.1186502397\n",
      "Epoch 2410, Loss: 0.1758477786\n",
      "Epoch 2411, Loss: 0.1844593767\n",
      "Epoch 2412, Loss: 0.1297024075\n",
      "Epoch 2413, Loss: 0.0890766807\n",
      "Epoch 2414, Loss: 0.0452272118\n",
      "Epoch 2415, Loss: 0.0428829856\n",
      "Epoch 2416, Loss: 0.0553049214\n",
      "Epoch 2417, Loss: 0.2922844261\n",
      "Epoch 2418, Loss: 0.0555452419\n",
      "Epoch 2419, Loss: 0.0933341876\n",
      "Epoch 2420, Loss: 0.0957698774\n",
      "Epoch 2421, Loss: 0.0299095966\n",
      "Epoch 2422, Loss: 0.1310166451\n",
      "Epoch 2423, Loss: 0.0879362333\n",
      "Epoch 2424, Loss: 0.0567014249\n",
      "Epoch 2425, Loss: 0.1262810165\n",
      "Epoch 2426, Loss: 0.1013382188\n",
      "Epoch 2427, Loss: 0.1054188131\n",
      "Epoch 2428, Loss: 0.1041313728\n",
      "Epoch 2429, Loss: 0.1300382473\n",
      "Epoch 2430, Loss: 0.1056728928\n",
      "Epoch 2431, Loss: 0.0300383605\n",
      "Epoch 2432, Loss: 0.0594590096\n",
      "Epoch 2433, Loss: 0.1525476836\n",
      "Epoch 2434, Loss: 0.1278225708\n",
      "Epoch 2435, Loss: 0.1032809556\n",
      "Epoch 2436, Loss: 0.0507716910\n",
      "Epoch 2437, Loss: 0.0897478111\n",
      "Epoch 2438, Loss: 0.0827338930\n",
      "Epoch 2439, Loss: 0.0559879399\n",
      "Epoch 2440, Loss: 0.0764481994\n",
      "Epoch 2441, Loss: 0.0325167001\n",
      "Epoch 2442, Loss: 0.0899042352\n",
      "Epoch 2443, Loss: 0.0450535171\n",
      "Epoch 2444, Loss: 0.0432118708\n",
      "Epoch 2445, Loss: 0.0566920889\n",
      "Epoch 2446, Loss: 0.0360823449\n",
      "Epoch 2447, Loss: 0.0658883787\n",
      "Epoch 2448, Loss: 0.0976678787\n",
      "Epoch 2449, Loss: 0.0161176987\n",
      "Epoch 2450, Loss: 0.0491851517\n",
      "Epoch 2451, Loss: 0.0227699701\n",
      "Epoch 2452, Loss: 0.0284845927\n",
      "Epoch 2453, Loss: 0.0605073808\n",
      "Epoch 2454, Loss: 0.0404700120\n",
      "Epoch 2455, Loss: 0.0219092403\n",
      "Epoch 2456, Loss: 0.0690386045\n",
      "Epoch 2457, Loss: 0.0106188468\n",
      "Epoch 2458, Loss: 0.0787177268\n",
      "Epoch 2459, Loss: 0.1968666630\n",
      "Epoch 2460, Loss: 0.0266277942\n",
      "Epoch 2461, Loss: 0.0471210360\n",
      "Epoch 2462, Loss: 0.0231433334\n",
      "Epoch 2463, Loss: 0.1355888620\n",
      "Epoch 2464, Loss: 0.0725760825\n",
      "Epoch 2465, Loss: 0.2632329383\n",
      "Epoch 2466, Loss: 0.2054586523\n",
      "Epoch 2467, Loss: 0.0344689392\n",
      "Epoch 2468, Loss: 0.0287489415\n",
      "Epoch 2469, Loss: 0.0456564474\n",
      "Epoch 2470, Loss: 0.0609838020\n",
      "Epoch 2471, Loss: 1.1650992594\n",
      "Epoch 2472, Loss: 0.3959856377\n",
      "Epoch 2473, Loss: 0.3773459840\n",
      "Epoch 2474, Loss: 0.3324918097\n",
      "Epoch 2475, Loss: 0.1399470373\n",
      "Epoch 2476, Loss: 0.1288418732\n",
      "Epoch 2477, Loss: 0.0497341125\n",
      "Epoch 2478, Loss: 0.0586432659\n",
      "Epoch 2479, Loss: 0.0363202129\n",
      "Epoch 2480, Loss: 0.0316136505\n",
      "Epoch 2481, Loss: 0.0847465933\n",
      "Epoch 2482, Loss: 0.2159639827\n",
      "Epoch 2483, Loss: 0.1051992801\n",
      "Epoch 2484, Loss: 0.0413610853\n",
      "Epoch 2485, Loss: 0.0730706131\n",
      "Epoch 2486, Loss: 0.0434539671\n",
      "Epoch 2487, Loss: 0.0550957154\n",
      "Epoch 2488, Loss: 0.0650276758\n",
      "Epoch 2489, Loss: 0.0672709388\n",
      "Epoch 2490, Loss: 0.0366611532\n",
      "Epoch 2491, Loss: 0.0512986871\n",
      "Epoch 2492, Loss: 0.0630062040\n",
      "Epoch 2493, Loss: 0.0458569544\n",
      "Epoch 2494, Loss: 0.1198951085\n",
      "Epoch 2495, Loss: 0.0502105360\n",
      "Epoch 2496, Loss: 0.0473749325\n",
      "Epoch 2497, Loss: 0.1176732186\n",
      "Epoch 2498, Loss: 0.0524336803\n",
      "Epoch 2499, Loss: 0.0569156400\n",
      "Epoch 2500, Loss: 0.0519759392\n",
      "Epoch 2501, Loss: 0.1203871723\n",
      "Epoch 2502, Loss: 0.0317843787\n",
      "Epoch 2503, Loss: 0.0381089316\n",
      "Epoch 2504, Loss: 0.0750200538\n",
      "Epoch 2505, Loss: 0.0383095941\n",
      "Epoch 2506, Loss: 0.0235221712\n",
      "Epoch 2507, Loss: 0.0414475420\n",
      "Epoch 2508, Loss: 0.0416806969\n",
      "Epoch 2509, Loss: 0.0431208128\n",
      "Epoch 2510, Loss: 0.0329129628\n",
      "Epoch 2511, Loss: 0.0379466073\n",
      "Epoch 2512, Loss: 0.1182161615\n",
      "Epoch 2513, Loss: 0.1001417612\n",
      "Epoch 2514, Loss: 0.1791415368\n",
      "Epoch 2515, Loss: 0.1106805838\n",
      "Epoch 2516, Loss: 0.0199414618\n",
      "Epoch 2517, Loss: 0.0247174262\n",
      "Epoch 2518, Loss: 0.0610080236\n",
      "Epoch 2519, Loss: 0.0774949500\n",
      "Epoch 2520, Loss: 0.0712105627\n",
      "Epoch 2521, Loss: 0.4928177257\n",
      "Epoch 2522, Loss: 0.7787658893\n",
      "Epoch 2523, Loss: 0.6848026572\n",
      "Epoch 2524, Loss: 0.1681856241\n",
      "Epoch 2525, Loss: 0.0935185744\n",
      "Epoch 2526, Loss: 0.1284441853\n",
      "Epoch 2527, Loss: 0.0788489460\n",
      "Epoch 2528, Loss: 0.1082931013\n",
      "Epoch 2529, Loss: 0.0725588372\n",
      "Epoch 2530, Loss: 0.0686038368\n",
      "Epoch 2531, Loss: 0.0653434815\n",
      "Epoch 2532, Loss: 0.0794952015\n",
      "Epoch 2533, Loss: 0.0754465781\n",
      "Epoch 2534, Loss: 0.0675335313\n",
      "Epoch 2535, Loss: 0.0742170824\n",
      "Epoch 2536, Loss: 0.2188621833\n",
      "Epoch 2537, Loss: 0.0828070297\n",
      "Epoch 2538, Loss: 0.0756317127\n",
      "Epoch 2539, Loss: 0.3033404003\n",
      "Epoch 2540, Loss: 0.2707545028\n",
      "Epoch 2541, Loss: 0.3171120061\n",
      "Epoch 2542, Loss: 0.2098490950\n",
      "Epoch 2543, Loss: 0.0976153939\n",
      "Epoch 2544, Loss: 0.1363426131\n",
      "Epoch 2545, Loss: 0.0764853757\n",
      "Epoch 2546, Loss: 0.0611910495\n",
      "Epoch 2547, Loss: 0.0758767319\n",
      "Epoch 2548, Loss: 0.0911598163\n",
      "Epoch 2549, Loss: 0.0773820231\n",
      "Epoch 2550, Loss: 0.0526775767\n",
      "Epoch 2551, Loss: 0.1084331284\n",
      "Epoch 2552, Loss: 0.1192908768\n",
      "Epoch 2553, Loss: 0.1174424635\n",
      "Epoch 2554, Loss: 0.1103571588\n",
      "Epoch 2555, Loss: 0.1850946265\n",
      "Epoch 2556, Loss: 0.0901339908\n",
      "Epoch 2557, Loss: 0.0818197791\n",
      "Epoch 2558, Loss: 0.0768926868\n",
      "Epoch 2559, Loss: 0.1296308319\n",
      "Epoch 2560, Loss: 0.0950199887\n",
      "Epoch 2561, Loss: 0.0631674315\n",
      "Epoch 2562, Loss: 0.1625825117\n",
      "Epoch 2563, Loss: 0.1678374531\n",
      "Epoch 2564, Loss: 0.0403619298\n",
      "Epoch 2565, Loss: 0.1168570950\n",
      "Epoch 2566, Loss: 0.0929749421\n",
      "Epoch 2567, Loss: 0.1777787771\n",
      "Epoch 2568, Loss: 0.0555436675\n",
      "Epoch 2569, Loss: 0.1485788073\n",
      "Epoch 2570, Loss: 0.1552860460\n",
      "Epoch 2571, Loss: 0.0954800580\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "best_weights_init = []\n",
    "best_biases_init = []\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "best_seed = 0\n",
    "\n",
    "for _ in range (1):\n",
    "    nn = NeuralNetwork_Adam(625, [256, 128, 64], 8)\n",
    "    nn.train(batches, 3, 0.001)\n",
    "    if nn.get_best_loss() < best_loss:\n",
    "        best_loss = nn.get_best_loss()\n",
    "        best_weights = nn.get_best_weights()\n",
    "        best_biases = nn.get_best_biases()\n",
    "        best_seed = nn.get_best_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006473117251765303\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "# print(best_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8, init_weights=best_weights, init_biases=best_biases, init_seed=best_seed)\n",
    "# nn.train(batches, 3, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.get_best_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Number of layers in the Neural Network\n",
    "N = 4  # Example value, replace with the actual number of layers\n",
    "\n",
    "# Initialize the dictionary\n",
    "weights_dict = {\n",
    "    'weights': {},\n",
    "    'bias': {}\n",
    "}\n",
    "\n",
    "weights = nn.get_best_weights()\n",
    "biases = nn.get_best_biases()\n",
    "\n",
    "# Populate the weights and bias dictionaries\n",
    "for i in range(N):\n",
    "    weights_dict['weights'][f'fc{i+1}'] = weights[i]\n",
    "    weights_dict['bias'][f'fc{i+1}'] = biases[i].flatten()\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09548005798612998\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'val' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))\n",
    "\n",
    "for X_val, Y_val in batches:\n",
    "    Y_pred= nn.predict(X_val)\n",
    "    print(cross_entropy_loss(Y_val,Y_pred)/len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
