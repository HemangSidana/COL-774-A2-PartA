{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Remember to import \"numpy_transforms\" functions if you wish to import these two classes in a different script.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "        label = row[\"class\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image), label\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image, label = self.dataset[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "        batch_labels = np.array(labels)\n",
    "\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(128, 625)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    print(labels.shape)  # Should be [batch_size]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# # ReLU activation and its derivative\n",
    "# def relu(x):\n",
    "#     return np.maximum(0, x)\n",
    "\n",
    "# def relu_derivative(x):\n",
    "#     return np.where(x > 0, 1, 0)\n",
    "\n",
    "# # Mean Squared Error loss\n",
    "# def mean_squared_error(y_true, y_pred):\n",
    "#     return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "# # Neural Network Class with ReLU in the Output Layer and Hidden Layers\n",
    "# class NeuralNetwork_Adam:\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "#         if init_seed is None:\n",
    "#             self.best_seed = int(time.time())\n",
    "#             np.random.seed(self.best_seed)\n",
    "#         else:\n",
    "#             np.random.seed(init_seed)\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "#         self.m_w = []\n",
    "#         self.v_w = []\n",
    "#         self.m_b = []\n",
    "#         self.v_b = []\n",
    "#         self.beta1 = beta1\n",
    "#         self.beta2 = beta2\n",
    "#         self.epsilon = epsilon\n",
    "#         self.t = 0  # Time step for Adam\n",
    "#         self.best_weights = []\n",
    "#         self.best_biases = []\n",
    "#         self.best_loss = float(\"inf\")\n",
    "\n",
    "#         layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "#         # Initialize weights, biases, and Adam parameters (m, v)\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             if (init_weights is not None) and (init_biases is not None):\n",
    "#                 self.weights.append(init_weights[i])\n",
    "#                 self.biases.append(init_biases[i])\n",
    "#             else:\n",
    "#                 self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "#                 self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "#             self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.best_weights = self.weights\n",
    "#             self.best_biases = self.biases\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         pre_activations = []\n",
    "\n",
    "#         # Pass through each layer except the output layer\n",
    "#         for i in range(len(self.weights) - 1):\n",
    "#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             pre_activations.append(z)\n",
    "#             a = relu(z)  # ReLU for hidden layers\n",
    "#             activations.append(a)\n",
    "\n",
    "#         # Pass through the output layer with ReLU\n",
    "#         z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         pre_activations.append(z)\n",
    "#         a = relu(z)  # ReLU for the output layer\n",
    "#         activations.append(a)\n",
    "\n",
    "#         return activations, pre_activations\n",
    "\n",
    "#     def backward(self, X, y, activations, pre_activations):\n",
    "#         grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "#         grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "#         # Start with output layer error\n",
    "#         delta = activations[-1] - y\n",
    "#         delta *= relu_derivative(pre_activations[-1])  # ReLU derivative for the output layer\n",
    "\n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "#             grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "#             if i > 0:\n",
    "#                 delta = np.dot(delta, self.weights[i].T) * relu_derivative(pre_activations[i - 1])\n",
    "\n",
    "#         return grad_w, grad_b\n",
    "\n",
    "#     def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "#         self.t += 1  # Increment time step for Adam\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             # Update biased first moment estimate\n",
    "#             self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "#             self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "#             # Update biased second moment estimate\n",
    "#             self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "#             self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "#             # Compute bias-corrected first moment estimate\n",
    "#             m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "#             m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "#             # Compute bias-corrected second moment estimate\n",
    "#             v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "#             v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "#             # Update weights and biases\n",
    "#             self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "#             self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "#     def train(self, batches, time_of_running, learning_rate):\n",
    "#         start_time = time.time()\n",
    "#         epoch = 0\n",
    "#         while True:\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 activations, pre_activations = self.forward(X_batch)\n",
    "#                 grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "#                 self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "#             # Calculate average loss over batches\n",
    "#             loss = 0\n",
    "#             z = 0\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 y_pred, _ = self.forward(X_batch)\n",
    "#                 loss += mean_squared_error(y_batch, y_pred[-1])\n",
    "#                 z += len(y_pred[-1])\n",
    "#             loss /= z\n",
    "            \n",
    "#             if loss < self.best_loss:\n",
    "#                 self.best_loss = loss\n",
    "#                 self.best_weights = self.weights\n",
    "#                 self.best_biases = self.biases\n",
    "#             print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "#             epoch += 1\n",
    "#             # if time elapsed is greater than 1 minute, break the loop\n",
    "#             if time.time() - start_time > 60 * time_of_running:\n",
    "#                 break\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "#     def get_best_weights(self):\n",
    "#         return self.best_weights\n",
    "\n",
    "#     def get_best_biases(self):\n",
    "#         return self.best_biases\n",
    "\n",
    "#     def get_best_loss(self):\n",
    "#         return self.best_loss\n",
    "\n",
    "#     def get_best_seed(self):\n",
    "#         return self.best_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'val' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset_val = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "# Create the DataLoader\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=len(dataset_val))\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches_val=[]\n",
    "for images,labels in dataloader_val:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches_val.append((images,one_hot_labels))\n",
    "\n",
    "accu=[]\n",
    "def get_stat():\n",
    "    for X_val, Y_val in batches_val:\n",
    "        Y_pred= nn.predict(X_val)\n",
    "        # print(cross_entropy_loss(Y_val,Y_pred)/len(dataset))\n",
    "        z=accuracy(Y_val, Y_pred)\n",
    "        accu.append(z)\n",
    "        print(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Leaky ReLU activation and its derivative\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Neural Network Class with Leaky ReLU and Adam Optimizer\n",
    "class NeuralNetwork_Adam:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, alpha=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        if init_seed is None:\n",
    "            self.best_seed = int(time.time())\n",
    "            np.random.seed(self.best_seed)\n",
    "        else:\n",
    "            np.random.seed(init_seed)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.m_w = []\n",
    "        self.v_w = []\n",
    "        self.m_b = []\n",
    "        self.v_b = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step for Adam\n",
    "        self.alpha = alpha  # Leaky ReLU parameter\n",
    "        self.best_weights = []\n",
    "        self.best_biases = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if init_weights is not None and init_biases is not None:\n",
    "                self.weights.append(init_weights[i])\n",
    "                self.biases.append(init_biases[i])\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "                self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "            self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "        # self.best_weights = self.weights\n",
    "        # self.best_biases = self.biases\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each hidden layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = leaky_relu(z, alpha=self.alpha)  # Leaky ReLU for hidden layers\n",
    "            # a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def forward_pred(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each hidden layer\n",
    "        for i in range(len(self.best_weights) - 1):\n",
    "            z = np.dot(activations[-1], self.best_weights[i]) + self.best_biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = leaky_relu(z, alpha=self.alpha)  # Leaky ReLU for hidden layers\n",
    "            # a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.best_weights[-1]) + self.best_biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * leaky_relu_derivative(pre_activations[i - 1], alpha=self.alpha)\n",
    "                # delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        self.t += 1  # Increment time step for Adam\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def train(self, batches, time_of_running, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while True:\n",
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
    "            loss /= z\n",
    "            \n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = [np.copy(w) for w in self.weights]  # Use np.copy to create independent copies\n",
    "                self.best_biases = [np.copy(b) for b in self.biases]  # Use np.copy for biases\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            get_stat()\n",
    "            epoch += 1\n",
    "            # if time elapsed is greater than 1 minute, break the loop\n",
    "            if time.time() - start_time > 60 * time_of_running:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_pred(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    \n",
    "    def get_best_biases(self):\n",
    "        return self.best_biases\n",
    "    \n",
    "    def get_best_loss(self):\n",
    "        return self.best_loss\n",
    "    \n",
    "    def get_best_seed(self):\n",
    "        return self.best_seed\n",
    "\n",
    "# Example usage:\n",
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8)\n",
    "# nn.train(batches, 1, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.1609061625\n",
      "0.12625\n",
      "Epoch 2, Loss: 2.1285123255\n",
      "0.125\n",
      "Epoch 3, Loss: 2.0867514542\n",
      "0.17875\n",
      "Epoch 4, Loss: 2.0593951172\n",
      "0.125\n",
      "Epoch 5, Loss: 2.0398176002\n",
      "0.12625\n",
      "Epoch 6, Loss: 2.0249242763\n",
      "0.21125\n",
      "Epoch 7, Loss: 2.0227918921\n",
      "0.24125\n",
      "Epoch 8, Loss: 2.0206058696\n",
      "0.19625\n",
      "Epoch 9, Loss: 1.9689863800\n",
      "0.23625\n",
      "Epoch 10, Loss: 1.9335954007\n",
      "0.23875\n",
      "Epoch 11, Loss: 1.8739352765\n",
      "0.38375\n",
      "Epoch 12, Loss: 1.8225156874\n",
      "0.2975\n",
      "Epoch 13, Loss: 1.7803616662\n",
      "0.2725\n",
      "Epoch 14, Loss: 1.7174082559\n",
      "0.29125\n",
      "Epoch 15, Loss: 1.7134289399\n",
      "0.34875\n",
      "Epoch 16, Loss: 1.6339820140\n",
      "0.39\n",
      "Epoch 17, Loss: 1.6041188120\n",
      "0.45125\n",
      "Epoch 18, Loss: 1.6012307639\n",
      "0.3875\n",
      "Epoch 19, Loss: 1.5653306189\n",
      "0.42\n",
      "Epoch 20, Loss: 1.5466052700\n",
      "0.43\n",
      "Epoch 21, Loss: 1.5281205680\n",
      "0.41375\n",
      "Epoch 22, Loss: 1.5200007172\n",
      "0.39875\n",
      "Epoch 23, Loss: 1.4736901671\n",
      "0.4425\n",
      "Epoch 24, Loss: 1.4464599568\n",
      "0.43875\n",
      "Epoch 25, Loss: 1.4202859503\n",
      "0.45375\n",
      "Epoch 26, Loss: 1.3826374651\n",
      "0.4825\n",
      "Epoch 27, Loss: 1.3905137525\n",
      "0.4825\n",
      "Epoch 28, Loss: 1.3481045328\n",
      "0.4575\n",
      "Epoch 29, Loss: 1.3541197791\n",
      "0.4575\n",
      "Epoch 30, Loss: 1.3679642762\n",
      "0.4575\n",
      "Epoch 31, Loss: 1.3323086710\n",
      "0.4725\n",
      "Epoch 32, Loss: 1.2844373440\n",
      "0.49375\n",
      "Epoch 33, Loss: 1.3236452946\n",
      "0.49375\n",
      "Epoch 34, Loss: 1.2953650327\n",
      "0.49375\n",
      "Epoch 35, Loss: 1.2444764016\n",
      "0.505\n",
      "Epoch 36, Loss: 1.2411419902\n",
      "0.49875\n",
      "Epoch 37, Loss: 1.2007634425\n",
      "0.51\n",
      "Epoch 38, Loss: 1.1781738306\n",
      "0.52875\n",
      "Epoch 39, Loss: 1.1636341559\n",
      "0.52625\n",
      "Epoch 40, Loss: 1.1688166937\n",
      "0.52625\n",
      "Epoch 41, Loss: 1.1575823839\n",
      "0.5175\n",
      "Epoch 42, Loss: 1.1326446790\n",
      "0.5375\n",
      "Epoch 43, Loss: 1.1211295449\n",
      "0.54625\n",
      "Epoch 44, Loss: 1.1133554483\n",
      "0.5475\n",
      "Epoch 45, Loss: 1.1043967122\n",
      "0.55125\n",
      "Epoch 46, Loss: 1.0982194004\n",
      "0.54\n",
      "Epoch 47, Loss: 1.0945437767\n",
      "0.54125\n",
      "Epoch 48, Loss: 1.0874732737\n",
      "0.54375\n",
      "Epoch 49, Loss: 1.0751207925\n",
      "0.5475\n",
      "Epoch 50, Loss: 1.0643979486\n",
      "0.5425\n",
      "Epoch 51, Loss: 1.0751568914\n",
      "0.5425\n",
      "Epoch 52, Loss: 1.0719910634\n",
      "0.5425\n",
      "Epoch 53, Loss: 1.0579780610\n",
      "0.545\n",
      "Epoch 54, Loss: 1.0600567597\n",
      "0.545\n",
      "Epoch 55, Loss: 1.0483980048\n",
      "0.56\n",
      "Epoch 56, Loss: 1.0461236208\n",
      "0.5725\n",
      "Epoch 57, Loss: 1.0102967581\n",
      "0.57125\n",
      "Epoch 58, Loss: 1.0200674255\n",
      "0.57125\n",
      "Epoch 59, Loss: 1.0111558509\n",
      "0.57125\n",
      "Epoch 60, Loss: 1.0066475082\n",
      "0.57625\n",
      "Epoch 61, Loss: 0.9992883988\n",
      "0.5575\n",
      "Epoch 62, Loss: 0.9927230513\n",
      "0.57375\n",
      "Epoch 63, Loss: 0.9765897822\n",
      "0.56375\n",
      "Epoch 64, Loss: 0.9714028533\n",
      "0.58375\n",
      "Epoch 65, Loss: 0.9594022236\n",
      "0.56375\n",
      "Epoch 66, Loss: 0.9569171704\n",
      "0.5825\n",
      "Epoch 67, Loss: 0.9914133287\n",
      "0.5825\n",
      "Epoch 68, Loss: 1.0323425695\n",
      "0.5825\n",
      "Epoch 69, Loss: 0.9848531648\n",
      "0.5825\n",
      "Epoch 70, Loss: 0.9412409148\n",
      "0.5725\n",
      "Epoch 71, Loss: 0.9561445530\n",
      "0.5725\n",
      "Epoch 72, Loss: 0.9468372948\n",
      "0.5725\n",
      "Epoch 73, Loss: 0.9620059875\n",
      "0.5725\n",
      "Epoch 74, Loss: 0.9390463328\n",
      "0.57625\n",
      "Epoch 75, Loss: 0.9638086769\n",
      "0.57625\n",
      "Epoch 76, Loss: 1.0278323534\n",
      "0.57625\n",
      "Epoch 77, Loss: 0.9495577210\n",
      "0.57625\n",
      "Epoch 78, Loss: 0.8991269456\n",
      "0.60375\n",
      "Epoch 79, Loss: 0.9099162959\n",
      "0.60375\n",
      "Epoch 80, Loss: 0.9246176214\n",
      "0.60375\n",
      "Epoch 81, Loss: 0.9076087605\n",
      "0.60375\n",
      "Epoch 82, Loss: 0.8632552445\n",
      "0.60125\n",
      "Epoch 83, Loss: 0.8629838458\n",
      "0.60375\n",
      "Epoch 84, Loss: 0.8801291583\n",
      "0.60375\n",
      "Epoch 85, Loss: 0.8852683918\n",
      "0.60375\n",
      "Epoch 86, Loss: 0.8651537267\n",
      "0.60375\n",
      "Epoch 87, Loss: 0.8537602787\n",
      "0.60375\n",
      "Epoch 88, Loss: 0.8790761504\n",
      "0.60375\n",
      "Epoch 89, Loss: 0.9119218307\n",
      "0.60375\n",
      "Epoch 90, Loss: 0.9585343548\n",
      "0.60375\n",
      "Epoch 91, Loss: 0.9376110288\n",
      "0.60375\n",
      "Epoch 92, Loss: 0.8881657671\n",
      "0.60375\n",
      "Epoch 93, Loss: 0.8472085986\n",
      "0.61625\n",
      "Epoch 94, Loss: 0.8684817967\n",
      "0.61625\n",
      "Epoch 95, Loss: 0.8370546129\n",
      "0.605\n",
      "Epoch 96, Loss: 0.8591410772\n",
      "0.605\n",
      "Epoch 97, Loss: 0.8562734862\n",
      "0.605\n",
      "Epoch 98, Loss: 0.8284692703\n",
      "0.6075\n",
      "Epoch 99, Loss: 0.8070610027\n",
      "0.61875\n",
      "Epoch 100, Loss: 0.7970863750\n",
      "0.6125\n",
      "Epoch 101, Loss: 0.8469719046\n",
      "0.6125\n",
      "Epoch 102, Loss: 1.1161681915\n",
      "0.6125\n",
      "Epoch 103, Loss: 0.9283321075\n",
      "0.6125\n",
      "Epoch 104, Loss: 1.0382396907\n",
      "0.6125\n",
      "Epoch 105, Loss: 0.9250674729\n",
      "0.6125\n",
      "Epoch 106, Loss: 1.0068327653\n",
      "0.6125\n",
      "Epoch 107, Loss: 0.9398736136\n",
      "0.6125\n",
      "Epoch 108, Loss: 0.9128656529\n",
      "0.6125\n",
      "Epoch 109, Loss: 0.9673424008\n",
      "0.6125\n",
      "Epoch 110, Loss: 1.0235747670\n",
      "0.6125\n",
      "Epoch 111, Loss: 0.9332477501\n",
      "0.6125\n",
      "Epoch 112, Loss: 0.9085705136\n",
      "0.6125\n",
      "Epoch 113, Loss: 0.8216858942\n",
      "0.6125\n",
      "Epoch 114, Loss: 0.8188862033\n",
      "0.6125\n",
      "Epoch 115, Loss: 0.8115881085\n",
      "0.6125\n",
      "Epoch 116, Loss: 0.8288611539\n",
      "0.6125\n",
      "Epoch 117, Loss: 0.7721862817\n",
      "0.63125\n",
      "Epoch 118, Loss: 0.8237527662\n",
      "0.63125\n",
      "Epoch 119, Loss: 0.8049671289\n",
      "0.63125\n",
      "Epoch 120, Loss: 0.7765361086\n",
      "0.63125\n",
      "Epoch 121, Loss: 0.7621272038\n",
      "0.63375\n",
      "Epoch 122, Loss: 0.7719425696\n",
      "0.63375\n",
      "Epoch 123, Loss: 0.8063150122\n",
      "0.63375\n",
      "Epoch 124, Loss: 0.7979902245\n",
      "0.63375\n",
      "Epoch 125, Loss: 0.7570209439\n",
      "0.61875\n",
      "Epoch 126, Loss: 0.7701242619\n",
      "0.61875\n",
      "Epoch 127, Loss: 0.7633087505\n",
      "0.61875\n",
      "Epoch 128, Loss: 0.7634750304\n",
      "0.61875\n",
      "Epoch 129, Loss: 0.7342316206\n",
      "0.60875\n",
      "Epoch 130, Loss: 0.7185896316\n",
      "0.6225\n",
      "Epoch 131, Loss: 0.7312343621\n",
      "0.6225\n",
      "Epoch 132, Loss: 0.7596994657\n",
      "0.6225\n",
      "Epoch 133, Loss: 0.7864278149\n",
      "0.6225\n",
      "Epoch 134, Loss: 0.7242648811\n",
      "0.6225\n",
      "Epoch 135, Loss: 0.7295626875\n",
      "0.6225\n",
      "Epoch 136, Loss: 0.7023897098\n",
      "0.63125\n",
      "Epoch 137, Loss: 0.7462063345\n",
      "0.63125\n",
      "Epoch 138, Loss: 0.7512803183\n",
      "0.63125\n",
      "Epoch 139, Loss: 0.7285542646\n",
      "0.63125\n",
      "Epoch 140, Loss: 0.7230948136\n",
      "0.63125\n",
      "Epoch 141, Loss: 0.6893892837\n",
      "0.62\n",
      "Epoch 142, Loss: 0.6919548830\n",
      "0.62\n",
      "Epoch 143, Loss: 0.7030787801\n",
      "0.62\n",
      "Epoch 144, Loss: 0.7687677242\n",
      "0.62\n",
      "Epoch 145, Loss: 0.7059187514\n",
      "0.62\n",
      "Epoch 146, Loss: 0.6856766156\n",
      "0.625\n",
      "Epoch 147, Loss: 0.6916021029\n",
      "0.625\n",
      "Epoch 148, Loss: 0.6934537079\n",
      "0.625\n",
      "Epoch 149, Loss: 0.7085146391\n",
      "0.625\n",
      "Epoch 150, Loss: 0.7476692637\n",
      "0.625\n",
      "Epoch 151, Loss: 0.6956914792\n",
      "0.625\n",
      "Epoch 152, Loss: 0.7136106058\n",
      "0.625\n",
      "Epoch 153, Loss: 0.7037501515\n",
      "0.625\n",
      "Epoch 154, Loss: 0.6957354181\n",
      "0.625\n",
      "Epoch 155, Loss: 0.7340356512\n",
      "0.625\n",
      "Epoch 156, Loss: 0.7213820687\n",
      "0.625\n",
      "Epoch 157, Loss: 0.7103672965\n",
      "0.625\n",
      "Epoch 158, Loss: 0.7669796869\n",
      "0.625\n",
      "Epoch 159, Loss: 0.8175367824\n",
      "0.625\n",
      "Epoch 160, Loss: 0.7824623846\n",
      "0.625\n",
      "Epoch 161, Loss: 0.7007341442\n",
      "0.625\n",
      "Epoch 162, Loss: 0.6702971827\n",
      "0.62625\n",
      "Epoch 163, Loss: 0.6659373793\n",
      "0.6225\n",
      "Epoch 164, Loss: 0.6589920429\n",
      "0.63375\n",
      "Epoch 165, Loss: 0.6753132970\n",
      "0.63375\n",
      "Epoch 166, Loss: 0.6409519996\n",
      "0.62125\n",
      "Epoch 167, Loss: 0.7132763244\n",
      "0.62125\n",
      "Epoch 168, Loss: 0.6692446689\n",
      "0.62125\n",
      "Epoch 169, Loss: 0.6738242674\n",
      "0.62125\n",
      "Epoch 170, Loss: 0.6133453744\n",
      "0.63\n",
      "Epoch 171, Loss: 0.6626561853\n",
      "0.63\n",
      "Epoch 172, Loss: 0.6447292873\n",
      "0.63\n",
      "Epoch 173, Loss: 0.6603662391\n",
      "0.63\n",
      "Epoch 174, Loss: 0.6972111259\n",
      "0.63\n",
      "Epoch 175, Loss: 0.6618482969\n",
      "0.63\n",
      "Epoch 176, Loss: 0.6715224811\n",
      "0.63\n",
      "Epoch 177, Loss: 0.6451732244\n",
      "0.63\n",
      "Epoch 178, Loss: 0.6380596504\n",
      "0.63\n",
      "Epoch 179, Loss: 0.6227069179\n",
      "0.63\n",
      "Epoch 180, Loss: 0.6238267244\n",
      "0.63\n",
      "Epoch 181, Loss: 0.6111841127\n",
      "0.6125\n",
      "Epoch 182, Loss: 0.6646557175\n",
      "0.6125\n",
      "Epoch 183, Loss: 0.6645786213\n",
      "0.6125\n",
      "Epoch 184, Loss: 0.6176634872\n",
      "0.6125\n",
      "Epoch 185, Loss: 0.6024701392\n",
      "0.63125\n",
      "Epoch 186, Loss: 0.6700655945\n",
      "0.63125\n",
      "Epoch 187, Loss: 0.5895450873\n",
      "0.63875\n",
      "Epoch 188, Loss: 0.6378008197\n",
      "0.63875\n",
      "Epoch 189, Loss: 0.5890146151\n",
      "0.6225\n",
      "Epoch 190, Loss: 0.6076291338\n",
      "0.6225\n",
      "Epoch 191, Loss: 0.5911516327\n",
      "0.6225\n",
      "Epoch 192, Loss: 0.6458230514\n",
      "0.6225\n",
      "Epoch 193, Loss: 0.6020583410\n",
      "0.6225\n",
      "Epoch 194, Loss: 0.5962107353\n",
      "0.6225\n",
      "Epoch 195, Loss: 0.5764523468\n",
      "0.615\n",
      "Epoch 196, Loss: 0.5763163601\n",
      "0.62125\n",
      "Epoch 197, Loss: 0.6180216255\n",
      "0.62125\n",
      "Epoch 198, Loss: 0.6227718314\n",
      "0.62125\n",
      "Epoch 199, Loss: 0.6006123827\n",
      "0.62125\n",
      "Epoch 200, Loss: 0.6370943779\n",
      "0.62125\n",
      "Epoch 201, Loss: 0.6129805174\n",
      "0.62125\n",
      "Epoch 202, Loss: 0.6403608223\n",
      "0.62125\n",
      "Epoch 203, Loss: 0.6095419873\n",
      "0.62125\n",
      "Epoch 204, Loss: 0.6530698253\n",
      "0.62125\n",
      "Epoch 205, Loss: 0.5975213507\n",
      "0.62125\n",
      "Epoch 206, Loss: 0.6612792611\n",
      "0.62125\n",
      "Epoch 207, Loss: 0.6703056125\n",
      "0.62125\n",
      "Epoch 208, Loss: 0.7252545125\n",
      "0.62125\n",
      "Epoch 209, Loss: 0.6676233926\n",
      "0.62125\n",
      "Epoch 210, Loss: 0.6636446082\n",
      "0.62125\n",
      "Epoch 211, Loss: 0.7228851452\n",
      "0.62125\n",
      "Epoch 212, Loss: 0.6979209954\n",
      "0.62125\n",
      "Epoch 213, Loss: 0.6495678437\n",
      "0.62125\n",
      "Epoch 214, Loss: 0.5885038755\n",
      "0.62125\n",
      "Epoch 215, Loss: 0.6156234356\n",
      "0.62125\n",
      "Epoch 216, Loss: 0.7274524761\n",
      "0.62125\n",
      "Epoch 217, Loss: 0.5954620094\n",
      "0.62125\n",
      "Epoch 218, Loss: 0.6150413806\n",
      "0.62125\n",
      "Epoch 219, Loss: 0.7229152447\n",
      "0.62125\n",
      "Epoch 220, Loss: 0.6014585503\n",
      "0.62125\n",
      "Epoch 221, Loss: 0.5921742390\n",
      "0.62125\n",
      "Epoch 222, Loss: 0.6129196816\n",
      "0.62125\n",
      "Epoch 223, Loss: 0.5335726183\n",
      "0.6225\n",
      "Epoch 224, Loss: 0.5450303151\n",
      "0.6225\n",
      "Epoch 225, Loss: 0.5459089417\n",
      "0.6225\n",
      "Epoch 226, Loss: 0.6076119040\n",
      "0.6225\n",
      "Epoch 227, Loss: 0.6321967725\n",
      "0.6225\n",
      "Epoch 228, Loss: 0.6165957526\n",
      "0.6225\n",
      "Epoch 229, Loss: 0.5601603061\n",
      "0.6225\n",
      "Epoch 230, Loss: 0.5832249680\n",
      "0.6225\n",
      "Epoch 231, Loss: 0.5769762953\n",
      "0.6225\n",
      "Epoch 232, Loss: 0.6040849413\n",
      "0.6225\n",
      "Epoch 233, Loss: 0.6781437499\n",
      "0.6225\n",
      "Epoch 234, Loss: 0.6482745945\n",
      "0.6225\n",
      "Epoch 235, Loss: 0.6953560164\n",
      "0.6225\n",
      "Epoch 236, Loss: 0.5726298785\n",
      "0.6225\n",
      "Epoch 237, Loss: 0.6497259495\n",
      "0.6225\n",
      "Epoch 238, Loss: 0.7514594595\n",
      "0.6225\n",
      "Epoch 239, Loss: 0.6861627673\n",
      "0.6225\n",
      "Epoch 240, Loss: 0.7188679131\n",
      "0.6225\n",
      "Epoch 241, Loss: 0.6467080505\n",
      "0.6225\n",
      "Epoch 242, Loss: 0.6084012093\n",
      "0.6225\n",
      "Epoch 243, Loss: 0.6599008307\n",
      "0.6225\n",
      "Epoch 244, Loss: 0.6122357129\n",
      "0.6225\n",
      "Epoch 245, Loss: 0.6641453708\n",
      "0.6225\n",
      "Epoch 246, Loss: 0.5649987784\n",
      "0.6225\n",
      "Epoch 247, Loss: 0.6748394957\n",
      "0.6225\n",
      "Epoch 248, Loss: 0.5979823038\n",
      "0.6225\n",
      "Epoch 249, Loss: 0.6033221750\n",
      "0.6225\n",
      "Epoch 250, Loss: 0.6617982629\n",
      "0.6225\n",
      "Epoch 251, Loss: 0.5905548228\n",
      "0.6225\n",
      "Epoch 252, Loss: 0.5895538304\n",
      "0.6225\n",
      "Epoch 253, Loss: 0.5701893833\n",
      "0.6225\n",
      "Epoch 254, Loss: 0.6647734574\n",
      "0.6225\n",
      "Epoch 255, Loss: 0.5880321060\n",
      "0.6225\n",
      "Epoch 256, Loss: 0.6095709130\n",
      "0.6225\n",
      "Epoch 257, Loss: 0.4995331233\n",
      "0.64125\n",
      "Epoch 258, Loss: 0.4831222062\n",
      "0.6225\n",
      "Epoch 259, Loss: 0.5040251116\n",
      "0.6225\n",
      "Epoch 260, Loss: 0.5797173382\n",
      "0.6225\n",
      "Epoch 261, Loss: 0.5276502396\n",
      "0.6225\n",
      "Epoch 262, Loss: 0.5097789216\n",
      "0.6225\n",
      "Epoch 263, Loss: 0.5153649578\n",
      "0.6225\n",
      "Epoch 264, Loss: 0.5569050957\n",
      "0.6225\n",
      "Epoch 265, Loss: 0.7089112586\n",
      "0.6225\n",
      "Epoch 266, Loss: 0.5414349054\n",
      "0.6225\n",
      "Epoch 267, Loss: 0.4895092370\n",
      "0.6225\n",
      "Epoch 268, Loss: 0.5140488510\n",
      "0.6225\n",
      "Epoch 269, Loss: 0.5195260604\n",
      "0.6225\n",
      "Epoch 270, Loss: 0.4837834582\n",
      "0.6225\n",
      "Epoch 271, Loss: 0.4692498994\n",
      "0.63125\n",
      "Epoch 272, Loss: 0.4954345555\n",
      "0.63125\n",
      "Epoch 273, Loss: 0.5191975822\n",
      "0.63125\n",
      "Epoch 274, Loss: 0.5324455630\n",
      "0.63125\n",
      "Epoch 275, Loss: 0.6512838004\n",
      "0.63125\n",
      "Epoch 276, Loss: 0.8246606645\n",
      "0.63125\n",
      "Epoch 277, Loss: 0.5299655974\n",
      "0.63125\n",
      "Epoch 278, Loss: 0.4951291804\n",
      "0.63125\n",
      "Epoch 279, Loss: 0.5335688740\n",
      "0.63125\n",
      "Epoch 280, Loss: 0.4817913168\n",
      "0.63125\n",
      "Epoch 281, Loss: 0.5204421860\n",
      "0.63125\n",
      "Epoch 282, Loss: 0.5303013534\n",
      "0.63125\n",
      "Epoch 283, Loss: 0.4580621201\n",
      "0.64875\n",
      "Epoch 284, Loss: 0.4660691845\n",
      "0.64875\n",
      "Epoch 285, Loss: 0.5304594607\n",
      "0.64875\n",
      "Epoch 286, Loss: 0.4510415921\n",
      "0.6025\n",
      "Epoch 287, Loss: 0.4359287758\n",
      "0.61375\n",
      "Epoch 288, Loss: 0.4369239663\n",
      "0.61375\n",
      "Epoch 289, Loss: 0.4653427781\n",
      "0.61375\n",
      "Epoch 290, Loss: 0.4334989204\n",
      "0.64375\n",
      "Epoch 291, Loss: 0.4599507090\n",
      "0.64375\n",
      "Epoch 292, Loss: 0.4920198382\n",
      "0.64375\n",
      "Epoch 293, Loss: 0.4656443338\n",
      "0.64375\n",
      "Epoch 294, Loss: 0.4285200460\n",
      "0.62625\n",
      "Epoch 295, Loss: 0.4548298866\n",
      "0.62625\n",
      "Epoch 296, Loss: 0.4384710407\n",
      "0.62625\n",
      "Epoch 297, Loss: 0.4065011496\n",
      "0.63875\n",
      "Epoch 298, Loss: 0.4510162836\n",
      "0.63875\n",
      "Epoch 299, Loss: 0.3950357423\n",
      "0.62875\n",
      "Epoch 300, Loss: 0.4122363494\n",
      "0.62875\n",
      "Epoch 301, Loss: 0.4263881538\n",
      "0.62875\n",
      "Epoch 302, Loss: 0.4510414872\n",
      "0.62875\n",
      "Epoch 303, Loss: 0.4587951838\n",
      "0.62875\n",
      "Epoch 304, Loss: 0.4545640147\n",
      "0.62875\n",
      "Epoch 305, Loss: 0.5480916860\n",
      "0.62875\n",
      "Epoch 306, Loss: 0.5770095815\n",
      "0.62875\n",
      "Epoch 307, Loss: 0.6058879046\n",
      "0.62875\n",
      "Epoch 308, Loss: 0.4938829944\n",
      "0.62875\n",
      "Epoch 309, Loss: 0.4956870597\n",
      "0.62875\n",
      "Epoch 310, Loss: 0.5234316737\n",
      "0.62875\n",
      "Epoch 311, Loss: 0.4868562181\n",
      "0.62875\n",
      "Epoch 312, Loss: 0.4688534108\n",
      "0.62875\n",
      "Epoch 313, Loss: 0.5949521625\n",
      "0.62875\n",
      "Epoch 314, Loss: 0.5655054809\n",
      "0.62875\n",
      "Epoch 315, Loss: 0.6030646964\n",
      "0.62875\n",
      "Epoch 316, Loss: 0.7446156032\n",
      "0.62875\n",
      "Epoch 317, Loss: 0.8929118271\n",
      "0.62875\n",
      "Epoch 318, Loss: 0.7104212619\n",
      "0.62875\n",
      "Epoch 319, Loss: 0.6020011282\n",
      "0.62875\n",
      "Epoch 320, Loss: 0.5112029468\n",
      "0.62875\n",
      "Epoch 321, Loss: 0.6175526333\n",
      "0.62875\n",
      "Epoch 322, Loss: 0.6002442558\n",
      "0.62875\n",
      "Epoch 323, Loss: 0.7380639983\n",
      "0.62875\n",
      "Epoch 324, Loss: 0.5489171904\n",
      "0.62875\n",
      "Epoch 325, Loss: 0.5420863911\n",
      "0.62875\n",
      "Epoch 326, Loss: 0.5370343967\n",
      "0.62875\n",
      "Epoch 327, Loss: 0.5464116808\n",
      "0.62875\n",
      "Epoch 328, Loss: 0.5303515058\n",
      "0.62875\n",
      "Epoch 329, Loss: 0.5394047971\n",
      "0.62875\n",
      "Epoch 330, Loss: 0.6259739275\n",
      "0.62875\n",
      "Epoch 331, Loss: 0.5264142465\n",
      "0.62875\n",
      "Epoch 332, Loss: 0.5692159633\n",
      "0.62875\n",
      "Epoch 333, Loss: 0.4431345388\n",
      "0.62875\n",
      "Epoch 334, Loss: 0.5061227306\n",
      "0.62875\n",
      "Epoch 335, Loss: 0.5707746277\n",
      "0.62875\n",
      "Epoch 336, Loss: 0.5012542265\n",
      "0.62875\n",
      "Epoch 337, Loss: 0.4175982653\n",
      "0.62875\n",
      "Epoch 338, Loss: 0.4244717508\n",
      "0.62875\n",
      "Epoch 339, Loss: 0.4610326724\n",
      "0.62875\n",
      "Epoch 340, Loss: 0.5162113006\n",
      "0.62875\n",
      "Epoch 341, Loss: 0.4909166527\n",
      "0.62875\n",
      "Epoch 342, Loss: 0.3957332895\n",
      "0.62875\n",
      "Epoch 343, Loss: 0.3861388722\n",
      "0.63875\n",
      "Epoch 344, Loss: 0.4352400676\n",
      "0.63875\n",
      "Epoch 345, Loss: 0.3889589569\n",
      "0.63875\n",
      "Epoch 346, Loss: 0.4266551072\n",
      "0.63875\n",
      "Epoch 347, Loss: 0.5022122426\n",
      "0.63875\n",
      "Epoch 348, Loss: 0.5611514616\n",
      "0.63875\n",
      "Epoch 349, Loss: 0.5573057930\n",
      "0.63875\n",
      "Epoch 350, Loss: 0.4627043117\n",
      "0.63875\n",
      "Epoch 351, Loss: 0.4553775151\n",
      "0.63875\n",
      "Epoch 352, Loss: 0.3485313903\n",
      "0.63875\n",
      "Epoch 353, Loss: 0.4264272374\n",
      "0.63875\n",
      "Epoch 354, Loss: 0.5166534595\n",
      "0.63875\n",
      "Epoch 355, Loss: 0.5354917365\n",
      "0.63875\n",
      "Epoch 356, Loss: 0.4920590957\n",
      "0.63875\n",
      "Epoch 357, Loss: 0.5732883352\n",
      "0.63875\n",
      "Epoch 358, Loss: 0.6968422663\n",
      "0.63875\n",
      "Epoch 359, Loss: 0.4985093000\n",
      "0.63875\n",
      "Epoch 360, Loss: 0.6649856299\n",
      "0.63875\n",
      "Epoch 361, Loss: 0.4996984980\n",
      "0.63875\n",
      "Epoch 362, Loss: 0.4141520585\n",
      "0.63875\n",
      "Epoch 363, Loss: 0.4449584940\n",
      "0.63875\n",
      "Epoch 364, Loss: 0.3805211278\n",
      "0.63875\n",
      "Epoch 365, Loss: 0.4870414351\n",
      "0.63875\n",
      "Epoch 366, Loss: 0.7569044150\n",
      "0.63875\n",
      "Epoch 367, Loss: 0.8381947334\n",
      "0.63875\n",
      "Epoch 368, Loss: 0.4085038474\n",
      "0.63875\n",
      "Epoch 369, Loss: 0.5243147468\n",
      "0.63875\n",
      "Epoch 370, Loss: 0.5657139250\n",
      "0.63875\n",
      "Epoch 371, Loss: 0.4637034959\n",
      "0.63875\n",
      "Epoch 372, Loss: 0.5750192065\n",
      "0.63875\n",
      "Epoch 373, Loss: 0.6370776478\n",
      "0.63875\n",
      "Epoch 374, Loss: 0.5233094953\n",
      "0.63875\n",
      "Epoch 375, Loss: 0.4810122980\n",
      "0.63875\n",
      "Epoch 376, Loss: 0.4423851113\n",
      "0.63875\n",
      "Epoch 377, Loss: 0.4803774188\n",
      "0.63875\n",
      "Epoch 378, Loss: 0.4561229106\n",
      "0.63875\n",
      "Epoch 379, Loss: 0.3830459210\n",
      "0.63875\n",
      "Epoch 380, Loss: 0.3935988112\n",
      "0.63875\n",
      "Epoch 381, Loss: 0.3577953298\n",
      "0.63875\n",
      "Epoch 382, Loss: 0.4426826504\n",
      "0.63875\n",
      "Epoch 383, Loss: 0.4800207488\n",
      "0.63875\n",
      "Epoch 384, Loss: 0.4604704553\n",
      "0.63875\n",
      "Epoch 385, Loss: 0.4862411895\n",
      "0.63875\n",
      "Epoch 386, Loss: 0.3843812396\n",
      "0.63875\n",
      "Epoch 387, Loss: 0.5699850950\n",
      "0.63875\n",
      "Epoch 388, Loss: 0.5325136224\n",
      "0.63875\n",
      "Epoch 389, Loss: 0.5076826306\n",
      "0.63875\n",
      "Epoch 390, Loss: 0.6267413000\n",
      "0.63875\n",
      "Epoch 391, Loss: 0.7121724569\n",
      "0.63875\n",
      "Epoch 392, Loss: 0.5155193813\n",
      "0.63875\n",
      "Epoch 393, Loss: 0.4511975932\n",
      "0.63875\n",
      "Epoch 394, Loss: 0.4949577661\n",
      "0.63875\n",
      "Epoch 395, Loss: 0.4599236678\n",
      "0.63875\n",
      "Epoch 396, Loss: 0.6236850200\n",
      "0.63875\n",
      "Epoch 397, Loss: 0.4330859159\n",
      "0.63875\n",
      "Epoch 398, Loss: 0.4126527948\n",
      "0.63875\n",
      "Epoch 399, Loss: 0.5858754677\n",
      "0.63875\n",
      "Epoch 400, Loss: 0.4404733805\n",
      "0.63875\n",
      "Epoch 401, Loss: 0.6341585119\n",
      "0.63875\n",
      "Epoch 402, Loss: 0.6503704682\n",
      "0.63875\n",
      "Epoch 403, Loss: 0.5541199171\n",
      "0.63875\n",
      "Epoch 404, Loss: 0.6964042120\n",
      "0.63875\n",
      "Epoch 405, Loss: 0.5436360237\n",
      "0.63875\n",
      "Epoch 406, Loss: 0.5414317422\n",
      "0.63875\n",
      "Epoch 407, Loss: 0.6940387863\n",
      "0.63875\n",
      "Epoch 408, Loss: 0.5192138579\n",
      "0.63875\n",
      "Epoch 409, Loss: 0.7349352429\n",
      "0.63875\n",
      "Epoch 410, Loss: 0.5319282687\n",
      "0.63875\n",
      "Epoch 411, Loss: 0.5596110045\n",
      "0.63875\n",
      "Epoch 412, Loss: 0.6584993927\n",
      "0.63875\n",
      "Epoch 413, Loss: 0.6379761053\n",
      "0.63875\n",
      "Epoch 414, Loss: 0.4952929660\n",
      "0.63875\n",
      "Epoch 415, Loss: 0.4859586453\n",
      "0.63875\n",
      "Epoch 416, Loss: 0.5870876540\n",
      "0.63875\n",
      "Epoch 417, Loss: 0.7946504400\n",
      "0.63875\n",
      "Epoch 418, Loss: 0.6804837569\n",
      "0.63875\n",
      "Epoch 419, Loss: 0.4989742019\n",
      "0.63875\n",
      "Epoch 420, Loss: 0.4747909585\n",
      "0.63875\n",
      "Epoch 421, Loss: 0.6411952309\n",
      "0.63875\n",
      "Epoch 422, Loss: 0.4059527836\n",
      "0.63875\n",
      "Epoch 423, Loss: 0.5997611630\n",
      "0.63875\n",
      "Epoch 424, Loss: 0.7631876598\n",
      "0.63875\n",
      "Epoch 425, Loss: 0.5384381862\n",
      "0.63875\n",
      "Epoch 426, Loss: 0.5738788903\n",
      "0.63875\n",
      "Epoch 427, Loss: 0.4581453674\n",
      "0.63875\n",
      "Epoch 428, Loss: 0.4440345474\n",
      "0.63875\n",
      "Epoch 429, Loss: 0.5904015890\n",
      "0.63875\n",
      "Epoch 430, Loss: 0.4739000298\n",
      "0.63875\n",
      "Epoch 431, Loss: 0.4821572406\n",
      "0.63875\n",
      "Epoch 432, Loss: 0.4712603203\n",
      "0.63875\n",
      "Epoch 433, Loss: 0.4492905463\n",
      "0.63875\n",
      "Epoch 434, Loss: 0.3480502149\n",
      "0.6475\n",
      "Epoch 435, Loss: 0.3765244546\n",
      "0.6475\n",
      "Epoch 436, Loss: 0.4217033109\n",
      "0.6475\n",
      "Epoch 437, Loss: 0.4635344636\n",
      "0.6475\n",
      "Epoch 438, Loss: 0.3945306575\n",
      "0.6475\n",
      "Epoch 439, Loss: 0.3186941089\n",
      "0.6375\n",
      "Epoch 440, Loss: 0.3334717736\n",
      "0.6375\n",
      "Epoch 441, Loss: 0.3174861453\n",
      "0.64\n",
      "Epoch 442, Loss: 0.2908928433\n",
      "0.635\n",
      "Epoch 443, Loss: 0.3044263734\n",
      "0.635\n",
      "Epoch 444, Loss: 0.3384555124\n",
      "0.635\n",
      "Epoch 445, Loss: 0.4667241678\n",
      "0.635\n",
      "Epoch 446, Loss: 0.3233404394\n",
      "0.635\n",
      "Epoch 447, Loss: 0.3065327231\n",
      "0.635\n",
      "Epoch 448, Loss: 0.3790098550\n",
      "0.635\n",
      "Epoch 449, Loss: 0.3460753439\n",
      "0.635\n",
      "Epoch 450, Loss: 0.3052801225\n",
      "0.635\n",
      "Epoch 451, Loss: 0.3186816472\n",
      "0.635\n",
      "Epoch 452, Loss: 0.5116326248\n",
      "0.635\n",
      "Epoch 453, Loss: 0.4622358409\n",
      "0.635\n",
      "Epoch 454, Loss: 0.4281915855\n",
      "0.635\n",
      "Epoch 455, Loss: 0.3870018411\n",
      "0.635\n",
      "Epoch 456, Loss: 0.3313206952\n",
      "0.635\n",
      "Epoch 457, Loss: 0.3060955435\n",
      "0.635\n",
      "Epoch 458, Loss: 0.3261691759\n",
      "0.635\n",
      "Epoch 459, Loss: 0.3235389656\n",
      "0.635\n",
      "Epoch 460, Loss: 0.3421850637\n",
      "0.635\n",
      "Epoch 461, Loss: 0.3404676799\n",
      "0.635\n",
      "Epoch 462, Loss: 0.3631095382\n",
      "0.635\n",
      "Epoch 463, Loss: 0.3539237519\n",
      "0.635\n",
      "Epoch 464, Loss: 0.3284136946\n",
      "0.635\n",
      "Epoch 465, Loss: 0.3868822225\n",
      "0.635\n",
      "Epoch 466, Loss: 0.3605284111\n",
      "0.635\n",
      "Epoch 467, Loss: 0.3650902074\n",
      "0.635\n",
      "Epoch 468, Loss: 0.3680561109\n",
      "0.635\n",
      "Epoch 469, Loss: 0.2833333357\n",
      "0.635\n",
      "Epoch 470, Loss: 0.3670224765\n",
      "0.635\n",
      "Epoch 471, Loss: 0.3699893435\n",
      "0.635\n",
      "Epoch 472, Loss: 0.4159510180\n",
      "0.635\n",
      "Epoch 473, Loss: 0.3911839348\n",
      "0.635\n",
      "Epoch 474, Loss: 0.3495863527\n",
      "0.635\n",
      "Epoch 475, Loss: 0.3584198705\n",
      "0.635\n",
      "Epoch 476, Loss: 0.3059237939\n",
      "0.635\n",
      "Epoch 477, Loss: 0.3769856490\n",
      "0.635\n",
      "Epoch 478, Loss: 0.3116580223\n",
      "0.635\n",
      "Epoch 479, Loss: 0.4299088829\n",
      "0.635\n",
      "Epoch 480, Loss: 0.3678390429\n",
      "0.635\n",
      "Epoch 481, Loss: 0.3242532714\n",
      "0.635\n",
      "Epoch 482, Loss: 0.3253094573\n",
      "0.635\n",
      "Epoch 483, Loss: 0.4464968226\n",
      "0.635\n",
      "Epoch 484, Loss: 0.3937424378\n",
      "0.635\n",
      "Epoch 485, Loss: 0.4550296333\n",
      "0.635\n",
      "Epoch 486, Loss: 0.2627849023\n",
      "0.6525\n",
      "Epoch 487, Loss: 0.3959967045\n",
      "0.6525\n",
      "Epoch 488, Loss: 0.4085882087\n",
      "0.6525\n",
      "Epoch 489, Loss: 0.3588665028\n",
      "0.6525\n",
      "Epoch 490, Loss: 0.3806297467\n",
      "0.6525\n",
      "Epoch 491, Loss: 0.2773907822\n",
      "0.6525\n",
      "Epoch 492, Loss: 0.3286014213\n",
      "0.6525\n",
      "Epoch 493, Loss: 0.4202081405\n",
      "0.6525\n",
      "Epoch 494, Loss: 0.3890173781\n",
      "0.6525\n",
      "Epoch 495, Loss: 0.4257851384\n",
      "0.6525\n",
      "Epoch 496, Loss: 0.4650866162\n",
      "0.6525\n",
      "Epoch 497, Loss: 0.3475298536\n",
      "0.6525\n",
      "Epoch 498, Loss: 0.3384273056\n",
      "0.6525\n",
      "Epoch 499, Loss: 0.3862699162\n",
      "0.6525\n",
      "Epoch 500, Loss: 0.2862098937\n",
      "0.6525\n",
      "Epoch 501, Loss: 0.3063395240\n",
      "0.6525\n",
      "Epoch 502, Loss: 0.3936881369\n",
      "0.6525\n",
      "Epoch 503, Loss: 0.2715825317\n",
      "0.6525\n",
      "Epoch 504, Loss: 0.3045843041\n",
      "0.6525\n",
      "Epoch 505, Loss: 0.2892994891\n",
      "0.6525\n",
      "Epoch 506, Loss: 0.3383218988\n",
      "0.6525\n",
      "Epoch 507, Loss: 0.3034638844\n",
      "0.6525\n",
      "Epoch 508, Loss: 0.3082563303\n",
      "0.6525\n",
      "Epoch 509, Loss: 0.2620632489\n",
      "0.64375\n",
      "Epoch 510, Loss: 0.2329996992\n",
      "0.65\n",
      "Epoch 511, Loss: 0.2941787310\n",
      "0.65\n",
      "Epoch 512, Loss: 0.3075620005\n",
      "0.65\n",
      "Epoch 513, Loss: 0.3768484767\n",
      "0.65\n",
      "Epoch 514, Loss: 0.4030800013\n",
      "0.65\n",
      "Epoch 515, Loss: 0.4965698063\n",
      "0.65\n",
      "Epoch 516, Loss: 0.6197986729\n",
      "0.65\n",
      "Epoch 517, Loss: 0.4786030978\n",
      "0.65\n",
      "Epoch 518, Loss: 0.4756843861\n",
      "0.65\n",
      "Epoch 519, Loss: 0.4411360284\n",
      "0.65\n",
      "Epoch 520, Loss: 0.4623460251\n",
      "0.65\n",
      "Epoch 521, Loss: 0.3983139326\n",
      "0.65\n",
      "Epoch 522, Loss: 0.3148049662\n",
      "0.65\n",
      "Epoch 523, Loss: 0.3238844992\n",
      "0.65\n",
      "Epoch 524, Loss: 0.6381344122\n",
      "0.65\n",
      "Epoch 525, Loss: 0.4435198575\n",
      "0.65\n",
      "Epoch 526, Loss: 0.5338453392\n",
      "0.65\n",
      "Epoch 527, Loss: 0.4240450870\n",
      "0.65\n",
      "Epoch 528, Loss: 0.5189389633\n",
      "0.65\n",
      "Epoch 529, Loss: 0.4042899330\n",
      "0.65\n",
      "Epoch 530, Loss: 0.3996294838\n",
      "0.65\n",
      "Epoch 531, Loss: 0.5427394696\n",
      "0.65\n",
      "Epoch 532, Loss: 0.5073213837\n",
      "0.65\n",
      "Epoch 533, Loss: 0.5068135527\n",
      "0.65\n",
      "Epoch 534, Loss: 0.5669321685\n",
      "0.65\n",
      "Epoch 535, Loss: 0.4758778114\n",
      "0.65\n",
      "Epoch 536, Loss: 0.4583926347\n",
      "0.65\n",
      "Epoch 537, Loss: 0.3860103381\n",
      "0.65\n",
      "Epoch 538, Loss: 0.3466254011\n",
      "0.65\n",
      "Epoch 539, Loss: 0.3692440775\n",
      "0.65\n",
      "Epoch 540, Loss: 0.3276937784\n",
      "0.65\n",
      "Epoch 541, Loss: 0.3826581412\n",
      "0.65\n",
      "Epoch 542, Loss: 0.3333268333\n",
      "0.65\n",
      "Epoch 543, Loss: 0.3590515745\n",
      "0.65\n",
      "Epoch 544, Loss: 0.3022546904\n",
      "0.65\n",
      "Epoch 545, Loss: 0.2695948805\n",
      "0.65\n",
      "Epoch 546, Loss: 0.3290608977\n",
      "0.65\n",
      "Epoch 547, Loss: 0.3354034707\n",
      "0.65\n",
      "Epoch 548, Loss: 0.3453193011\n",
      "0.65\n",
      "Epoch 549, Loss: 0.3520767119\n",
      "0.65\n",
      "Epoch 550, Loss: 0.3106114764\n",
      "0.65\n",
      "Epoch 551, Loss: 0.3949814066\n",
      "0.65\n",
      "Epoch 552, Loss: 0.4650184119\n",
      "0.65\n",
      "Epoch 553, Loss: 0.5023072200\n",
      "0.65\n",
      "Epoch 554, Loss: 0.3448759932\n",
      "0.65\n",
      "Epoch 555, Loss: 0.5072869533\n",
      "0.65\n",
      "Epoch 556, Loss: 0.3660764746\n",
      "0.65\n",
      "Epoch 557, Loss: 0.3651201803\n",
      "0.65\n",
      "Epoch 558, Loss: 0.3327646522\n",
      "0.65\n",
      "Epoch 559, Loss: 0.3685554427\n",
      "0.65\n",
      "Epoch 560, Loss: 0.3457860393\n",
      "0.65\n",
      "Epoch 561, Loss: 0.3317534234\n",
      "0.65\n",
      "Epoch 562, Loss: 0.3218655372\n",
      "0.65\n",
      "Epoch 563, Loss: 0.2967884074\n",
      "0.65\n",
      "Epoch 564, Loss: 0.2717528558\n",
      "0.65\n",
      "Epoch 565, Loss: 0.2762808404\n",
      "0.65\n",
      "Epoch 566, Loss: 0.2388755685\n",
      "0.65\n",
      "Epoch 567, Loss: 0.2877281824\n",
      "0.65\n",
      "Epoch 568, Loss: 0.2648445285\n",
      "0.65\n",
      "Epoch 569, Loss: 0.2897351769\n",
      "0.65\n",
      "Epoch 570, Loss: 0.3750635567\n",
      "0.65\n",
      "Epoch 571, Loss: 0.2845060433\n",
      "0.65\n",
      "Epoch 572, Loss: 0.2609526713\n",
      "0.65\n",
      "Epoch 573, Loss: 0.2863111372\n",
      "0.65\n",
      "Epoch 574, Loss: 0.3477445343\n",
      "0.65\n",
      "Epoch 575, Loss: 0.3392150907\n",
      "0.65\n",
      "Epoch 576, Loss: 0.3283398526\n",
      "0.65\n",
      "Epoch 577, Loss: 0.3317556737\n",
      "0.65\n",
      "Epoch 578, Loss: 0.2451013728\n",
      "0.65\n",
      "Epoch 579, Loss: 0.2597048629\n",
      "0.65\n",
      "Epoch 580, Loss: 0.3016267564\n",
      "0.65\n",
      "Epoch 581, Loss: 0.3118714504\n",
      "0.65\n",
      "Epoch 582, Loss: 0.3431556429\n",
      "0.65\n",
      "Epoch 583, Loss: 0.3388432093\n",
      "0.65\n",
      "Epoch 584, Loss: 0.3696155639\n",
      "0.65\n",
      "Epoch 585, Loss: 0.3747405432\n",
      "0.65\n",
      "Epoch 586, Loss: 0.3385748364\n",
      "0.65\n",
      "Epoch 587, Loss: 0.3415577109\n",
      "0.65\n",
      "Epoch 588, Loss: 0.4007841321\n",
      "0.65\n",
      "Epoch 589, Loss: 0.3860093628\n",
      "0.65\n",
      "Epoch 590, Loss: 0.3871443514\n",
      "0.65\n",
      "Epoch 591, Loss: 0.2541636090\n",
      "0.65\n",
      "Epoch 592, Loss: 0.3231169948\n",
      "0.65\n",
      "Epoch 593, Loss: 0.3408513835\n",
      "0.65\n",
      "Epoch 594, Loss: 0.3779067396\n",
      "0.65\n",
      "Epoch 595, Loss: 0.4432190906\n",
      "0.65\n",
      "Epoch 596, Loss: 0.5828004424\n",
      "0.65\n",
      "Epoch 597, Loss: 0.6319809571\n",
      "0.65\n",
      "Epoch 598, Loss: 0.7361629497\n",
      "0.65\n",
      "Epoch 599, Loss: 0.6229221208\n",
      "0.65\n",
      "Epoch 600, Loss: 0.5122131641\n",
      "0.65\n",
      "Epoch 601, Loss: 0.5945859151\n",
      "0.65\n",
      "Epoch 602, Loss: 0.5297926594\n",
      "0.65\n",
      "Epoch 603, Loss: 0.3349434059\n",
      "0.65\n",
      "Epoch 604, Loss: 0.3960353019\n",
      "0.65\n",
      "Epoch 605, Loss: 0.4646611970\n",
      "0.65\n",
      "Epoch 606, Loss: 0.4340175458\n",
      "0.65\n",
      "Epoch 607, Loss: 0.5261436951\n",
      "0.65\n",
      "Epoch 608, Loss: 0.4174603280\n",
      "0.65\n",
      "Epoch 609, Loss: 0.4033577991\n",
      "0.65\n",
      "Epoch 610, Loss: 0.3058066204\n",
      "0.65\n",
      "Epoch 611, Loss: 0.3715540943\n",
      "0.65\n",
      "Epoch 612, Loss: 0.3283310623\n",
      "0.65\n",
      "Epoch 613, Loss: 0.3343940850\n",
      "0.65\n",
      "Epoch 614, Loss: 0.3579591195\n",
      "0.65\n",
      "Epoch 615, Loss: 0.3604206246\n",
      "0.65\n",
      "Epoch 616, Loss: 0.3935151619\n",
      "0.65\n",
      "Epoch 617, Loss: 0.3560615952\n",
      "0.65\n",
      "Epoch 618, Loss: 0.4006610544\n",
      "0.65\n",
      "Epoch 619, Loss: 0.2748684492\n",
      "0.65\n",
      "Epoch 620, Loss: 0.3125467364\n",
      "0.65\n",
      "Epoch 621, Loss: 0.3767120669\n",
      "0.65\n",
      "Epoch 622, Loss: 0.4689989449\n",
      "0.65\n",
      "Epoch 623, Loss: 0.3508917738\n",
      "0.65\n",
      "Epoch 624, Loss: 0.3188163236\n",
      "0.65\n",
      "Epoch 625, Loss: 0.5110832365\n",
      "0.65\n",
      "Epoch 626, Loss: 0.3688224093\n",
      "0.65\n",
      "Epoch 627, Loss: 0.3518767000\n",
      "0.65\n",
      "Epoch 628, Loss: 0.3619446446\n",
      "0.65\n",
      "Epoch 629, Loss: 0.3501015482\n",
      "0.65\n",
      "Epoch 630, Loss: 0.4913101756\n",
      "0.65\n",
      "Epoch 631, Loss: 0.3509901867\n",
      "0.65\n",
      "Epoch 632, Loss: 0.2871074381\n",
      "0.65\n",
      "Epoch 633, Loss: 0.3092299231\n",
      "0.65\n",
      "Epoch 634, Loss: 0.3065550728\n",
      "0.65\n",
      "Epoch 635, Loss: 0.3483899940\n",
      "0.65\n",
      "Epoch 636, Loss: 0.3708122663\n",
      "0.65\n",
      "Epoch 637, Loss: 0.3145498509\n",
      "0.65\n",
      "Epoch 638, Loss: 0.2481514539\n",
      "0.65\n",
      "Epoch 639, Loss: 0.2239754933\n",
      "0.655\n",
      "Epoch 640, Loss: 0.2319566611\n",
      "0.655\n",
      "Epoch 641, Loss: 0.2240944016\n",
      "0.655\n",
      "Epoch 642, Loss: 0.1876808912\n",
      "0.64625\n",
      "Epoch 643, Loss: 0.2123340393\n",
      "0.64625\n",
      "Epoch 644, Loss: 0.2277247700\n",
      "0.64625\n",
      "Epoch 645, Loss: 0.2522614354\n",
      "0.64625\n",
      "Epoch 646, Loss: 0.2658365317\n",
      "0.64625\n",
      "Epoch 647, Loss: 0.3335111487\n",
      "0.64625\n",
      "Epoch 648, Loss: 0.2793669034\n",
      "0.64625\n",
      "Epoch 649, Loss: 0.3446473712\n",
      "0.64625\n",
      "Epoch 650, Loss: 0.2503267525\n",
      "0.64625\n",
      "Epoch 651, Loss: 0.2315010484\n",
      "0.64625\n",
      "Epoch 652, Loss: 0.2729186980\n",
      "0.64625\n",
      "Epoch 653, Loss: 0.3293108852\n",
      "0.64625\n",
      "Epoch 654, Loss: 0.2837269706\n",
      "0.64625\n",
      "Epoch 655, Loss: 0.2587260459\n",
      "0.64625\n",
      "Epoch 656, Loss: 0.2579280458\n",
      "0.64625\n",
      "Epoch 657, Loss: 0.2556747586\n",
      "0.64625\n",
      "Epoch 658, Loss: 0.3118140392\n",
      "0.64625\n",
      "Epoch 659, Loss: 0.3601600526\n",
      "0.64625\n",
      "Epoch 660, Loss: 0.4084209350\n",
      "0.64625\n",
      "Epoch 661, Loss: 0.3471245186\n",
      "0.64625\n",
      "Epoch 662, Loss: 0.4032518911\n",
      "0.64625\n",
      "Epoch 663, Loss: 0.4184586646\n",
      "0.64625\n",
      "Epoch 664, Loss: 0.2388532501\n",
      "0.64625\n",
      "Epoch 665, Loss: 0.4267450387\n",
      "0.64625\n",
      "Epoch 666, Loss: 0.3311972661\n",
      "0.64625\n",
      "Epoch 667, Loss: 0.2898186352\n",
      "0.64625\n",
      "Epoch 668, Loss: 0.3389290955\n",
      "0.64625\n",
      "Epoch 669, Loss: 0.2371044378\n",
      "0.64625\n",
      "Epoch 670, Loss: 0.2983386810\n",
      "0.64625\n",
      "Epoch 671, Loss: 0.3004000347\n",
      "0.64625\n",
      "Epoch 672, Loss: 0.3149814273\n",
      "0.64625\n",
      "Epoch 673, Loss: 0.2735692449\n",
      "0.64625\n",
      "Epoch 674, Loss: 0.3000318022\n",
      "0.64625\n",
      "Epoch 675, Loss: 0.3024228383\n",
      "0.64625\n",
      "Epoch 676, Loss: 0.2984740225\n",
      "0.64625\n",
      "Epoch 677, Loss: 0.3336014471\n",
      "0.64625\n",
      "Epoch 678, Loss: 0.4858014348\n",
      "0.64625\n",
      "Epoch 679, Loss: 0.3671750985\n",
      "0.64625\n",
      "Epoch 680, Loss: 0.3545949629\n",
      "0.64625\n",
      "Epoch 681, Loss: 0.3639438445\n",
      "0.64625\n",
      "Epoch 682, Loss: 0.4048011193\n",
      "0.64625\n",
      "Epoch 683, Loss: 0.2777843025\n",
      "0.64625\n",
      "Epoch 684, Loss: 0.2411725954\n",
      "0.64625\n",
      "Epoch 685, Loss: 0.2661021045\n",
      "0.64625\n",
      "Epoch 686, Loss: 0.4059535904\n",
      "0.64625\n",
      "Epoch 687, Loss: 0.3659228317\n",
      "0.64625\n",
      "Epoch 688, Loss: 0.4342921077\n",
      "0.64625\n",
      "Epoch 689, Loss: 0.3218499798\n",
      "0.64625\n",
      "Epoch 690, Loss: 0.3891180682\n",
      "0.64625\n",
      "Epoch 691, Loss: 0.2853558798\n",
      "0.64625\n",
      "Epoch 692, Loss: 0.2854634231\n",
      "0.64625\n",
      "Epoch 693, Loss: 0.3318390707\n",
      "0.64625\n",
      "Epoch 694, Loss: 0.2744491189\n",
      "0.64625\n",
      "Epoch 695, Loss: 0.4090024792\n",
      "0.64625\n",
      "Epoch 696, Loss: 0.2950635344\n",
      "0.64625\n",
      "Epoch 697, Loss: 0.3361261623\n",
      "0.64625\n",
      "Epoch 698, Loss: 0.2571179561\n",
      "0.64625\n",
      "Epoch 699, Loss: 0.2306397574\n",
      "0.64625\n",
      "Epoch 700, Loss: 0.2789097730\n",
      "0.64625\n",
      "Epoch 701, Loss: 0.2332822736\n",
      "0.64625\n",
      "Epoch 702, Loss: 0.2396329332\n",
      "0.64625\n",
      "Epoch 703, Loss: 0.2312352472\n",
      "0.64625\n",
      "Epoch 704, Loss: 0.3523611356\n",
      "0.64625\n",
      "Epoch 705, Loss: 0.2770822532\n",
      "0.64625\n",
      "Epoch 706, Loss: 0.3931266798\n",
      "0.64625\n",
      "Epoch 707, Loss: 0.4680719876\n",
      "0.64625\n",
      "Epoch 708, Loss: 0.2264844299\n",
      "0.64625\n",
      "Epoch 709, Loss: 0.3745733100\n",
      "0.64625\n",
      "Epoch 710, Loss: 0.2682154361\n",
      "0.64625\n",
      "Epoch 711, Loss: 0.2503538544\n",
      "0.64625\n",
      "Epoch 712, Loss: 0.2635185572\n",
      "0.64625\n",
      "Epoch 713, Loss: 0.3707534552\n",
      "0.64625\n",
      "Epoch 714, Loss: 0.3011558600\n",
      "0.64625\n",
      "Epoch 715, Loss: 0.2617526297\n",
      "0.64625\n",
      "Epoch 716, Loss: 0.2551533497\n",
      "0.64625\n",
      "Epoch 717, Loss: 0.2251517034\n",
      "0.64625\n",
      "Epoch 718, Loss: 0.1598255900\n",
      "0.65125\n",
      "Epoch 719, Loss: 0.2177108985\n",
      "0.65125\n",
      "Epoch 720, Loss: 0.2664038232\n",
      "0.65125\n",
      "Epoch 721, Loss: 0.2117009483\n",
      "0.65125\n",
      "Epoch 722, Loss: 0.2447186632\n",
      "0.65125\n",
      "Epoch 723, Loss: 0.2496384801\n",
      "0.65125\n",
      "Epoch 724, Loss: 0.2101512213\n",
      "0.65125\n",
      "Epoch 725, Loss: 0.2642718893\n",
      "0.65125\n",
      "Epoch 726, Loss: 0.3377183494\n",
      "0.65125\n",
      "Epoch 727, Loss: 0.1906208105\n",
      "0.65125\n",
      "Epoch 728, Loss: 0.3114918545\n",
      "0.65125\n",
      "Epoch 729, Loss: 0.2093981330\n",
      "0.65125\n",
      "Epoch 730, Loss: 0.2153506247\n",
      "0.65125\n",
      "Epoch 731, Loss: 0.2393284752\n",
      "0.65125\n",
      "Epoch 732, Loss: 0.2296682722\n",
      "0.65125\n",
      "Epoch 733, Loss: 0.2726126893\n",
      "0.65125\n",
      "Epoch 734, Loss: 0.1769024723\n",
      "0.65125\n",
      "Epoch 735, Loss: 0.1579979277\n",
      "0.64\n",
      "Epoch 736, Loss: 0.2582479732\n",
      "0.64\n",
      "Epoch 737, Loss: 0.1865821941\n",
      "0.64\n",
      "Epoch 738, Loss: 0.3001365330\n",
      "0.64\n",
      "Epoch 739, Loss: 0.2745217316\n",
      "0.64\n",
      "Epoch 740, Loss: 0.3356645049\n",
      "0.64\n",
      "Epoch 741, Loss: 0.2521573801\n",
      "0.64\n",
      "Epoch 742, Loss: 0.4105384981\n",
      "0.64\n",
      "Epoch 743, Loss: 0.3943767636\n",
      "0.64\n",
      "Epoch 744, Loss: 0.3535148694\n",
      "0.64\n",
      "Epoch 745, Loss: 0.3571613614\n",
      "0.64\n",
      "Epoch 746, Loss: 0.4156591308\n",
      "0.64\n",
      "Epoch 747, Loss: 0.2938960016\n",
      "0.64\n",
      "Epoch 748, Loss: 0.2894967280\n",
      "0.64\n",
      "Epoch 749, Loss: 0.3024604481\n",
      "0.64\n",
      "Epoch 750, Loss: 0.3005158000\n",
      "0.64\n",
      "Epoch 751, Loss: 0.3296236327\n",
      "0.64\n",
      "Epoch 752, Loss: 0.1867311791\n",
      "0.64\n",
      "Epoch 753, Loss: 0.2899785736\n",
      "0.64\n",
      "Epoch 754, Loss: 0.2359724389\n",
      "0.64\n",
      "Epoch 755, Loss: 0.1934131059\n",
      "0.64\n",
      "Epoch 756, Loss: 0.3193581441\n",
      "0.64\n",
      "Epoch 757, Loss: 0.3715861238\n",
      "0.64\n",
      "Epoch 758, Loss: 0.3087952252\n",
      "0.64\n",
      "Epoch 759, Loss: 0.3044961437\n",
      "0.64\n",
      "Epoch 760, Loss: 0.3359192100\n",
      "0.64\n",
      "Epoch 761, Loss: 0.2746520169\n",
      "0.64\n",
      "Epoch 762, Loss: 0.3153843798\n",
      "0.64\n",
      "Epoch 763, Loss: 0.3997749523\n",
      "0.64\n",
      "Epoch 764, Loss: 0.4301292502\n",
      "0.64\n",
      "Epoch 765, Loss: 0.4178428937\n",
      "0.64\n",
      "Epoch 766, Loss: 0.2503349811\n",
      "0.64\n",
      "Epoch 767, Loss: 0.3364327163\n",
      "0.64\n",
      "Epoch 768, Loss: 0.3545752879\n",
      "0.64\n",
      "Epoch 769, Loss: 0.3022948553\n",
      "0.64\n",
      "Epoch 770, Loss: 0.2115109036\n",
      "0.64\n",
      "Epoch 771, Loss: 0.3910025903\n",
      "0.64\n",
      "Epoch 772, Loss: 0.3106226178\n",
      "0.64\n",
      "Epoch 773, Loss: 0.3841986735\n",
      "0.64\n",
      "Epoch 774, Loss: 0.3467642695\n",
      "0.64\n",
      "Epoch 775, Loss: 0.5328971088\n",
      "0.64\n",
      "Epoch 776, Loss: 0.4785537433\n",
      "0.64\n",
      "Epoch 777, Loss: 0.4238402437\n",
      "0.64\n",
      "Epoch 778, Loss: 0.3888663695\n",
      "0.64\n",
      "Epoch 779, Loss: 0.2490019952\n",
      "0.64\n",
      "Epoch 780, Loss: 0.4824442968\n",
      "0.64\n",
      "Epoch 781, Loss: 0.5291686731\n",
      "0.64\n",
      "Epoch 782, Loss: 0.7080610172\n",
      "0.64\n",
      "Epoch 783, Loss: 0.2712365098\n",
      "0.64\n",
      "Epoch 784, Loss: 0.2509070257\n",
      "0.64\n",
      "Epoch 785, Loss: 0.3751125621\n",
      "0.64\n",
      "Epoch 786, Loss: 0.6537515770\n",
      "0.64\n",
      "Epoch 787, Loss: 0.6002606880\n",
      "0.64\n",
      "Epoch 788, Loss: 0.4316439862\n",
      "0.64\n",
      "Epoch 789, Loss: 0.4622238418\n",
      "0.64\n",
      "Epoch 790, Loss: 0.3503936767\n",
      "0.64\n",
      "Epoch 791, Loss: 0.2606179508\n",
      "0.64\n",
      "Epoch 792, Loss: 0.2164233301\n",
      "0.64\n",
      "Epoch 793, Loss: 0.2763762107\n",
      "0.64\n",
      "Epoch 794, Loss: 0.2738024532\n",
      "0.64\n",
      "Epoch 795, Loss: 0.2745528963\n",
      "0.64\n",
      "Epoch 796, Loss: 0.3073866352\n",
      "0.64\n",
      "Epoch 797, Loss: 0.3263024916\n",
      "0.64\n",
      "Epoch 798, Loss: 0.2701717615\n",
      "0.64\n",
      "Epoch 799, Loss: 0.2737086503\n",
      "0.64\n",
      "Epoch 800, Loss: 0.3550697577\n",
      "0.64\n",
      "Epoch 801, Loss: 0.2896064391\n",
      "0.64\n",
      "Epoch 802, Loss: 0.2739197562\n",
      "0.64\n",
      "Epoch 803, Loss: 0.4582166858\n",
      "0.64\n",
      "Epoch 804, Loss: 0.2490204479\n",
      "0.64\n",
      "Epoch 805, Loss: 0.2560138127\n",
      "0.64\n",
      "Epoch 806, Loss: 0.3455252749\n",
      "0.64\n",
      "Epoch 807, Loss: 0.1897484232\n",
      "0.64\n",
      "Epoch 808, Loss: 0.2152453275\n",
      "0.64\n",
      "Epoch 809, Loss: 0.1943281156\n",
      "0.64\n",
      "Epoch 810, Loss: 0.2090951519\n",
      "0.64\n",
      "Epoch 811, Loss: 0.1465270433\n",
      "0.6275\n",
      "Epoch 812, Loss: 0.1928654056\n",
      "0.6275\n",
      "Epoch 813, Loss: 0.1542420029\n",
      "0.6275\n",
      "Epoch 814, Loss: 0.1994725453\n",
      "0.6275\n",
      "Epoch 815, Loss: 0.1836544184\n",
      "0.6275\n",
      "Epoch 816, Loss: 0.1941631614\n",
      "0.6275\n",
      "Epoch 817, Loss: 0.2035641521\n",
      "0.6275\n",
      "Epoch 818, Loss: 0.2328463817\n",
      "0.6275\n",
      "Epoch 819, Loss: 0.2509646197\n",
      "0.6275\n",
      "Epoch 820, Loss: 0.2442500738\n",
      "0.6275\n",
      "Epoch 821, Loss: 0.2367275272\n",
      "0.6275\n",
      "Epoch 822, Loss: 0.2808973887\n",
      "0.6275\n",
      "Epoch 823, Loss: 0.4003061374\n",
      "0.6275\n",
      "Epoch 824, Loss: 0.2451703722\n",
      "0.6275\n",
      "Epoch 825, Loss: 0.1475630023\n",
      "0.6275\n",
      "Epoch 826, Loss: 0.1582318461\n",
      "0.6275\n",
      "Epoch 827, Loss: 0.1580993172\n",
      "0.6275\n",
      "Epoch 828, Loss: 0.1713906042\n",
      "0.6275\n",
      "Epoch 829, Loss: 0.2732417940\n",
      "0.6275\n",
      "Epoch 830, Loss: 0.2124654504\n",
      "0.6275\n",
      "Epoch 831, Loss: 0.1778711093\n",
      "0.6275\n",
      "Epoch 832, Loss: 0.1845834722\n",
      "0.6275\n",
      "Epoch 833, Loss: 0.1732024826\n",
      "0.6275\n",
      "Epoch 834, Loss: 0.1808903618\n",
      "0.6275\n",
      "Epoch 835, Loss: 0.2199224758\n",
      "0.6275\n",
      "Epoch 836, Loss: 0.2575637676\n",
      "0.6275\n",
      "Epoch 837, Loss: 0.2230749229\n",
      "0.6275\n",
      "Epoch 838, Loss: 0.1658444290\n",
      "0.6275\n",
      "Epoch 839, Loss: 0.1246356091\n",
      "0.63\n",
      "Epoch 840, Loss: 0.1447476263\n",
      "0.63\n",
      "Epoch 841, Loss: 0.1776154186\n",
      "0.63\n",
      "Epoch 842, Loss: 0.2220244554\n",
      "0.63\n",
      "Epoch 843, Loss: 0.1642254822\n",
      "0.63\n",
      "Epoch 844, Loss: 0.2238920865\n",
      "0.63\n",
      "Epoch 845, Loss: 0.1738605594\n",
      "0.63\n",
      "Epoch 846, Loss: 0.1279366714\n",
      "0.63\n",
      "Epoch 847, Loss: 0.1551438121\n",
      "0.63\n",
      "Epoch 848, Loss: 0.1513364397\n",
      "0.63\n",
      "Epoch 849, Loss: 0.1394749886\n",
      "0.63\n",
      "Epoch 850, Loss: 0.1403374883\n",
      "0.63\n",
      "Epoch 851, Loss: 0.1185630113\n",
      "0.61625\n",
      "Epoch 852, Loss: 0.2323068504\n",
      "0.61625\n",
      "Epoch 853, Loss: 0.1174597723\n",
      "0.63375\n",
      "Epoch 854, Loss: 0.1384389171\n",
      "0.63375\n",
      "Epoch 855, Loss: 0.1643574595\n",
      "0.63375\n",
      "Epoch 856, Loss: 0.1686474994\n",
      "0.63375\n",
      "Epoch 857, Loss: 0.1545457522\n",
      "0.63375\n",
      "0.11745977230775956\n"
     ]
    }
   ],
   "source": [
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=256)\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_weights_init = []\n",
    "best_biases_init = []\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "best_seed = 0\n",
    "\n",
    "for _ in range (1):\n",
    "    nn = NeuralNetwork_Adam(625, [256,128,64], 8, beta1=0.95, beta2=0.999)\n",
    "    nn.train(batches,14,learning_rate=0.0015)\n",
    "    if nn.get_best_loss() < best_loss:\n",
    "        best_loss = nn.get_best_loss()\n",
    "        best_weights = nn.get_best_weights()\n",
    "        best_biases = nn.get_best_biases()\n",
    "        best_seed = nn.get_best_seed()\n",
    "\n",
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = range(len(accu))\n",
    "\n",
    "# Plot the array\n",
    "plt.plot(x, accu, marker='o',markersize=1)  # 'o' adds markers at each data point\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plot')\n",
    "\n",
    "# Show grid\n",
    "plt.grid()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11745977230775956\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "# print(best_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8, init_weights=best_weights, init_biases=best_biases, init_seed=best_seed)\n",
    "# nn.train(batches, 3, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.get_best_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Number of layers in the Neural Network\n",
    "N = 4  # Example value, replace with the actual number of layers\n",
    "\n",
    "# Initialize the dictionary\n",
    "weights_dict = {\n",
    "    'weights': {},\n",
    "    'bias': {}\n",
    "}\n",
    "\n",
    "weights = nn.get_best_weights()\n",
    "biases = nn.get_best_biases()\n",
    "\n",
    "# Populate the weights and bias dictionaries\n",
    "for i in range(N):\n",
    "    weights_dict['weights'][f'fc{i+1}'] = weights[i]\n",
    "    weights_dict['bias'][f'fc{i+1}'] = biases[i].flatten()\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9331559330982713\n",
      "0.63375\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'val' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "# Create the DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))\n",
    "\n",
    "for X_val, Y_val in batches:\n",
    "    Y_pred= nn.predict(X_val)\n",
    "    print(cross_entropy_loss(Y_val,Y_pred)/len(dataset))\n",
    "    print(accuracy(Y_val, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
