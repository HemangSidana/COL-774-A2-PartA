{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "#Remember to import \"numpy_transforms\" functions if you wish to import these two classes in a different script.\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "class TrainImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "        label = row[\"class\"]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image), label\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class TrainDataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image, label = self.dataset[idx]\n",
    "            images.append(image)\n",
    "            labels.append(label)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "        batch_labels = np.array(labels)\n",
    "\n",
    "        return batch_images, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"dataset_for_A2/multi_dataset\" #Path to the dataset directory\n",
    "csv = os.path.join(root_dir, \"train.csv\") #The csv file will always be placed inside the dataset directory.\n",
    "#Please ensure that you set the csv file path accordingly in all parts of the assignment so that it gets loaded correctly.\n",
    "#While evaluation, the train.csv will have same name while the test set csv will be renamed to \"val.csv\" to be compatible with the setting here.\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = TrainImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)  #Remember to import \"numpy_transforms\" functions.\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = TrainDataLoader(dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(256, 625)\n",
      "(256,)\n",
      "(128, 625)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the DataLoader\n",
    "for images, labels in dataloader:\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    print(labels.shape)  # Should be [batch_size]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import time\n",
    "\n",
    "# # ReLU activation and its derivative\n",
    "# def relu(x):\n",
    "#     return np.maximum(0, x)\n",
    "\n",
    "# def relu_derivative(x):\n",
    "#     return np.where(x > 0, 1, 0)\n",
    "\n",
    "# # Mean Squared Error loss\n",
    "# def mean_squared_error(y_true, y_pred):\n",
    "#     return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "# # Neural Network Class with ReLU in the Output Layer and Hidden Layers\n",
    "# class NeuralNetwork_Adam:\n",
    "#     def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "#         if init_seed is None:\n",
    "#             self.best_seed = int(time.time())\n",
    "#             np.random.seed(self.best_seed)\n",
    "#         else:\n",
    "#             np.random.seed(init_seed)\n",
    "#         self.weights = []\n",
    "#         self.biases = []\n",
    "#         self.m_w = []\n",
    "#         self.v_w = []\n",
    "#         self.m_b = []\n",
    "#         self.v_b = []\n",
    "#         self.beta1 = beta1\n",
    "#         self.beta2 = beta2\n",
    "#         self.epsilon = epsilon\n",
    "#         self.t = 0  # Time step for Adam\n",
    "#         self.best_weights = []\n",
    "#         self.best_biases = []\n",
    "#         self.best_loss = float(\"inf\")\n",
    "\n",
    "#         layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "#         # Initialize weights, biases, and Adam parameters (m, v)\n",
    "#         for i in range(len(layer_sizes) - 1):\n",
    "#             if (init_weights is not None) and (init_biases is not None):\n",
    "#                 self.weights.append(init_weights[i])\n",
    "#                 self.biases.append(init_biases[i])\n",
    "#             else:\n",
    "#                 self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "#                 self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "#             self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "#             self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "#             self.best_weights = self.weights\n",
    "#             self.best_biases = self.biases\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         activations = [X]\n",
    "#         pre_activations = []\n",
    "\n",
    "#         # Pass through each layer except the output layer\n",
    "#         for i in range(len(self.weights) - 1):\n",
    "#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "#             pre_activations.append(z)\n",
    "#             a = relu(z)  # ReLU for hidden layers\n",
    "#             activations.append(a)\n",
    "\n",
    "#         # Pass through the output layer with ReLU\n",
    "#         z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "#         pre_activations.append(z)\n",
    "#         a = relu(z)  # ReLU for the output layer\n",
    "#         activations.append(a)\n",
    "\n",
    "#         return activations, pre_activations\n",
    "\n",
    "#     def backward(self, X, y, activations, pre_activations):\n",
    "#         grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "#         grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "#         # Start with output layer error\n",
    "#         delta = activations[-1] - y\n",
    "#         delta *= relu_derivative(pre_activations[-1])  # ReLU derivative for the output layer\n",
    "\n",
    "#         for i in reversed(range(len(self.weights))):\n",
    "#             grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "#             grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "#             if i > 0:\n",
    "#                 delta = np.dot(delta, self.weights[i].T) * relu_derivative(pre_activations[i - 1])\n",
    "\n",
    "#         return grad_w, grad_b\n",
    "\n",
    "#     def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "#         self.t += 1  # Increment time step for Adam\n",
    "\n",
    "#         for i in range(len(self.weights)):\n",
    "#             # Update biased first moment estimate\n",
    "#             self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "#             self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "#             # Update biased second moment estimate\n",
    "#             self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "#             self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "#             # Compute bias-corrected first moment estimate\n",
    "#             m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "#             m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "#             # Compute bias-corrected second moment estimate\n",
    "#             v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "#             v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "#             # Update weights and biases\n",
    "#             self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "#             self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "#     def train(self, batches, time_of_running, learning_rate):\n",
    "#         start_time = time.time()\n",
    "#         epoch = 0\n",
    "#         while True:\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 activations, pre_activations = self.forward(X_batch)\n",
    "#                 grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "#                 self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "#             # Calculate average loss over batches\n",
    "#             loss = 0\n",
    "#             z = 0\n",
    "#             for X_batch, y_batch in batches:\n",
    "#                 y_pred, _ = self.forward(X_batch)\n",
    "#                 loss += mean_squared_error(y_batch, y_pred[-1])\n",
    "#                 z += len(y_pred[-1])\n",
    "#             loss /= z\n",
    "            \n",
    "#             if loss < self.best_loss:\n",
    "#                 self.best_loss = loss\n",
    "#                 self.best_weights = self.weights\n",
    "#                 self.best_biases = self.biases\n",
    "#             print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "#             epoch += 1\n",
    "#             # if time elapsed is greater than 1 minute, break the loop\n",
    "#             if time.time() - start_time > 60 * time_of_running:\n",
    "#                 break\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         activations, _ = self.forward(X)\n",
    "#         return activations[-1]\n",
    "\n",
    "#     def get_best_weights(self):\n",
    "#         return self.best_weights\n",
    "\n",
    "#     def get_best_biases(self):\n",
    "#         return self.best_biases\n",
    "\n",
    "#     def get_best_loss(self):\n",
    "#         return self.best_loss\n",
    "\n",
    "#     def get_best_seed(self):\n",
    "#         return self.best_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'val' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset_val = TrainImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "# Create the DataLoader\n",
    "dataloader_val = TrainDataLoader(dataset_val, batch_size=len(dataset_val))\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches_val=[]\n",
    "for images,labels in dataloader_val:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches_val.append((images,one_hot_labels))\n",
    "\n",
    "accu=[]\n",
    "def get_stat():\n",
    "    for X_val, Y_val in batches_val:\n",
    "        Y_pred= nn.predict(X_val)\n",
    "        # print(cross_entropy_loss(Y_val,Y_pred)/len(dataset))\n",
    "        z=accuracy(Y_val, Y_pred)\n",
    "        accu.append(z)\n",
    "        print(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Leaky ReLU activation and its derivative\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    exps = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
    "\n",
    "# Cross-entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)  # Avoid log(0)\n",
    "    return -np.sum(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(np.argmax(y_true, axis=1) == np.argmax(y_pred, axis=1))\n",
    "\n",
    "# Neural Network Class with Leaky ReLU and Adam Optimizer\n",
    "class NeuralNetwork_Adam:\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, init_weights=None, init_biases=None, init_seed=None, alpha=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        # if init_seed is None:\n",
    "        #     self.best_seed = int(time.time())\n",
    "        #     np.random.seed(self.best_seed)\n",
    "        # else:\n",
    "        #     np.random.seed(init_seed)\n",
    "        np.random.seed(0)\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.m_w = []\n",
    "        self.v_w = []\n",
    "        self.m_b = []\n",
    "        self.v_b = []\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # Time step for Adam\n",
    "        self.alpha = alpha  # Leaky ReLU parameter\n",
    "        self.best_weights = []\n",
    "        self.best_biases = []\n",
    "        self.best_loss = float(\"inf\")\n",
    "\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Initialize weights, biases, and Adam parameters (m, v)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            if init_weights is not None and init_biases is not None:\n",
    "                self.weights.append(init_weights[i])\n",
    "                self.biases.append(init_biases[i])\n",
    "            else:\n",
    "                self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i + 1]).astype(np.float64) * np.sqrt(2 / layer_sizes[i]))\n",
    "                self.biases.append(np.zeros((1, layer_sizes[i + 1]), dtype=np.float64))\n",
    "            self.m_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.v_w.append(np.zeros_like(self.weights[-1]))\n",
    "            self.m_b.append(np.zeros_like(self.biases[-1]))\n",
    "            self.v_b.append(np.zeros_like(self.biases[-1]))\n",
    "        # self.best_weights = self.weights\n",
    "        # self.best_biases = self.biases\n",
    "\n",
    "    def forward(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each hidden layer\n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = leaky_relu(z, alpha=self.alpha)  # Leaky ReLU for hidden layers\n",
    "            # a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def forward_pred(self, X):\n",
    "        activations = [X]\n",
    "        pre_activations = []\n",
    "\n",
    "        # Pass through each hidden layer\n",
    "        for i in range(len(self.best_weights) - 1):\n",
    "            z = np.dot(activations[-1], self.best_weights[i]) + self.best_biases[i]\n",
    "            pre_activations.append(z)\n",
    "            a = leaky_relu(z, alpha=self.alpha)  # Leaky ReLU for hidden layers\n",
    "            # a = sigmoid(z)\n",
    "            activations.append(a)\n",
    "\n",
    "        # Pass through the output layer with softmax\n",
    "        z = np.dot(activations[-1], self.best_weights[-1]) + self.best_biases[-1]\n",
    "        pre_activations.append(z)\n",
    "        a = softmax(z, axis=1)  # Softmax for the output layer\n",
    "        activations.append(a)\n",
    "\n",
    "        return activations, pre_activations\n",
    "\n",
    "    def backward(self, X, y, activations, pre_activations):\n",
    "        grad_w = [np.zeros_like(w) for w in self.weights]\n",
    "        grad_b = [np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "        # Start with output layer error\n",
    "        delta = activations[-1] - y\n",
    "\n",
    "        for i in reversed(range(len(self.weights))):\n",
    "            grad_w[i] = np.dot(activations[i].T, delta) / delta.shape[0]\n",
    "            grad_b[i] = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\n",
    "\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * leaky_relu_derivative(pre_activations[i - 1], alpha=self.alpha)\n",
    "                # delta = np.dot(delta, self.weights[i].T) * sigmoid_derivative(pre_activations[i - 1])\n",
    "\n",
    "        return grad_w, grad_b\n",
    "\n",
    "    def update_parameters(self, grad_w, grad_b, learning_rate):\n",
    "        self.t += 1  # Increment time step for Adam\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * grad_w[i]\n",
    "            self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * grad_b[i]\n",
    "\n",
    "            # Update biased second moment estimate\n",
    "            self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (grad_w[i] ** 2)\n",
    "            self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (grad_b[i] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate\n",
    "            m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "            m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "\n",
    "            # Compute bias-corrected second moment estimate\n",
    "            v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "            v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "            self.biases[i] -= learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    def train(self, batches, time_of_running, learning_rate):\n",
    "        start_time = time.time()\n",
    "        epoch = 0\n",
    "        while epoch < 3000:\n",
    "            for X_batch, y_batch in batches:\n",
    "                activations, pre_activations = self.forward(X_batch)\n",
    "                grad_w, grad_b = self.backward(X_batch, y_batch, activations, pre_activations)\n",
    "                self.update_parameters(grad_w, grad_b, learning_rate)\n",
    "\n",
    "            # Calculate average loss over batches\n",
    "            loss = 0\n",
    "            z = 0\n",
    "            for X_batch, y_batch in batches:\n",
    "                y_pred, _ = self.forward(X_batch)\n",
    "                loss += cross_entropy_loss(y_batch, y_pred[-1])\n",
    "                z += len(y_pred[-1])\n",
    "            loss /= z\n",
    "            \n",
    "            if loss < self.best_loss:\n",
    "                self.best_loss = loss\n",
    "                self.best_weights = [np.copy(w) for w in self.weights]  # Use np.copy to create independent copies\n",
    "                self.best_biases = [np.copy(b) for b in self.biases]  # Use np.copy for biases\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss:.10f}\")\n",
    "            get_stat()\n",
    "            epoch += 1\n",
    "            # if time elapsed is greater than 1 minute, break the loop\n",
    "            if time.time() - start_time > 60 * time_of_running:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_pred(X)\n",
    "        return activations[-1]\n",
    "    \n",
    "    def get_best_weights(self):\n",
    "        return self.best_weights\n",
    "    \n",
    "    def get_best_biases(self):\n",
    "        return self.best_biases\n",
    "    \n",
    "    def get_best_loss(self):\n",
    "        return self.best_loss\n",
    "    \n",
    "    # def get_best_seed(self):\n",
    "    #     return self.best_seed\n",
    "\n",
    "# Example usage:\n",
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8)\n",
    "# nn.train(batches, 1, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.0978879989\n",
      "0.14125\n",
      "Epoch 2, Loss: 2.0890121683\n",
      "0.14875\n",
      "Epoch 3, Loss: 2.0599367418\n",
      "0.1625\n",
      "Epoch 4, Loss: 2.0518723766\n",
      "0.17125\n",
      "Epoch 5, Loss: 2.0267792617\n",
      "0.22875\n",
      "Epoch 6, Loss: 2.0022242717\n",
      "0.3\n",
      "Epoch 7, Loss: 2.0026478192\n",
      "0.3\n",
      "Epoch 8, Loss: 1.9703399189\n",
      "0.2875\n",
      "Epoch 9, Loss: 1.9413749748\n",
      "0.2375\n",
      "Epoch 10, Loss: 1.8956135204\n",
      "0.2525\n",
      "Epoch 11, Loss: 1.8169289713\n",
      "0.31875\n",
      "Epoch 12, Loss: 1.7760009955\n",
      "0.25125\n",
      "Epoch 13, Loss: 1.7339833771\n",
      "0.2525\n",
      "Epoch 14, Loss: 1.7175438272\n",
      "0.28625\n",
      "Epoch 15, Loss: 1.7103136822\n",
      "0.28375\n",
      "Epoch 16, Loss: 1.6980575906\n",
      "0.25625\n",
      "Epoch 17, Loss: 1.5958425652\n",
      "0.3675\n",
      "Epoch 18, Loss: 1.5685995027\n",
      "0.39\n",
      "Epoch 19, Loss: 1.5548741217\n",
      "0.38625\n",
      "Epoch 20, Loss: 1.5545749622\n",
      "0.3875\n",
      "Epoch 21, Loss: 1.5429673454\n",
      "0.40375\n",
      "Epoch 22, Loss: 1.5253120746\n",
      "0.415\n",
      "Epoch 23, Loss: 1.4976801123\n",
      "0.43\n",
      "Epoch 24, Loss: 1.4671446761\n",
      "0.4625\n",
      "Epoch 25, Loss: 1.4591594246\n",
      "0.475\n",
      "Epoch 26, Loss: 1.4427275925\n",
      "0.48125\n",
      "Epoch 27, Loss: 1.4293024314\n",
      "0.46625\n",
      "Epoch 28, Loss: 1.4049688074\n",
      "0.4775\n",
      "Epoch 29, Loss: 1.3953102155\n",
      "0.47625\n",
      "Epoch 30, Loss: 1.3736403266\n",
      "0.475\n",
      "Epoch 31, Loss: 1.3739840225\n",
      "0.475\n",
      "Epoch 32, Loss: 1.3490624267\n",
      "0.47625\n",
      "Epoch 33, Loss: 1.3299172239\n",
      "0.5075\n",
      "Epoch 34, Loss: 1.3245483700\n",
      "0.48875\n",
      "Epoch 35, Loss: 1.3103817635\n",
      "0.495\n",
      "Epoch 36, Loss: 1.2993842788\n",
      "0.495\n",
      "Epoch 37, Loss: 1.2714011312\n",
      "0.5025\n",
      "Epoch 38, Loss: 1.2745370757\n",
      "0.5025\n",
      "Epoch 39, Loss: 1.2553082405\n",
      "0.505\n",
      "Epoch 40, Loss: 1.2441998362\n",
      "0.495\n",
      "Epoch 41, Loss: 1.2364668822\n",
      "0.50875\n",
      "Epoch 42, Loss: 1.2208334626\n",
      "0.51\n",
      "Epoch 43, Loss: 1.2111890309\n",
      "0.51125\n",
      "Epoch 44, Loss: 1.1991944854\n",
      "0.51\n",
      "Epoch 45, Loss: 1.1950063688\n",
      "0.52875\n",
      "Epoch 46, Loss: 1.1833074736\n",
      "0.51875\n",
      "Epoch 47, Loss: 1.1754717606\n",
      "0.52375\n",
      "Epoch 48, Loss: 1.1712308480\n",
      "0.52625\n",
      "Epoch 49, Loss: 1.1691695293\n",
      "0.51875\n",
      "Epoch 50, Loss: 1.1656389608\n",
      "0.52625\n",
      "Epoch 51, Loss: 1.1387214423\n",
      "0.53625\n",
      "Epoch 52, Loss: 1.1310216294\n",
      "0.5375\n",
      "Epoch 53, Loss: 1.1174407471\n",
      "0.53625\n",
      "Epoch 54, Loss: 1.1132301365\n",
      "0.53375\n",
      "Epoch 55, Loss: 1.1051724078\n",
      "0.55\n",
      "Epoch 56, Loss: 1.1097777744\n",
      "0.55\n",
      "Epoch 57, Loss: 1.1299981766\n",
      "0.55\n",
      "Epoch 58, Loss: 1.1220834571\n",
      "0.55\n",
      "Epoch 59, Loss: 1.0989558443\n",
      "0.53375\n",
      "Epoch 60, Loss: 1.1024257807\n",
      "0.53375\n",
      "Epoch 61, Loss: 1.0662350562\n",
      "0.54\n",
      "Epoch 62, Loss: 1.0642519200\n",
      "0.53875\n",
      "Epoch 63, Loss: 1.0565470175\n",
      "0.55375\n",
      "Epoch 64, Loss: 1.0629265841\n",
      "0.55375\n",
      "Epoch 65, Loss: 1.0859951965\n",
      "0.55375\n",
      "Epoch 66, Loss: 1.0992023159\n",
      "0.55375\n",
      "Epoch 67, Loss: 1.0457163504\n",
      "0.54\n",
      "Epoch 68, Loss: 1.0300761877\n",
      "0.54875\n",
      "Epoch 69, Loss: 1.0267315160\n",
      "0.54625\n",
      "Epoch 70, Loss: 1.0390314281\n",
      "0.54625\n",
      "Epoch 71, Loss: 1.0481740900\n",
      "0.54625\n",
      "Epoch 72, Loss: 1.0183749099\n",
      "0.55625\n",
      "Epoch 73, Loss: 1.0065342182\n",
      "0.54875\n",
      "Epoch 74, Loss: 0.9969100505\n",
      "0.5475\n",
      "Epoch 75, Loss: 1.0171986421\n",
      "0.5475\n",
      "Epoch 76, Loss: 1.0880897557\n",
      "0.5475\n",
      "Epoch 77, Loss: 1.0192933942\n",
      "0.5475\n",
      "Epoch 78, Loss: 0.9779670891\n",
      "0.55875\n",
      "Epoch 79, Loss: 0.9866379596\n",
      "0.55875\n",
      "Epoch 80, Loss: 0.9853293369\n",
      "0.55875\n",
      "Epoch 81, Loss: 0.9629047000\n",
      "0.5575\n",
      "Epoch 82, Loss: 1.0320945560\n",
      "0.5575\n",
      "Epoch 83, Loss: 1.0128382722\n",
      "0.5575\n",
      "Epoch 84, Loss: 1.0571249615\n",
      "0.5575\n",
      "Epoch 85, Loss: 0.9565989673\n",
      "0.57125\n",
      "Epoch 86, Loss: 0.9616703444\n",
      "0.57125\n",
      "Epoch 87, Loss: 0.9498854340\n",
      "0.55375\n",
      "Epoch 88, Loss: 0.9645145568\n",
      "0.55375\n",
      "Epoch 89, Loss: 0.9380420594\n",
      "0.57125\n",
      "Epoch 90, Loss: 0.9509546874\n",
      "0.57125\n",
      "Epoch 91, Loss: 0.9231733374\n",
      "0.58\n",
      "Epoch 92, Loss: 0.9672084464\n",
      "0.58\n",
      "Epoch 93, Loss: 0.9103592246\n",
      "0.585\n",
      "Epoch 94, Loss: 0.9473363556\n",
      "0.585\n",
      "Epoch 95, Loss: 0.9370282308\n",
      "0.585\n",
      "Epoch 96, Loss: 0.9723859574\n",
      "0.585\n",
      "Epoch 97, Loss: 0.9268932834\n",
      "0.585\n",
      "Epoch 98, Loss: 0.9732914447\n",
      "0.585\n",
      "Epoch 99, Loss: 0.9412222983\n",
      "0.585\n",
      "Epoch 100, Loss: 0.9873154420\n",
      "0.585\n",
      "Epoch 101, Loss: 0.9251604991\n",
      "0.585\n",
      "Epoch 102, Loss: 0.8866979662\n",
      "0.5775\n",
      "Epoch 103, Loss: 0.8828488728\n",
      "0.57\n",
      "Epoch 104, Loss: 1.0568886672\n",
      "0.57\n",
      "Epoch 105, Loss: 0.8962089376\n",
      "0.57\n",
      "Epoch 106, Loss: 0.9052535042\n",
      "0.57\n",
      "Epoch 107, Loss: 0.9225399404\n",
      "0.57\n",
      "Epoch 108, Loss: 0.8756588624\n",
      "0.57\n",
      "0.8756588624122636\n"
     ]
    }
   ],
   "source": [
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "mode = 'train' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "    csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "elif mode == 'val':\n",
    "    csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = TrainImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = TrainDataLoader(dataset, batch_size=256)\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images,labels in dataloader:\n",
    "    one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append((images,one_hot_labels))\n",
    "\n",
    "best_loss = float('inf')\n",
    "best_weights_init = []\n",
    "best_biases_init = []\n",
    "best_weights = []\n",
    "best_biases = []\n",
    "best_seed = 0\n",
    "\n",
    "for _ in range (1):\n",
    "    nn = NeuralNetwork_Adam(625, [256,128,64], 8, beta1=0.95, beta2=0.999)\n",
    "    nn.train(batches,0.5,learning_rate=0.0015)\n",
    "    if nn.get_best_loss() < best_loss:\n",
    "        best_loss = nn.get_best_loss()\n",
    "        best_weights = nn.get_best_weights()\n",
    "        best_biases = nn.get_best_biases()\n",
    "        # best_seed = nn.get_best_seed()\n",
    "\n",
    "print(best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = range(len(accu))\n",
    "\n",
    "# Plot the array\n",
    "plt.plot(x, accu, marker='o',markersize=1)  # 'o' adds markers at each data point\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Plot')\n",
    "\n",
    "# Show grid\n",
    "plt.grid()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8756588624122636\n"
     ]
    }
   ],
   "source": [
    "print(best_loss)\n",
    "# print(best_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn = NeuralNetwork_Adam(625, [512, 256, 128, 32], 8, init_weights=best_weights, init_biases=best_biases, init_seed=best_seed)\n",
    "# nn.train(batches, 3, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(nn.get_best_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Number of layers in the Neural Network\n",
    "N = 4  # Example value, replace with the actual number of layers\n",
    "\n",
    "# Initialize the dictionary\n",
    "weights_dict = {\n",
    "    'weights': {},\n",
    "    'bias': {}\n",
    "}\n",
    "\n",
    "weights = nn.get_best_weights()\n",
    "biases = nn.get_best_biases()\n",
    "\n",
    "# Populate the weights and bias dictionaries\n",
    "for i in range(N):\n",
    "    weights_dict['weights'][f'fc{i+1}'] = weights[i]\n",
    "    weights_dict['bias'][f'fc{i+1}'] = biases[i].flatten()\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open('weights.pkl', 'wb') as f:\n",
    "    pickle.dump(weights_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "class TestImageDataset:\n",
    "    def __init__(self, root_dir, csv, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the subfolders.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.df = pd.read_csv(csv)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, row[\"Path\"])\n",
    "        image = Image.open(img_path).convert(\"L\") #Convert image to greyscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return np.array(image)\n",
    "\n",
    "# Transformations using NumPy\n",
    "def resize(image, size):\n",
    "    # return np.array(Image.fromarray(image).resize(size))\n",
    "    return np.array(image.resize(size))\n",
    "\n",
    "def to_tensor(image):\n",
    "    return image.astype(np.float32) / 255.0\n",
    "\n",
    "def numpy_transform(image, size=(25, 25)):\n",
    "    image = resize(image, size)\n",
    "    image = to_tensor(image)\n",
    "    image = image.flatten()\n",
    "    return image\n",
    "\n",
    "class TestDataLoader:\n",
    "    def __init__(self, dataset, batch_size=1):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        # if self.shuffle:\n",
    "        #     np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.start_idx = 0\n",
    "        return self\n",
    "    def __len__(self):\n",
    "        return int(len(self.dataset)/self.batch_size)\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.start_idx >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "\n",
    "        end_idx = min(self.start_idx + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[self.start_idx:end_idx]\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            image = self.dataset[idx]\n",
    "            images.append(image)\n",
    "\n",
    "        self.start_idx = end_idx\n",
    "\n",
    "        # Stack images and labels to create batch tensors\n",
    "        batch_images = np.stack(images, axis=0)\n",
    "\n",
    "        return batch_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n",
      "(1, 625)\n"
     ]
    }
   ],
   "source": [
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"dataset_for_A2/multi_dataset\" #Path to the dataset directory\n",
    "csv = os.path.join(root_dir, \"val.csv\") #The csv file will always be placed inside the dataset directory.\n",
    "#Please ensure that you set the csv file path accordingly in all parts of the assignment so that it gets loaded correctly.\n",
    "#While evaluation, the train.csv will have same name while the test set csv will be renamed to \"val.csv\" to be compatible with the setting here.\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = TestImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)  #Remember to import \"numpy_transforms\" functions.\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = TestDataLoader(dataset, batch_size=1)\n",
    "\n",
    "# Iterate through the DataLoader [No labels will be returned in the test loader]\n",
    "for idx, images in enumerate(dataloader):\n",
    "    print(images.shape)  # Should be [batch_size, 625]\n",
    "    #Data being loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = \"./dataset_for_A2/multi_dataset\"\n",
    "# mode = 'val' #Set mode to 'train' for loading the train set for training. Set mode to 'val' for testing your model after training. \n",
    "\n",
    "# if mode == 'train': # Set mode to train when using the dataloader for training the model.\n",
    "#     csv = os.path.join(root_dir, \"train.csv\")\n",
    "\n",
    "# elif mode == 'val':\n",
    "#     csv = os.path.join(root_dir, \"val.csv\")\n",
    "\n",
    "# # Create the custom dataset\n",
    "# dataset = CustomImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)\n",
    "# # Create the DataLoader\n",
    "# dataloader = DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    # Convert y to a 2D one-hot encoding matrix\n",
    "    y_one_hot = np.zeros((len(y), num_classes))\n",
    "    y_one_hot[np.arange(len(y)), y] = 1\n",
    "    return y_one_hot\n",
    "\n",
    "batches=[]\n",
    "for images in dataloader:\n",
    "    # one_hot_labels= one_hot_encode(labels,8)\n",
    "    batches.append(images)\n",
    "\n",
    "for X_val in batches:\n",
    "    Y_pred= nn.predict(X_val)\n",
    "    # print(cross_entropy_loss(Y_val,Y_pred)/len(dataset))\n",
    "    # print(accuracy(Y_val, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting accuracy from predictions.pkl\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Load the predictions from the pickle file\n",
    "with open('predictions.pkl', 'rb') as f:\n",
    "    predictions = pickle.load(f)\n",
    "\n",
    "# Root directory containing the 8 subfolders\n",
    "root_dir = \"dataset_for_A2/multi_dataset\" #Path to the dataset directory\n",
    "csv = os.path.join(root_dir, \"val.csv\") #The csv file will always be placed inside the dataset directory.\n",
    "#Please ensure that you set the csv file path accordingly in all parts of the assignment so that it gets loaded correctly.\n",
    "#While evaluation, the train.csv will have same name while the test set csv will be renamed to \"val.csv\" to be compatible with the setting here.\n",
    "\n",
    "# Create the custom dataset\n",
    "dataset = TrainImageDataset(root_dir=root_dir, csv = csv, transform=numpy_transform)  #Remember to import \"numpy_transforms\" functions.\n",
    "\n",
    "# Create the DataLoader\n",
    "dataloader = TrainDataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2] [4, 0, 7, 3, 0, 1, 4, 1, 1, 5]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m             count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m count \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_true)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[60], line 14\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     12\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_true)):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_true[i] \u001b[38;5;241m==\u001b[39m \u001b[43my_pred\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     15\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m count \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_true)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "actual = []\n",
    "\n",
    "for (images, labels)in (dataloader):\n",
    "    actual.append(labels)\n",
    "\n",
    "actual = [label.tolist()[0] for label in actual]\n",
    "\n",
    "print(predictions[:10], actual[:10])\n",
    "# print(predictions.shape, actual.shape)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    count = 0\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] == y_pred[i]:\n",
    "            count += 1\n",
    "    return count / len(y_true)\n",
    "\n",
    "print(accuracy(actual, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
